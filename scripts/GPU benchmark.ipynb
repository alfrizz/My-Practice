{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab28c4-545c-4cc3-bd09-ff7ace35554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########      PYTORCH       ###############\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ecc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32: 34.81 TFLOPS\n",
      "torch.float16: 73.34 TFLOPS\n",
      "\n",
      "ENVIRONMENT\n",
      "===========\n",
      "Python       3.12.3\n",
      "OS           Linux 6.6.87.2-microsoft-standard-WSL2\n",
      "Reference    NVIDIA GeForce RTX 5080 Laptop GPU specs\n",
      "Driver       576.80\n",
      "GPU Count    1\n",
      "Device[0]   NVIDIA GeForce RTX 5080 Laptop GPU\n",
      "  ComputeCap  (12, 0)  Reference (12, 0)\n",
      "  Memory      17.1 GB  Expected (12, 24) GB\n",
      "\n",
      "1) CPU → GPU Bandwidth\n",
      "======================\n",
      "Measured     26901 MB/s  Expected 10000–20000 MB/s\n",
      "\n",
      "2) GPU → CPU Bandwidth\n",
      "======================\n",
      "Measured     26457 MB/s  Expected 10000–30000 MB/s\n",
      "\n",
      "3) GPU Memory Triad (A+B*scalar)\n",
      "================================\n",
      "Measured     316972 MB/s  Expected 400000–600000 MB/s\n",
      "\n",
      "4) GPU Vector Add\n",
      "=================\n",
      "Measured     45 GEOPS  Expected 50–100 GEOPS\n",
      "\n",
      "7) MATRIX MULTIPLY THROUGHPUT\n",
      "=============================\n",
      "FP32: Measured 33693 GFLOPS  Expected 20000–40000 GFLOPS\n",
      "FP16: Measured 64630 GFLOPS  Expected 40000–80000 GFLOPS\n",
      "\n",
      "5) cuDNN Conv2D Throughput\n",
      "==========================\n",
      "Measured     30683 imgs/s  Expected 20000–40000 imgs/s\n",
      "\n",
      "6) cuDNN-Fused LSTM Latency\n",
      "===========================\n",
      "Measured     2.2 ms  Expected 2.0–6.0 ms\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "# Allow TF32 on matmul/cublas and on cuDNN convs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Let cuDNN autotune the best algorithm\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    use_nvml = True\n",
    "except ImportError:\n",
    "    use_nvml = False\n",
    "\n",
    "REFERENCES = {\n",
    "    \"device_name\":    \"NVIDIA GeForce RTX 5080 Laptop GPU\",\n",
    "    \"compute_cap\":    \"(12, 0)\",          # Blackwell sm_120\n",
    "    \"total_mem_gb\":   (12, 24),           # GB\n",
    "\n",
    "    # WSL2 real‐world → CPU↔GPU BW\n",
    "    \"bw_cpu_gpu\":     (10000, 20000),       # MB/s\n",
    "    \"bw_gpu_cpu\":     (10000, 30000),     # MB/s\n",
    "\n",
    "    # Native‐Linux specs for on‐GPU tests:\n",
    "    \"mem_triad\":      (400_000, 600_000), # MB/s\n",
    "    \"vec_add\":        (50, 100),          # GEOPS\n",
    "    \"mat32\":          (20000, 40000),     # GFLOPS\n",
    "    \"mat16\":          (40000, 80000),     # GFLOPS\n",
    "    \"conv2d\":         (20000, 40000),     # imgs/s\n",
    "    \"lstm\":           (2.0, 6.0),         # ms\n",
    "}\n",
    "\n",
    "\n",
    "def header(title):\n",
    "    print(f\"\\n{title}\\n{'='*len(title)}\")\n",
    "\n",
    "def report_environment():\n",
    "    header(\"ENVIRONMENT\")\n",
    "    print(f\"Python       {platform.python_version()}\")\n",
    "    print(f\"OS           {platform.system()} {platform.release()}\")\n",
    "    print(f\"Reference    {REFERENCES['device_name']} specs\")\n",
    "    if use_nvml:\n",
    "        drv = pynvml.nvmlSystemGetDriverVersion().decode()\n",
    "        print(f\"Driver       {drv}\")\n",
    "        cnt = pynvml.nvmlDeviceGetCount()\n",
    "        print(f\"GPU Count    {cnt}\")\n",
    "        for i in range(cnt):\n",
    "            h = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(h).decode()\n",
    "            cap = torch.cuda.get_device_capability(i)\n",
    "            mem = pynvml.nvmlDeviceGetMemoryInfo(h)\n",
    "            print(f\"Device[{i}]   {name}\")\n",
    "            print(f\"  ComputeCap  {cap}  Reference {REFERENCES['compute_cap']}\")\n",
    "            print(f\"  Memory      {mem.total/1e9:.1f} GB  Expected {REFERENCES['total_mem_gb']} GB\")\n",
    "    else:\n",
    "        cnt = torch.cuda.device_count()\n",
    "        print(f\"GPU Count    {cnt}\")\n",
    "        for i in range(cnt):\n",
    "            name = torch.cuda.get_device_name(i)\n",
    "            cap = torch.cuda.get_device_capability(i)\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"Device[{i}]   {name}\")\n",
    "            print(f\"  ComputeCap  {cap}  Reference {REFERENCES['compute_cap']}\")\n",
    "            print(f\"  Memory      {props.total_memory/1e9:.1f} GB  Expected {REFERENCES['total_mem_gb']} GB\")\n",
    "\n",
    "def measure_bandwidth(src, dst, size_mb=512):\n",
    "    # calculate exact number of float32 elements for size_mb bytes\n",
    "    n = size_mb * 1024 * 1024 // 4\n",
    "    x = torch.randn(n, device=src)\n",
    "    if src == \"cpu\":\n",
    "        x = x.pin_memory()\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    _ = x.to(dst, non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    return size_mb / (time.perf_counter() - t0)\n",
    "\n",
    "def cpu_to_gpu_bw():\n",
    "    header(\"1) CPU → GPU Bandwidth\")\n",
    "    lo, hi = REFERENCES[\"bw_cpu_gpu\"]\n",
    "    bw = measure_bandwidth(\"cpu\", \"cuda\", size_mb=512)\n",
    "    print(f\"Measured     {bw:.0f} MB/s  Expected {lo:.0f}–{hi:.0f} MB/s\")\n",
    "\n",
    "def gpu_to_cpu_bw():\n",
    "    header(\"2) GPU → CPU Bandwidth\")\n",
    "    size_mb = 512\n",
    "    n = size_mb * 1024 * 1024 // 4\n",
    "    x = torch.randn(n, device=\"cuda\")\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    _ = x.to(\"cpu\", non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    bw = size_mb / (time.perf_counter() - t0)\n",
    "    lo, hi = REFERENCES[\"bw_gpu_cpu\"]\n",
    "    print(f\"Measured     {bw:.0f} MB/s  Expected {lo:.0f}–{hi:.0f} MB/s\")\n",
    "\n",
    "def memory_triad():\n",
    "    header(\"3) GPU Memory Triad (A+B*scalar)\")\n",
    "    n = 200_000_000\n",
    "    a = torch.randn(n, device=\"cuda\")\n",
    "    b = torch.randn(n, device=\"cuda\")\n",
    "    scalar = 0.123\n",
    "    for _ in range(3):\n",
    "        _ = a + b * scalar\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        _ = a + b * scalar\n",
    "    torch.cuda.synchronize()\n",
    "    dt = (time.perf_counter() - t0)/10\n",
    "    mb = 3 * n * 4 / 1e6\n",
    "    bw = mb / dt\n",
    "    lo, hi = REFERENCES[\"mem_triad\"]\n",
    "    print(f\"Measured     {bw:.0f} MB/s  Expected {lo:.0f}–{hi:.0f} MB/s\")\n",
    "\n",
    "def vector_add():\n",
    "    header(\"4) GPU Vector Add\")\n",
    "    n = 100_000_000\n",
    "    a = torch.randn(n, device=\"cuda\")\n",
    "    b = torch.randn(n, device=\"cuda\")\n",
    "    for _ in range(3):\n",
    "        _ = a + b\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        _ = a + b\n",
    "    torch.cuda.synchronize()\n",
    "    dt = (time.perf_counter() - t0)/10\n",
    "    geops = n / dt / 1e9\n",
    "    lo, hi = REFERENCES[\"vec_add\"]\n",
    "    print(f\"Measured     {geops:.0f} GEOPS  Expected {lo:.0f}–{hi:.0f} GEOPS\")\n",
    "\n",
    "def matmul_bench(dtype, label, key, steps=20):\n",
    "    a = torch.randn(2048, 2048, device=\"cuda\", dtype=dtype)\n",
    "    b = torch.randn(2048, 2048, device=\"cuda\", dtype=dtype)\n",
    "    for _ in range(5): _ = a @ b\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(steps): _ = a @ b\n",
    "    torch.cuda.synchronize()\n",
    "    dt = (time.perf_counter() - t0) / steps\n",
    "    gflops = 2 * 2048**3 / (dt * 1e9)\n",
    "    lo, hi = REFERENCES[key]\n",
    "    print(f\"{label}: Measured {gflops:.0f} GFLOPS  Expected {lo:.0f}–{hi:.0f} GFLOPS\")\n",
    "\n",
    "def conv2d_bench():\n",
    "    header(\"5) cuDNN Conv2D Throughput\")\n",
    "    inp = torch.randn(32, 3, 224, 224, device=\"cuda\")\n",
    "    conv = torch.nn.Conv2d(3, 64, 7, stride=2, padding=3).cuda()\n",
    "    for _ in range(5): conv(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(20): conv(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    dt = (time.perf_counter() - t0) / 20\n",
    "    imgs_s = 32 / dt\n",
    "    lo, hi = REFERENCES[\"conv2d\"]\n",
    "    print(f\"Measured     {imgs_s:.0f} imgs/s  Expected {lo:.0f}–{hi:.0f} imgs/s\")\n",
    "\n",
    "def lstm_bench():\n",
    "    header(\"6) cuDNN-Fused LSTM Latency\")\n",
    "    B, S, F, H = 64, 100, 128, 512\n",
    "    m = torch.nn.LSTM(F, H, batch_first=True).cuda()\n",
    "    inp = torch.randn(B, S, F, device=\"cuda\")\n",
    "    for _ in range(3): m(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10): m(inp)\n",
    "    torch.cuda.synchronize()\n",
    "    ms = (time.perf_counter() - t0) * 1e3 / 10\n",
    "    lo, hi = REFERENCES[\"lstm\"]\n",
    "    print(f\"Measured     {ms:.1f} ms  Expected {lo:.1f}–{hi:.1f} ms\")\n",
    "\n",
    "def run_benchmark(mat_size, dtype, n_iters=50):\n",
    "    # Prepare random matrices\n",
    "    A = torch.randn((mat_size, mat_size), device='cuda', dtype=dtype)\n",
    "    B = torch.randn((mat_size, mat_size), device='cuda', dtype=dtype)\n",
    "    torch.cuda.synchronize()\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = torch.mm(A, B)\n",
    "    torch.cuda.synchronize()\n",
    "    # Timed loop\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(n_iters):\n",
    "        _ = torch.mm(A, B)\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    # Compute TFLOPS: 2·N³ operations per multiplication\n",
    "    ops = 2 * (mat_size ** 3) * n_iters\n",
    "    tflops = ops / elapsed / 1e12\n",
    "    return tflops\n",
    "\n",
    "for dtype in (torch.float32, torch.float16):\n",
    "    tflops = run_benchmark(4096, dtype)\n",
    "    print(f\"{dtype}: {tflops:.2f} TFLOPS\")\n",
    "\n",
    "# Run all tests\n",
    "report_environment()\n",
    "cpu_to_gpu_bw()\n",
    "gpu_to_cpu_bw()\n",
    "memory_triad()\n",
    "vector_add()\n",
    "header(\"7) MATRIX MULTIPLY THROUGHPUT\")\n",
    "matmul_bench(torch.float32, \"FP32\", \"mat32\")\n",
    "matmul_bench(torch.float16, \"FP16\", \"mat16\")\n",
    "conv2d_bench()\n",
    "lstm_bench()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0a02d-d037-410e-a748-d611b8becd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########      TENSORFLOW    ###############\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a06dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version    2.17.0\n",
      "CUDA version  12.8\n",
      "cuDNN version 9\n",
      "GPUs           [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device name   NVIDIA GeForce RTX 5080 Laptop GPU\n",
      "Compute cap.  (12, 0)\n",
      "\n",
      "ENVIRONMENT\n",
      "===========\n",
      "Python      3.12.3\n",
      "OS          Linux 6.6.87.2-microsoft-standard-WSL2\n",
      "TensorFlow  2.17.0\n",
      "CUDA        12.8\n",
      "cuDNN       9\n",
      "GPU Count   1\n",
      "Device      NVIDIA GeForce RTX 5080 Laptop GPU cap= (12, 0)\n",
      "\n",
      "1) CPU → GPU Bandwidth\n",
      "======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751754172.988245      81 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1751754172.989065      81 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measured   829 MB/s  Expected 10000–20000\n",
      "\n",
      "2) GPU → CPU Bandwidth\n",
      "======================\n",
      "Measured   1996 MB/s  Expected 10000–30000\n",
      "\n",
      "5) MATRIX MULTIPLY THROUGHPUT\n",
      "=============================\n",
      "FP32:     1589 GFLOPS  Expected 20000–40000\n",
      "FP16:     4203 GFLOPS  Expected 40000–80000\n",
      "\n",
      "6) cuDNN Conv2D Throughput\n",
      "==========================\n",
      "Measured   546 imgs/s  Expected 20000–40000\n",
      "\n",
      "7) cuDNN-Fused LSTM Latency\n",
      "===========================\n",
      "⚠️  LSTM skipped: Exception encountered when calling layer 'cu_dnnlstm_2' (type CuDNNLSTM).\n",
      "\n",
      "{{function_node __wrapped__CudnnRNNV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} No algorithm worked!\n",
      "\t [[{{node CudnnRNNV2}}]] [Op:CudnnRNNV2]\n",
      "\n",
      "Call arguments received by layer 'cu_dnnlstm_2' (type CuDNNLSTM):\n",
      "  • inputs=tf.Tensor(shape=(64, 100, 128), dtype=float32)\n",
      "  • mask=None\n",
      "  • training=None\n",
      "  • initial_state=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 22:22:57.895046: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at cudnn_rnn_ops.cc:1756 : INTERNAL: No algorithm worked!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import time, platform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# get the dict of build‐time settings\n",
    "build = tf.sysconfig.get_build_info()\n",
    "\n",
    "print(\"TF version   \", tf.__version__)\n",
    "print(\"CUDA version \", build[\"cuda_version\"])\n",
    "print(\"cuDNN version\", build[\"cudnn_version\"])\n",
    "\n",
    "# list visible GPUs\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs          \", gpus)\n",
    "\n",
    "# and per‐GPU details (compute capability, name) via the device details API\n",
    "if gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    print(\"Device name  \", details[\"device_name\"])\n",
    "    print(\"Compute cap. \", details[\"compute_capability\"])\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import time, platform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# EXACTLY the same spec‐based ranges as PyTorch\n",
    "REFERENCES = {\n",
    "    \"bw_cpu_gpu\": (10000, 20000),   # MB/s (WSL2 measured)\n",
    "    \"bw_gpu_cpu\": (10000,30000),   # MB/s\n",
    "    \"mem_triad\":  (400_000,600_000),# MB/s\n",
    "    \"vec_add\":    (50,  100),      # GEOPS\n",
    "    \"mat32\":      (20000,40000),   # GFLOPS\n",
    "    \"mat16\":      (40000,80000),   # GFLOPS\n",
    "    \"conv2d\":     (20000,40000),   # imgs/s\n",
    "    \"lstm\":       (2.0,   6.0),    # ms\n",
    "}\n",
    "\n",
    "def header(title):\n",
    "    print(f\"\\n{title}\\n{'='*len(title)}\")\n",
    "\n",
    "def report_env():\n",
    "    header(\"ENVIRONMENT\")\n",
    "    print(\"Python     \", platform.python_version())\n",
    "    print(\"OS         \", platform.system(), platform.release())\n",
    "    print(\"TensorFlow \", tf.__version__)\n",
    "    build = tf.sysconfig.get_build_info()\n",
    "    print(\"CUDA       \", build[\"cuda_version\"])\n",
    "    print(\"cuDNN      \", build[\"cudnn_version\"])\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    print(\"GPU Count  \", len(gpus))\n",
    "    if gpus:\n",
    "        d = tf.config.experimental.get_device_details(gpus[0])\n",
    "        print(\"Device     \", d[\"device_name\"], \"cap=\", d[\"compute_capability\"])\n",
    "\n",
    "def cpu_to_gpu_bw(size_mb=512):\n",
    "    header(\"1) CPU → GPU Bandwidth\")\n",
    "    # 512 MB of float32 → element count\n",
    "    n = size_mb * 1024 * 1024 // 4\n",
    "    host = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # force copy onto GPU\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        dev = tf.constant(host)\n",
    "    _ = dev.numpy()   # sync back\n",
    "    bw = size_mb / (time.perf_counter() - t0)\n",
    "\n",
    "    lo, hi = REFERENCES[\"bw_cpu_gpu\"]\n",
    "    print(f\"Measured   {bw:.0f} MB/s  Expected {lo}–{hi}\")\n",
    "\n",
    "def gpu_to_cpu_bw(size_mb=512):\n",
    "    header(\"2) GPU → CPU Bandwidth\")\n",
    "    n = size_mb * 1024 * 1024 // 4\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        dev = tf.random.uniform([n], dtype=tf.float32)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # pull back to host\n",
    "    host = dev.numpy()\n",
    "    bw = size_mb / (time.perf_counter() - t0)\n",
    "\n",
    "    lo, hi = REFERENCES[\"bw_gpu_cpu\"]\n",
    "    print(f\"Measured   {bw:.0f} MB/s  Expected {lo}–{hi}\")\n",
    "\n",
    "def memory_triad():\n",
    "    header(\"3) GPU Memory Triad (A + B*scalar)\")\n",
    "    n = 200_000_000\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        A = tf.random.normal([n])\n",
    "        B = tf.random.normal([n])\n",
    "    scalar = 0.123\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(3):\n",
    "        _ = A + B * scalar\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        _ = A + B * scalar\n",
    "    dt = (time.perf_counter() - t0) / 10\n",
    "\n",
    "    # 3 streams × n elements × 4 bytes → MB\n",
    "    mb = 3 * n * 4 / 1e6\n",
    "    bw = mb / dt\n",
    "    lo, hi = REFERENCES[\"mem_triad\"]\n",
    "    print(f\"Measured   {bw:.0f} MB/s  Expected {lo}–{hi}\")\n",
    "\n",
    "def vector_add():\n",
    "    header(\"4) GPU Vector Add\")\n",
    "    n = 100_000_000\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        A = tf.random.normal([n])\n",
    "        B = tf.random.normal([n])\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(3):\n",
    "        _ = A + B\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        _ = A + B\n",
    "    dt = (time.perf_counter() - t0) / 10\n",
    "\n",
    "    geops = n / dt / 1e9\n",
    "    lo, hi = REFERENCES[\"vec_add\"]\n",
    "    print(f\"Measured   {geops:.0f} GEOPS  Expected {lo}–{hi}\")\n",
    "\n",
    "def matmul_bench():\n",
    "    header(\"5) MATRIX MULTIPLY THROUGHPUT\")\n",
    "    for label, dtype, key in [(\"FP32\", tf.float32, \"mat32\"),\n",
    "                              (\"FP16\", tf.float16, \"mat16\")]:\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            A = tf.random.normal([2048,2048], dtype=dtype)\n",
    "            B = tf.random.normal([2048,2048], dtype=dtype)\n",
    "        # warm-up\n",
    "        for _ in range(5):\n",
    "            _ = tf.linalg.matmul(A, B).numpy()\n",
    "        # timed\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(20):\n",
    "            _ = tf.linalg.matmul(A, B).numpy()\n",
    "        dt = (time.perf_counter() - t0) / 20\n",
    "\n",
    "        # GFLOPS = 2·N³ / (dt·1e9)\n",
    "        gflops = 2 * 2048**3 / (dt * 1e9)\n",
    "        lo, hi = REFERENCES[key]\n",
    "        print(f\"{label}:     {gflops:.0f} GFLOPS  Expected {lo}–{hi}\")\n",
    "\n",
    "def conv2d_bench():\n",
    "    header(\"6) cuDNN Conv2D Throughput\")\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        X = tf.random.normal([32,224,224,3])\n",
    "        conv = tf.keras.layers.Conv2D(64, 7, strides=2, padding=\"same\")\n",
    "    # warm-up\n",
    "    for _ in range(5):\n",
    "        _ = conv(X).numpy()\n",
    "    # timed\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(20):\n",
    "        _ = conv(X).numpy()\n",
    "    dt = (time.perf_counter() - t0) / 20\n",
    "\n",
    "    imgs = 32 / dt\n",
    "    lo, hi = REFERENCES[\"conv2d\"]\n",
    "    print(f\"Measured   {imgs:.0f} imgs/s  Expected {lo}–{hi}\")\n",
    "\n",
    "def lstm_bench():\n",
    "    header(\"7) cuDNN-Fused LSTM Latency\")\n",
    "    try:\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            lstm = tf.compat.v1.keras.layers.CuDNNLSTM(512)\n",
    "            X    = tf.random.normal([64,100,128])\n",
    "        # warm-up\n",
    "        for _ in range(3):\n",
    "            _ = lstm(X).numpy()\n",
    "        # timed\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            _ = lstm(X).numpy()\n",
    "        ms = (time.perf_counter() - t0) * 1e3 / 10\n",
    "\n",
    "        lo, hi = REFERENCES[\"lstm\"]\n",
    "        print(f\"Measured   {ms:.1f} ms     Expected {lo:.1f}–{hi:.1f} ms\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️  LSTM skipped:\", e)\n",
    "\n",
    "report_env()\n",
    "cpu_to_gpu_bw()\n",
    "gpu_to_cpu_bw()\n",
    "matmul_bench()\n",
    "conv2d_bench()\n",
    "lstm_bench()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abdc0c-a184-4958-a398-ab02c2696b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63a1342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ab7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d0f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
