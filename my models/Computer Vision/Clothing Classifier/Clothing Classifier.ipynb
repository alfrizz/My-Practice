{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa02648-9eae-45ba-893f-88440e8e4235",
   "metadata": {},
   "source": [
    "![clothing_classification](clothing_classification.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a988c-1095-485a-a88c-002400a872be",
   "metadata": {},
   "source": [
    "Fashion Forward is a new AI-based e-commerce clothing retailer.\n",
    "They want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n",
    "\n",
    "As a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1ab317-f3e4-4e5f-93a7-9c27677c5ffb",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 872,
    "lastExecutedAt": 1713283114116,
    "lastExecutedByKernel": "ab12a397-c392-4a15-94ff-20186a93f541",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Run the cells below first"
   },
   "outputs": [],
   "source": [
    "# Run the cells below first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e7dae3-c192-4267-a0ed-18d1ac56c861",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 35793,
    "lastExecutedAt": 1713429490432,
    "lastExecutedByKernel": "613ad597-3acd-485b-b4bd-0a12e756578b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install torchmetrics\n!pip install torchvision",
    "outputsMetadata": {
     "0": {
      "height": 544,
      "type": "stream"
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torchmetrics\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8065b7-84fc-4376-afef-6db731dec4b3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3827,
    "lastExecutedAt": 1713429494261,
    "lastExecutedByKernel": "613ad597-3acd-485b-b4bd-0a12e756578b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662e1bf1-943f-4243-9fd4-02ce11609e8d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6117,
    "lastExecutedAt": 1713429500380,
    "lastExecutedByKernel": "613ad597-3acd-485b-b4bd-0a12e756578b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     },
     "1": {
      "height": 38,
      "type": "stream"
     },
     "2": {
      "height": 122,
      "type": "stream"
     },
     "3": {
      "height": 38,
      "type": "stream"
     },
     "4": {
      "height": 122,
      "type": "stream"
     },
     "5": {
      "height": 38,
      "type": "stream"
     },
     "6": {
      "height": 122,
      "type": "stream"
     },
     "7": {
      "height": 38,
      "type": "stream"
     },
     "8": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "# The FashionMNIST dataset is a collection of grayscale images of clothing items\n",
    "# The images are transformed into PyTorch tensors using transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a107eee4-caa3-4eff-a33a-842428f2d0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6f94b1-4502-4fed-b8f2-fab3ceda0a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0] # Access the first element in the dataset\n",
    "image.shape, label\n",
    "\n",
    "# Image: Each image is a tensor of shape [1, 28, 28], representing 1 channel (grayscale) and 28x28 pixels.\n",
    "# Label: Each label is an integer representing the class of the clothing item (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc345644-9ff8-4032-9fb9-bcc5e7596bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize Example image and label\n",
    "\n",
    "# Convert the tensor to a NumPy array and plot it\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b354f1-198d-40d4-b6b6-4e23042d7886",
   "metadata": {},
   "source": [
    "The labels correspond to the following classes:\n",
    "\n",
    "0 T-shirt/top\n",
    "\n",
    "1 Trouser\n",
    "\n",
    "2 Pullover\n",
    "\n",
    "3 Dress\n",
    "\n",
    "4 Coat\n",
    "\n",
    "5 Sandal\n",
    "\n",
    "6 Shirt\n",
    "\n",
    "7 Sneaker\n",
    "\n",
    "8 Bag\n",
    "\n",
    "9 Ankle boot\n",
    "\n",
    "Automate product tagging for the e-commerce store using CNNs.\n",
    "\n",
    "Once trained (keeping the epochs to 1 or 2 to keep the run time down), store your predictions on the test set in a list named predictions.\n",
    "\n",
    "Calculate the accuracy, and per-class precision and recall for your classifier based on the predictions obtained. Store your metrics in variables named accuracy, precision, and recall. Use lists of the appropriate length for the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19843ff-d1c5-4d2a-bcd5-205901a33a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of classes\n",
    "classes = train_data.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89732df-4043-49ca-9001-f5f555ee2fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554e823a-393e-46a1-ae6c-05a91b6f6517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[0][0].shape)\n",
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ded695-ddb0-4f69-9e99-16fdc249a5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = train_data[0][0].shape[1]\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f733bb04-a982-4d63-b5ae-a72fab91f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some relevant variables\n",
    "num_input_channels = 1 # 1 channel for grayscale, 3 for RGB\n",
    "num_output_channels = 16 # number of filters or kernels to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffea0124-a7dc-4c01-8cdb-51cafb8c39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN\n",
    "class MultiClassImageClassifier(nn.Module):\n",
    "  \n",
    "    # Define the init method\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiClassImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Create a fully connected layer\n",
    "        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes) \n",
    "        # //2 because, when using a stride of 2, each pooling operation reduces the dimensions of the feature map by a factor of 2 (then we square to get all values)\n",
    "        # We multiply by num_output_channels to account for all the feature maps produced by the convolutional layer (each filter (or kernel) applied to the input produces one feature map)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass inputs through each layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f64b00a-fec9-426c-bb21-6b0769b4d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for features, labels in train_data:\n",
    "    print(features.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae441e1-2481-4f0e-aa4a-1b59e04d5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# Define the training set DataLoader\n",
    "dataloader_train = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab68a583-8621-43da-ad79-bf6aa9f35cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "tensor([4, 2, 0, 3, 4, 0, 8, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for features, labels in dataloader_train:\n",
    "    print(features.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7c79034-defe-4605-b1c4-3db06d7c2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_model(optimizer, net, num_epochs):\n",
    "    num_processed = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        num_processed = 0\n",
    "        \n",
    "        for features, labels in dataloader_train:\n",
    "            optimizer.zero_grad() # Resets the gradients of all the model parameters to zero. This prevents gradients from accumulating, ensuring that each optimization step uses fresh gradients.\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() # computes the gradient of the loss with respect to each parameter (weight) in the model.\n",
    "            optimizer.step() # updates the parameters of the model based on the computed gradients.\n",
    "            running_loss += loss.item()\n",
    "            num_processed += len(labels)\n",
    "            print('epoch:', epoch+1, \n",
    "                  'batch_size:',batch_size,\n",
    "                  'len_dataloader_train:', len(dataloader_train), \n",
    "                  'num_processed:', num_processed,\n",
    "                  'running_loss:', running_loss / num_processed,\n",
    "                  end='\\r')\n",
    "            \n",
    "    train_loss = running_loss / len(dataloader_train)\n",
    "    print('epoch:', epoch+1, \n",
    "          'batch_size:', batch_size,\n",
    "          'len_dataloader_train:', len(dataloader_train), \n",
    "          'num_processed:', num_processed,\n",
    "          'train_loss:', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9593f5f8-a6bf-45b0-a682-2aeed7b82fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch_size: 10 len_dataloader_train: 6000 num_processed: 60000 train_loss: 0.39553910159443817826 batch_size: 10 len_dataloader_train: 6000 num_processed: 1310 running_loss: 0.11438860176628783 1 batch_size: 10 len_dataloader_train: 6000 num_processed: 38350 running_loss: 0.043292765229857845\n"
     ]
    }
   ],
   "source": [
    "# Train for 1 epoch\n",
    "net = MultiClassImageClassifier(num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    optimizer=optimizer,\n",
    "    net=net,\n",
    "    num_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c98f9ce-3b39-48f6-befb-3fababbe05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the test set\n",
    "              \n",
    "# Define the test set DataLoader\n",
    "dataloader_test = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da2c8600-05f8-4cc2-a078-8c57148305ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "for features, labels in dataloader_test:\n",
    "    print(features.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef22b5c-ff65-4a70-afbc-472f43906cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics (average=None computes metrics globally by counting the total true positives, false negatives, and false positives)\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "precision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\n",
    "recall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd333b7-921f-4e87-a881-c7512a46abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10, 10])\n",
      "tensor([5, 6, 8, 9, 1, 9, 1, 8, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Run model on test set\n",
    "net.eval()\n",
    "predictions = []\n",
    "for i, (features, labels) in enumerate(dataloader_test):\n",
    "\n",
    "    # output = net.forward(features.reshape(-1, 1, image_size, image_size))\n",
    "    output = net.forward(features)\n",
    "    cat = torch.argmax(output, dim=-1)\n",
    "    predictions.extend(cat.tolist())\n",
    "    # below we calculate the scoring metrics at each iteration, and then we use the method compute() afterwards to calculate their total value\n",
    "    # this way we don't need to create specific lists to store all predictions and labels at each iteration\n",
    "    accuracy_metric(cat, labels)\n",
    "    precision_metric(cat, labels)\n",
    "    recall_metric(cat, labels)\n",
    "    print('iter:', i, end='\\r')\n",
    "\n",
    "# just printing the shapes of the last iteration\n",
    "print(features.shape)\n",
    "print(features.reshape(-1, 1, image_size, image_size).shape)\n",
    "print(output.shape) # 10 predictions * 10 batches\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbc6b541-7e30-4298-9563-cc1f05f150fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8790)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ca47f9-1408-4f51-8460-ed82f0aad8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8984, 0.9740, 0.8951, 0.8770, 0.7888, 0.9818, 0.6177, 0.9486, 0.9343,\n",
       "        0.9424])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd81e353-8ab0-4e7d-93ae-6ae50d658460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7340, 0.9750, 0.7250, 0.8980, 0.8480, 0.9690, 0.7530, 0.9420, 0.9810,\n",
       "        0.9650])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "961c4a39-aa1e-493b-b78d-6520abe539c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8790000081062317\n",
      "Precision (per class): [0.898408830165863, 0.9740259647369385, 0.895061731338501, 0.876953125, 0.788837194442749, 0.9817629456520081, 0.6177194714546204, 0.9486404657363892, 0.9342857003211975, 0.9423828125]\n",
      "Recall (per class): [0.734000027179718, 0.9750000238418579, 0.7250000238418579, 0.8980000019073486, 0.8479999899864197, 0.968999981880188, 0.753000020980835, 0.9419999718666077, 0.9810000061988831, 0.9649999737739563]\n"
     ]
    }
   ],
   "source": [
    "# Compute the metrics\n",
    "accuracy = accuracy_metric.compute().item()\n",
    "precision = precision_metric.compute().tolist()\n",
    "recall = recall_metric.compute().tolist()\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision (per class):', precision)\n",
    "print('Recall (per class):', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3053d-6cfb-4ff0-be57-b7f7a0aece00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12424b9e-2c64-4e3b-ac28-ceed1cada413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f080f2-8ef8-4c1a-a2a8-2490b1fd02d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28e77-c15b-4ad5-ba35-6cd61e4a5db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab43cc-3487-4ff2-9587-a5443f43de38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e31476-8f9d-436c-bf25-83cba85f75d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5883f4-3c2c-4cd0-a66a-d1515699a31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa8756-2e11-41e2-b85c-e0cb6f457b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a158b70-7511-4c2d-a45f-6a450d2a1691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469d0c3-1b56-4196-a551-3a95a6b22155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b147c-f413-4a80-abe4-e2f100764093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015ec6e-35fd-4595-96ed-19807db16ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce896e-79b1-4702-86bc-35887f9d519e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf529823-825d-4699-9549-22666f5eae84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a76633-2015-47d5-a729-84168a34b448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd617aea-ee01-4651-8a93-7f406279a060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e50f9-5838-47fa-b3aa-bc9939de3cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9cca7-7b8f-4132-8cbd-07779b4a9add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690ea7d-1397-44f6-9372-403d7960e3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caab473-4627-400a-be7f-69d66beb1946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e165d15-8036-47a7-9a79-f287f59bc90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64416638-2e1c-40f6-9136-f6fafe6056cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ff2f8-8816-43a7-bc1a-e35b15c11eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
