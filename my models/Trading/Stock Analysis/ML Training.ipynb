{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:30:00</th>\n",
       "      <td>250.5906</td>\n",
       "      <td>250.6435</td>\n",
       "      <td>250.5244</td>\n",
       "      <td>250.5753</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>250.5001</td>\n",
       "      <td>250.6505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:31:00</th>\n",
       "      <td>250.5806</td>\n",
       "      <td>250.6317</td>\n",
       "      <td>250.5121</td>\n",
       "      <td>250.5606</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>250.4854</td>\n",
       "      <td>250.6358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:32:00</th>\n",
       "      <td>250.5712</td>\n",
       "      <td>250.6200</td>\n",
       "      <td>250.4938</td>\n",
       "      <td>250.5453</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>250.4701</td>\n",
       "      <td>250.6205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:33:00</th>\n",
       "      <td>250.5580</td>\n",
       "      <td>250.6094</td>\n",
       "      <td>250.4762</td>\n",
       "      <td>250.5347</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>250.4595</td>\n",
       "      <td>250.6099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:34:00</th>\n",
       "      <td>250.5491</td>\n",
       "      <td>250.5994</td>\n",
       "      <td>250.4600</td>\n",
       "      <td>250.5168</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>250.4416</td>\n",
       "      <td>250.5919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:56:00</th>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3500</td>\n",
       "      <td>203.2450</td>\n",
       "      <td>203.3200</td>\n",
       "      <td>189023.0</td>\n",
       "      <td>203.2590</td>\n",
       "      <td>203.3810</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:57:00</th>\n",
       "      <td>203.3200</td>\n",
       "      <td>203.4200</td>\n",
       "      <td>203.3050</td>\n",
       "      <td>203.3800</td>\n",
       "      <td>222383.0</td>\n",
       "      <td>203.3190</td>\n",
       "      <td>203.4410</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:58:00</th>\n",
       "      <td>203.3800</td>\n",
       "      <td>203.4300</td>\n",
       "      <td>203.3322</td>\n",
       "      <td>203.3750</td>\n",
       "      <td>279702.0</td>\n",
       "      <td>203.3140</td>\n",
       "      <td>203.4360</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:59:00</th>\n",
       "      <td>203.3700</td>\n",
       "      <td>203.4100</td>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>724307.0</td>\n",
       "      <td>203.2790</td>\n",
       "      <td>203.4010</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 21:00:00</th>\n",
       "      <td>203.3288</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>202.8400</td>\n",
       "      <td>203.1993</td>\n",
       "      <td>11076221.0</td>\n",
       "      <td>203.1383</td>\n",
       "      <td>203.2603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46904 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close      volume  \\\n",
       "2025-01-02 13:30:00  250.5906  250.6435  250.5244  250.5753      2259.0   \n",
       "2025-01-02 13:31:00  250.5806  250.6317  250.5121  250.5606      2351.0   \n",
       "2025-01-02 13:32:00  250.5712  250.6200  250.4938  250.5453      2455.0   \n",
       "2025-01-02 13:33:00  250.5580  250.6094  250.4762  250.5347      2474.0   \n",
       "2025-01-02 13:34:00  250.5491  250.5994  250.4600  250.5168      2792.0   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2025-06-03 20:56:00  203.2500  203.3500  203.2450  203.3200    189023.0   \n",
       "2025-06-03 20:57:00  203.3200  203.4200  203.3050  203.3800    222383.0   \n",
       "2025-06-03 20:58:00  203.3800  203.4300  203.3322  203.3750    279702.0   \n",
       "2025-06-03 20:59:00  203.3700  203.4100  203.2500  203.3400    724307.0   \n",
       "2025-06-03 21:00:00  203.3288  203.3400  202.8400  203.1993  11076221.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2025-01-02 13:30:00  250.5001  250.6505             0             0.00   \n",
       "2025-01-02 13:31:00  250.4854  250.6358             0             0.00   \n",
       "2025-01-02 13:32:00  250.4701  250.6205             0             0.00   \n",
       "2025-01-02 13:33:00  250.4595  250.6099             0             0.00   \n",
       "2025-01-02 13:34:00  250.4416  250.5919             0             0.00   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-03 20:56:00  203.2590  203.3810             0             1.99   \n",
       "2025-06-03 20:57:00  203.3190  203.4410             0             1.99   \n",
       "2025-06-03 20:58:00  203.3140  203.4360             0             1.99   \n",
       "2025-06-03 20:59:00  203.2790  203.4010             0             1.99   \n",
       "2025-06-03 21:00:00  203.1383  203.2603             0             1.99   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2025-01-02 13:30:00        0.000                 0.0  \n",
       "2025-01-02 13:31:00        0.000                 0.0  \n",
       "2025-01-02 13:32:00        0.000                 0.0  \n",
       "2025-01-02 13:33:00        0.000                 0.0  \n",
       "2025-01-02 13:34:00        0.000                 0.0  \n",
       "...                          ...                 ...  \n",
       "2025-06-03 20:56:00        0.942                 0.0  \n",
       "2025-06-03 20:57:00        0.882                 0.0  \n",
       "2025-06-03 20:58:00        0.887                 0.0  \n",
       "2025-06-03 20:59:00        0.922                 0.0  \n",
       "2025-06-03 21:00:00        1.062                 0.0  \n",
       "\n",
       "[46904 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/merged_{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Turn a multi-day minute-bar DataFrame into leakage-free tensors that an\n",
    "# LSTM can digest.  \n",
    "# Key guarantees\n",
    "#   • NO window ever crosses a midnight boundary              (⇒ no leak)\n",
    "#   • Each feature column is standard-scaled *inside the same day*       \n",
    "#   • The label column can be scaled the same way\n",
    "#   • Final dtype = float32  (GPU-friendly, half the RAM of float64)\n",
    "# ======================================================================\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,                           # ← everything after this must be keyworded\n",
    "    look_back: int = 60,         # minutes of history the LSTM should see\n",
    "    feature_cols: Sequence[str] = [\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
    "    label_col: str = \"signal_smooth_norm\",\n",
    "    rth_start: dt.time = dt.time(14, 30),    # 09:30 ET expressed in CET\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        All sessions stacked vertically, index = DateTimeIndex.\n",
    "    look_back : int\n",
    "        Window length (in rows/minutes) fed into the LSTM.\n",
    "    feature_cols : sequence[str]\n",
    "        Predictor columns.\n",
    "    label_col : str\n",
    "        Column the model must predict (1-step-ahead, same-bar, etc.).\n",
    "    rth_start : datetime.time\n",
    "        All bars ≥ rth_start count as “Regular Trading Hours”; only those\n",
    "        become *targets*.  Earlier bars are context only.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray  shape (samples, look_back, n_features)\n",
    "    y : np.ndarray  shape (samples,)\n",
    "    \"\"\"\n",
    "\n",
    "    X_windows, y_targets = [], []     # will collect windows from *all* days\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1)  iterate ONE calendar day at a time —> main anti-leak safeguard\n",
    "    # ------------------------------------------------------------------\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "        # ----- keep rows in chronological order -----\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # ----- fit a per-day StandardScaler on *features* -----\n",
    "        f_scaler = StandardScaler()\n",
    "        day_df.loc[:, feature_cols] = f_scaler.fit_transform(day_df[feature_cols])\n",
    "\n",
    "        # ---- pull NumPy views & cast to float32 (no scaling here) ----\n",
    "        feat_np = day_df[feature_cols].values.astype(\"float32\")\n",
    "        label_np = day_df[label_col].values.astype(\"float32\")\n",
    "\n",
    "        # indices of bars that are INSIDE the regular session\n",
    "        mask_rth = day_df.index.time >= rth_start      # ← keeps ONLY ≥ 14 : 30 CET\n",
    "        idx_rth  = np.flatnonzero(mask_rth)            # ← integer positions of those rows\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 2)  build windows **within this single day only**\n",
    "        #     => Monday 16:00 can never join Tuesday 08:30\n",
    "        # --------------------------------------------------------------\n",
    "        for i in idx_rth:            # ← iterate *only* over RTH bars\n",
    "            if i < look_back:        # need full context; skip if not\n",
    "                continue\n",
    "                \n",
    "            # the network learns “given the previous look_back candles, predict the label at the next minute.”\n",
    "            window = feat_np[i - look_back : i]   # shape (look_back, n_feat) --- rows t-look_back … t-1\n",
    "            target = label_np[i]                  # 1-D float --- row t  (always RTH)\n",
    "\n",
    "            X_windows.append(window)\n",
    "            y_targets.append(target)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3)  final stacking → tensors ready for model.fit()\n",
    "    # ------------------------------------------------------------------\n",
    "    X = np.stack(X_windows, dtype=\"float32\")      # (samples, look_back, n_feat)\n",
    "    y = np.asarray(y_targets, dtype=\"float32\")    # (samples,)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40664, 60, 5)\n",
      "(40664,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float = 0.70,\n",
    "    val_prop: float = 0.15,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],   # train\n",
    "        Tuple[np.ndarray, np.ndarray],   # val\n",
    "        Tuple[np.ndarray, np.ndarray],   # test\n",
    "        List[int]                        # samples per day\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Split minute-bar tensors into chronological train / val / test\n",
    "    without hard-coding “391 windows per day”.\n",
    "\n",
    "    X, y            — tensors from build_lstm_tensors  \n",
    "    df              — the same multi-day DataFrame used to create X, y  \n",
    "    look_back       — window length you used (e.g. 60)  \n",
    "    rth_start       — session open time (≥ labels)  \n",
    "    train_prop      — fraction of days → train  \n",
    "    val_prop        — fraction of days → validation  \n",
    "                     remainder goes to test\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️⃣  count windows per calendar day *exactly as build_lstm_tensors did*\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # How many indices are valid labels (time ≥ rth_start)?\n",
    "        rth_rows = (day_df.index.time >= rth_start).sum()\n",
    "\n",
    "        # No extra subtraction: the first RTH index is already ≥ look_back\n",
    "        samples_per_day.append(rth_rows)\n",
    "\n",
    "    # 2️⃣  map every sample in X to its day-number\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "    if day_id.size != len(X):\n",
    "        raise ValueError(\n",
    "            f\"Mismatch: computed {day_id.size} samples from df \"\n",
    "            f\"but X has {len(X)} rows\"\n",
    "        )\n",
    "\n",
    "    # 3️⃣  calculate day cut-points (chronological)\n",
    "    last_day = day_id.max()                      # e.g. 103\n",
    "    train_cut = int(last_day * train_prop)\n",
    "    val_cut   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    train_mask =  day_id <= train_cut\n",
    "    val_mask   = (day_id > train_cut) & (day_id <= val_cut)\n",
    "    test_mask  =  day_id > val_cut\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val,   y_val   = X[val_mask],   y[val_mask]\n",
    "    X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), samples_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1249c7d7-d100-42ea-8bc0-79c17a9ba8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-day windows   : [391, 391, 391, 391, 391] …\n",
      "Set shapes        : train (28543, 60, 5), val (5865, 60, 5), test (6256, 60, 5)\n"
     ]
    }
   ],
   "source": [
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_pd = chronological_split(\n",
    "    X, y, df,\n",
    "    look_back=60,\n",
    "    rth_start=dt.time(14, 30),   # 09:30 ET in CET\n",
    "    train_prop=0.70,\n",
    "    val_prop=0.15, # let's create only train an test sets for the moment\n",
    ")\n",
    "\n",
    "print(f\"Per-day windows   : {samples_pd[:5]} …\")\n",
    "print(f\"Set shapes        : train {X_tr.shape}, val {X_val.shape}, test {X_te.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db049a1-42a8-47ad-bf15-c3eb1edca173",
   "metadata": {},
   "source": [
    "######################################################################\n",
    "\n",
    "GOAL\n",
    "----- \n",
    "\n",
    "Build a **stateful** LSTM that remembers the whole RTH session,\n",
    "then flushes (“forgets”) at every midnight so yesterday’s state\n",
    "never bleeds into today’s open.  \n",
    "\n",
    "We still keep a 60-bar\n",
    "look-back, which acts as the “short-term” component.\n",
    "\n",
    "KEY DESIGN CHANGES vs the stateless baseline\n",
    "\n",
    "───────────────────────────────────────────────────────────────\n",
    "\n",
    "1. `stateful=True`  →  hidden–cell state is preserved from one\n",
    "   training batch to the next.\n",
    "2. Fixed `batch_size`.  Keras requires you to specify it in the\n",
    "   `batch_input_shape` when the layer is stateful.\n",
    "3. Data must arrive in strict chronological order (`shuffle=False`).\n",
    "4. We call `model.reset_states()` after finishing every *day*.\n",
    "5. We feed one **day** per `model.fit(...)` call, so that the callback\n",
    "   machinery (early stopping, LR scheduler) still works automatically.\n",
    "   \n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e65b13-7949-443a-9439-6d9feeac18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "# 1)  MAKE A DAY-WISE DATASET  (generator that yields one day at a time)\n",
    "########################################################################\n",
    "def make_day_dataset(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    day_id: np.ndarray,\n",
    "    batch_size: int = 1\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Yields windows in *chronological* order, grouped by calendar day.\n",
    "    Each element = (X_day, y_day) where\n",
    "        X_day shape → (n_samples_in_that_day, 60, n_feats)\n",
    "        y_day shape → (n_samples_in_that_day,)\n",
    "    \"\"\"\n",
    "    unique_days = np.unique(day_id)\n",
    "\n",
    "    def gen():\n",
    "        for d in unique_days:\n",
    "            idx = np.flatnonzero(day_id == d)\n",
    "            yield X[idx], y[idx]\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, X.shape[1], X.shape[2]), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None,),                        dtype=tf.float32)\n",
    "        )\n",
    "    ).batch(1, drop_remainder=False)   # 1 \"sequence\" (the day) per step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cf1a2-adfc-49af-a00f-4f5b6beb36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# 2)  BUILD THE STATEFUL  LSTM\n",
    "#########################################################\n",
    "look_back  = X_tr.shape[1]       # 60\n",
    "n_feats    = X_tr.shape[2]       # 5\n",
    "batch_size = 1                   # 1 day per training step\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # note batch_input_shape instead of input_shape\n",
    "    tf.keras.layers.Input(\n",
    "        batch_shape=(batch_size, look_back, n_feats)\n",
    "    ),\n",
    "    tf.keras.layers.LSTM(\n",
    "        64,\n",
    "        stateful=True,           # <── remembers across batches\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2\n",
    "    ),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a2e13-f8ed-4dec-aa5e-565941f1c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# 3)  PREPARE day_id ARRAYS  FOR  TRAIN / VAL / TEST SETS\n",
    "#########################################################\n",
    "# chronological_split already built the masks; we can reuse its logic.\n",
    "# For brevity we recompute here:\n",
    "def build_day_id(df: pd.DataFrame, rth_start: dt.time, look_back: int) -> np.ndarray:\n",
    "    samples_per_day = []\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "        rth_rows = (day_df.index.time >= rth_start).sum()\n",
    "        samples_per_day.append(rth_rows)\n",
    "    return np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "day_id_full = build_day_id(df, dt.time(14, 30), look_back=60)\n",
    "day_id_tr   = day_id_full[:len(X_tr)]\n",
    "day_id_val  = day_id_full[len(X_tr): len(X_tr)+len(X_val)]\n",
    "day_id_te   = day_id_full[-len(X_te):]\n",
    "\n",
    "ds_train = make_day_dataset(X_tr,  y_tr,  day_id_tr)\n",
    "ds_val   = make_day_dataset(X_val, y_val, day_id_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fcb8d-9514-4306-8668-1d345e56d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 4)  TRAIN  —  Reset hidden state after every day automatically\n",
    "################################################################\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_rmse\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_rmse\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5\n",
    "    )\n",
    "]\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---- TRAIN on every day sequentially ---------------------------------\n",
    "    for X_day, y_day in ds_train:\n",
    "        model.train_on_batch(X_day, y_day)\n",
    "        model.reset_states()        # flush memory overnight\n",
    "    \n",
    "    # ---- VALIDATE on every validation day -------------------------------\n",
    "    val_losses = []\n",
    "    val_rmses  = []\n",
    "    for X_day, y_day in ds_val:\n",
    "        loss, rmse = model.evaluate(X_day, y_day, verbose=0)\n",
    "        val_losses.append(loss); val_rmses.append(rmse)\n",
    "        model.reset_states()\n",
    "    val_rmse_epoch = np.mean(val_rmses)\n",
    "    print(f\"Epoch {epoch:3d}   val_RMSE = {val_rmse_epoch:.5f}\")\n",
    "    \n",
    "    # ---- Early-stopping bookkeeping -------------------------------------\n",
    "    callbacks[0].on_epoch_end(epoch, logs={\"val_rmse\": val_rmse_epoch})\n",
    "    callbacks[1].on_epoch_end(epoch, logs={\"val_rmse\": val_rmse_epoch})\n",
    "    if callbacks[0].stopped_epoch > 0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe3add-c8bb-4f85-ada2-393104407bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# 5)  FINAL  EVALUATION  ON CHRONOLOGICAL TEST SET\n",
    "#########################################################\n",
    "test_rmses = []\n",
    "for X_day, y_day in make_day_dataset(X_te, y_te, day_id_te):\n",
    "    loss, rmse = model.evaluate(X_day, y_day, verbose=0)\n",
    "    test_rmses.append(rmse)\n",
    "    model.reset_states()\n",
    "\n",
    "print(f\"\\nFINAL  TEST  RMSE  =  {np.mean(test_rmses):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07405d53-9e14-4f8d-b3c0-856aaf754fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ea52-a04f-4fff-bb69-73c1717dc459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
