{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics\n",
    "\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:30:00</th>\n",
       "      <td>250.5906</td>\n",
       "      <td>250.6435</td>\n",
       "      <td>250.5244</td>\n",
       "      <td>250.5753</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>250.5001</td>\n",
       "      <td>250.6505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:31:00</th>\n",
       "      <td>250.5806</td>\n",
       "      <td>250.6317</td>\n",
       "      <td>250.5121</td>\n",
       "      <td>250.5606</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>250.4854</td>\n",
       "      <td>250.6358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:32:00</th>\n",
       "      <td>250.5712</td>\n",
       "      <td>250.6200</td>\n",
       "      <td>250.4938</td>\n",
       "      <td>250.5453</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>250.4701</td>\n",
       "      <td>250.6205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:33:00</th>\n",
       "      <td>250.5580</td>\n",
       "      <td>250.6094</td>\n",
       "      <td>250.4762</td>\n",
       "      <td>250.5347</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>250.4595</td>\n",
       "      <td>250.6099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:34:00</th>\n",
       "      <td>250.5491</td>\n",
       "      <td>250.5994</td>\n",
       "      <td>250.4600</td>\n",
       "      <td>250.5168</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>250.4416</td>\n",
       "      <td>250.5919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:56:00</th>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3500</td>\n",
       "      <td>203.2450</td>\n",
       "      <td>203.3200</td>\n",
       "      <td>189023.0</td>\n",
       "      <td>203.2590</td>\n",
       "      <td>203.3810</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:57:00</th>\n",
       "      <td>203.3200</td>\n",
       "      <td>203.4200</td>\n",
       "      <td>203.3050</td>\n",
       "      <td>203.3800</td>\n",
       "      <td>222383.0</td>\n",
       "      <td>203.3190</td>\n",
       "      <td>203.4410</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:58:00</th>\n",
       "      <td>203.3800</td>\n",
       "      <td>203.4300</td>\n",
       "      <td>203.3322</td>\n",
       "      <td>203.3750</td>\n",
       "      <td>279702.0</td>\n",
       "      <td>203.3140</td>\n",
       "      <td>203.4360</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:59:00</th>\n",
       "      <td>203.3700</td>\n",
       "      <td>203.4100</td>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>724307.0</td>\n",
       "      <td>203.2790</td>\n",
       "      <td>203.4010</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 21:00:00</th>\n",
       "      <td>203.3288</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>202.8400</td>\n",
       "      <td>203.1993</td>\n",
       "      <td>11076221.0</td>\n",
       "      <td>203.1383</td>\n",
       "      <td>203.2603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46904 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close      volume  \\\n",
       "2025-01-02 13:30:00  250.5906  250.6435  250.5244  250.5753      2259.0   \n",
       "2025-01-02 13:31:00  250.5806  250.6317  250.5121  250.5606      2351.0   \n",
       "2025-01-02 13:32:00  250.5712  250.6200  250.4938  250.5453      2455.0   \n",
       "2025-01-02 13:33:00  250.5580  250.6094  250.4762  250.5347      2474.0   \n",
       "2025-01-02 13:34:00  250.5491  250.5994  250.4600  250.5168      2792.0   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2025-06-03 20:56:00  203.2500  203.3500  203.2450  203.3200    189023.0   \n",
       "2025-06-03 20:57:00  203.3200  203.4200  203.3050  203.3800    222383.0   \n",
       "2025-06-03 20:58:00  203.3800  203.4300  203.3322  203.3750    279702.0   \n",
       "2025-06-03 20:59:00  203.3700  203.4100  203.2500  203.3400    724307.0   \n",
       "2025-06-03 21:00:00  203.3288  203.3400  202.8400  203.1993  11076221.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2025-01-02 13:30:00  250.5001  250.6505             0             0.00   \n",
       "2025-01-02 13:31:00  250.4854  250.6358             0             0.00   \n",
       "2025-01-02 13:32:00  250.4701  250.6205             0             0.00   \n",
       "2025-01-02 13:33:00  250.4595  250.6099             0             0.00   \n",
       "2025-01-02 13:34:00  250.4416  250.5919             0             0.00   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-03 20:56:00  203.2590  203.3810             0             1.99   \n",
       "2025-06-03 20:57:00  203.3190  203.4410             0             1.99   \n",
       "2025-06-03 20:58:00  203.3140  203.4360             0             1.99   \n",
       "2025-06-03 20:59:00  203.2790  203.4010             0             1.99   \n",
       "2025-06-03 21:00:00  203.1383  203.2603             0             1.99   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2025-01-02 13:30:00        0.000                 0.0  \n",
       "2025-01-02 13:31:00        0.000                 0.0  \n",
       "2025-01-02 13:32:00        0.000                 0.0  \n",
       "2025-01-02 13:33:00        0.000                 0.0  \n",
       "2025-01-02 13:34:00        0.000                 0.0  \n",
       "...                          ...                 ...  \n",
       "2025-06-03 20:56:00        0.942                 0.0  \n",
       "2025-06-03 20:57:00        0.882                 0.0  \n",
       "2025-06-03 20:58:00        0.887                 0.0  \n",
       "2025-06-03 20:59:00        0.922                 0.0  \n",
       "2025-06-03 21:00:00        1.062                 0.0  \n",
       "\n",
       "[46904 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/merged_{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"signal_smooth_norm\"\n",
    "\n",
    "feature_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "raw_F = len(feature_cols)\n",
    "look_back = 60\n",
    "n_feats = raw_F * look_back \n",
    "\n",
    "rth_start = dt.time(14, 30) \n",
    "save_dir   = Path(\"dfs training\") \n",
    "ckpt_path = save_dir / f\"best_{ticker}.weights.h5\" \n",
    "\n",
    "train_prop = 0.7\n",
    "val_prop = 0.15\n",
    "batch_size = 1\n",
    "units = 128\n",
    "\n",
    "dropout = 0.10\n",
    "recurrent_dropout = 0.05\n",
    "initial_lr = 1e-3\n",
    "min_lr     = 1e-5   # two orders of magnitude lower\n",
    "loss = \"mse\"\n",
    "\n",
    "max_epochs             = 60\n",
    "early_stop_patience    = 15    # epochs without improvement\n",
    "lr_reduce_patience     = 5    # epochs without improvement → halve LR\n",
    "\n",
    "STEPS_PER_PLOT = 20              # refresh tqdm postfix every n trading sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# build_lstm_tensors                                                   #\n",
    "# ---------------------------------------------------------------------\n",
    "# • Converts a multi-day minute-bar DataFrame into leakage-free NumPy   #\n",
    "#   tensors suitable for our stateful LSTM.                            #\n",
    "# • One *sample*  = last `look_back` candles ➜ predict current candle. #\n",
    "# • Windows NEVER cross midnight.                                      #\n",
    "# • Features are standard-scaled *within each day* to kill day-to-day  #\n",
    "#   level shifts without leaking information between sessions.         #\n",
    "# • Output dtype = float32 (saves RAM & plays nice with GPUs).         #\n",
    "# =====================================================================\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,                         # ← force keyword args for safety\n",
    "    look_back: int,            # length of sliding window (minutes)\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,        # e.g. 14:30 CET for US stocks\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray, shape (samples , look_back·n_feats)\n",
    "        Flattened window for every RTH bar.\n",
    "    y : np.ndarray, shape (samples ,)\n",
    "        One-step-ahead target (same length as samples).\n",
    "    \"\"\"\n",
    "\n",
    "    X_windows, y_targets = [], []             # collectors\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 1) Iterate one calendar day at a time  –– main leak stopper\n",
    "    # -------------------------------------------------------------\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "\n",
    "        # --- chronological order is critical for windows ----------\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # --- per-day standardisation of *features* -----------------\n",
    "        scaler = StandardScaler()\n",
    "        day_df[feature_cols] = scaler.fit_transform(day_df[feature_cols])\n",
    "\n",
    "        # --- pull NumPy views (cheap) ------------------------------\n",
    "        feat_np  = day_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "        label_np = day_df[label_col]      .to_numpy(dtype=np.float32)\n",
    "\n",
    "        # --- locate bars inside Regular Trading Hours --------------\n",
    "        mask_rth = day_df.index.time >= rth_start\n",
    "        rth_idx  = np.flatnonzero(mask_rth)   # integer positions\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 2) Build sliding windows that stay *within the same day*\n",
    "        # -----------------------------------------------------------\n",
    "        for i in rth_idx:\n",
    "            if i < look_back:            # not enough history yet\n",
    "                continue\n",
    "\n",
    "            # rows t-look_back … t-1 become the window\n",
    "            window_3d = feat_np[i - look_back : i]         # (L , F)\n",
    "\n",
    "            # ── choose representation ───────────────────────────\n",
    "            # OPTION A: keep 3-D window  (uncomment next two lines\n",
    "            #          and comment the \"flatten\" line if you prefer)\n",
    "            # X_windows.append(window_3d)\n",
    "            # ----------------------------------------------------\n",
    "            # OPTION B: flatten to 1-D (default – lighter & matches\n",
    "            #          our make_day_dataset reshape)\n",
    "            X_windows.append(window_3d.reshape(-1))        # (L·F,)\n",
    "            # ----------------------------------------------------\n",
    "\n",
    "            y_targets.append(label_np[i])                  # scalar\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 3) Stack lists → final tensors\n",
    "    # -------------------------------------------------------------\n",
    "    X = np.stack(X_windows, dtype=np.float32)    # (N , L·F)  or (N , L , F)\n",
    "    y = np.asarray(y_targets, dtype=np.float32)  # (N ,)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40664, 300)\n",
      "(40664,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=look_back,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split                                                         #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Purpose                                                                     #\n",
    "#   Chronologically split the sliding-window tensors (X, y) into              #\n",
    "#       • train      – first `train_prop`   fraction of *days*                #\n",
    "#       • validation – next  `val_prop`     fraction of *days*                #\n",
    "#       • test       – remainder                                              #\n",
    "#                                                                            #\n",
    "# Why we have to count *windows per day* again                                #\n",
    "#   `build_lstm_tensors()` creates `X` by looping minute-by-minute **inside** #\n",
    "#   each day and it skips the first `look_back` indices (they have no full    #\n",
    "#   context).  Therefore the number of samples coming out of one day is       #\n",
    "#                                                                            #\n",
    "#         (# RTH rows in that day)  minus  (how many of those rows have       #\n",
    "#                                      global-index  < look_back)            #\n",
    "#                                                                            #\n",
    "#   We reproduce exactly that logic here so that the vector `day_id` we build #\n",
    "#   matches X *one-to-one*.                                                   #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],   # train\n",
    "        Tuple[np.ndarray, np.ndarray],   # val\n",
    "        Tuple[np.ndarray, np.ndarray],   # test\n",
    "        List[int],                       # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray   # day_id train/val/test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X , y    Sliding-window tensors returned by `build_lstm_tensors`.\n",
    "    df       Same DataFrame used to create X, y (index = DateTimeIndex).\n",
    "    look_back\n",
    "              Length of each window (minutes).\n",
    "    rth_start\n",
    "              First bar that counts as a *target* (e.g. 14:30:00 CET).\n",
    "    train_prop , val_prop\n",
    "              Fractions of *days* that go into train / val.  Remainder\n",
    "              becomes the test split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train , y_train) , (X_val , y_val) , (X_test , y_test)\n",
    "    samples_per_day      - list[int] , number of usable windows per day\n",
    "    day_id_tr , day_id_val , day_id_te\n",
    "              - arrays mapping each sample back to its calendar day\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 1) How many windows did each calendar day contribute?\n",
    "    #    We must replicate the *same* \"skip first look_back rows\" rule\n",
    "    #    that build_lstm_tensors() applied, otherwise lengths won't match.\n",
    "    # -----------------------------------------------------------------\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "        day_df = day_df.sort_index()                       # chronological\n",
    "\n",
    "        # integer row numbers of bars that are inside RTH for *this* day\n",
    "        idx_rth = np.flatnonzero(day_df.index.time >= rth_start)\n",
    "\n",
    "        # keep only those indices that have at least `look_back` rows of\n",
    "        # history *within the same day*  (identical IF-condition as before)\n",
    "        idx_valid = idx_rth[idx_rth >= look_back]\n",
    "\n",
    "        samples_per_day.append(len(idx_valid))\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2) Build `day_id` – one integer tag for every sample in X\n",
    "    # -----------------------------------------------------------------\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    if len(day_id) != len(X):\n",
    "        raise ValueError(\n",
    "            f\"Mismatch: day_id length = {len(day_id)}  but  X length = {len(X)}.\\n\"\n",
    "            \"Check that build_lstm_tensors() and chronological_split() \"\n",
    "            \"apply the *same* look_back and rth_start logic.\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 3) Determine split boundaries in *day space* (not in sample space)\n",
    "    # -----------------------------------------------------------------\n",
    "    last_day_index = len(samples_per_day) - 1        # 0-based\n",
    "    train_cut_day  = int(last_day_index * train_prop)\n",
    "    val_cut_day    = int(last_day_index * (train_prop + val_prop))\n",
    "\n",
    "    train_mask =  day_id <= train_cut_day\n",
    "    val_mask   = (day_id > train_cut_day) & (day_id <= val_cut_day)\n",
    "    test_mask  =  day_id > val_cut_day\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 4) Slice tensors\n",
    "    # -----------------------------------------------------------------\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val,   y_val   = X[val_mask],   y[val_mask]\n",
    "    X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 5) Return everything – keeping `day_id` arrays lets downstream\n",
    "    #    builders (make_day_dataset) stitch windows back into full days.\n",
    "    # -----------------------------------------------------------------\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), \\\n",
    "           samples_per_day,                                       \\\n",
    "           day_id[train_mask], day_id[val_mask], day_id[test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1249c7d7-d100-42ea-8bc0-79c17a9ba8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-day windows   : [391, 391, 391, 391, 391] …\n",
      "Set shapes        : train (28543, 300), val (5865, 300), test (6256, 300)\n",
      "day_id_tr [ 0  0  0 ... 72 72 72] day_id_val [73 73 73 ... 87 87 87] day_id_te [ 88  88  88 ... 103 103 103]\n"
     ]
    }
   ],
   "source": [
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_pd, day_id_tr, day_id_val, day_id_te = \\\n",
    "chronological_split(\n",
    "                    X, y, df,\n",
    "                    look_back=look_back,\n",
    "                    rth_start=rth_start,  \n",
    "                    train_prop=train_prop,\n",
    "                    val_prop=val_prop \n",
    "                    )\n",
    "\n",
    "print(f\"Per-day windows   : {samples_pd[:5]} …\")\n",
    "print(f\"Set shapes        : train {X_tr.shape}, val {X_val.shape}, test {X_te.shape}\")\n",
    "print('day_id_tr', day_id_tr, 'day_id_val', day_id_val, 'day_id_te', day_id_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset                                                             #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *per-minute* sliding-window arrays (X, y, day_id) into a        #\n",
    "# tf.data.Dataset that streams ONE element per trading session.               #\n",
    "#                                                                              #\n",
    "# Output per element                                                           #\n",
    "#   x_day        : (batch = 1 , T , n_feats)   – full intraday feature tensor  #\n",
    "#   y_day        : (batch = 1 , T)             – matching label vector         #\n",
    "#   new_day_flag : scalar bool  (always True)  – lets the training loop call   #\n",
    "#                                              reset_states() explicitly       #\n",
    "#                                                                              #\n",
    "# Important:                                                                   \n",
    "#   • `X` already contains the *flattened* 60-bar window for every minute      #\n",
    "#     (shape per sample = look_back × raw_feats).  Therefore we NO LONGER      #\n",
    "#     slice `[:, -1, :]`; we keep the vector “as is”.                          #\n",
    "#   • If you switched back to the 3-D window variant, replace the “reshape”    #\n",
    "#     comment below accordingly.                                               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def make_day_dataset(\n",
    "        X: np.ndarray,           # (N , L·F)   – flattened 60-bar window\n",
    "        y: np.ndarray,           # (N ,)       – scalar target per sample\n",
    "        day_id: np.ndarray,      # (N ,)       – integer day tag\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Groups minute-level samples into per-day tensors and returns a\n",
    "    stateful-ready Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 1.  Chronological ordering  (just in case the caller shuffled)\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    sort_idx  = np.argsort(day_id, kind=\"stable\")\n",
    "    X_sorted  = X[sort_idx]\n",
    "    y_sorted  = y[sort_idx]\n",
    "    d_sorted  = day_id[sort_idx]\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 2.  Slice contiguous blocks that share the same day_id\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    day_change = np.where(np.diff(d_sorted) != 0)[0] + 1\n",
    "    split_ids  = np.split(np.arange(len(d_sorted)), day_change)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 3.  Generator – one yield == one full RTH session\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    def gen():\n",
    "        for idx_block in split_ids:\n",
    "            # All minutes for one calendar day\n",
    "            x_block = X_sorted[idx_block]          # (T , L·F)\n",
    "            y_block = y_sorted[idx_block]          # (T ,)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # If you kept the 3-D window (60 , F) variant in\n",
    "            # build_lstm_tensors(), replace the next two lines with:\n",
    "            #     x_day = x_block.reshape(T , look_back , raw_F)\n",
    "            #     n_feats = look_back * raw_F  <-- adjust below\n",
    "            # ---------------------------------------------------------\n",
    "            x_day = x_block                        # (T , n_feats) – already flat\n",
    "\n",
    "            # Label vector is already one-dimensional\n",
    "            y_day = y_block                        # (T ,)\n",
    "\n",
    "            # Add leading batch dim = 1 for stateful=True\n",
    "            x_day = np.expand_dims(x_day, 0)       # (1 , T , n_feats)\n",
    "            y_day = np.expand_dims(y_day, 0)       # (1 , T)\n",
    "\n",
    "            yield x_day.astype(np.float32), y_day.astype(np.float32), True\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 4.  Wrap in tf.data.Dataset\n",
    "    #     T is variable (None) because each day has a different #bars\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    n_feats = X.shape[-1]                 # here n_feats = look_back · raw_F\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(1, None, n_feats), dtype=tf.float32),  # x_day\n",
    "        tf.TensorSpec(shape=(1, None),       dtype=tf.float32),     # y_day\n",
    "        tf.TensorSpec(shape=(),              dtype=tf.bool)         # flag\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)     # overlaps data prep & GPU work\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b435b87f-ec62-4e25-b0f5-60ef82fd6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(1, None, 300), dtype=tf.float32, name=None), TensorSpec(shape=(1, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(1, None, 300), dtype=tf.float32, name=None), TensorSpec(shape=(1, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(1, None, 300), dtype=tf.float32, name=None), TensorSpec(shape=(1, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>\n",
      "X  shape : (1, None, 300) dtype: <dtype: 'float32'>\n",
      "y  shape : (1, None) dtype: <dtype: 'float32'>\n",
      "flag     : () dtype: <dtype: 'bool'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Prepare train / validation / test datasets\n",
    "# ---------------------------------------------------------------------------\n",
    "# We now switch from “one-window” streams to **“one-day” streams**.\n",
    "# Each iterator element already contains the leading batch-dim = 1, so the\n",
    "# `batch_size` argument is no longer required.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# build the datasets\n",
    "ds_train = make_day_dataset(X_tr,  y_tr,  day_id_tr)   # one element = one day\n",
    "ds_val   = make_day_dataset(X_val, y_val, day_id_val)\n",
    "ds_test  = make_day_dataset(X_te,  y_te,  day_id_te)\n",
    "\n",
    "# quick sanity-prints\n",
    "print(ds_train)     # e.g. <_GeneratorDataset element_spec=...>\n",
    "print(ds_val)\n",
    "print(ds_test)\n",
    "\n",
    "# inspect the TensorSpec coming out of the generator\n",
    "x_spec, y_spec, flag_spec = ds_train.element_spec\n",
    "\n",
    "print(\"X  shape :\", x_spec.shape, \"dtype:\", x_spec.dtype)   # (1, None, n_feats)\n",
    "print(\"y  shape :\", y_spec.shape, \"dtype:\", y_spec.dtype)   # (1, None)\n",
    "print(\"flag     :\", flag_spec.shape, \"dtype:\", flag_spec.dtype)  # scalar bool\n",
    "\n",
    "\n",
    "# 1) save test Dataset  →  will AUTO-create \"dfs training/ds_test_data/\"\n",
    "ds_test.save(str(save_dir / \"ds_test_data\"), compression=\"GZIP\")\n",
    "\n",
    "# 2) store element_spec right next to it\n",
    "(save_dir / f\"ds_test_{ticker}.pkl\").write_bytes(\n",
    "    pickle.dumps(ds_test.element_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│                ❶  NETWORK WEIGHTS   (global knowledge)                │\\n│   • Millions of parameters learned across *all* historic days.        │\\n│   • Do **not** change during inference.                               │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│              ❷  CELL STATE  cₜ   (“within-day long-term”)             │\\n│   • Slow-changing store; integrator.                                   │\\n│   • Carries patterns from early-morning bars to late-afternoon bars.   │\\n│   • Reset to 0 at day boundary.                                        │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│              ❸  HIDDEN STATE  hₜ  (“within-day short-term”)           │\\n│   • Quickly reacts to the last few bars (momentum, spikes).            │\\n│   • Also reset to 0 overnight.                                         │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│              ❹  CURRENT INPUT WINDOW  xₜ  (60 latest bars)            │\\n│   • Raw OHLCV features for minutes t-60 … t-1.                         │\\n│   • Injected into the gates together with hₜ to produce the output.    │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                       Predicted signal  ŷₜ\\n\\n\\n\\n\\nSource\\t                                                Typical influence on ŷₜ\\n\\nNetwork weights (“what patterns matter in general?”)\\t≈ 50 %\\nCell state cₜ (earlier bars this day)\\t                  20 – 30 %\\nHidden state hₜ (very recent bars)\\t                      10 – 20 %\\nCurrent 60-bar raw input\\t                              10 – 20 %\\n\\n\\n\\nDay i\\n ───────────────────────────────────────────────────────────────────────\\n t=0        t=200          t=390 (close)\\n │──────────│──────────────│\\n cₜ:   0 → growing memory → carries full-day context ─┐\\n hₜ:   0 → wobbles quickly → tracks local moves      │\\n                                                     │\\n model.reset_states()  ◄─────────────────────────────┘\\n Day i+1   (both c and h back to zero)\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                ❶  NETWORK WEIGHTS   (global knowledge)                │\n",
    "│   • Millions of parameters learned across *all* historic days.        │\n",
    "│   • Do **not** change during inference.                               │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              ❷  CELL STATE  cₜ   (“within-day long-term”)             │\n",
    "│   • Slow-changing store; integrator.                                   │\n",
    "│   • Carries patterns from early-morning bars to late-afternoon bars.   │\n",
    "│   • Reset to 0 at day boundary.                                        │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              ❸  HIDDEN STATE  hₜ  (“within-day short-term”)           │\n",
    "│   • Quickly reacts to the last few bars (momentum, spikes).            │\n",
    "│   • Also reset to 0 overnight.                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              ❹  CURRENT INPUT WINDOW  xₜ  (60 latest bars)            │\n",
    "│   • Raw OHLCV features for minutes t-60 … t-1.                         │\n",
    "│   • Injected into the gates together with hₜ to produce the output.    │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                       Predicted signal  ŷₜ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Source\t                                                Typical influence on ŷₜ\n",
    "\n",
    "Network weights (“what patterns matter in general?”)\t≈ 50 %\n",
    "Cell state cₜ (earlier bars this day)\t                  20 – 30 %\n",
    "Hidden state hₜ (very recent bars)\t                      10 – 20 %\n",
    "Current 60-bar raw input\t                              10 – 20 %\n",
    "\n",
    "\n",
    "\n",
    "Day i\n",
    " ───────────────────────────────────────────────────────────────────────\n",
    " t=0        t=200          t=390 (close)\n",
    " │──────────│──────────────│\n",
    " cₜ:   0 → growing memory → carries full-day context ─┐\n",
    " hₜ:   0 → wobbles quickly → tracks local moves      │\n",
    "                                                     │\n",
    " model.reset_states()  ◄─────────────────────────────┘\n",
    " Day i+1   (both c and h back to zero)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072a3d6a-9bd7-4f5b-871f-4dc3143abf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4. build_stateful_lstm                                                      #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "#   Create **one** compact, stateful, seq-to-seq LSTM that:                   #\n",
    "#       • receives  (B=1 , T , n_feats)   where n_feats = look_back × F       #\n",
    "#       • emits     (B=1 , T , 1)          → one prediction *per minute*      #\n",
    "#       • keeps its hidden state (h, c) through the entire RTH session        #\n",
    "#       • is compiled and ready for `train_on_batch` / `model.fit`            #\n",
    "#                                                                             #\n",
    "# MEMORY-RETENTION TWEAKS (explained in comments)                             #\n",
    "# ──────────────────────────────────────────────────────────────────────────── #\n",
    "#   1) `return_sequences=True`               – emit a vector each minute.     #\n",
    "#   2) `unit_forget_bias=True` (default)     – sets the forget-gate bias ≈ +1 #\n",
    "#      which empirically lets cₜ remember 2-3× longer.                        #\n",
    "#   3) `recurrent_dropout=0.05`              – low value so gates aren’t      #\n",
    "#      zeroed too often → longer usable horizon.                              #\n",
    "#   4) Optional: bump `units` ≥128 for even longer recall if GPU allows.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_stateful_lstm(\n",
    "        look_back: int,         # kept only for API symmetry (ignored here)\n",
    "        n_feats:   int,         # = look_back * raw_feature_count (flattened)\n",
    "        *,\n",
    "        batch_size:       int = 1,   # MUST be 1 when stateful=True\n",
    "        units:            int = 128, # ↑units ⇒ ↑long-term capacity\n",
    "        dropout:          float = 0.10,\n",
    "        recurrent_dropout:float = 0.05,\n",
    "        initial_lr:       float = 1e-3,\n",
    "        loss:             str   = \"mse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.Model\n",
    "        A compiled, stateful LSTM that outputs a scalar per minute.\n",
    "    ------------------------------------------------------------------\n",
    "    Input  : (batch=1 , T , n_feats)\n",
    "    Output : (batch=1 , T , 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # ─────────────────────── NETWORK CORE ────────────────────────── #\n",
    "    model = models.Sequential([\n",
    "\n",
    "        # Fixed batch size (1) for stateful=True; variable sequence length.\n",
    "        layers.Input(batch_shape=(batch_size, None, n_feats)),\n",
    "\n",
    "        layers.LSTM(\n",
    "            units             = units,\n",
    "            stateful          = True,        # keep (h, c) across minutes\n",
    "            return_sequences  = True,        # one vector EACH minute\n",
    "            dropout           = dropout,\n",
    "            recurrent_dropout = recurrent_dropout,\n",
    "            # `unit_forget_bias=True` (default) sets forget-gate bias to +1\n",
    "            # which already extends cₜ memory; no need for manual hack.\n",
    "        ),\n",
    "\n",
    "        # Map every time-step’s hidden vector → one scalar prediction\n",
    "        layers.TimeDistributed(layers.Dense(1, activation=\"linear\"))\n",
    "    ])\n",
    "\n",
    "    # ─────────────────────── COMPILE ─────────────────────────────── #\n",
    "    model.compile(\n",
    "        optimizer = optimizers.Adam(learning_rate=initial_lr),\n",
    "        loss      = loss,\n",
    "        metrics   = [metrics.RootMeanSquaredError(name=\"rmse\")]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c6ff11-2bda-4a5e-994a-cf44f0fe6383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=True>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_stateful_lstm(\n",
    "        look_back=look_back,\n",
    "        n_feats=n_feats,\n",
    "        batch_size=batch_size,\n",
    "        units=units,\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=recurrent_dropout,\n",
    "        initial_lr=initial_lr)\n",
    "\n",
    "#save model\n",
    "model.save(save_dir / f\"model_{ticker}.keras\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ccba0e-2c38-4ba3-90b3-6a13164c8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LSTM name=lstm, built=True>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = model.layers[0]          # if your first layer is the stateful LSTM\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  LiveRMSEPlot – works on every Jupyter backend                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    Live, inline RMSE curves.  Call `update(train, val)` once per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        plt.ioff()                                             # quiet backend\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "\n",
    "        self.line_tr , = self.ax.plot([], [], label=\"train RMSE\")\n",
    "        self.line_val, = self.ax.plot([], [], label=\"val RMSE\")\n",
    "\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\",\n",
    "                    title=\"Training progress\")\n",
    "        self.ax.grid(True); self.ax.legend()\n",
    "\n",
    "        self.epochs, self.train, self.val = [], [], []\n",
    "\n",
    "        # One DisplayHandle → in-place updates (no flicker)\n",
    "        self._handle = display(self.fig, display_id=True)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _push(self):\n",
    "        \"Force-refresh the figure, backend-agnostic.\"\n",
    "        try:\n",
    "            self._handle.update(self.fig)          # >= IPython-8\n",
    "        except AttributeError:                     # very old IPython\n",
    "            clear_output(wait=True); display(self.fig)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        ep = len(self.epochs) + 1\n",
    "        self.epochs.append(ep); self.train.append(train_rmse); self.val.append(val_rmse)\n",
    "\n",
    "        self.line_tr.set_data(self.epochs, self.train)\n",
    "        self.line_val.set_data(self.epochs, self.val)\n",
    "        self.ax.relim(); self.ax.autoscale_view()\n",
    "\n",
    "        self._push()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0270ee25-3dbc-4cd5-8adf-6b71cb30319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# custom_stateful_training_loop  –  ONE-DAY-PER-BATCH, MINUTE-LEVEL           #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# What this loop does                                                         #\n",
    "#   • Receives a Dataset that yields exactly *one* element per calendar day   #\n",
    "#       X_day : (1 , T , n_feats)      all minutes of the session             #\n",
    "#       y_day : (1 , T)                scalar label per minute                #\n",
    "#       flag  : bool                   always True (explicit reset signal)    #\n",
    "#   • Keeps the LSTM state (h, c) alive through the session, then resets      #\n",
    "#     at the closing bell.                                                   #\n",
    "#   • Trains with `model.train_on_batch()`  (fast, no extra graph-tracing).   #\n",
    "#   • Prints a tqdm bar + optional live Matplotlib plot.                      #\n",
    "#   • Implements early-stopping + “halve-on-plateau” LR schedule.             #\n",
    "#   • Saves the best weights to `ckpt_path` and reloads them at the end.      #\n",
    "###############################################################################\n",
    "\n",
    "def custom_stateful_training_loop(\n",
    "        model: tf.keras.Model,\n",
    "        ds_train: tf.data.Dataset,\n",
    "        ds_val:   tf.data.Dataset,\n",
    "        *,\n",
    "        max_epochs:          int,\n",
    "        early_stop_patience: int,\n",
    "        lr_reduce_patience:  int,\n",
    "        min_lr:              float,\n",
    "        ckpt_path:           str\n",
    ") -> float:\n",
    "    # ── ensure Keras-3 filename requirement (.weights.h5) ────────────\n",
    "    if not str(ckpt_path).endswith(\".weights.h5\"):\n",
    "        ckpt_path = Path(str(ckpt_path) + \".weights.h5\")\n",
    "\n",
    "    # ── live plot (fallback stub if display fails) ───────────────────\n",
    "    try:\n",
    "        plotter = LiveRMSEPlot()\n",
    "    except Exception as e:\n",
    "        print(\"Live plotting disabled →\", e)\n",
    "        class _Stub:                       # noqa: D401\n",
    "            def update(self, *_, **__): pass\n",
    "        plotter = _Stub()\n",
    "\n",
    "    lstm_layers = [l for l in model.layers if hasattr(l, \"reset_states\")]\n",
    "\n",
    "    N_train = sum(1 for _ in ds_train)\n",
    "    ds_train = ds_train.cache()\n",
    "\n",
    "    # LR must be a tf.Variable so we can .assign()\n",
    "    if not isinstance(model.optimizer.learning_rate, tf.Variable):\n",
    "        model.optimizer.learning_rate = tf.Variable(model.optimizer.learning_rate, tf.float32)\n",
    "    lr_var = model.optimizer.learning_rate\n",
    "\n",
    "    best_val, pat_no_imp, pat_lr = np.inf, 0, 0   # early stop bookkeeping\n",
    "\n",
    "    # ─────────────────────────── EPOCH LOOP ──────────────────────────\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "        # 1) TRAIN ----------------------------------------------------\n",
    "        batch_rmse = []\n",
    "        pbar = tqdm(ds_train, total=N_train, unit=\"day\",\n",
    "                    desc=f\"Epoch {epoch}\", leave=False)\n",
    "\n",
    "        for step, (X_day, y_day, new_day) in enumerate(pbar, 1):\n",
    "            if bool(new_day):\n",
    "                for l in lstm_layers: l.reset_states()\n",
    "\n",
    "            logs = model.train_on_batch(X_day, y_day, return_dict=True)\n",
    "            batch_rmse.append(logs[\"rmse\"])\n",
    "\n",
    "            if step % STEPS_PER_PLOT == 0:\n",
    "                pbar.set_postfix(train_rmse=f\"{np.mean(batch_rmse[-STEPS_PER_PLOT:]):.4f}\")\n",
    "        pbar.close()\n",
    "        epoch_train = float(np.mean(batch_rmse))\n",
    "\n",
    "        # 2) VALIDATE -------------------------------------------------\n",
    "        val_rmse = []\n",
    "        for X_day, y_day, new_day in ds_val:\n",
    "            if bool(new_day):\n",
    "                for l in lstm_layers: l.reset_states()\n",
    "\n",
    "            y_pred = tf.squeeze(model(X_day, training=False), axis=[0, -1])\n",
    "            y_true = tf.squeeze(y_day, axis=0)\n",
    "            rmse   = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmse.append(float(rmse))\n",
    "        epoch_val = float(np.mean(val_rmse))\n",
    "\n",
    "        # 3) LOG + PLOT ----------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f}\")\n",
    "        plotter.update(epoch_train, epoch_val)\n",
    "\n",
    "        # 4) CHECKPOINT + LR SCHEDULE --------------------------------\n",
    "        if epoch_val < best_val:\n",
    "            best_val, pat_no_imp, pat_lr = epoch_val, 0, 0\n",
    "            model.save_weights(ckpt_path)\n",
    "        else:\n",
    "            pat_no_imp += 1; pat_lr += 1\n",
    "            if pat_lr >= lr_reduce_patience:\n",
    "                new_lr = max(float(lr_var.numpy()) * 0.5, min_lr)\n",
    "                lr_var.assign(new_lr); pat_lr = 0\n",
    "                print(f\"    ↳ LR halved to {new_lr:.1e}\")\n",
    "\n",
    "        if pat_no_imp >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\"); break\n",
    "\n",
    "    # 5) reload champion weights & return best metric -----------------\n",
    "    if Path(ckpt_path).exists():\n",
    "        model.load_weights(ckpt_path)\n",
    "\n",
    "    return best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6041b99-b5b3-4ff1-a981-578c8f630c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1508a67d44604e91933ee09b6246da25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/73 [00:00<?, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_rmse = custom_stateful_training_loop(\n",
    "                                            model=model,\n",
    "                                            ds_train=ds_train,\n",
    "                                            ds_val=ds_val,\n",
    "                                            max_epochs=max_epochs,\n",
    "                                            early_stop_patience=early_stop_patience,\n",
    "                                            lr_reduce_patience=lr_reduce_patience,\n",
    "                                            min_lr=min_lr,\n",
    "                                            ckpt_path=ckpt_path\n",
    "                                            )\n",
    "best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ea52-a04f-4fff-bb69-73c1717dc459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
