{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 20:05:51.560229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750269951.647043   12228 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750269951.677386   12228 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750269951.785182   12228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750269951.785370   12228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750269951.785376   12228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750269951.785380   12228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:30:00</th>\n",
       "      <td>250.5906</td>\n",
       "      <td>250.6435</td>\n",
       "      <td>250.5244</td>\n",
       "      <td>250.5753</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>250.5001</td>\n",
       "      <td>250.6505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:31:00</th>\n",
       "      <td>250.5806</td>\n",
       "      <td>250.6317</td>\n",
       "      <td>250.5121</td>\n",
       "      <td>250.5606</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>250.4854</td>\n",
       "      <td>250.6358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:32:00</th>\n",
       "      <td>250.5712</td>\n",
       "      <td>250.6200</td>\n",
       "      <td>250.4938</td>\n",
       "      <td>250.5453</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>250.4701</td>\n",
       "      <td>250.6205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:33:00</th>\n",
       "      <td>250.5580</td>\n",
       "      <td>250.6094</td>\n",
       "      <td>250.4762</td>\n",
       "      <td>250.5347</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>250.4595</td>\n",
       "      <td>250.6099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:34:00</th>\n",
       "      <td>250.5491</td>\n",
       "      <td>250.5994</td>\n",
       "      <td>250.4600</td>\n",
       "      <td>250.5168</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>250.4416</td>\n",
       "      <td>250.5919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:56:00</th>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3500</td>\n",
       "      <td>203.2450</td>\n",
       "      <td>203.3200</td>\n",
       "      <td>189023.0</td>\n",
       "      <td>203.2590</td>\n",
       "      <td>203.3810</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:57:00</th>\n",
       "      <td>203.3200</td>\n",
       "      <td>203.4200</td>\n",
       "      <td>203.3050</td>\n",
       "      <td>203.3800</td>\n",
       "      <td>222383.0</td>\n",
       "      <td>203.3190</td>\n",
       "      <td>203.4410</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:58:00</th>\n",
       "      <td>203.3800</td>\n",
       "      <td>203.4300</td>\n",
       "      <td>203.3322</td>\n",
       "      <td>203.3750</td>\n",
       "      <td>279702.0</td>\n",
       "      <td>203.3140</td>\n",
       "      <td>203.4360</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:59:00</th>\n",
       "      <td>203.3700</td>\n",
       "      <td>203.4100</td>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>724307.0</td>\n",
       "      <td>203.2790</td>\n",
       "      <td>203.4010</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 21:00:00</th>\n",
       "      <td>203.3288</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>202.8400</td>\n",
       "      <td>203.1993</td>\n",
       "      <td>11076221.0</td>\n",
       "      <td>203.1383</td>\n",
       "      <td>203.2603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46904 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close      volume  \\\n",
       "2025-01-02 13:30:00  250.5906  250.6435  250.5244  250.5753      2259.0   \n",
       "2025-01-02 13:31:00  250.5806  250.6317  250.5121  250.5606      2351.0   \n",
       "2025-01-02 13:32:00  250.5712  250.6200  250.4938  250.5453      2455.0   \n",
       "2025-01-02 13:33:00  250.5580  250.6094  250.4762  250.5347      2474.0   \n",
       "2025-01-02 13:34:00  250.5491  250.5994  250.4600  250.5168      2792.0   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2025-06-03 20:56:00  203.2500  203.3500  203.2450  203.3200    189023.0   \n",
       "2025-06-03 20:57:00  203.3200  203.4200  203.3050  203.3800    222383.0   \n",
       "2025-06-03 20:58:00  203.3800  203.4300  203.3322  203.3750    279702.0   \n",
       "2025-06-03 20:59:00  203.3700  203.4100  203.2500  203.3400    724307.0   \n",
       "2025-06-03 21:00:00  203.3288  203.3400  202.8400  203.1993  11076221.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2025-01-02 13:30:00  250.5001  250.6505             0             0.00   \n",
       "2025-01-02 13:31:00  250.4854  250.6358             0             0.00   \n",
       "2025-01-02 13:32:00  250.4701  250.6205             0             0.00   \n",
       "2025-01-02 13:33:00  250.4595  250.6099             0             0.00   \n",
       "2025-01-02 13:34:00  250.4416  250.5919             0             0.00   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-03 20:56:00  203.2590  203.3810             0             1.99   \n",
       "2025-06-03 20:57:00  203.3190  203.4410             0             1.99   \n",
       "2025-06-03 20:58:00  203.3140  203.4360             0             1.99   \n",
       "2025-06-03 20:59:00  203.2790  203.4010             0             1.99   \n",
       "2025-06-03 21:00:00  203.1383  203.2603             0             1.99   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2025-01-02 13:30:00        0.000                 0.0  \n",
       "2025-01-02 13:31:00        0.000                 0.0  \n",
       "2025-01-02 13:32:00        0.000                 0.0  \n",
       "2025-01-02 13:33:00        0.000                 0.0  \n",
       "2025-01-02 13:34:00        0.000                 0.0  \n",
       "...                          ...                 ...  \n",
       "2025-06-03 20:56:00        0.942                 0.0  \n",
       "2025-06-03 20:57:00        0.882                 0.0  \n",
       "2025-06-03 20:58:00        0.887                 0.0  \n",
       "2025-06-03 20:59:00        0.922                 0.0  \n",
       "2025-06-03 21:00:00        1.062                 0.0  \n",
       "\n",
       "[46904 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/merged_{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = \"signal_smooth_norm\"\n",
    "\n",
    "feature_cols   = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "LOOK_BACK      = 60                                # minutes of history\n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "import datetime as dt\n",
    "rth_start      = dt.time(14, 30)                   # US cash-open in CET/CEST\n",
    "\n",
    "from pathlib import Path\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"best_{ticker}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1 ·  MODEL HYPER-PARAMETERS (tuned defaults)                                 #\n",
    "###############################################################################\n",
    "# ── architecture ───────────────────────────────────────────────────────\n",
    "SHORT_UNITS        = 32      # double → better GPU fill, little over-fit risk\n",
    "LONG_UNITS         = 96\n",
    "DROPOUT_SHORT      = 0.20    # applied AFTER the LSTM layer\n",
    "DROPOUT_LONG       = 0.15\n",
    "REC_DROP_SHORT     = 0.0     # MUST be 0.0 for fused kernel\n",
    "REC_DROP_LONG      = 0.0\n",
    "\n",
    "# ── optimiser : cosine-decay-restarts ──────────────────────────────────\n",
    "INITIAL_LR         = 4e-4    # slightly cooler for bigger network\n",
    "FIRST_DECAY_EPOCHS = 3       # first valley arrives early\n",
    "T_MUL              = 2.0\n",
    "M_MUL              = 1.0\n",
    "ALPHA              = 0.05\n",
    "LOSS_FN            = \"mse\"\n",
    "CLIPNORM           = 1.0\n",
    "\n",
    "# ── training control ───────────────────────────────────────────────────\n",
    "TRAIN_BATCH        = 16      # GPU now uses more threads; fits on 8-12 GB\n",
    "VAL_BATCH          = 1\n",
    "MAX_EPOCHS         = 200\n",
    "EARLY_STOP_PATIENCE= 20\n",
    "USE_FP16           = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1-A. per-day standard-scaling of *features*\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(\n",
    "                                   day_df[feature_cols])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col]     .to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40664, 300)\n",
      "(40664,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 64  (multiple of 16)\n",
      "Validation days    : 15\n",
      "Test days          : 16\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (TRAIN_BATCH , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750269960.354586   12228 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=TRAIN_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "\n",
    "    x = layers.LSTM(short_units,\n",
    "                    stateful=True,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0, #dropout_short,\n",
    "                    recurrent_dropout=0, #rec_drop_short,\n",
    "                    kernel_initializer=\"orthogonal\",\n",
    "                    name=\"short_lstm\")(inp)\n",
    "    x = layers.Dropout(dropout_short)(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "\n",
    "    x = layers.LSTM(long_units,\n",
    "                    stateful=True,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0, #dropout_long,\n",
    "                    recurrent_dropout=0, #rec_drop_long,\n",
    "                    kernel_initializer=\"orthogonal\",\n",
    "                    name=\"long_lstm\")(x)\n",
    "    x = layers.Dropout(dropout_long)(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    model = models.Model(inp, out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE                       #\n",
    "###############################################################################\n",
    "\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    Blue  = train RMSE   (line + latest-dot)\n",
    "    Orange= val   RMSE   (line + latest-dot)\n",
    "    Works with `%matplotlib inline`, `%matplotlib widget`, `%matplotlib notebook`\n",
    "    without spawning a new image every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        self._build_figure()\n",
    "        # display once and keep display_id so we can overwrite instead of spawn\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "\n",
    "        self.e, self.tr, self.va = [], [], []      # epoch → metric history\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        # 1. append data\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "\n",
    "        # 2. update lines\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "\n",
    "        # 3. update latest dots\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            self.va_dot.set_offsets([])            # hide dot if NaN\n",
    "\n",
    "        # 4. rescale axes\n",
    "        self.ax.relim();  self.ax.autoscale_view()\n",
    "\n",
    "        # 5. redraw WITHOUT spawning new figure\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:                                       # inline / notebook png\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)           # overwrite same output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# custom_stateful_training_loop                                               #\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  WHAT THIS ROUTINE DOES END-TO-END                                         #\n",
    "#  • Trains a **dual-memory, stateful** LSTM that ingests entire trading     #\n",
    "#    days (or fat-batches of 32 days) in one go.                             #\n",
    "#  • Two stacked layers:                                                     #\n",
    "#        short_lstm – intraday memory  → reset at the **start of every day** #\n",
    "#        long_lstm  – multi-day memory → reset only once per **week-end**    #\n",
    "#  • Works with three tf.data streams:                                       #\n",
    "#        – TRAIN  : (32 , T_max , F)  padded batches,     FP16, CuDNN        #\n",
    "#        – VAL/TE : (1  , T      , F)  unbatched per day, FP32               #\n",
    "#  • Displays one persistent tqdm bar that counts *calendar days* so users   #\n",
    "#    see meaningful progress.                                                #\n",
    "#  • LiveRMSEPlot() drops a train / val RMSE dot after every epoch.          #\n",
    "#  • **Early stopping** on stagnant val-RMSE; the learning-rate is governed  #\n",
    "#    entirely by the **Cosine-Decay-Restarts** schedule baked into the model #\n",
    "#                                                                             #\n",
    "#  SPEED / STABILITY DECISIONS                                               #\n",
    "#  ────────────────────────────────────────────────────────────────────────── #\n",
    "#  • Uses CuDNN-fused LSTM kernels (fastest path)  ➜  therefore the loop is  #\n",
    "#    compiled in graph mode **without** `jit_compile=True` (XLA + CuDNN      #\n",
    "#    RNNs remains unsupported).                                              #\n",
    "#  • Mixed precision ON: float16 activations + float32 weights deliver about #\n",
    "#    2× lower GPU memory use and ~1.5× speed-up during back-prop.            #\n",
    "#  • Gradient clipping (`clipnorm = 1.0`) avoids FP16 blow-ups.              #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  low-level helper  (unchanged)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.cast(model(xb, training=True), tf.float32)\n",
    "        loss   = loss_fn(yb, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return tf.sqrt(loss)           # RMSE\n",
    "\n",
    "\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Return the *scalar* learning-rate, regardless of whether `opt` is\n",
    "    • a normal optimizer,\n",
    "    • wrapped by LossScaleOptimizer (mixed-precision),\n",
    "    • driven by a LearningRateSchedule.\n",
    "    \"\"\"\n",
    "    # 1 ─ unwrap if mixed-precision\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer       # (older TF: opt._optimizer)\n",
    "\n",
    "    # 2 ─ constant vs. schedule\n",
    "    lr = opt.learning_rate\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:                               # tf.Variable or Python float\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,                    # (32 day , T_max , F)\n",
    "    ds_val,                              # (1  day , T     , F)\n",
    "    *,\n",
    "    n_train_days: int,                   # concrete # calendar days / epoch\n",
    "    max_epochs: int,\n",
    "    early_stop_patience: int,\n",
    "    baseline_val_rmse: float,   \n",
    "    ckpt_path) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Fast GPU-optimal loop:\n",
    "      • CuDNN fused LSTM kernels (mixed-precision, no XLA)\n",
    "      • One fat-batch = 32 calendar days\n",
    "      • Outer tqdm bar counts days; no inner bars, no fancy prints\n",
    "    Returns the best validation RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    loss_fn   = tf.keras.losses.MeanSquaredError()\n",
    "    opt       = model_train.optimizer\n",
    "    opt.clipnorm = 1.0                                   # FP16 stability\n",
    "    live_plot = LiveRMSEPlot()                           # tiny matplotlib helper\n",
    "\n",
    "    # quick handles to stateful layers for manual resets\n",
    "    short_tr  = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr   = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers   if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers   if l.name == \"long_lstm\"]\n",
    "\n",
    "    best_val_rmse, patience_ctr = math.inf, 0\n",
    "    \n",
    "    # ============================================================================\n",
    "    # E P O C H   L O O P\n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    # • Outer loop = epoch\n",
    "    # • One epoch = all training days once + full validation pass\n",
    "    # • A tqdm bar counts calendar-days during TRAIN; we keep the bar visible\n",
    "    #   until the epoch summary is printed, then we close it (leave=False)\n",
    "    # ============================================================================\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "    \n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        # 0 ▸ PROGRESS-BAR  – one tick == one calendar day\n",
    "        #   leave=False  ➜  the bar line is cleared when .close() is called\n",
    "        #   We create it *before* TRAIN starts and close it *after* we print\n",
    "        #   the epoch-level metrics, so the user always sees 100 % + numbers.\n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "    \n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        # 1 ▸ TRAIN PHASE\n",
    "        #   • Iterate over fat batches (TRAIN_BATCH calendar days each)\n",
    "        #   • Manual state resets (daily + weekend)\n",
    "        #   • Single CuDNN kernel per batch for max throughput\n",
    "        #   • Progress-bar advances +TRAIN_BATCH ticks at once\n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        batch_rmses, prev_wd_train = [], None\n",
    "    \n",
    "        for xb, yb, wd_batch in ds_train_batched:                # xb.shape[0] == TRAIN_BATCH\n",
    "            # 1.1  DAILY reset  – short-term tier\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # 1.2  WEEKEND reset – long-term tier (detect Fri→Mon wrap-around)\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # 1.3  FORWARD + BACKWARD  (CuDNN fused kernel, FP16 inside)\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # 1.4  PROGRESS-BAR  – jump +TRAIN_BATCH calendar-day ticks\n",
    "            pbar.update(int(xb.shape[0]))                        # +TRAIN_BATCH\n",
    "    \n",
    "        epoch_train = float(np.mean(batch_rmses))                # TRAIN metric\n",
    "    \n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        # 2 ▸ VALIDATION  – pure FP32 forward pass, batch_size = 1 (one day)\n",
    "        #   • Copy weights from train-graph to val-graph\n",
    "        #   • Same state-reset logic as train-graph\n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        model_val.set_weights(model_train.get_weights())         # sync FP32 twin\n",
    "        val_rmses, prev_wd_val = [], None\n",
    "    \n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # 2.1  DAILY reset  – short-term tier\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # 2.2  WEEKEND reset – long-term tier\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # 2.3  FORWARD-ONLY\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]),  tf.float32)\n",
    "            val_rmses.append(float(tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))))\n",
    "    \n",
    "        epoch_val = float(np.mean(val_rmses))                    # VAL metric\n",
    "        impr_pct  = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "\n",
    "        current_lr   = current_lr = current_lr_from(opt)\n",
    "        grad_norm    = np.mean(batch_rmses) / current_lr      # crude proxy; optional\n",
    "    \n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        # 3 ▸ LOGGING  – print after validation while bar is still visible\n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} • \"\n",
    "              f\"train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "\n",
    "        \n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        # 4 ▸ CLOSE BAR  – now that epoch summary is printed, clear the line\n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        pbar.close()\n",
    "    \n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        # 5 ▸ EARLY-STOP LOGIC\n",
    "        # ─────────────────────────────────────────────────────────────────────\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse, patience_ctr = epoch_val, 0\n",
    "            model_train.save_weights(ckpt_path)                 # keep champion\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    # ================ END EPOCH LOOP =============================================\n",
    "    \n",
    "    # restore champion weights before returning\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.295175\n",
      "Training sees 64 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGuCAYAAACNwjLpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAQ6wAAEOsBUJTofAAAeJRJREFUeJzt3XdcleX/x/HXYQ9lKENcuBVXao5y4DZzi5mZZZpWNrSyrL4tNX9lw/qWLc36ujVzW5paLswcmSMH7omCKENUNpzfH7eSBCqHgHOA9/PxOA84F/d98zmf7uLTdV33dZnMZrMZEREREbktO2sHICIiIlIUqGgSERERyQUVTSIiIiK5oKJJREREJBdUNImIiIjkgoomERERkVxQ0SQiIiKSCyqaRERERHJBRZOIiIhILqhoEhEREckFFU0i8q9UqVKFdu3a5fn8jRs3YjKZmDFjRr7FJCJSEFQ0iRQjJpMp16+NGzdaO1wRkSLFpA17RYqPOXPmZHkfFhbGe++9R5s2bXjyySez/Kxz5874+/v/69+ZnJyMyWTCyckpT+dnZGSQkpKCo6Mj9vb2/zoeEZGCoqJJpBjbuHEj7du357HHHrvj8Ne1a9dwd3cvnMCKqbS0NNLT03F2di7U35uRkUFycjKurq6F+ntFShoNz4mUQDfmIf311190794db29vSpUqBRh/gN977z3atWtHQEAATk5OVKhQgaFDhxIeHn7La+XUduTIEXr37o2npyelSpWie/fuHD9+PMuxOc1purlt9uzZNGzYEBcXFypUqMAbb7xBenp6tjhWr15NixYtcHV1xc/PjyeeeIKYmBhMJhNDhgy5Y05u/p1ff/01QUFBuLi4UKVKFd555x3S0tKyHD9kyBBMJhPR0dE8+eSTBAQE4OzszNatWwGIi4tj9OjRVK1aFWdnZ/z9/Rk4cCBHjx7N9rtTUlJ48803qVy5Mi4uLtStW5dvvvmGGTNmZBtKHTduHCaTiYMHD/LKK68QGBiIk5MTCxYsAMBsNjNt2jSaN2+Ou7s77u7utGzZkmXLluWYsw4dOuDn54eLiwsVK1akW7du/P7775nHxMbGMmbMGGrWrImrqyve3t40aNCAl1566Y45FSluHKwdgIhYx9mzZ2nXrh19+vRh4sSJREZGAsYf8A8++ICQkBC6d++Op6cnf/31F//73/9Yt24de/fuxdvb+47XP3fuHMHBwfTq1YsPPviAo0eP8vnnn9OrVy/27duHnd2d/59t6tSpnDt3juHDh+Pr68uSJUt47733KF26NK+99lrmcStWrKBv374EBATw2muv4e3tzfLly+natavFefniiy8IDw9nxIgRlClThqVLlzJ27FhOnDiRY29dp06d8PHx4bXXXiMjI4Ny5cpx5coVWrVqxcGDBxk0aBAtW7bk+PHjfPXVV6xevZotW7ZQt27dzGsMGjSIRYsW0blzZ8aMGUN0dDRjx46lUqVKt4xz0KBBODo68txzz+Hu7k7t2rUBGDp0KLNmzaJ3794MGjQIgCVLltC3b1++/vprRowYAUBoaCg9evSgbt26jBkzhrJlyxIZGcmWLVvYs2cPLVu2BODBBx9kw4YNPPnkkzRu3Jjk5GSOHz/OunXrLM6tSJFnFpFia8OGDWbA/Nhjj2VpDwwMNAPmb775Jts5GRkZ5mvXrmVr/+WXX8yA+aOPPsp2rbZt2+Z4/Xnz5mVpnzhxohkwr1mzJluM06dPz9ZWrlw5c0xMTGZ7enq6OSgoyBwQEJDZlpaWZq5cubLZw8PDfP78+SzH9u7dO8fPn5Mbv9PNzc186tSpLNfp2bOnGTBv3rw5s/2xxx4zA+ZBgwZlu9Zbb71lBswffPBBlvaNGzeaAXPHjh0z29auXWsGzA8++KA5IyMjs/3MmTNmd3d3M2DesGFDZvvYsWPNgDk4ONicmpqa5frLli0zA+ZPPvkkW0w9evQwe3h4mOPj481ms9n84osvmgFzZGTkLXMSFxdnNplM5hEjRtzyGJGSRMNzIiVUmTJlePzxx7O1m0wm3NzcAGOoLi4ujkuXLtGoUSO8vLzYtm1brq5fvnx5Bg4cmKWtc+fOABw5ciRX13j88cez9GrZ2dnRsWNHIiIiuHr1KgB//vknZ86cYfDgwQQEBGQ59tVXX83V77nZI488QmBgYJbr3OjVWrx4cbbjX3755WxtixcvxtPTk+effz5Le9u2bWnfvj3r168nNjYWgKVLlwLwyiuvYDKZMo+tVKlSZk9RTl588UUcHLIOFsyePRtXV1cGDBjApUuXsrz69u1LfHx85vChl5cXAAsXLsw29HiDq6srzs7ObN++nRMnTtwyFpGSQkWTSAlVvXr1Wz6ttmzZMlq2bJk5h8XX1xdfX1/i4uKIiYnJ1fWrVauWra1s2bIAREdH59s1bvwxr1OnTrZjg4KCcvV7bnbzsNk/244dO5btZ7Vq1crWduLECWrUqJHjhPAGDRpgNps5efJk5rFgefw5/d6wsDASExOpUKFC5j+zG69hw4YBcOHCBQCee+45mjZtysiRIylTpgz33Xcf7777bmZcAE5OTkyePJmDBw9SvXp16taty/Dhw1myZEmO88pEijvNaRIpoW70Jv3TsmXL6Nu3L02bNuWTTz6hcuXKmU9lPfTQQ2RkZOTq+rdbPsCcy4d2LbnGzb00t2vLixvXyel6t8pjfv3uW8np92ZkZODp6cmiRYtueV69evUAo6dx+/btbNmyhV9//ZXffvuN8ePHM378eObMmcODDz4IwBNPPEGvXr1YtWoVmzdv5pdffuG7776jefPmbNq0CRcXl4L5gCI2SEWTiGQxa9YsXFxc2LRpU5Y/zNeuXcscUrIlN3qjwsLCsv3s4MGDFl8vp3MOHDgAGL1zuY3p6NGjJCcnZ+tt2r9/PyaTiapVq2YeC3Do0CHuvvvuLMfm9Jlup1atWhw6dIjGjRtn9sjdjp2dHW3atKFNmzYAnD59miZNmvD6669nFk0A/v7+DB06lKFDh2I2m3nllVeYNGkSixYt4pFHHrEoRpGiTMNzIpKFg4MDJpMpW4/ShAkTct3LVJjuvvtuKlWqxOzZs4mIiMhsN5vNfPjhhxZfb86cOZw+fTrzfUZGBu+//z4AISEhubpGSEgIly9f5vPPP8/S/ttvv7F+/Xrat2+fOVerT58+AHz44YdZes/Onj3L3LlzLYp98ODBgDE/KqfevBtDcwAXL17M9vPKlSvj6+ubOfSZkJBAQkJClmNMJhNNmjQBcj/MKlJcqKdJRLJ44IEHWLhwIW3btmXIkCGYzWbWrFnDwYMH8fHxsXZ42djb2zN58mT69etHs2bNePLJJ/Hy8mL58uWZk8UtGSoLCgqiRYsWPP3005lLDmzYsIFHHnkks0fmTl555RWWLFnCmDFj2Lt3b5YlBzw9PbMUU126dKFv37788MMPxMbG0rNnT2JiYpgyZQr16tVjx44duY6/X79+PPHEE0ybNo29e/fSp08fypUrx/nz59m5cyc///wzqampADz55JOcOXOG++67j8DAQNLS0lixYgWHDx/mhRdeAIwJ+8HBwfTp04f69evj4+PD8ePHmTJlCh4eHvTt2zfXeRUpDlQ0iUgWDz74IFevXuW///0vr7zyCqVLl6Zz585s3ryZ1q1bWzu8HPXp04cff/yRcePG8d577+Hh4UHv3r158803qVKlikUrZT/33HMkJCQwefJkTp48Sbly5Rg7dixvvvlmrq9RunRpfvvtN9555x2WLl3KggUL8PT0pHfv3owfPz7bJO758+czfvx4Zs+ezaZNm6hevTrvvPMOSUlJ7Nixw6L4v/nmGzp06MDUqVOZNGkSiYmJ+Pv7U79+/SzF2qOPPsqsWbOYPXs2Fy9exM3NjZo1a/LNN99kThqvVKkSw4cPZ+PGjfz0008kJCQQEBBA7969ee2116hcuXKu4xIpDrSNiogUW3/88QfNmzfn/fffv+PyAze2nJk+fXquVhAvDM8++yxfffUVkZGR+bJPoIj8O5rTJCJFXmpqara1hm5sBwNw3333WSOsXPvnvCGAM2fOMGvWLO666y4VTCI2QsNzIlLknT59mvbt2/PQQw9Rs2ZNoqOjWbZsGTt27GDw4ME0atTI2iHe1sSJE9myZQsdO3bEz8+Po0ePMm3aNJKSkvjoo4+sHZ6IXKeiSUSKvLJlyxIcHMyiRYu4cOECZrOZWrVqMWnSpMxJzbasdevWbNmyhc8++4zY2FhKly7Nvffey+uvv26z88hESiLNaRIRERHJBc1pEhEREckFmymajh07xogRI2jUqBEODg7Ur18/V+e9+uqr1KtXj9KlS+Ph4UGzZs34/vvvCzhaERERKWlsZk7TgQMHWLlyJS1atCAjIyPXKw9fu3aNESNGULt2bcxmM4sWLWLgwIFkZGTw8MMP5/r3JyUlsW/fPnx9fbPtHC4iIiLFU1paGhcvXqRBgwZ33EvRZuY0ZWRkYGdndHwNGTKEnTt3sn///jxdq1WrVri7u7N27dpcn3NjPRcREREpeXbs2EGzZs1ue4zNdKncKJjyQ9myZbly5YpF5/j6+gJG0gICAvItFoDExERCQ0MJDg62aGXfkk55s5xyljfKm+WUs7xR3ixX0DmLiIigefPmmXXA7dhM0fRvmM1m0tPTuXr1Kj/++CNr165lzpw5tz0nPj6e+Pj4zPc3Nq/08vLK1e7glkhKSsLHxwcfH587dv3J35Q3yylneaO8WU45yxvlzXIFnbPExESAXE3NsZnhuZtZOjz366+/0rlzZ8D40F988QVPPfXUbc8ZN24c48ePz9b+7bff2uSmpCIiIpL/Ll26xPDhwzl79iwVK1a87bHFoqepRYsW/PHHH1y+fJmff/6Z5557DgcHh8xNJ3MyevRohg8fnvn+RvdccHDwHZNmqaSkpMyuRf2fRe4pb5ZTzvJGebOccpY3ypvlCjpn4eHhuT62WBRNpUuXpmnTpgB07NiR5ORkRo8ezZAhQ7C3t8/xHA8PDzw8PLK1u7q6Ftg4s4uLi8aw80B5s5xyljfKm+WUs7xR3ixXUDmz5Jo2s05Tfrr77ruJj4/PnKckIiIi8m8Vi56mf/rtt9/w8PDQ3CQREbFZZrOZS5cukZSURHp6urXDsVnp6el4e3tz/vz5W44e5cTe3h4XFxd8fHwwmUz5EovNFE0JCQmsWrUKMHYsj4+PZ9GiRQC0bdsWX19fhg0bxsyZM0lLSwPgr7/+4tVXX6V///5UqVIl8+m57777jvfff1+LVIqIiE0ym82cO3eOK1eu4OTkZFExUNLY2dlRrlw5i5cmSklJ4erVqyQnJ1OhQoV8KZxspqqIioqif//+WdpuvN+wYQPt2rUjPT09SzXu7++Pl5cX77zzDpGRkXh6elKnTh2WLVtG7969CzV+ERGR3IqNjeXKlSv4+fnl+zI3xU1GRgbx8fF4eHhYXDhFR0cTFRXFpUuXcrUO053YTNFUpUoV7rT6wYwZM5gxY0bme39/f+bPn1/AkYmIiOSvlJQUnJycVDAVsLJlyxIXF0dSUlK+XK9YTgQXERGxZRkZGRqSKyT29vb5NmdMRZOIiIhILqhoEhEREckFFU2FIOJyEstO2ZGcpkdKRUREiioVTQUsNT2D0Yv2syHCjoHf/cmpS9esHZKIiEi+WrZsGV999VW+XrNdu3b06NEjX6/5b6loKmAmoFmgNwBhkVfp8flvrPwrwrpBiYiI5KOCKJq++uorPv7443y95r+loqmAOdjbMbpTdZ6sk46nqwNXk9N4dt4u3l6+X8N1IiJSYpjNZpKTk3N9fN26daldu3YBRmQ5FU2FpJ63maVPNadJZS8AZm09zQNfb+VMdIJ1AxMREfkXhgwZwsyZMzlw4AAmkwmTycSQIUMYMmQI9evXZ9WqVdx11104OzuzYsUKrl27xnPPPUft2rVxc3OjSpUqjBgxgsuXL2e57j+H58aPH0+pUqX466+/aN26NW5ubtSvX581a9YU2me1mcUtS4IATxcWPHUvk9YcZmroCfadu0z3yZv58IGG3N8gwNrhiYiIFaWkZXAuLtHaYVDByxUnh9z3qbz11ltcvHiRQ4cOMXfuXAB8fX2ZMGEC58+f5/nnn+fNN9+kUqVKVKpUiYSEBNLT03n33Xfx9fXl7NmzvPvuu/Tt25f169ff9nelpqbyyCOPMGrUKN566y0mTpxIv379OH36dKEsFKqiqZA52tvxn25BNK9ahtE/7OVyYipPz93FkJZV+E+3Ojg7aLEzEZGS6FxcIu0nbbR2GGx4uR1VfdxzfXz16tXx9fXl9OnT3HPPPVl+Fhsby+rVq2nevHmW9q+//jrz+7S0NKpWrUrr1q05cuQItWrVuuXvSklJ4f3336dbt26Zv7tmzZr8/PPPPPLII7mOOa80PGclHYP8WfV8GxpfH66b8fspHpyylbMxGq4TEZHiwcfHJ1vBBDB79mwaN25MqVKlcHR0pHXr1gAcOXLkttezs7OjU6dOme9r1KiBk5MT4eHh+Rv4LainyYoqeLmy4Ml7+XD1Ib797SR7w43huo/638V99cpZOzwRESlEFbxc2fByO2uHQQUv13y7lp+fX7a2pUuXMnjwYJ588kneffddypYtS0REBH379r3jHnGurq44OTllaXN0dMy3veXuREWTlTk52PFmj7o0r1qGlxfuJT4pjadm/8mw1lV5tWsdi8aVRUSk6HJysLNoWKwoMJlM2doWLlxIo0aNmDp1ambbpk2bCjOsPNNfZBvRpV45Vo5qw12VvAD47reTPDh1K+GxGq4TERHb5uTklOvensTExGy9RTcmkNs6FU02pFIZNxY+dS+Pt6oKwJ6zcXT7bDO/HLxg5chERERuLSgoiFOnTjF//nx27tzJqVOnbnls586d2bFjB++88w6//vorL730EuvWrSu8YP8FFU02xsnBjrd71mXKI3dT2sWB+KQ0npi1k3dXHiQ1PcPa4YmIiGQzbNgw+vfvz8iRI2nWrBnjxo275bFPPfUUL730El988QUhISGcOXOGefPmFV6w/4LmNNmorvXLUa+8B8/O28Vf4ZeZtvkkO0/H8sXDTfJ1kp6IiMi/5eHhwfz583N1rL29PZMmTWLSpElZ2s1mc5b3GzduBCAjw+gwGDt2LOPHj892vatXr+Yh4rxRT5MNq1TGjYUj7mVIyyoA7D4TR/fJm1l/SMN1IiIihU1Fk41zdrBnXK96fD2oCaWdHYhLSOXxGTuZ+HOYhutEREQKkYqmIuL+BgH8NKo19St4ADB10wke+mYb521gyX0REZGSQEVTERJY1p3FT7dk8L2BAPx5Opbukzez4XCUlSMTEREp/lQ0FTHODva807s+XzzcmFLODsQmpDJ0+h98sPoQaRquExERKTAqmoqoHg3L8+PI1tQNMIbrvt54nIHTthF5uXCWkhcRESlpVDQVYVV93FnyTEseuacyAH+ciqXb5M1sOnLRypGJiIgUPyqaijgXR3v+r08DJg9sjLuTPTHXUnjsfzv4aI2G60RERPKTiqZiotddxnBdnXKlAfhyw3EGfbudC/EarhMREckPKpqKkWq+pVj2bCsGNjeG67afjKHbZ5vZfFTDdSIiIv+WiqZixsXRnokhDfh0QCPcnOyJvpbC4P/t4JO1h0nPMN/5AiIiIlZw6tQpTCYTixYtsnYot2QzRdOxY8cYMWIEjRo1wsHBgfr169/xnPj4eMaNG0eLFi3w8vLC19eXrl27smvXrkKI2Lb1aVyBFc+1prZ/acxmmLz+GIO+3UaUhutERETyxGaKpgMHDrBy5Upq1KhB3bp1c3XOmTNnmDp1Kp06dWLBggVMnz6d9PR0WrZsqcIJqOFnDNcNaFoJgG0nYug2+Te2HLtk5chERESKHgdrB3BDz5496d27NwBDhgxh586ddzynatWqHD9+HDc3t8y2Tp06Ua1aNT7//HOmT59eYPEWFa5O9nzwQENaVCvDG0v3c+lqMo98t53nO9ZkZIea2NuZrB2iiIgApKXA5bPWjgI8K4GDU64PnzFjBsOHD+fcuXP4+/tntsfExFCuXDk+/fRTGjduzMSJE9m5cyeXL1+mZs2avPTSSzz66KMF8QkKjM0UTXZ2lnd6ubu7Z2tzcXEhKCiI8+fP50dYxUZIk4o0rOjJM3N3ceTCVT799Sh/nIrh0wGN8S3tbO3wRETk8ln4vIm1o4CRu6Bs9VwfHhISwtNPP83ChQt57rnnMtsXL16M2Wymf//+rFu3jlatWjFixAhcXFzYsmULw4YNw2w2M3jw4IL4FAXCZoqm/HLt2jV27959x38I8fHxxMfHZ76PiIgAIDExkcTE/N0ENykpKctXa6lQ2oH5j9/N//18mKV7ItlyLJr7PwtlUkg9WlT1tmpsObGVvBUlylneKG+WU87y5ka+0tPTsbOzIyPjpvX0MjJsYs5MRkYGZOR+nb9SpUpx//33M3/+fJ555pnM9vnz59OhQwfKli3Lgw8+mNluNptp3bo1Z8+eZcqUKTzyyCN//97rX2/Oy83teWE2m8nIyLjl33ZL/uYXu6LpzTffJCEhIUu1m5NPPvmE8ePHZ2sPDQ3Fx8enQGILDQ0tkOtaqp0ruFY3sfCkHZeupjB01i7ur5RB5wpmbHG0zlbyVpQoZ3mjvFlOOcubyMhIypUrl+V/3rHzxO6xjVaL6YYMO0+4Oa5c6N27N0OHDuXAgQNUqlSJCxcusGnTJr788kvi4+OJi4tj4sSJrFq1ioiICNLT0wEoU6ZMZg6uXr0KGEVMfA6//8bPLZWamkpkZCT79+/P8eeXLuV+nm+xKprmzZvHp59+ypdffkmNGjVue+zo0aMZPnx45vuIiAiaN29OcHAwFStWzNe4kpKSCA0NJTg4GBcXl3y9dl51AR6KusqLC/dz/FICq87aE+fozYch9Sjrnvux7IJki3mzdcpZ3ihvllPO8uZG3sqVK4ejoyMeHh5ZD/AumP9pL2j9+/dn1KhRrFq1ijFjxjBjxgycnJx46KGH8PDwYPDgwWzdupW33nqLunXr4uHhwZQpU/jhhx8yc1CqVCkAXF1ds+QlIyODq1evUqpUqTxN5YmNjaVChQo0a9Ysx5+Hh4fn+lrFpmj65ZdfGDp0KGPGjMnSPXgrHh4e2W9WjH9Yrq6uBREiLi4uBXbtvGgY6MqPo9rw5rL9LNl1jt9PxBIy9Q8mD2zMPdXKWju8TLaWt6JAOcsb5c1yylne2NvbYzKZ8lQE2CI3Nzf69OnDggULePXVV1mwYAHdu3fHy8uLpKQkVq1axccff8yoUaMyz/n666+Bv+c03/w1p7zcqv1OTCYT9vb2t7xPLbl/i8U/rR07dhASEkL//v354IMPrB1OkeLm5MDH/e/iw34NcXawI+pKMg9P28aXG46RocUwRUQklwYOHMju3btZs2YN27Zt4+GHHwYgOTmZ9PR0nJz+HsW4cuUKK1assFaoeVbke5rCwsLo1q0brVq1Yvr06ZhMNjgpx8aZTCYebFaJhpWMp+tOXLzGR2sOs/1kDP998C7KltLTdSIicnudOnXC19eXxx9/HA8PD7p16waAp6cnzZo14/3338fX1xcHBwfef/99PD09iYqKsnLUlrGZnqaEhAQWLVrEokWLOH36NPHx8ZnvL1409k4bNmwYDg5/13lRUVHcd999ODo6MmbMGP7880+2bdvGtm3b2L17t7U+SpFVp5wHPz7Xmj6NygMQeuQi3SZvZsfJGCtHJiIits7BwYH+/ftz/vx5+vbtm2Wu27x586hevTqPPfYYo0aN4oEHHihSSw3cYDM9TVFRUfTv3z9L2433GzZsoF27dqSnp2fOuAc4ePAgZ88aC4F16tQpy7mBgYGcOnWqYIMuhtydHfjvgEbcU60sY1cc4EJ8MgOnbeOlLrUYEVwdO1t8vE5ERGzCl19+yZdffpmtvUaNGqxfvz5b+7hx4zK/r1KlCmazbU8LsZmephvJyunVrl07wFh19OaEtmvX7pbnqGDKO5PJxEPNK7Ps2VZU83EnPcPMh6sPM2zmH8RcS7F2eCIiIlZhM0WT2J6gAA9WjGxNz7uM4boNhy/SffJmdp7ScJ2IiJQ8Kprktko5OzD5oUa827c+Tg52RFxOYsA325i66bierhMRkRJFRZPckclkYlCLQJY+05IqZd1IzzAz8edDDJ+1k1gN14mISAmhoklyrV55T34c2ZruDQMAWH8oiu6TN7PrTKyVIxMRKVrs7OyyPNgkBSc9PR17e/t8uZaKJrFIaRdHvhjYmAl96uNkb8f5y0k8OGUr00JP2PxTDyIitsLJyYmUlBSio6OtHUqxFh0dTUpKSr5t9WMzSw5I0WEymXj0nkAaV/Limbm7OBOTwLurwth+MoZJ/Rvi5WYbe9eJiNgqb29vzGYzUVFRxMXF5VtPSHFkNptJTU0lNjbWogWs09PTSUlJoXTp0vj45M+efuppkjyrX8GTn0a15v765QD4NewC3Sf/xm4N14mI3JbJZKJChQr4+Phk2V5EssvIyCAyMpKMjAyLznNycsLHx4cKFSrk224h6mmSf8XDxZGvBjVh1tbTvLsyjHNxiTw4dSuv3R/E462qaFsbEZFbMJlM+Pr6WjsMm5eYmMj+/ftp1qyZ1TeHVk+T/Gsmk4nHWlZh0dP3UqmMK6npZib8dJCnZv/J5YRUa4cnIiKSL1Q0Sb5pWNGLn0a24b56/gCsPXiB7p9vZu/ZOOsGJiIikg9UNEm+8nR1ZMojd/N2j7o42psIj03kgSm/M2PLST1dJyIiRZqKJsl3JpOJx1tXZeGIllTwMobrxv14kKfn7OJyoobrRESkaFLRJAWmUSUvVo1qQ+e6xnDd6gOR9Pz8N/aFX7ZyZCIiIpZT0SQFytPNkW8evZs3uwfhYGfiTEwC/b7+nVlbT2m4TkREihQVTVLgTCYTw9tU44cR91LBy5WU9AzeXn6A5+btJj5Jw3UiIlI0qGiSQtOksjcrR7WmU5AfACv3RdDz89/Yf07DdSIiYvtUNEmh8nJzYtrgprzerQ72diZORycQ8tXvzN52WsN1IiJi01Q0SaEzmUw8GVydH566h/KeLqSkZ/DWsv2MnL+bKxquExERG6WiSazm7sAyrBzVhva1jW0Efvorgl5fbOHg+XgrRyYiIpKdiiaxKm93J757rBmv3W8M1528dI0+X21h3vYzGq4TERGboqJJrM7OzsSIttX5/sl7KOfhQkpaBq8v3ccrSw+SlG7t6ERERAwqmsRmNKtShpWjWtO21vXhun0X+Pgvew5fuGrlyERERFQ0iY0pW8qZ6UOa8UrX2tibTEQlmRjw7U6+36HhOhERsS4VTWJz7OxMPNOuBjMea4yno5nktAxeW7KP0T/s5VpymrXDExGREkpFk9ispoFejLkrnVbVywCwdPc5en3xG4cjr1g5MhERKYlUNIlNK+0I3wy6i5e71MLOBMcvXqP3l7/xw86z1g5NRERKGBVNYvPsTCae61CTucPvwbe0M0mpGbyy6C9e+mEvCSkarhMRkcKhokmKjHurl2XVqDa0ruEDwOJd4fT+YgtHL2i4TkRECp7NFE3Hjh1jxIgRNGrUCAcHB+rXr5+r8xYsWEC/fv2oUKECJpOJSZMmFXCkYk2+pZ2Z+XhzXuxUC5MJjkZdpdcXW1j8Z7i1QxMRkWLOZoqmAwcOsHLlSmrUqEHdunVzfd6iRYs4ceIEPXv2LMDoxJbY25l4vlNN5g5rgU8pZxJT03lp4V5eWbSXxBSthikiIgXDZoqmnj17cvbsWRYtWkSTJk1yfd6CBQvYvXs3U6ZMKcDoxBa1rOHDqudbc2+1sgD8sDOcPl9u4ViUFsMUEZH8ZzNFk51d3kLJ63lSPPiVdmHO8BaM6lgTkwkOX7hCry9+Y+luDdeJiEj+crB2ANYSHx9PfHx85vuIiAgAEhMTSUxMzNfflZSUlOWr5I4leXu6dSXuKu/OK0sOEH0tlRcX7GXLkSjeuL8WLo72BR2qzdC9ljfKm+WUs7xR3ixX0Dmz5G9+iS2aPvnkE8aPH5+tPTQ0FB8fnwL5naGhoQVy3eLOkrw9XwdmHbXjWLwdi3ZHsOXweYbWSsfftQADtEG61/JGebOccpY3ypvlCipnly5dyvWxJbZoGj16NMOHD898HxERQfPmzQkODqZixYr5+ruSkpIIDQ0lODgYFxeXfL12cZbXvPXNyODLjaeYuvkUEQkmPj3ozDs9atO9QbkCjNY26F7LG+XNcspZ3ihvlivonIWH5346R4ktmjw8PPDw8MjW7urqiqtrwXRLuLi4FNi1i7O85O217vVoWdOPFxbsIeZaCi8vOcif4VcZ27NuiRiu072WN8qb5ZSzvFHeLFdQObPkmppFLcVWcC1fVo1qQ/Mqxt5183ecoe9Xv3Py0jUrRyYiIkWRiiYp1sp5ujDviRY82746AGER8fSYvJkf9563cmQiIlLU2MzwXEJCAqtWrQLg9OnTxMfHs2jRIgDatm2Lr68vw4YNY+bMmaSl/b3f2MGDBzl48GDm+3379rFo0SLc3d25//77C/dDiE1ysLdjzH11aFalDC8u2ENsQioj5+9m+8lo3uxeMobrRETk37OZoikqKor+/ftnabvxfsOGDbRr14709HTS07Ou+PzDDz9keQpu1qxZzJo1i8DAQE6dOlXgcUvR0a62H6ueb8PIebvZeTqWOdvOsPtMHF8+3IQqPu7WDk9ERGyczQzPValSBbPZnOOrXbt2AMyYMQOz2ZzlvHHjxuV4jgomyUmApyvzn7yHEW2N4boD5+Pp8flvrPwrwsqRiYiIrbOZokmksDja2/Ha/XX435CmeLk5cjU5jWfn7eLt5ftJTtPedSIikjMVTVJidajjz6pRbWhS2QuAWVtP88DXWzkTnWDdwERExCapaJISrbyXKwueupengqsBsO/cZbp/vpnV+zVcJyIiWalokhLP0d6O/3QL4tvBTfF0deRKUhoj5uxi3IoDGq4TEZFMKppErutU15+Vo1rTqJIXADN+P8WDU7ZyNkbDdSIioqJJJIuK3m788NS9DG9dFYC94ZfpPnkzaw5EWjkyERGxNhVNIv/g5GDHmz3q8s2jd+Ph4kB8UhpPzf6TCT8dJCUtw9rhiYiIlahoErmFLvXKsXJUG+66Plz33W8neXDqVsJjNVwnIlISqWgSuY1KZdxY+NS9DG1VBYA9Z+PoPvk3fj14wbqBiYhIoVPRJHIHTg52jO1ZjymP3E1pFwcuJ6YyfNZO3l15kNR0DdeJiJQUKppEcqlr/XKsHNmGBhU8AZi22RiuOxeXaOXIRESkMKhoErFA5bJuLHr6Xoa0rALA7jNxdJ+8mfWHNFwnIlLcqWgSsZCzgz3jetXjq0FNKO3sQFxCKo/P2MnEn8M0XCciUoypaBLJo24NAvhpVGvqV/AAYOqmEwz8ZhvnNVwnIlIsqWgS+RcCy7qzaERLHr0nEICdp2PpPnkzGw5HWTkyERHJbyqaRP4lF0d7JvSpz+cDG1PK2YHYhFSGTv+DD1YfIk3DdSIixYaKJpF80vOu8vw4sjV1A4zhuq83HufhaduJvJxk5chERCQ/qGgSyUdVfdxZ8kxLBrWoDMCOUzF0m7yZTUcuWjkyERH5t1Q0ieQzF0d73u3bgM8eaoS7kz0x11IYMn0Hk9Yc1nCdiEgRpqJJpID0blSBFSNbU6dcacxm+GLDMQZ9u50L8RquExEpilQ0iRSg6r6lWPZsKwY2rwTA9pMxdPtsM5uParhORKSoUdEkUsBcHO2ZGNKQTwc0ws3JnuhrKQz+3w4+WXuY9AyztcMTEZFcUtEkUkj6NK7AiudaU9vfGK6bvP4Yj3y7nSgN14mIFAkqmkQKUQ0/Y7huQFNjuG7riWi6Tf6NLccuWTkyERG5ExVNIoXM1cmeDx5oyMf978LV0Z5LV5N55LvtfPrrEQ3XiYjYMBVNIlbS7+6KrHiuFTX9SmE2w6e/HmXw/7Zz8UqytUMTEZEcqGgSsaKa/qVZ/lwrHri7IgBbjkXTbfJmth6PtnJkIiLyTyqaRKzMzcmBSf3v4qMHGuLiaMfFK8kM+nYbn687quE6EREbYjNF07FjxxgxYgSNGjXCwcGB+vXr5/rcmTNnUqdOHVxcXKhfvz4LFy4swEhFCkb/ppVY8VxraviVIsMMH/9yhCHTd3DpqobrRERsgc0UTQcOHGDlypXUqFGDunXr5vq8RYsWMWTIEPr27cvPP/9Mx44dGTBgAGvXri3AaC2UlkTVi2vBrC005PZq+Zdm+bOtCGlcAYDNRy/R7bPNbDuh4ToREWuzmaKpZ8+enD17lkWLFtGkSZNcn/fWW2/Rv39/Jk6cSPv27fnss8/o3Lkzb7/9dgFGa4HUJJwWD6Fh+Bwc17wCGSqc5PbcnR34+MG7+KBfA5wd7Ii6kszD07bx5YZjZGi4TkTEamymaLKzszyUkydPcujQIQYOHJil/eGHH2bHjh1cumQDa9/YO2Iu5QuAw945sHK0Cie5I5PJxIBmlVn+XCuq+bqTYYaP1hxmyIw/iNZwnYiIVThYO4B/IywsDICgoKAs7XXr1sVsNnPo0CFat26d47nx8fHEx8dnvo+IiAAgMTGRxMTEfI0zqf37XD13jkqxv8Of00nLyCC18/tgMuXr7ylukpKSsnwtiQI9Hflh+N2M/+kwP+67QOiRi3T7bDMfP1CPuyt7ZTteOcsb5c1yylneKG+WK+icWfI3v0gXTbGxsQB4eXllaff29gYgJibmlud+8sknjB8/Plt7aGgoPj4++RfkDYFPAmYqxW7FYfdMzpw9x76Kj6pwyoXQ0FBrh2B1Hd3BrZqJxSftuHAlmcHT/6Rb5Qw6ljdjl8MtpJzljfJmOeUsb5Q3yxVUziwZlSrSRdMNpn8UHmazOcf2m40ePZrhw4dnvo+IiKB58+YEBwdTsWLFfI0vKSmJ0NBQSj0ym7Rfx+AQtpRql36lcmAgqR3/T4XTLdzIW3BwMC4uLtYOx+ruAx6KvMILC/dzOiaRn87YE+9clvf7BOHt5gQoZ3mlvFlOOcsb5c1yBZ2z8PDwXB9bpIumGz1KsbGx+Pv7Z7bHxcVl+XlOPDw88PDwyNbu6uqKq6tr/gZ6nYtbKRwe+BaW2sH+xTj8+R0ODk7QdaIKp9twcXEpsH8mRU3jqq6sfD6Y/yzZx497zxN6NJp+3+zk84GNaVqlTOZxylneKG+WU87yRnmzXEHlzJJr2sxE8Ly4MZfpxtymGw4ePIjJZKJOnTrWCOv27B2g7zdQr6/xfvvXsOZ1MOupKMmdUs4OTH6oEe/2rY+Tgx0Rl5MY8M02pm46TobuIxGRAlOki6aqVatSp04dFixYkKV9/vz5NG/evGDmJuUHewcI+Rbq9jHeb/sK1r6pwklyzWQyMahFIEuebkmVsm6kZ5iZ+PMhnv3+L66lWjs6EZHiyWaG5xISEli1ahUAp0+fJj4+nkWLFgHQtm1bfH19GTZsGDNnziQtLS3zvHfeeYcBAwZQvXp1OnfuzPLly1m7di2rV6+2yufINXsH6PetseBl2ArY+oUxRNd5gobqJNfqV/Dkx5GteW3xPlbui2DjkWj2ONmTWu4cfZoE4unmaO0QRUSKDZspmqKioujfv3+WthvvN2zYQLt27UhPTyc9PT3bMQkJCbz33ntMmjSJGjVqsGDBArp06VJoseeZvSM88D9YOAQO/QS/fw4mO+g0XoWT5FppF0e+eLgx92wrwzs/HSQuBcb+dJh3Vx+lc5A/IU0qEFzLF0f7It2xLCJidTZTNFWpUiXzqbdbmTFjBjNmzMjW/thjj/HYY48VUGQFzN4RHpgOi4YahdOWzwATdBqnwklyzWQy8ei9Vajr78b4RdsJi7MnJS2DlfsiWLkvgrLuTvS8qzz9mlSkfgWP2z5ZKiIiOdP/etoCByejcKrdzXi/5VNY947mOInF6gaUZljtDDa91JoJferT+PoCmNHXUpjx+yl6fvEbXf4bytcbjxNxOX8XcRURKe5UNNkKByfoPxNq3W+8/+0TWP9/KpwkT7zdHHn0nkCWPtOK9S+1ZVSHGlTwMh6rPRp1lQ9WH6Ll++sZ9O02Fv8ZzrXktDtcUUREVDTZEgcneHAm1LzPeL95EmycaN2YpMir5luK0V1qs/mV9ix48h4GNK1EKWcHzGbYciyalxbupen//croBXv47egl0rUpsIhIjmxmTpNc5+AMA2bDgkfg6FrY9IExObzda9aOTIo4OzsTLaqVpUW1sozrVY9fwi6wZFc4m49eIjE1nSW7z7Fk9zn8PZzp07gC/ZpUpJZ/aWuHLSJiM1Q02SIHZ3hwNiwYBMd+vd7bZIJ2r1o7MikmXJ3s6XVXeXrdVZ6oK0ms2HOeJbvOcTAingvxyUzddIKpm05Qv4IHIY0r0qtReXxKOVs7bBERq1LRZKscXWDAXPh+IBxfDxvfM3qc2o6xdmRSzPiVdmF4m2oMb1ONsIh4lu4+x7Ld54i6ksz+c/HsP3eQd1eF0a6WL32bVKBTkD8ujvbWDltEpNCpaLJlji7w0DyYPxBObIAN1zf3DX7Z2pFJMRUU4EFQgAevdq3DlmOXWLIrnNUHIklKzWDdoSjWHYqitIsDPRoGENKkIk0DvbV8gYiUGCqabJ2jKwycD/MfghMbYf0Eo8epzWhrRybFmL2dieBavgTX8uVqcho/74tgya5zbD0RzZWkNObvOMv8HWepXMaNvo0rENKkAoFl3a0dtohIgVLRVBQ4usJD82H+ADgZCuvGG4VT6xesHZmUAKWcHejftBL9m1YiPDaB5XvOs3hXOCcuXuNMTAKfrTvKZ+uOcnegNyFNKtCjQXlt3yIixZKKpqLCyQ0GLoB5D8KpzfDrWKNwajXK2pFJCVLR241n29fgmXbV+Sv8Mkt2hbNi73liE1L583Qsf56OZfyKg3Sq60dI44q0ra3tW0Sk+FDRVJQ4ucHDC2Dug3D6N/jlLaNwavmctSOTEsZkMnFXJS/uquTFG93rsvFwFEt3n2NdWBQp6Rms2hfJqn2RlHF3otdd5QlpUoEGFTw1/0lEijQVTUWNkzsM+gHm9ofTW2DtG0bhdO8z1o5MSignBzu61CtHl3rliEtI4ae/IliyK5xdZ+KIub59y4zfT1HDrxQhTSrQp1EFyl9fnVxEpChRv3lR5OQOD/8Ale813q/5D2ybYt2YRAAvNyceuSeQJc+0YsPL7RjVoQYVvY0C6VjUVT5cfZhWH2j7FhEpmlQ0FVXOpWDQQqh0j/F+9auw/RvrxiRyk6o+7ozuUpvQMcb2LQ81q0TpHLZveXHBHjYfvajtW0TE5ml4rihzLg2PLII5/eDsdvh5jLGOU/MnrB2ZSKZs27ccNLZvCb2+fcvS3edYetP2LSGNK1K7nLZvERHbo6KpqHMuDYMWwZwQCP8DVr1sFE7Nhls7MpFsXBzt6XlXeXretH3L0t3nOHA+6/Yt9cp7ENKkIr3uKo9vaW3fIiK2QUVTceDiAY8shtkhcG4nrHzJmBze9HFrRyZySzdv33IoMp6lu4wep6gryRw4H8+B8wd5b1UYbWv5EqLtW0TEBqhoKi5cPOHRJTC7L5z7E3560Sic7h5i7chE7qhOOQ/+082DV65v37J09zlW748kMTWd9YeiWH99+5buDYztW5pV0fYtIlL4LJoIfv78edLS7vy0y5UrVwgNDc1zUJJHLp7wyBIo39h4/+PzsGuWdWMSscCN7Vv+O6ARf7zZiUn976Jl9bKYTHAlKY3v/zjLg1O3EvzRBj755QinLl2zdsgiUoJYVDRVqlSJXbt2Zb7PyMigWrVqHDhwIMtxBw8epH379vkToVjG1QseXQoBjYz3K0bBrtnWjEgkT0o5O/DA3RWZ98Q9/PZqB8bcV5vqvsb+dmdjEpm87ijtJm0k5KstzNl2mriEFCtHLCLFnUXDc2azOdv7U6dOkZycnK9Byb/k6g2Dl8Gs3hCxF1aMNIbqGg+ydmQieVLByzVz+5Z95y6zZNc5lu85R2xCKrvOxLHrTBzv/HiQjkF+hDSpSNtavjg5aEUVEclfmtNUXLl6w6PLjMIp8i9Y/qxRODUaaO3IRPLMZDLRsKIXDSt68Xq3IDYduciSXeGZ27f8vD+Sn/f/vX1L38YVaFhR27eISP5Q0VScuZWBwcthVi+I3AfLnjYKp7sGWDsykX/NycGOznX96VzXn8sJqfy07zxLdp3jz9OxWbZvqe7rTkiTivRtrO1bROTfUf91cedWBgavAP8GgBmWjYC/frB2VCL5ytPNkUEtAln8dEtj+5aONalUxiiQjl+8xkdrjO1bHp62jUV/hnNV27eISB5Y3NP08ccf4+/vD/w9x+mjjz7C19c385gLFy7kU3iSL270OM3sCVEHYOlTRo9TgwesHZlIvqvq487ozrV4sVNNdp6OZcmucH7aG8GV5DR+Px7N78ejeXPZPrrWK0dIk4q0quFj7ZBFpIiwqGiqXLkyO3bsyNIWGBjItm3bcjxWbIh7WXhsxfXC6SAsecJYObx+P2tHJlIgTCYTzaqUoVmVMoztWY9fwy6wZNc5Nh25SFJqBsv2nGfZnvP4ezjTvb4/vgnWjlhEbJ1FRdOpU6cKKAwpFO4+xlDdzJ5wMQwWPwGYoH6ItSMTKVAujvb0aFieHg3Lc/FKMiv2nmfp7nD2nzO2b/nf72cAB1Zc2EG/uyvRu1EFbd8iItloTlNJU8oXHvsRfOuAOR0WD4cDy6wdlUih8S3tzLDWVflpZBvWvBDMU22r4VfaCYCwyKv838ow7pm4jqHTd/Dj3vMkpaZbOWIRsRUWFU2pqanEx8dna4+MjOTll1+me/fuDB8+nJ07d1ocyJEjR+jatSvu7u74+fnx/PPPk5iYeMfzUlJSePXVVylfvjyurq40b96cdevWWfz7S5QbhZNPbaNwWvQ4HFxu7ahECl3tcqX5z/1BrH+hFU8HpdOzoT+ujvakZ5jZcPgiI+fvptn//cqri/5i+4loMjLMd76oiBRbFg3PjR49mrVr13L48OHMtujoaJo0aUJkZCRlypTh8uXLzJ07l61bt9KoUaNcXTcuLo4OHToQGBjI4sWLiYqKYvTo0URHRzNnzpzbnvvCCy8wa9Ys3n33XerUqcP06dPp1q0bW7dupUmTJpZ8vJKllJ9ROM3sAZeOGIVT/xkQ1NPakYkUOns7E3W8zIzqUo+Mfo6s3h/Jkt3h/H48mivJaSzYeZYFO89S0duVkMYV6NukIlV93K0dtogUMot6mjZv3syjjz6ape3jjz8mMjKSadOmcenSJc6dO0fNmjWZOHFirq87depUYmNjWb58OV27dmXw4MFMnjyZuXPnEhYWdsvzzp07xzfffMPEiRN5/vnnue+++5g/fz61a9dm/Pjxlny0kqm0v1E4la0BGWmwcAiE/WTtqESsyt3ZgX53V2Tu8HvY8moHXulamxp+pQAIj01k8vpjtNf2LSIlkkVF05kzZ7L1Hi1fvpzatWszbNgwAPz8/HjppZeyPWV3O6tWraJTp074+Pz96G+/fv1wdnZm1apVtzzvr7/+Ij09nfvuuy+zzWQy0aVLF9asWUNKiv5jdkely8FjP0GZ6tcLp8fg0EprRyViE8p7ufJMuxr88mIwK55rxZCWVSjjbsx/2nUmjjeX7af5u+sYMftP1h6IJCUtw8oRi0hBsmh4LjU1FTc3t8z3cXFxHDp0iBEjRmQ5rlq1ahat1RQWFsbjjz+epc3Z2Znq1avftqcpKSkJACcnp2znJicnc/LkSWrXrp3jufHx8VnmZ0VERACQmJiYq7lUlrgR542vNsfRCx5ahPP8EOxiT2L+4TFS+n5HRo0uVg3L5vNmg5SzvMlN3mqWdebVztUY3aEKvx2LYflfEaw/fImU9AxWH4hk9YFIvFwd6V7fn953laN++dLFevsW3Wt5o7xZrqBzZsnffIuKpurVq7N161Y6dOgAwJo1awDo2LFjluNiYmLw9vbO9XVjY2Px8vLK1u7t7U1MTMwtz6tVqxYAO3bsoEqVKpntN9aNut25n3zySY5DeKGhoVl6vPJTaGhogVw3v7iUf55W196jVEoUjkuGsqPqKC54NrZ2WDafN1uknOWNJXnr5gntmsDuaBN/XLTj5BUTcYmpzP0jnLl/hOPvaqapTwZNfc2UKcarF+heyxvlzXIFlbNLly7l+liLiqZhw4bx2muvAVCuXDkmTJiAv78/999/f5bjNmzYQJ06dSy5dI7/R2Y2m2/7f2r16tWjXbt2vPrqq1SsWJHatWszffp0Nm3aBICd3a1HH0ePHs3w4cMz30dERNC8eXOCg4OpWLGiRbHfSVJSEqGhoQQHB+Pi4pKv185vpvg2ZMwPwS7uNC1Of0lK3/+RUb2TVWIpSnmzFcpZ3vybvPW5/vV0TAI//hXJ8r2RhMclcSHRxMqz9qw6C82reNH7rgC6BPni7lw8tvzUvZY3ypvlCjpn4eHhuT7Won97n3nmGQ4cOMA777xDamoqlStXZv78+bi6/r0JZlxcHLNmzeI///lPrq/r7e1NbGxstva4uDiCgoJue+6MGTPo378/rVq1AowVyt9++23Gjh1LuXLlbnmeh4cHHh4e2dpdXV2zfJ785OLiUmDXzjeuNWDISpjRHVPcaZyXPg4PzYea1imcoIjkzcYoZ3nzb/JWp4IrdSqU5eWudf/evuWvCK4kpbH9VBzbT8XxzqrDdK1Xjr5NKtK6hg/2dkV/+E73Wt4ob5YrqJxZck2LJoLb29szZcoU4uLiiIqK4tSpU7Rt2zbLMaVKleLo0aO88MILub5uUFBQtrlLycnJHD9+/I5FU2BgIDt27ODkyZMcOHCA48eP4+rqSkBAAIGBgbmOQW7iVQmG/ARelSE9Bb5/GI79au2oRIqEG9u3TAxpyB9vdOLLh5vQsY4f9namzO1bHvvfDu6duI73VoVxKDL72nciYpvytCK4q6vrLef9ODg4ULZsWRwdHXN9vW7durFu3Tqio6Mz25YuXUpycjLdunXL1TWqVKlC3bp1SUlJ4bvvvssy9CZ54FXZeKrOszKkJ8P8h+GYFg0VsYSLoz3dGwbw3ZBmbH+9I2/3qEv9CkYPd9SVZL4JPUHXTzfT7bPNfLv5BFFXNDlYxJZZNDy3ZMkSiy4eEpK7Pc2eeuopPv/8c3r37s1bb72VubjloEGDsvQ0DRs2jJkzZ5KWlpbZ9sUXX+Dp6UmlSpU4deoUn3zyCS4uLrz66qsWxSo58A6EIT/CjB5w+azR4zTwe6je3tqRiRQ5PqWcebx1VR5vXZUjF66wZNc5lu0+R2R8Egcj4jm4Mp6JPx+iTU0fQppUpEtdf1wc7a0dtojcxKKi6YEHHsicmG023347AZPJRHp67vZs8vLyYv369YwcOZKQkBDc3NwYOHAgH3zwQZbj0tPTs10zOTmZcePGER4eTtmyZQkJCWHChAm4u2u13nzhXcVYAHNGD4gPh/kPwcMLoFo7a0cmUmTV8i/Na/fXYcx9tdl6PJolu8L5eX8kianpbDx8kY2HL1La2YFuDQLo26QCzauUwa4YzH8SKeosKprs7Oxwc3Ojb9++PPzwwxY/IXc7tWrVylzC4FZmzJjBjBkzsrS99NJLvPTSS/kWh+SgTNW/e5ziz8G8h2DQD1A12NqRiRRp9nYmWtf0oXVNHyb0SWPNgUiW7DrHluOXsmzfUsHLlZAmFejbuALVfEtZO2yREsuiouncuXN8//33zJs3j27dutGoUSMGDRrEwIEDCQgIKKgYxRaUqfZ3j9OV8zD3QRi0EKq2sXZkIsWCu7MDIU0qEtKkIhGXE1m2+zxLdoVzNOoq5+IS+Xz9MT5ff4zGlb0IaVKRng0D8HJzuvOFRSTfWDQR3N/fn+eff57t27dz+PBhevfuzbRp06hUqRIdOnTg22+/JS4uroBCFasrW914qq50AKQlwrwH4dQWa0clUuwEeLrydLvqrH0xmB+fa83QVlUoe337lt1n4nhr2X6avfsrI2b/yRpt3yJSaPL09BxAjRo1ePvttwkLC2PHjh0EBQXx9NNPZ+5BJ8VU2erGU3WlykFqAsztD6d/t3ZUIsWSyWSiQUVPxvasx7bXO/LdY03p3iAAJ3s7UtPNrD4QyVOz/6TFe7/y9vL97Dkbd8f5piJFSWJKOr8cNLZl++XgBRJTcjdXuqD8q6VpMzIy+OWXX5g3bx5Lly7F09OTNm00XFPs+dQwepxmdIerF2DOA/DIYgi819qRiRRbjvZ2dAzyp2OQP5cTUlm5L4Ilu8LZeTqW2IRUZm09zaytp6nm605I4wr0aVyBit5ud76wiI2aseUkH609jIM5jXfuhreW7+f1FYd5uUsthrSqapWY8tTT9PvvvzNy5EgCAgLo168fqampzJs3j8jISIsWtZQizKfm9R4nf0i9BnMfgDPbrR2VSIng6ebIwy0qs+jplmwa044XOtWkchmjQDpx8RqT1h6h9QcbeOibrfyw8yxXklKtHLGIZWZsOcm4Hw9yLTlrz9LV5DTG/XiQGVtOWiUui4qm119/nWrVqtG+fXtOnjzJJ598woULF5g3bx49evTAwaF47KkkueRby5gc7u4HKVdhTj84u8PaUYmUKIFl3XmhUy02jWnHohH3MrB5ZTxcjP8WbzsRwyuL/qLZu7/y/Pe72XTkImnpmv8kti0hJY0P1xzKfJ9hhvMJxtcbJq09YpWhOouqnPfff5/SpUvTr18/fHx82L59O9u359y7YDKZ+Oyzz/IlSLFhvrWNwmlmD7h2EWaHwKNLoVIza0cmUqKYTCaaVilD0yplGNuzLusPRbFkVzgbD18kKTWD5XvOs3zPeXxLO9OnUXlCmlQkKCD7/psilsrIMJOYms61lDQSkq9/TUnnWvI/vt7885yOu95+OTGVxNS/i/srqSY+2OuAk93fVdPV5DR+DbtAz7vKF+pntahoqly5MiaTia1bt97xWBVNJYhfnb+XI0i4BHNC4NFlUPFua0cmUiK5ONrTrUEA3RoEcOlqMj/uPc+SXefYd+4yF68kM23zSaZtPklQgAf9mlSgV6Py+JXO/93jxfakpWdwLSWdhJQ0riX/42tKOgnJ//h6q5/f1J5QSD0+/3zE4eKV5EL5vTezqGg6depUro+9cuWKpbFIUeYX9HePU0I0zO4Lg5dCBRVOItbkU8qZoa2qMrRVVY5euMKS3edYusvYviUsIp7/WxnPe6vCaFPTl5AmFehStxyuTtq+xdrMZjPJaRm37qm5U9Fzi58X9vIUzg52uDs74OZkj7uTA27O17862f/d/o+fH468wqytpzOvUcrBzEsN0/nvPntu6oDCt7RzoX4W+JdPz+UkKiqKTz/9lClTphATE5Pflxdb5l/37x6nxBiY1RcGL4MKTawdmYgANf1L82rXOrzcpTbbTkSzeFc4q/dHkpCSzqYjF9l05CKlnB3o1qAcIU0qavuWXLJkeOrytSQOnbQjdEUYyencpihKJz2jcJePcHeyx83Zwfjq5IC78z++3vHnNxVFzva4OdrjYG/582aJKeks3hWeOQnc3g7KOIPppluxlLMDnYL88+uj55rFRdO2bduYOXMmZ86coUaNGowaNYrq1atz4cIF3nnnHaZPn05KSgoDBw4siHjF1vnXg8dWwMxeRuE0uw8MXgHlG1k7MhG5zt7ORKsaPrSq4cOE3sb2LUt3n+O3Y5e4mpzGDzvD+WFneLHcvsU2hqfsIDIiz5/B3s6U2TPj7vzPnpq8FT0uDvY2UyC7Otkzpkttxv148JbHvNylllV6RC0qmn7++Wd69uyJ2WzG19c3c42m2bNn8+ijjxIbG8vAgQN56623qFWrVkHFLLauXAMYvBxm9YLEWJjV2yikAu6ydmQi8g//3L5l+Z7zLP4z+/YtjSp50a9JBXo0LI+3e8Fv32I2m0lJz8j9xOJc/jy5kIennBzsshQpLg52JF2No1I5P0q7OuW+5+amYSxnBztMJtsocArKjXWYJq09Aua/l8wo5exg1XWaLCqa3nvvPe6++26WL19OuXLluHr1Kk899RS9evUiICCANWvW0KSJhmIECGhoFE4ze0FSnFE4DV5htIuITQrwdGVE2+o8FVyNA+fjWbwrnBV7zhN9LYU9Z+PYczaOd346SIc6fnSv58eN+sNsvj48lYc5NsbPcy560gp5eMrtn8XLrXpmbm6/TdHj5mSP4z+GpxITE1m7di1dujTE1dW1UD9fUTOkVVUGNKvML/vOYg7fy4Te9encoJJV59xZVDQdOnSIadOmUa5cOQBKlSrF+++/z/z583n//fdVMElWAXf9o8eplzHnqVwDa0cmIrdhMpmoX8GT+hU8eb1bEJuPXmTxrnP8cvACKWkZrDlwgTUHLuBkZ89buzeRmJpOYe7eYmcyeshuObH4H3Nr7jTx2N3JAVdH2xmekr+5OtnTua4/a8Ohc11/qz+kYFHRFB0dTfnyWddEuPG+Zs2a+ReVFB/lG10vnHobhdPMG4VTfWtHJiK54GhvR4c6/nSo48/lxFRWXd++5Y9TsaRkmEi5w3yefw5P5XlicQkbnhLbZPFE8FvdqPb2ekRVbqF8Y2PBy1l9rz9V18vYgsW/rrUjExELeLo6MrB5ZQY2r8yR8zHMXPkbjRs1xLuUa66Hp0SKMouLpvbt22Nnl/1fgjZt2mRpN5lMXL58+d9FJ8VHhbuNwml2H2Mdp5k9jU1//YKsHZmI5EElb1ea+5npUt9fc3OkxLCoaBo7dmxBxSElQcXrhdOsPsbK4TN7Gj1OfnWsHZmIiMgdqWiSwlWxKTy6xNij7trFv3ucfGtbOzIREZHb0mCzFL5KzeGRxeBUCq5FGSuIXzxi7ahERERuS0WTWEflFkbh5OhuFE4ze8Clo9aOSkRE5JZUNIn1VL4HHllkFE5XLxg9TpeOWTsqERGRHKloEusKbAmDFoKjG1yNNHqcoo9bOyoREZFsVDSJ9VVp9XfhdCXC6HFS4SQiIjZGRZPYhiqt4eEF4OAKV87DzJ6YYk9ZOyoREZFMKprEdlQNvl44uUD8OZzm98MtOcraUYmIiAAqmsTWVGsLA78HBxfsrpyj1dGJmOJOWzsqERERFU1ig6q3h4HzMds745YajdP8fhCrwklERKzLZoqmI0eO0LVrV9zd3fHz8+P5558nMTHxjuddu3aN1157jerVq+Pm5kbNmjUZN24cycnJhRC1FJjqHUgJmUG6yRG7+HDjqbq4M9aOSkRESjCLN+wtCHFxcXTo0IHAwEAWL15MVFQUo0ePJjo6mjlz5tz23Keffpply5bx7rvvUr9+fXbs2MFbb71FTEwMkydPLqRPIAUho1p7dlR7nntOTcYUd8Z4qm7ISvCqZO3QRESkBLKJomnq1KnExsayZ88efHx8AHBwcGDQoEG88cYbBAUF5XheWloaCxcu5JVXXmHkyJEAtG/fntOnT7NgwQIVTcVAlEdDUvpOx3npUIg7bfQ4DVkJnhWtHZqIiJQwNjE8t2rVKjp16pRZMAH069cPZ2dnVq1adcvzzGYzaWlpeHp6Zmn38vLCbDYXWLxSuDKqd4QBc8HeCWJPGT1Ol89ZOywRESlhbKKnKSwsjMcffzxLm7OzM9WrVycsLOyW5zk6OjJ06FA+//xzWrVqRb169fjjjz+YNm1aZs/TrcTHxxMfH5/5PiIiAoDExMRczaWyRFJSUpavkjtZ8lapDXZ9vsVp6TBMsSfJmNGd5IFLoHSAlaO0LbrX8kZ5s5xyljfKm+UKOmeW/M03mW2gS8bR0ZEJEybw2muvZWlv3bo1fn5+LFmy5JbnpqenM2LECL799tvMtpEjR95xaG7cuHGMHz8+W/u3336bpcdLbIv/5d00PzkZO3M6V5392VLzdZIcva0dloiIFFGXLl1i+PDhnD17looVbz/1wyZ6mgBMJlO2NrPZnGP7zV577TV++uknvvnmG2rXrs2ff/7J2LFj8fb2zrEoumH06NEMHz48831ERATNmzcnODj4jkmzVFJSEqGhoQQHB+Pi4pKv1y7Ocs5bF1KP3oXTsuGUSr5A5/OTjR6nUv5WjdVW6F7LG+XNcspZ3ihvlivonIWHh+f6WJsomry9vYmNjc3WHhcXd8tJ4AD79+9n0qRJLF++nF69egEQHByMnZ0dL7/8Ms8++yx+fn45nuvh4YGHh0e2dldXV1xdXfP4SW7PxcWlwK5dnGXLW8O+4OgAC4dgF3Mc1+8fMCaHl1bhdIPutbxR3iynnOWN8ma5gsqZJde0iYngQUFB2eYuJScnc/z48dsWTQcPHgSgUaNGWdobNWpEWloap09rQcRiK6gnPDAdTPYQfdR4qu6qtlwREZGCYxNFU7du3Vi3bh3R0dGZbUuXLiU5OZlu3brd8rzAwEAA/vzzzyztO3fuBKBKlSr5H6zYjrq94IH/GYXTpSMws6cKJxERKTA2UTQ99dRTeHl50bt3b9asWcPs2bMZOXIkgwYNytLTNGzYMBwc/h5RbNq0Kc2bN2fEiBFMmTKFDRs28OGHHzJ27FgGDBiAr6+vNT6OFKZ6feCB74zC6eIhmNkLrl60dlQiIlIM2UTR5OXlxfr163F3dyckJITRo0czcOBApk2bluW49PR00tPTM9/b29vz448/0qdPHz744AO6devGt99+y8iRI7M8TSfFXL2+0G8amOzgYhjM6gXXLlk7KhERKWZsYiI4QK1atVizZs1tj5kxYwYzZszI0ubn58fUqVMLMDIpEur3A7MZljwBUQeNHqfHfgT3staOTEREigmb6GkSyRcNHoC+3xg9TlEHjB6nhBhrRyUiIsWEiiYpXhr2hz5TABNc2K/CSURE8o2KJil+7hoAfb4GTBC5D2b1VuEkIiL/moomKZ4aDYQ+X2EUTn/B7D6QmH0BVRERkdxS0STFV6OHofcXgAki9sKsPpAYZ+WgRESkqFLRJMVb40eg1/XNmyP2wOy+KpxERCRPVDRJ8ddkMPT8zPj+/C6YEwJJl60bk4iIFDkqmqRkuHsI9PjU+P7cnzBbhZOIiFhGRZOUHE2HQvdPjO/P7YQ5/SAp3roxiYhIkaGiSUqWZsOg2yTj+/A/jMIp+Yp1YxIRkSJBRZOUPM2fgPs/Mr4P3wFzHlDhJCIid6SiSUqmFk9C1w+M789ug7n9IfmqdWMSERGbpqJJSq57RsB9E43vz2xV4SQiIreloklKtnufgfveM74/8zvMGwAp16wbk4iI2CQVTSL3Pgtd/s/4/vRv1wunBOvGJCIiNkdFkwhAy5HQ+R3j+1ObYb4KJxERyUpFk8gNrZ6HTuON70+GwvyHIDXRujGJiIjNUNEkcrPWL0DHscb3JzfB/IEqnEREBFDRJJJdm9HQ4S3j+xMb4PuHITXJujGJiIjVqWgSyUnwy9D+TeP74+thwSAVTiIiJZyKJpFbaTsG2r1ufH/sV1jwCKQlWzcmERGxGhVNIrfT7lVo+5rx/bFfYMGjKpxEREooFU0id9LuNQh+xfj+6Br4YbAKJxGREkhFk8idmEzQ/nVo87Lx/shqWDgE0lKsGpaIiBQuFU0iuWEyQYc3ofVo4/3hVSqcRERKGBVNIrllMkHHt6HVC8b7wyth0VBIT7VqWCIiUjhUNIlYwmSCTuOg5Sjj/aGfYNHjKpxEREoAFU0iljKZjH3q7n3OeB+2AhYPU+EkIlLM2UzRdOTIEbp27Yq7uzt+fn48//zzJCbefvuKU6dOYTKZcnw5OzsXUuRSIplM0OX/4J5njfcHl8OSJyA9zbpxiYhIgXGwdgAAcXFxdOjQgcDAQBYvXkxUVBSjR48mOjqaOXPm3PK8gIAAtm7dmqXNbDZz//330759+4IOW0o6kwnuexfMGbD9aziwFEx20PcbsLeJf7VERCQf2cR/2adOnUpsbCx79uzBx8cHAAcHBwYNGsQbb7xBUFBQjuc5Oztzzz33ZGnbuHEjly9f5uGHHy7wuEUwmaDrRKNw2jEV9i8GTNB3qgonEZFixiaG51atWkWnTp0yCyaAfv364ezszKpVqyy61rx58/Dw8KBnz575HaZIzkwmuP8DaPaE8X7/Ilj2NGSkWzcuERHJVzZRNIWFhWXrTXJ2dqZ69eqEhYXl+jqpqaksXryYvn374uLikt9hityayQTdPoKmw4z3+36AZc+ocBIRKUZsYvwgNjYWLy+vbO3e3t7ExMTk+jo///wzMTExuRqai4+PJz4+PvN9REQEAImJiXecgG6ppKSkLF8ld4pk3jpMwDEtFYc9s+Cv70nLSCf1/k/Bzr5Qfn2RzJkNUN4sp5zljfJmuYLOmSV/822iaAIwmUzZ2sxmc47ttzJ37lz8/f3p2LHjHY/95JNPGD9+fLb20NDQLMOE+Sk0NLRArlvcFb28deCusmepEr0Bh/0LOX8+kt2VhxmTxAtJ0cuZbVDeLKec5Y3yZrmCytmlS5dyfaxNFE3e3t7ExsZma4+Li7vlJPB/unr1Kj/99BPDhw/H3v7O/1c/evRohg8fnvk+IiKC5s2bExwcTMWKFXMffC4kJSURGhpKcHCwhg0tUKTzZu5C2uoxOPw1l8oxmylfoSKp939c4IVTkc6ZFSlvllPO8kZ5s1xB5yw8PDzXx9pE0RQUFJRt7lJycjLHjx/n8ccfz9U1li5dSkJCQq6fmvPw8MDDwyNbu6urK66urrm6hqVcXFwK7NrFWZHNW58vwN4Eu+fgsG8+Do6O0OMzsCv4HqcimzMrU94sp5zljfJmuYLKmSXXtImiqVu3bkyYMIHo6GjKli0LGEVQcnIy3bp1y9U15s2bR/Xq1WnRokVBhiqSe3Z20PNzMJthz1zYNQswQY9PC6VwEsk3aSkQfQwuhsHFwxAVhvOFMLrGR2Kf3hNaPw9+dawdpUiBs4mi6amnnuLzzz+nd+/evPXWW5mLWw4aNCjL8NywYcOYOXMmaWlZV12+ePEiv/76K6+99lphhy5ye3Z20Ot64bR3HuyaaTxp1/2/KpzE9qSnQvRxoziKOvT315jjkJH1v7t2gDPAX/OMV43O0PI5qNrWuMdFiiGbKJq8vLxYv349I0eOJCQkBDc3NwYOHMgHH3yQ5bj09HTS07M/wv3DDz+QlpamBS3FNtnZQ+8vjAUw//oe/pxhzG3q/on+uIh1pKdCzAmICoOLh4xX1CGjNynjNnso2jtB2ZrgV4dU7xocO3ac2ld/x+7KeTj2i/Eq18DYl7FeCDg4Fd5nEikENlE0AdSqVYs1a9bc9pgZM2YwY8aMbO3PPvsszz77bAFFJpIP7Oyhz1dG4bTvB9j5P6Nw6jZJhZMUnPQ0iD35d3F04+ulo7cvjuwcwacm+NYxXn51wDcIylTLXOk+LTGRI1fXUqXjf3E9sRp+/xwi/4LIfbD0Kfh1PLR4Eu4eCq5ehfN5RQqYzRRNIsWenT30nQKYYd9C+ONbo3C6/0MVTvLvZKRDzMnsw2rRRyE95dbn2TlA2RrXC6Ogv7+WqQb2jrn73faO0PBBaNAfTm2G37+Ao2vgynn4dRxs+giaPAr3PA3eVfLj04pYjYomkcJkZw99phhznPYvgh3fGIVT1/dVOMmdZaRD7KnrPUY3JmUfgktHID351ueZ7I3i6EaPkW/t68VR9fwbQjOZoGqw8bp4GLZ+CXu/h9RrsH2Kca8H9YR7R0KlZvnzO0UKmYomkcJm72Bs6GvOgANLjD8oXN/4V4WTAGRkQNyprL1GF8OMYbW026yKbLI3eoluFEc3vpatUbjzi3xrQ6/J0OEto0f1j2mQEA0HlxuvSi2MeU91uhfaavki+UFFk4g12DtAyDSjcDq4DLZ/bfQ43feuCqeSJCMD4k5nnW8UdaM4us3WDiY7ozi6eVjNt44xD8nBufDiv5NSvtD+P9D6BaPXaeuXxpDh2e3Gy7sq3PMMNB4ETu7WjlbkjlQ0iViLvQP0+xYwG//3ve1Lo2Dq8n8qnIqbjAy4fCZzjaO/i6MjkJpw6/NMdkZhcfNkbL86xhNsjkVoNWlHV2g6FJo8BkfXwtYvjPlPsSfh5zGw4V1o+ji0eApKl7N2tCK3pKJJxJrsHaHfd0aPU9iPxh8Tkx10fkeFU1FkNsPls9mH1S4eMeb23JLJmCR982TsGz1HjsVo1Wg7O6jd1Xid32Pc7/uXQFIc/PaJ8QReg/7Gek/+9awdrUg2KppErM3eER6YDguHwKGf4PfJRuHUaZwKJ1tlNsPl8KzDahcPGT1JKVdvc6IJvANvmm90Y1itFji5FVr4NqF8I6OntdM4Y17fnzMhOd5YBHbvPKjW3iieqnfUvwdiM1Q0idiCmwunwythy6dG4dTxbf3BsCazGeLPZ3+U/+JhSLly+3O9KmedjO13ozjS3J0sPCsaQ9LBr8Du2bBtijGUeWKD8fKrC/c+a/RA2dJ8LSmRVDSJ2AoHJ+g/A34YDEd+NoYrTHbQ4U0VTgXNbMYlJQa7kxvh8s0rZR82ej9ux7Py371Gftcf5/epDc6lCiX0YsPFwyiOmj8FYcuN9Z7O74Kog7D8WVj3DjR/ApoOA7cy1o5WSigVTSK2xMEJHpx5vXBaDZsnGY9kt3/d2pEVD2YzXIm8qcfIeLlEhXFfcjwcuM25npWMgiizOAoC31rgXLrQwi8R7B2gfj9jG5YzW43i6fAquHoB1v8fbP4EGj1sPHVXtrq1o5USRkWTiK1xcIYHZ8GCR4wnjTZ9YPQ4tdOG1LlmNsPVqByG1cIg6XK2w7P043lUyL5Ctk8toydECo/JBIEtjVf0cWO5gj3zjKcN//gW/vjOWOfp3ueg8j3qjZVCoaJJxBY5OMODs43C6dgvsHEiYIJ2r1o7MttiNsO1i//YW+2wURwlxt7+3NLlM4fVUryqs+1EHM26PYqrl3/hxC65V7Y69PjEGKre+R1s/wauRRkPThz6CSrcbRRPQb0y98YTKQi6u0RslaMLDJgD3z8Mx9fBxveMHqe2Y6wdmXVcvfj3U2o3F0mJMbc/r1S57Ctk+9bOsolsemIisRfWgrN6k2yaWxkIHgMtRxn7N/7+hVEgn/sTFg01Jt+3eNrY607DplIAVDSJ2DJHF3hoHnw/EI6vhw3XF74MftnakRWca9HXh9NuFEbXh9USom9/Xin/7Ctk+9UBV+/CiVsKj4MzNH4EGg0y/ofi98/hxEaIOwNr/gMb34e7H4MWI8CzgrWjlWJERZOIrbtROM1/yPjDsH6C0ePUZrS1I/t3EmL+sfHs9SLp2sXbn+fu9/eGszcXSXqiquQxmaBGJ+MVuc+Y97RvESRfNtY72/aVMaG85XMQcJe1o5ViQEWTSFHg6AoPzTcKp5ObYN14o3Bq/YK1I7uzxNjsk7GjDhlzUm7HzeemwujGsFodcC9bOHFL0VKuAfSdAh3Hwo6psPN/xqT/fT8YryptoOVIqNHZWJlcJA9UNIkUFU5uMPB7mPegsW/Xr2ONwqnVKGtHZkiMyz7f6OJhuBp5+/Pcyv49z+jm3iN3n0IJW4oZjwBjlfE2L8OeuUZvU+wp49+ZU5uNJyHvfRYaPlS09u8Tm6CiSaQocXKDhxfAvAHGH4Bf3jIKp5bPFV4MSZezrHGUWSRdibj9ea7e2VfI9g2CUr6FE7eULM6ljA2Amw2/vj3RFxC+w9gk+cfnYd0EY7HMZsNVoEuuqWgSKWqc3I3CaW5/OL0F1r5hFE73PpO/vycp/u/H928eVrty/vbnuXhln4ztGwSl/LSWjhQ+O3uo29t4nd1hTBo/9BMkXDKW8vjtv3DXQ8aSBT41rR2t2DgVTSJFkZM7PPyDUTid+d14YshkB3c9Zvm1kq9knYh944m1+PDbn+fs+Y/tQ65/LeWv4khsU6XmMGA2xJyEbV/D7jmQeg3+nGG8anU1iqcqrXUPS45UNIkUVc6lYNBCmPuAsd3E6lexT0sDKuV8fPJVuHT4H5OyD8Hls3f4PR5Ze4xuFEqlA/SHRYqmMlWh24fQ/j9GsbR9qjG8fGS18Qq4C+4dCfX6GJtpi1ynokmkKLtROM3pB2e34/TrG1SrMAhThB/En8y61tHlM7e/llPp6wVR7axzjjzKqziS4snVG1q/CPc8CweWGPOeLuyDiL2wZDj8Os6YF3X3Y+Diae1oxQaoaBIp6pxLw6BFRuEUvoMG5+bCrLm3Pt6pVPbCyK+OseeaiiMpiRycjHlNDQcYS3r8/oWxfVF8uPGwxaYPoclguGeEseq4lFgqmkSKAxcPeGQxGbP6YHf+T6PN0f2mx/hvKpI8K6k4EsmJyQTV2hmvqDBjscy/FkDKFdj2JWyfYkwob/mcsd+dlDgqmkSKCxcPkgcu5s/l39C0Ux9c/GpqET+RvPILgt5fQMe3Ycc0+ONbY5/DA0uMV+WWRvFU6379e1aC6J+0SHHi4EJ06TqYPSvrP+Qi+aGUH3R4A148AN0/hjLVjfYzvxubaX/R1CioUhKsG6cUCv1XVURE5E6c3IyFMJ/baewFWbml0R5zHFa+BP+tB+v/D65csG6cUqBUNImIiOSWnR3U6Q6P/wxPrDc2BDbZG0N3oR/Bp/Vh+bPGnCgpdmymaDpy5Ahdu3bF3d0dPz8/nn/+eRITE3N1bkxMDM888wwBAQG4uLhQq1Ytpk6dWsARi4hIiVbhbug/HZ7fYyyK6VQa0lOMRTO/usd4ovX4BjCbrR2p5BObmAgeFxdHhw4dCAwMZPHixURFRTF69Giio6OZM2fObc+9evUqbdu2xdXVlc8++ww/Pz+OHj1KampqIUUvIiIlmldluO9daPsK7JoF26YYyxUc+9V4+dc3iqr6/awdqfxLNlE0TZ06ldjYWPbs2YOPj7FxooODA4MGDeKNN94gKCjolue+9957JCYmsmPHDlxdXQFo165dYYQtIiLyNxdPaDkSWoyAg8uNfe4i9sCF/bBsBKwbj0OTx3FMq2jtSCWPbGJ4btWqVXTq1CmzYALo168fzs7OrFq16rbn/u9//2PYsGGZBZOIiIhV2TtCgwfgyY0wZKWxLAHAlQgcN71LlwMv4PjrG8YeeFKk2ETRFBYWlq03ydnZmerVqxMWduvJdCdPnuTChQt4e3vTo0cPnJ2dKVu2LM8++2yu50OJiIgUCJPJ2Pz34e+Np+7uHorZwQWHjGQc/vwOPm8CCx6FszusHankkk0Mz8XGxuLl5ZWt3dvbm5iYmFueFxkZCcCYMWPo378/q1at4uDBg/znP/8hJSWFadOm3fLc+Ph44uPjM99HREQAkJiYmO8FV1JSUpavkjvKm+WUs7xR3iynnFnIvSJ0mkhyk+eI+PFdal/ehF1iNIStgLAVpJdvSlrzEWTUvB/s7K0drU0p6HvNkr/5NlE0AZhy2NbBbDbn2H5DRkYGAEFBQfzvf/8DoGPHjqSmpjJmzBgmTJhAuXLlcjz3k08+Yfz48dnaQ0NDswwT5qfQ0NACuW5xp7xZTjnLG+XNcspZHgT04Zh/NyrG/E6Ni6spnXQe+/M7sV82nGtOfhz368KZMsGk27tYO1KbUlD32qVLl3J9rE0UTd7e3sTGxmZrj4uLu+0k8DJlygDQoUOHLO0dOnQgIyODsLCwWxZNo0ePZvjw4ZnvIyIiaN68OcHBwVSsmL+T9JKSkggNDSU4OBgXF/1LkFvKm+WUs7xR3iynnOXNjby1btcJF5ceYP4/kk+sx+GPKdif/g33lCgahs+hwcUfSWs8mLQmw6B0zn/HSoqCvtfCw8NzfaxNFE1BQUHZ5i4lJydz/PhxHn/88VueV716dZycnLK1m6+viWF3m20kPDw88PDwyNbu6upaYJPKXVxcNGE9D5Q3yylneaO8WU45y5sseavf03hF7DU2Cd6/GFPyZRy3fY7jjinGpPJ7n4VyDawbtJUV1L1myTVtYiJ4t27dWLduHdHR0ZltS5cuJTk5mW7dut3yPCcnJzp37sy6deuytK9btw4HBwfq1q1bYDGLiIjkq4C7IOQbeP4vaPU8OHtCRirsnQ9TWsOs3nD0Vy2WaUU2UTQ99dRTeHl50bt3b9asWcPs2bMZOXIkgwYNyjI8N2zYMBwcsnaOvf322+zdu5fBgwezdu1aPv30U8aOHctzzz2Hr69vYX8UERGRf8ezAnR+B0YfgK7vG4tnApzYCHP7wVf3wq7ZkJZs1TBLIpsomry8vFi/fj3u7u6EhIQwevRoBg4cmO3pt/T0dNLT07O0NW/enJUrV3Lw4EF69uzJhx9+yMiRI/nwww8L8yOIiIjkL+fScM/TMHI39J8BFZoa7RfDYMVz8N/6sOkjuBZ928tI/rGJOU0AtWrVYs2aNbc9ZsaMGcyYMSNbe+fOnencuXMBRSYiImJF9g5Qry/U7QNntxsrjR9aCdeiYMP/weaPodFAuOdZ8Klh7WiLNZvoaRIREZE7MJmg8j3w0FwY+Sc0ewIc3SAtEXb+D75oCvMHwqktmvdUQFQ0iYiIFDVlq0P3SfDiAejwFpTyB8xweBXM6AbT2sO+RZCeZu1IixUVTSIiIkWVWxkIfhle2Ae9vwK/60+Nn98Ni4fB5Ebw+xeQFH/by0juqGgSEREp6hycofEgePp3eGQJVL++6PPls7D2DfhvPVj7JlzO/UKOkp2KJhERkeLCZIIaHeHRpUYB1WgQ2DlCcrwxgfyzu2DxcDi/x9qRFkkqmkRERIoj/3rQ5yt4cT+0eQlcvCAjDfYthG/awowecHg1XN/HVe5MRZOIiEhxVrocdHwbRh+EbpPAu6rRfmozzB8AXzaHndMhNdG6cRYBKppERERKAid3aP6EsVzBgDlQ6R6jPfoo/PSCMe9pw3tw9aJVw7RlKppERERKEjt7COoJw9bA8HXGopkmO0iIhk0fGMXTipFw8bC1I7U5KppERERKqopN4cGZMGo3tHgaHN0hPRl2zTKG7eb2hxObtFjmdSqaRERESjrvKnD/+8a8p07joXR5o/3oWpjVC6YGw94FkJ5q1TCtTUWTiIiIGFy9oPUL8Pxe6PsNlGtgtEf+BUufhE8bwm+fQmKc9WK0IhVNIiIikpWDE9w1AJ7aDINXQM0uRvuV8/DrWGPe08+vQexp68ZZyFQ0iYiISM5MJqjWFgYthGe2Q5PBYO8MKVdh+9fGNi0/PAbhO60daaFQ0SQiIiJ35lcHen1uLJbZ9lVwKwvmDDi4DL7tCN/dB2E/Qka6tSMtMCqaREREJPdK+UH71+HFA9Djv1C2htF+dhsseAS+aAo7pkHKNevGWQBUNImIiIjlHF2h6ePw7B8w8HsIbG20x5yAVS8b857WvQNXIq0bZz5S0SQiIiJ5Z2cHte+HoSvhyY1Q/wEw2UNiLGz+GD5tAMuegQsHrR3pv6aiSURERPJH+cbwwHfGkgX3PgdOpSE9BfbMha/vhdl94di6IrtYpoomERERyV9eleC+d43FMru8C56VjPbj62FOCHzdCnbPhbRk68ZpIRVNIiIiUjBcPKDlczBqD/T7zuiJAog6AMufMRbLDJ0ECTFWDTO3VDSJiIhIwbJ3gAYPwBMbYMgqqN0NMMHVSFg/wZg0vvJlYxL5zVIS4PAq4/vDq4z3VqSiSURERAqHyQRVWsHA+fDcTuPpOwcXSE2AP6bB5Cbw/SA4sw22TYFJNWHlS8a5K1+Cj2vB9qlWC9/Bar9ZRERESi6fGsY6T+3fhJ3fwY5v4NpFOPST8brBwevv75OvwM+vGN+3eKpQwwX1NImIiIg1uZeFtq/AC/uNFcd9amX5sXNaPNWi1mR94m79BKsM1aloEhEREetzdDH2tmv7apZmOzJocG4uTulX/25MvgJHfi7kAFU0iYiIiC25djHL2zSTExnYk27nlPW4q1GFGJRBRZOIiIjYjlJ+Wd6mOrjzS72PSTc53fa4wqCiSURERGxHrfvBqVSWpiSnMsaTdzc4lzaOK2Q2UzQdOXKErl274u7ujp+fH88//zyJiYl3PK9du3aYTKZsr0OHDhVC1CIiIpKvnNyg49u3P6bDW8ZxhcwmlhyIi4ujQ4cOBAYGsnjxYqKiohg9ejTR0dHMmTPnjue3atWKSZMmZWmrUqVKAUUrIiIiBerGcgLrJ0DGTe3OpY2CyQrLDYCNFE1Tp04lNjaWPXv24OPjA4CDgwODBg3ijTfeICgo6Lbne3l5cc899xRGqCIiIlIYWjwFjR+Fgz/DaaD7x1D3fqv0MN1gE8Nzq1atolOnTpkFE0C/fv1wdnZm1apVVoxMRERErMbJ7fqWKxhfrVgwgY30NIWFhfH4449naXN2dqZ69eqEhYXd8fxNmzbh7u5Oeno6LVq0YMKECQQHB9/2nPj4eOLj4zPfR0REAJCYmJiruVSWSEpKyvJVckd5s5xyljfKm+WUs7xR3ixX0Dmz5G++TRRNsbGxeHl5ZWv39vYmJub2Ox+3bduWwYMHU7NmTc6fP8+kSZPo1KkTmzZt4t57773leZ988gnjx4/P1h4aGpqlxys/hYaGFsh1izvlzXLKWd4ob5ZTzvJGebNcQeXs0qVLuT7WJoomANPNjxJeZzabc2y/2T8Lnx49elCvXj0mTJhw26G90aNHM3z48Mz3ERERNG/enODgYCpWrGhh9LeXlJREaGgowcHBuLi45Ou1izPlzXLKWd4ob5ZTzvJGebNcQecsPDw818faRNHk7e1NbGxstva4uLg7TgL/J3d3d7p3786iRYtue5yHhwceHh7Z2l1dXXF1dbXod+aWi4tLgV27OFPeLKec5Y3yZjnlLG+UN8sVVM4suaZNTAQPCgrKNncpOTmZ48ePW1w0gdFDJSIiIpKfbKJo6tatG+vWrSM6OjqzbenSpSQnJ9OtWzeLrnXt2jVWrlxJs2bN8jtMERERKcFsomh66qmn8PLyonfv3qxZs4bZs2czcuRIBg0alKWnadiwYTg4/D2iuHnzZnr37s2MGTPYsGEDc+fOpU2bNkRGRvL223dYTVRERETEAjYxp8nLy4v169czcuRIQkJCcHNzY+DAgXzwwQdZjktPTyc9PT3zfUBAAMnJyfznP/8hOjoad3d3WrZsyZQpU2jevLlFMaSlpQF/Lz2QnxITE7l06RLh4eEaw7aA8mY55SxvlDfLKWd5o7xZrqBzduPv/o064HZMZk0AAuCPP/6wuNASERGR4mHHjh13nNqjoum6pKQk9u3bh6+vb5YhwPxwYzmDHTt2EBAQkK/XLs6UN8spZ3mjvFlOOcsb5c1yBZ2ztLQ0Ll68SIMGDe64pIFNDM/ZAhcXlwKfPB4QEJDva0CVBMqb5ZSzvFHeLKec5Y3yZrmCzFmVKlVydZxNTAQXERERsXUqmkRERERyQUVTIfDw8GDs2LE5rkAut6a8WU45yxvlzXLKWd4ob5azpZxpIriIiIhILqinSURERCQXVDSJiIiI5IKKJhEREZFcUNEkIiIikgsqmkRERERyQUXTv3Ts2DFGjBhBo0aNcHBwoH79+rk+d+bMmdSpUwcXFxfq16/PwoULCzBS25HXnLVr1w6TyZTtdejQoQKO2PoWLlxInz59qFSpEu7u7jRs2JCvv/6ajIyMO55bUu8zyHveSvK9tmbNGtq2bYuvry/Ozs5Uq1aN0aNHc/ny5TueW5LvtbzmrSTfa/909epVKlasiMlkYufOnXc83hr3m7ZR+ZcOHDjAypUradGiBRkZGbn6IwawaNEihgwZwmuvvUaXLl1YtmwZAwYMwNPTky5duhRw1NaV15wBtGrVikmTJmVpy+3y90XZxx9/TGBgIB999BH+/v5s2LCBUaNGceLECT766KNbnleS7zPIe96g5N5rMTExtGzZkhdeeAFvb2/279/PuHHj2L9/P2vXrr3leSX9Xstr3qDk3mv/NGHCBNLS0nJ1rNXuN7P8K+np6ZnfP/bYY+Z69erl6rw6deqY+/fvn6WtS5cu5hYtWuRrfLYorzlr27atuXv37gUVlk2LiorK1vbiiy+aXVxczElJSbc8ryTfZ2Zz3vNWku+1nHzzzTdmwHzu3LlbHlPS77Wc5CZvutcMYWFhZnd3d/OUKVPMgPmPP/647fHWut80PPcv2dlZnsKTJ09y6NAhBg4cmKX94YcfZseOHVy6dCm/wrNJeclZSefr65utrXHjxiQlJRETE5PjOSX9PoO85U2yK1u2LACpqak5/lz3Ws7ulDf526hRoxgxYgS1a9e+47HWvN/018sKwsLCAAgKCsrSXrduXcxmc4kcy86tTZs24e7ujouLC23btiU0NNTaIVnN5s2bKVOmDH5+fjn+XPdZzu6UtxtK+r2Wnp5OUlISu3bt4p133qFnz54EBgbmeKzutb9ZkrcbSvq9tmjRIvbu3cvbb7+dq+Oteb+paLKC2NhYALy8vLK0e3t7A+j/gG+hbdu2fPbZZ6xevZqZM2eSkJBAp06d2Lp1q7VDK3Q7d+5k+vTpvPjii9jb2+d4jO6z7HKTN9C9BhAYGIirqyt33303AQEBzJ8//5bH6l77myV5A91rCQkJjB49mokTJ+Z6bzlr3m+aCG5FJpMpy3vz9W0A/9kuhvHjx2d536NHD+rVq8eECRNYtWqVlaIqfJGRkfTr14/mzZvz6quv3vF43WcGS/Kmew1WrVrF1atXOXDgABMmTKBnz5788ssvty02da9ZnreSfq/93//9H/7+/gwZMsTic61xv6mnyQpuVMM3quUb4uLisvxcbs/d3Z3u3bvz559/WjuUQnP58mXuv/9+3NzcWLFiBY6Ojrc8VvfZ3yzJW05K4r3WsGFDWrZsyRNPPMHSpUvZsGEDS5cuzfFY3Wt/syRvOSlJ99rp06f5+OOPGT9+PPHx8cTFxXH16lXAWH7gxvf/ZM37TUWTFdwYh70xLnvDwYMHMZlM1KlTxxphFUk3/s+iJEhKSqJXr15cuHCB1atXZ04yvRXdZwZL83YrJele+6dGjRphb2/PsWPHcvy57rWc3Slvt1JS7rWTJ0+SkpJC9+7d8fb2xtvbm549ewLQvn17OnXqlON51rzfVDRZQdWqValTpw4LFizI0j5//nyaN2+Oj4+PlSIrWq5du8bKlStp1qyZtUMpcGlpaTz44IPs3buX1atX33FiKeg+g7zlLScl6V7LydatW0lPT6datWo5/lz3Ws7ulLeclKR7rVGjRmzYsCHL67///S8AU6ZM4auvvsrxPGveb5rT9C8lJCRkjjufPn2a+Ph4Fi1aBJC5OuywYcOYOXNmlkW73nnnHQYMGED16tXp3Lkzy5cvZ+3ataxevdoqn6Mw5SVnmzdvZtKkSfTt25fAwEDOnz/Pxx9/TGRkZIlYdfjZZ5/lxx9/5MMPPyQhIYFt27Zl/qxu3bp4eHjoPstBXvJW0u+1kJAQmjZtSsOGDXF1dWXv3r18+OGHNGzYkD59+gDoXstBXvJW0u81Ly8v2rVrl+PP7r77bpo0aQLY2P1WoKtAlQAnT540Azm+NmzYYDabjQUcc0r1jBkzzLVq1TI7OTmZ69ata/7hhx8KOXrryEvOjh49ar7vvvvM5cqVMzs6Opq9vLzM3bp1M2/fvt1Kn6JwBQYG6j7Lg7zkraTfaxMnTjQ3atTIXLp0abO7u7u5Xr165rfeest8+fLlzGN0r2WXl7yV9HstJxs2bMi2uKUt3W8ms7mEDJ6KiIiI/Aua0yQiIiKSCyqaRERERHJBRZOIiIhILqhoEhEREckFFU0iIiIiuaCiSURERCQXVDSJiIiI5IKKJhEREZFcUNEkIvIvjBs3jlKlSlk7DBEpBCqaRERERHJBRZOIiIhILqhoEpEiZ+vWrXTo0AF3d3c8PT15+OGHiYqKAuDUqVOYTCZmzpzJsGHD8PT0pEyZMowePTrLLukA+/fvp2vXrpQqVQoPDw969+7NsWPHshyTkZHBJ598QlBQEM7OzpQrV47+/ftz+fLlLMf99ddftG7dGjc3N+rXr8+aNWsKNgkiUuhUNIlIkbJ161batWuHp6cnCxYs4JtvvuGPP/6gV69eWY57/fXXycjI4IcffmDMmDF8/vnnvPnmm5k/P3v2LG3atOHChQvMnDmTb7/9liNHjtCmTRsuXryYedzIkSN55ZVX6NGjBz/++CNffvklpUuX5urVq5nHpKam8sgjjzBkyBCWLl2Kj48P/fr1Izo6uuATIiKFxywiUoQEBwebW7Zsac7IyMhs279/v9lkMplXrlxpPnnypBkwt2nTJst5b775ptnNzc0cExNjNpvN5hdffNHs5uZmjoqKyjzm1KlTZkdHR/PYsWPNZrPZfPjwYbPJZDK/9957t4xn7NixZsC8cuXKzLajR4+aAfPs2bPz4yOLiI1QT5OIFBkJCQls2bKF/v37k56eTlpaGmlpadSuXZuAgAD++OOPzGP79u2b5dyQkBASEhLYt28fAJs3b6ZDhw74+vpmHhMYGEjLli3ZvHkzAOvXr8dsNjNs2LDbxmVnZ0enTp0y39eoUQMnJyfCw8P/9WcWEduhoklEiozY2FjS09N58cUXcXR0zPI6f/48Z8+ezTzWz88vy7k33kdERGReq1y5ctl+R7ly5YiJiQEgOjoaBweHbNf6J1dXV5ycnLK0OTo6kpSUZPmHFBGb5WDtAEREcsvLywuTycTrr79Onz59sv3cx8cn8/sbE8P/+T4gIACAMmXKcOHChWzXiIyMpEyZMgCULVuWtLQ0oqKi7lg4iUjxp54mESky3N3duffeewkLC6Np06bZXlWqVMk8dunSpVnOXbJkCW5ubjRo0ACA1q1bs27duiyTtc+ePcvvv/9OmzZtAOjQoQMmk4np06cX/IcTEZunniYRKVI++ugjOnTowIABA3jooYfw9vYmPDycX375haFDh2YWTsePH2fo0KE89NBD7Nq1iw8++IAXXngBb29vAF588UWmT59Oly5deOONN0hPT2fs2LGUKVOGZ599FoBatWoxYsQI3nzzTWJiYujYsSMJCQmsXLmScePGUaFCBWulQUSsQEWTiBQpLVu25LfffmPs2LEMHTqUlJQUKlasSMeOHalRo0bmWkzvvvsuGzdupH///tjb2/PMM8/w7rvvZl6nUqVKhIaG8vLLL/Poo49iZ2dH+/bt+fjjj7NMDv/iiy+oWrUq06ZN47///S9ly5albdu2lC5dutA/u4hYl8lsNputHYSISH45deoUVatWZeHChTzwwAPWDkdEihHNaRIRERHJBRVNIiIiIrmg4TkRERGRXFBPk4iIiEguqGgSERERyQUVTSIiIiK5oKJJREREJBdUNImIiIjkgoomERERkVxQ0SQiIiKSCyqaRERERHJBRZOIiIhILvw/K8B7+gUXiqEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ae1596003949568dc1a30a7a414d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750269978.258700   12293 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=1.268338 • val=1.005418 • impr=-240.6% • lr=3.75e-04 • g≈3386.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818395a8b6fa4dd7b55686df4785bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.984784 • val=0.625688 • impr=-112.0% • lr=4.55e-05 • g≈21664.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7a7f13357143e4b860469beee4bf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.846660 • val=0.688830 • impr=-133.4% • lr=3.44e-04 • g≈2458.72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf7d30b4c424bea9c9de967bab29adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.864327 • val=0.519567 • impr=-76.0% • lr=1.61e-04 • g≈5374.35\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "import math, numpy as np, tensorflow as tf\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "today     = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "file_path = save_dir / f\"model_{ticker}_{today}.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ea52-a04f-4fff-bb69-73c1717dc459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
