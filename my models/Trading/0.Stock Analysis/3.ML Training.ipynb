{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 14:13:54.099579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750421634.187728   29512 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750421634.215582   29512 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750421634.329159   29512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750421634.329251   29512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750421634.329254   29512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750421634.329257   29512 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import stockanalibs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:30:00</th>\n",
       "      <td>573.3900</td>\n",
       "      <td>573.8499</td>\n",
       "      <td>571.364</td>\n",
       "      <td>571.6900</td>\n",
       "      <td>74953.0</td>\n",
       "      <td>571.5185</td>\n",
       "      <td>571.8615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.182741e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:31:00</th>\n",
       "      <td>571.6600</td>\n",
       "      <td>572.0200</td>\n",
       "      <td>571.010</td>\n",
       "      <td>571.0100</td>\n",
       "      <td>17681.0</td>\n",
       "      <td>570.8387</td>\n",
       "      <td>571.1813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.738614e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:32:00</th>\n",
       "      <td>571.0520</td>\n",
       "      <td>571.5000</td>\n",
       "      <td>569.400</td>\n",
       "      <td>569.4000</td>\n",
       "      <td>21106.0</td>\n",
       "      <td>569.2292</td>\n",
       "      <td>569.5708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.715507e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:33:00</th>\n",
       "      <td>569.3700</td>\n",
       "      <td>569.9640</td>\n",
       "      <td>568.580</td>\n",
       "      <td>569.9640</td>\n",
       "      <td>16628.0</td>\n",
       "      <td>569.7930</td>\n",
       "      <td>570.1350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.616680e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:34:00</th>\n",
       "      <td>570.5499</td>\n",
       "      <td>572.0500</td>\n",
       "      <td>569.980</td>\n",
       "      <td>570.3600</td>\n",
       "      <td>12734.0</td>\n",
       "      <td>570.1889</td>\n",
       "      <td>570.5311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.559958e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.3750</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.215</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.5129</td>\n",
       "      <td>173.6171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>3.876</td>\n",
       "      <td>4.528500e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.5650</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.240</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.3280</td>\n",
       "      <td>173.4320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.061</td>\n",
       "      <td>3.019000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.3900</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.200</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.2580</td>\n",
       "      <td>173.3620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.131</td>\n",
       "      <td>2.012667e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.3150</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.230</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.2280</td>\n",
       "      <td>173.3320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.161</td>\n",
       "      <td>1.341778e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.3000</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.170</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.5576</td>\n",
       "      <td>173.6618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>3.831</td>\n",
       "      <td>8.945186e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5895811 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high      low     close     volume  \\\n",
       "2014-04-03 13:30:00  573.3900  573.8499  571.364  571.6900    74953.0   \n",
       "2014-04-03 13:31:00  571.6600  572.0200  571.010  571.0100    17681.0   \n",
       "2014-04-03 13:32:00  571.0520  571.5000  569.400  569.4000    21106.0   \n",
       "2014-04-03 13:33:00  569.3700  569.9640  568.580  569.9640    16628.0   \n",
       "2014-04-03 13:34:00  570.5499  572.0500  569.980  570.3600    12734.0   \n",
       "...                       ...       ...      ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.3750  173.6771  173.215  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.5650  173.5900  173.240  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.3900  173.4100  173.200  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.3150  173.4000  173.230  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.3000  174.0500  173.170  173.6097  7649838.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:30:00  571.5185  571.8615           0.0            0.000   \n",
       "2014-04-03 13:31:00  570.8387  571.1813           0.0            0.000   \n",
       "2014-04-03 13:32:00  569.2292  569.5708           0.0            0.000   \n",
       "2014-04-03 13:33:00  569.7930  570.1350           0.0            0.000   \n",
       "2014-04-03 13:34:00  570.1889  570.5311           0.0            0.000   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.5129  173.6171           0.0            1.166   \n",
       "2025-06-18 20:57:00  173.3280  173.4320           0.0            1.166   \n",
       "2025-06-18 20:58:00  173.2580  173.3620           0.0            1.166   \n",
       "2025-06-18 20:59:00  173.2280  173.3320           0.0            1.166   \n",
       "2025-06-18 21:00:00  173.5576  173.6618           0.0            1.166   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2014-04-03 13:30:00        0.000        1.182741e-02  \n",
       "2014-04-03 13:31:00        0.000        1.738614e-02  \n",
       "2014-04-03 13:32:00        0.000        3.715507e-02  \n",
       "2014-04-03 13:33:00        0.000        7.616680e-02  \n",
       "2014-04-03 13:34:00        0.000        1.559958e-01  \n",
       "...                          ...                 ...  \n",
       "2025-06-18 20:56:00        3.876        4.528500e-09  \n",
       "2025-06-18 20:57:00        4.061        3.019000e-09  \n",
       "2025-06-18 20:58:00        4.131        2.012667e-09  \n",
       "2025-06-18 20:59:00        4.161        1.341778e-09  \n",
       "2025-06-18 21:00:00        3.831        8.945186e-10  \n",
       "\n",
       "[5895811 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/merged_{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = \"signal_smooth_norm\"\n",
    "\n",
    "feature_cols   = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "LOOK_BACK      = 60                                # minutes of history\n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "import datetime as dt\n",
    "rth_start      = dt.time(14, 30)                   # US cash-open in CET/CEST\n",
    "\n",
    "from pathlib import Path\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"best_{ticker}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1 ·  MODEL HYPER-PARAMETERS (tuned defaults)                                 #\n",
    "###############################################################################\n",
    "# ── architecture ───────────────────────────────────────────────────────\n",
    "SHORT_UNITS        = 64      # double → better GPU fill, little over-fit risk\n",
    "LONG_UNITS         = 128\n",
    "DROPOUT_SHORT      = 0.20    # applied AFTER the LSTM layer\n",
    "DROPOUT_LONG       = 0.15\n",
    "REC_DROP_SHORT     = 0.0     # MUST be 0.0 for fused kernel\n",
    "REC_DROP_LONG      = 0.0\n",
    "\n",
    "# ── optimiser : cosine-decay-restarts ──────────────────────────────────\n",
    "INITIAL_LR         = 4e-4    # slightly cooler for bigger network\n",
    "FIRST_DECAY_EPOCHS = 5       # first valley arrives early\n",
    "T_MUL              = 2.0\n",
    "M_MUL              = 1.0\n",
    "ALPHA              = 0.05\n",
    "LOSS_FN            = \"mse\"\n",
    "CLIPNORM           = 1.0\n",
    "\n",
    "# ── training control ───────────────────────────────────────────────────\n",
    "TRAIN_BATCH        = 64      # GPU now uses more threads; fits on 8-12 GB\n",
    "VAL_BATCH          = 1\n",
    "MAX_EPOCHS         = 90\n",
    "EARLY_STOP_PATIENCE= 15\n",
    "USE_FP16           = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1-A. per-day standard-scaling of *features*\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(\n",
    "                                   day_df[feature_cols])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col]     .to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2333971, 300)\n",
      "(2333971,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 2816  (multiple of 64)\n",
      "Validation days    : 614\n",
      "Test days          : 615\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (TRAIN_BATCH , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750421693.195245   29512 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=TRAIN_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "\n",
    "    x = layers.LSTM(short_units,\n",
    "                    stateful=True,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0, #dropout_short,\n",
    "                    recurrent_dropout=0, #rec_drop_short,\n",
    "                    kernel_initializer=\"orthogonal\",\n",
    "                    name=\"short_lstm\")(inp)\n",
    "    x = layers.Dropout(dropout_short)(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "\n",
    "    x = layers.LSTM(long_units,\n",
    "                    stateful=True,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0, #dropout_long,\n",
    "                    recurrent_dropout=0, #rec_drop_long,\n",
    "                    kernel_initializer=\"orthogonal\",\n",
    "                    name=\"long_lstm\")(x)\n",
    "    x = layers.Dropout(dropout_long)(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    model = models.Model(inp, out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,              # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,   # Total calendar days in the training epoch\n",
    "    max_epochs: int,     # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    ckpt_path           # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(ckpt_path)  # Save checkpoint of the best model.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.170713\n",
      "Training sees 2816 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886447ec49b34483bcb9372ba06bc9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750421720.120481   29588 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.635655 • val=0.226919 • impr=-32.9% • lr=3.54e-04 • g≈1793.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a7a55a7cd049408a87281c3145e502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.482743 • val=0.185564 • impr= -8.7% • lr=3.76e-04 • g≈1284.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c4ad27e696411795275d65b77ff0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.432474 • val=0.168692 • impr=  1.2% • lr=9.24e-05 • g≈4681.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e830bd712b6f4e379cf2c9754fc1946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.409593 • val=0.168083 • impr=  1.5% • lr=3.84e-04 • g≈1066.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f71d4f4f0d444abea594201e125d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.371635 • val=0.157641 • impr=  7.7% • lr=2.65e-04 • g≈1401.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686dfbcaf5d54ff6a1eb7bcb68562897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.345498 • val=0.149102 • impr= 12.7% • lr=1.08e-04 • g≈3212.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6e9bfe504246ff9f2a285a1c580abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/2816 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "today     = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "file_path = save_dir / f\"model_{ticker}_{today}.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ea52-a04f-4fff-bb69-73c1717dc459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
