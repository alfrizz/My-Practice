{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 18:08:08.837068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750608488.857398   58346 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750608488.864341   58346 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750608488.885127   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750608488.885165   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750608488.885168   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750608488.885170   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import stockanalibs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:07:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:08:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:09:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:10:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:11:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:49:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>174.02</td>\n",
       "      <td>173.8900</td>\n",
       "      <td>173.99</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>173.9378</td>\n",
       "      <td>174.0422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:50:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>173.98</td>\n",
       "      <td>163.4137</td>\n",
       "      <td>173.92</td>\n",
       "      <td>524.0</td>\n",
       "      <td>173.8678</td>\n",
       "      <td>173.9722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:51:00</th>\n",
       "      <td>173.970</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8933</td>\n",
       "      <td>173.90</td>\n",
       "      <td>3898.0</td>\n",
       "      <td>173.8478</td>\n",
       "      <td>173.9522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:52:00</th>\n",
       "      <td>173.908</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8600</td>\n",
       "      <td>173.97</td>\n",
       "      <td>779.0</td>\n",
       "      <td>173.9178</td>\n",
       "      <td>174.0222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:53:00</th>\n",
       "      <td>173.960</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8911</td>\n",
       "      <td>174.00</td>\n",
       "      <td>939.0</td>\n",
       "      <td>173.9478</td>\n",
       "      <td>174.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296484 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open    high       low   close  volume       bid  \\\n",
       "2014-04-03 13:07:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:08:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:09:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:10:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:11:00  570.500  570.50  570.5000  570.50   100.0  570.3288   \n",
       "...                      ...     ...       ...     ...     ...       ...   \n",
       "2025-06-18 20:49:00  173.980  174.02  173.8900  173.99  1604.0  173.9378   \n",
       "2025-06-18 20:50:00  173.980  173.98  163.4137  173.92   524.0  173.8678   \n",
       "2025-06-18 20:51:00  173.970  174.00  173.8933  173.90  3898.0  173.8478   \n",
       "2025-06-18 20:52:00  173.908  174.00  173.8600  173.97   779.0  173.9178   \n",
       "2025-06-18 20:53:00  173.960  174.00  173.8911  174.00   939.0  173.9478   \n",
       "\n",
       "                          ask  trade_action  StrategyEarning  EarningDiff  \\\n",
       "2014-04-03 13:07:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:08:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:09:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:10:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:11:00  570.6712             0              0.0        0.000   \n",
       "...                       ...           ...              ...          ...   \n",
       "2025-06-18 20:49:00  174.0422             0              0.0        1.395   \n",
       "2025-06-18 20:50:00  173.9722             0              0.0        1.465   \n",
       "2025-06-18 20:51:00  173.9522             0              0.0        1.485   \n",
       "2025-06-18 20:52:00  174.0222             0              0.0        1.415   \n",
       "2025-06-18 20:53:00  174.0522             0              0.0        1.385   \n",
       "\n",
       "                     signal_smooth_adjusted  \n",
       "2014-04-03 13:07:00                0.071363  \n",
       "2014-04-03 13:08:00                0.078887  \n",
       "2014-04-03 13:09:00                0.087231  \n",
       "2014-04-03 13:10:00                0.096444  \n",
       "2014-04-03 13:11:00                0.106627  \n",
       "...                                     ...  \n",
       "2025-06-18 20:49:00                0.000000  \n",
       "2025-06-18 20:50:00                0.000000  \n",
       "2025-06-18 20:51:00                0.000000  \n",
       "2025-06-18 20:52:00                0.000000  \n",
       "2025-06-18 20:53:00                0.000000  \n",
       "\n",
       "[1296484 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = \"signal_smooth_adjusted\"\n",
    "feature_cols   = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.25     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.20     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-4     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 3e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.05     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 64       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a eg 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1. per-day standard-scaling\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols])\n",
    "        day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col].to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1043134, 450)\n",
      "(1043134,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 1920  (multiple of 64)\n",
      "Validation days    : 422\n",
      "Test days          : 423\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (TRAIN_BATCH , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750608522.179188   58346 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=TRAIN_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,              # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,   # Total calendar days in the training epoch\n",
    "    max_epochs: int,     # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    ckpt_path           # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(ckpt_path)  # Save checkpoint of the best model.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.907639\n",
      "Training sees 1920 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d622bf387e6e4e3fb7f0a1fdb439dfd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750608545.197639   58421 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=1.176972 • val=0.825945 • impr=  9.0% • lr=5.67e-05 • g≈20744.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3377b531541450791de1f80e413ceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=1.052638 • val=0.784371 • impr= 13.6% • lr=1.03e-04 • g≈10223.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ea654acab4f05b50dec87e57b4a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=1.007424 • val=0.770674 • impr= 15.1% • lr=2.76e-04 • g≈3650.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb2dfd753fc436fa9db4d03f2b0267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.963808 • val=0.758859 • impr= 16.4% • lr=1.30e-04 • g≈7431.07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4988d3ab064b53a63bdf0f99588e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.940563 • val=0.756372 • impr= 16.7% • lr=1.77e-05 • g≈53025.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843512c0a9be4c4aae36f8776774079a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.925790 • val=0.754644 • impr= 16.9% • lr=2.83e-04 • g≈3269.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829f230634b9475299e4e3f6432edc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.897981 • val=0.742247 • impr= 18.2% • lr=2.25e-04 • g≈3996.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1862e17b33b4023bb672b475ff16aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 • train=0.877592 • val=0.741674 • impr= 18.3% • lr=1.44e-04 • g≈6114.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6334313824c568944631afc05552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 • train=0.866898 • val=0.741178 • impr= 18.3% • lr=6.71e-05 • g≈12919.69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1837811e20724a6cb54f85d54fc4f566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 • train=0.863116 • val=0.738399 • impr= 18.6% • lr=2.11e-05 • g≈40836.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f01ce70e3a426fbf0349e58c907c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 • train=0.858693 • val=0.743241 • impr= 18.1% • lr=2.98e-04 • g≈2877.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcbf3287de747918c3c1f53033098df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 • train=0.852004 • val=0.746422 • impr= 17.8% • lr=2.86e-04 • g≈2975.72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a9b289d8bc4ac896616f1bb81ffbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 • train=0.838488 • val=0.731849 • impr= 19.4% • lr=2.63e-04 • g≈3187.13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe46eedce884834bf72792fdbf9aa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 • train=0.826016 • val=0.724887 • impr= 20.1% • lr=2.31e-04 • g≈3579.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd768f1b86547dcad516f5bcf4e2e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 • train=0.816842 • val=0.723035 • impr= 20.3% • lr=1.92e-04 • g≈4251.63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b9345f861e42f99f877961b47d5b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 • train=0.811036 • val=0.722178 • impr= 20.4% • lr=1.51e-04 • g≈5388.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c8f1ee41544f65adf899eb91cab292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 • train=0.805597 • val=0.721502 • impr= 20.5% • lr=1.09e-04 • g≈7357.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abf30b0121d4188bce60ede3d7f31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 • train=0.801333 • val=0.722327 • impr= 20.4% • lr=7.26e-05 • g≈11035.69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be6122687e74db8bcaaef2d16147166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 • train=0.799972 • val=0.722249 • impr= 20.4% • lr=4.30e-05 • g≈18585.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730f47dcaeab4f1f8d43e27876c14a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 • train=0.799058 • val=0.720142 • impr= 20.7% • lr=2.33e-05 • g≈34250.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1921fa6ae48e49c997d958d958c54950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 • train=0.797729 • val=0.719837 • impr= 20.7% • lr=1.52e-05 • g≈52580.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636cf8d82a3d4082bd9a1577c17aac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 • train=0.798396 • val=0.725563 • impr= 20.1% • lr=2.99e-04 • g≈2670.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4019b2d7c7641288239553f6848d9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 • train=0.793793 • val=0.723351 • impr= 20.3% • lr=2.95e-04 • g≈2692.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53061262967241d890aabbcf1aaea0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 • train=0.789625 • val=0.726690 • impr= 19.9% • lr=2.88e-04 • g≈2743.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0aa5f7cf4e49409470512cc874d587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 • train=0.784571 • val=0.732374 • impr= 19.3% • lr=2.78e-04 • g≈2823.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea422cf7eb6b42858bd07a1be5d58aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 • train=0.779494 • val=0.720042 • impr= 20.7% • lr=2.65e-04 • g≈2937.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880c56d981f841a5a0ef4d27141f2f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 • train=0.774835 • val=0.708650 • impr= 21.9% • lr=2.51e-04 • g≈3092.20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f73cbf0502e4a0987b52d0e55289977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 • train=0.770460 • val=0.705814 • impr= 22.2% • lr=2.34e-04 • g≈3296.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3729188b0924b0290f5a9b1f8364947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 • train=0.765740 • val=0.704442 • impr= 22.4% • lr=2.15e-04 • g≈3557.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0c401e89c44a6e91f795ea6ef82af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 • train=0.762634 • val=0.701734 • impr= 22.7% • lr=1.96e-04 • g≈3900.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4beabe4600ee4cebb604c28e64993827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 • train=0.759706 • val=0.699960 • impr= 22.9% • lr=1.75e-04 • g≈4342.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ed309c9c3949589f8793519ce63117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 • train=0.756282 • val=0.697611 • impr= 23.1% • lr=1.54e-04 • g≈4910.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa4dc9e16154a8aba5dd7d5008d015c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 • train=0.753899 • val=0.699215 • impr= 23.0% • lr=1.33e-04 • g≈5662.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691b936522ee48389307e0aafb0c9205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 • train=0.752000 • val=0.698449 • impr= 23.0% • lr=1.13e-04 • g≈6666.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0479bd294e174fd397cde697afb3cee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "file_path = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
