{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f06a4f-691a-4a84-a305-e7212eb879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python’s garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "# 3) If you’re using PyTorch + CUDA, free any lingering GPU memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "from libs import models, plots, params\n",
    "importlib.reload(models)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "ticker         = params.ticker\n",
    "look_back      = params.look_back \n",
    "features_cols  = params.features_cols\n",
    "label_col      = params.label_col\n",
    "save_path      = params.save_path\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "regular_start_pred  = params.regular_start_pred\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = params.train_prop, params.val_prop\n",
    "\n",
    "# USE GPU if available, otherwise fallback to CPU\n",
    "device = params.device\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (attention-augmented tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64           # hidden size of each daily LSTM layer\n",
    "LONG_UNITS          = 96           # hidden size of the weekly LSTM\n",
    "DROPOUT_SHORT       = 0.3          # dropout after residual+attention block\n",
    "DROPOUT_LONG        = 0.4          # dropout after weekly LSTM outputs\n",
    "ATT_HEADS           = 4            # number of self-attention heads\n",
    "ATT_DROPOUT         = 0.2          # dropout rate inside attention\n",
    "WEIGHT_DECAY        = 1e-4         # L2 weight decay on all model weights\n",
    "\n",
    "# ── Training Control Parameters ────────────────────────────────────────────\n",
    "TRAIN_BATCH         = params.train_batch           \n",
    "VAL_BATCH           = params.val_batch  \n",
    "NUM_WORKERS         = params.num_workers\n",
    "MAX_EPOCHS          = 70          # upper limit on training epochs\n",
    "EARLY_STOP_PATIENCE = 7           # stop if no val-improve for this many epochs\n",
    "\n",
    "# ── Optimizer Settings ─────────────────────────────────────────────────────\n",
    "LR_EPOCHS_WARMUP    = 0            # epochs to wait before decreasing the LR  \n",
    "INITIAL_LR          = 1.2e-3       # AdamW initial learning rate\n",
    "CLIPNORM            = 0.5          # max-norm gradient clipping\n",
    "\n",
    "# ── CosineAnnealingWarmRestarts Scheduler ─────────────────────────────────\n",
    "T_0                 = MAX_EPOCHS   # epochs before first cosine restart\n",
    "T_MULT              = 1            # cycle length multiplier after each restart\n",
    "ETA_MIN             = 5e-5         # floor LR in each cosine cycle\n",
    "\n",
    "# ── ReduceLROnPlateau Scheduler ───────────────────────────────────────────\n",
    "PLATEAU_FACTOR      = 0.9          # multiply LR by this factor on plateau\n",
    "PLATEAU_PATIENCE    = 0            # epochs with no val-improve before LR cut\n",
    "MIN_LR              = 1e-6         # lower bound on LR after reductions\n",
    "PLAT_EPOCHS_WARMUP  = MAX_EPOCHS   # epochs to wait before triggering plateau logic      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08b80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step D: saving final CSV …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>r_1</th>\n",
       "      <th>r_5</th>\n",
       "      <th>r_15</th>\n",
       "      <th>vol_15</th>\n",
       "      <th>volume_spike</th>\n",
       "      <th>vwap_dev</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:06:00</th>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>4580.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.568641</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.636251</td>\n",
       "      <td>28.653438</td>\n",
       "      <td>0.327384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:07:00</th>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>4540.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.570338</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.631098</td>\n",
       "      <td>28.648282</td>\n",
       "      <td>0.328556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:08:00</th>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.574408</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.625944</td>\n",
       "      <td>28.643125</td>\n",
       "      <td>0.329833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:09:00</th>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>4460.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.581017</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.620791</td>\n",
       "      <td>28.637968</td>\n",
       "      <td>0.331213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:10:00</th>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>4420.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.590413</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.615637</td>\n",
       "      <td>28.632811</td>\n",
       "      <td>0.332697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.375000</td>\n",
       "      <td>173.677100</td>\n",
       "      <td>173.215000</td>\n",
       "      <td>173.565000</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>2.462713</td>\n",
       "      <td>1.248428</td>\n",
       "      <td>17.019768</td>\n",
       "      <td>173.512900</td>\n",
       "      <td>173.617100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.565000</td>\n",
       "      <td>173.590000</td>\n",
       "      <td>173.240000</td>\n",
       "      <td>173.380000</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.005063</td>\n",
       "      <td>-0.010671</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>2.154838</td>\n",
       "      <td>1.246015</td>\n",
       "      <td>11.648165</td>\n",
       "      <td>173.328000</td>\n",
       "      <td>173.432000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.390000</td>\n",
       "      <td>173.410000</td>\n",
       "      <td>173.200000</td>\n",
       "      <td>173.310000</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.011816</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>1.439161</td>\n",
       "      <td>1.245096</td>\n",
       "      <td>11.384870</td>\n",
       "      <td>173.258000</td>\n",
       "      <td>173.362000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.315000</td>\n",
       "      <td>173.400000</td>\n",
       "      <td>173.230000</td>\n",
       "      <td>173.280000</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>-0.011932</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>2.836382</td>\n",
       "      <td>1.244678</td>\n",
       "      <td>11.830567</td>\n",
       "      <td>173.228000</td>\n",
       "      <td>173.332000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.300000</td>\n",
       "      <td>174.050000</td>\n",
       "      <td>173.170000</td>\n",
       "      <td>173.609700</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>8.568493</td>\n",
       "      <td>1.248745</td>\n",
       "      <td>22.962317</td>\n",
       "      <td>173.557600</td>\n",
       "      <td>173.661800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1651679 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           open        high         low       close  \\\n",
       "2014-04-03 12:06:00   28.644845   28.644845   28.644845   28.644845   \n",
       "2014-04-03 12:07:00   28.639690   28.639690   28.639690   28.639690   \n",
       "2014-04-03 12:08:00   28.634534   28.634534   28.634534   28.634534   \n",
       "2014-04-03 12:09:00   28.629379   28.629379   28.629379   28.629379   \n",
       "2014-04-03 12:10:00   28.624224   28.624224   28.624224   28.624224   \n",
       "...                         ...         ...         ...         ...   \n",
       "2025-06-18 20:56:00  173.375000  173.677100  173.215000  173.565000   \n",
       "2025-06-18 20:57:00  173.565000  173.590000  173.240000  173.380000   \n",
       "2025-06-18 20:58:00  173.390000  173.410000  173.200000  173.310000   \n",
       "2025-06-18 20:59:00  173.315000  173.400000  173.230000  173.280000   \n",
       "2025-06-18 21:00:00  173.300000  174.050000  173.170000  173.609700   \n",
       "\n",
       "                        volume       r_1       r_5      r_15    vol_15  \\\n",
       "2014-04-03 12:06:00     4580.0 -0.000180 -0.000180 -0.000180  0.000046   \n",
       "2014-04-03 12:07:00     4540.0 -0.000180 -0.000360 -0.000360  0.000063   \n",
       "2014-04-03 12:08:00     4500.0 -0.000180 -0.000540 -0.000540  0.000075   \n",
       "2014-04-03 12:09:00     4460.0 -0.000180 -0.000720 -0.000720  0.000082   \n",
       "2014-04-03 12:10:00     4420.0 -0.000180 -0.000900 -0.000900  0.000088   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "2025-06-18 20:56:00   621199.0  0.001124 -0.004226 -0.009661  0.001493   \n",
       "2025-06-18 20:57:00   624198.0 -0.001066 -0.005063 -0.010671  0.001487   \n",
       "2025-06-18 20:58:00   454542.0 -0.000404 -0.005811 -0.011816  0.001436   \n",
       "2025-06-18 20:59:00  1094746.0 -0.000173 -0.004434 -0.011932  0.001432   \n",
       "2025-06-18 21:00:00  7649838.0  0.001901  0.001382 -0.009290  0.001592   \n",
       "\n",
       "                     volume_spike  vwap_dev     rsi_14         bid  \\\n",
       "2014-04-03 12:06:00      0.568641 -0.000177   0.000000   28.636251   \n",
       "2014-04-03 12:07:00      0.570338 -0.000352   0.000000   28.631098   \n",
       "2014-04-03 12:08:00      0.574408 -0.000524   0.000000   28.625944   \n",
       "2014-04-03 12:09:00      0.581017 -0.000694   0.000000   28.620791   \n",
       "2014-04-03 12:10:00      0.590413 -0.000862   0.000000   28.615637   \n",
       "...                           ...       ...        ...         ...   \n",
       "2025-06-18 20:56:00      2.462713  1.248428  17.019768  173.512900   \n",
       "2025-06-18 20:57:00      2.154838  1.246015  11.648165  173.328000   \n",
       "2025-06-18 20:58:00      1.439161  1.245096  11.384870  173.258000   \n",
       "2025-06-18 20:59:00      2.836382  1.244678  11.830567  173.228000   \n",
       "2025-06-18 21:00:00      8.568493  1.248745  22.962317  173.557600   \n",
       "\n",
       "                            ask  signal_smooth  \n",
       "2014-04-03 12:06:00   28.653438       0.327384  \n",
       "2014-04-03 12:07:00   28.648282       0.328556  \n",
       "2014-04-03 12:08:00   28.643125       0.329833  \n",
       "2014-04-03 12:09:00   28.637968       0.331213  \n",
       "2014-04-03 12:10:00   28.632811       0.332697  \n",
       "...                         ...            ...  \n",
       "2025-06-18 20:56:00  173.617100       0.000000  \n",
       "2025-06-18 20:57:00  173.432000       0.000000  \n",
       "2025-06-18 20:58:00  173.362000       0.000000  \n",
       "2025-06-18 20:59:00  173.332000       0.000000  \n",
       "2025-06-18 21:00:00  173.661800       0.000000  \n",
       "\n",
       "[1651679 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURES ENGINEERING\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_ready.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# 1) engineer top intraday features on your 1-min bars\n",
    "for lag in (1, 5, 15):\n",
    "    df[f\"r_{lag}\"] = np.log(df[\"close\"] / df[\"close\"].shift(lag))\n",
    "\n",
    "df[\"vol_15\"]        = df[\"r_1\"].rolling(15).std()\n",
    "df[\"volume_spike\"]  = df[\"volume\"] / df[\"volume\"].rolling(15).mean()\n",
    "\n",
    "typ_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "vwap      = (typ_price * df[\"volume\"]).cumsum() / df[\"volume\"].cumsum()\n",
    "df[\"vwap_dev\"]      = (df[\"close\"] - vwap) / vwap\n",
    "\n",
    "delta     = df[\"close\"].diff()\n",
    "gain      = delta.clip(lower=0)\n",
    "loss      = -delta.clip(upper=0)\n",
    "avg_gain  = gain.rolling(14).mean()\n",
    "avg_loss  = loss.rolling(14).mean()\n",
    "rs        = avg_gain / avg_loss\n",
    "df[\"rsi_14\"]        = 100 - (100 / (1 + rs))\n",
    "\n",
    "# 2) keep only your label plus the most predictive features\n",
    "\n",
    "df = df[features_cols  + ['bid', 'ask'] + [label_col]].dropna()\n",
    "\n",
    "print(\"\\n Step D: saving final CSV …\")\n",
    "out_path = save_path / f\"{ticker}_final.csv\"\n",
    "df.to_csv(out_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = models.build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=look_back,\n",
    "    features_cols=features_cols,\n",
    "    label_col=label_col,\n",
    "    regular_start=regular_start_pred\n",
    ")\n",
    "\n",
    "# 1)\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) \n",
    "\n",
    "\n",
    "# 2) quick shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"  X         =\", X.shape,    \"(samples, features, look_back)\")\n",
    "print(\"  y         =\", y.shape,    \"(samples,)\")\n",
    "print(\"  raw_close =\", raw_close.shape)\n",
    "print(\"  raw_bid   =\", raw_bid.shape)\n",
    "print(\"  raw_ask   =\", raw_ask.shape)\n",
    "\n",
    "# 3) rebuild the list of label‐timestamps (window‐ends)\n",
    "ends = []\n",
    "for day, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "    day_df = day_df.sort_index()\n",
    "    # candidate ends at positions look_back .. end\n",
    "    idxs = day_df.index[look_back:]\n",
    "    # only keep those at/after regular_start_pred\n",
    "    mask = [t >= regular_start_pred for t in idxs.time]\n",
    "    ends.extend(idxs[mask])\n",
    "\n",
    "# 4) show first few ends\n",
    "print(\"\\nFirst 5 window‐end timestamps:\")\n",
    "for ts in ends[:5]:\n",
    "    print(\" \", ts)\n",
    "\n",
    "# 5) show exactly which minutes X[0] contains\n",
    "first_end   = ends[0]\n",
    "first_start = first_end - pd.Timedelta(minutes=look_back)\n",
    "print(f\"\\nFirst window covers {look_back} bars from\")\n",
    "print(f\"  {first_start}  →  {first_end - pd.Timedelta(minutes=1)}\")\n",
    "print(f\"and predicts the bar at {first_end}\")\n",
    "\n",
    "print(\"\\nThose bars (timestamps):\")\n",
    "print(pd.date_range(first_start, first_end - pd.Timedelta(minutes=1), freq=\"1min\"))\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "y_np         = y.cpu().numpy()\n",
    "print(\"First 5 values y:\",         y_np[:5])\n",
    "print(\"First 5 signal_smooth values, on regular trade time:\")\n",
    "df.signal_smooth.iloc[look_back:look_back+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test by calendar day\n",
    "(X_tr, y_tr), \\\n",
    "(X_val, y_val), \\\n",
    "(X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = models.chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back   = look_back,\n",
    "    regular_start   = regular_start_pred,\n",
    "    train_prop  = TRAIN_PROP,\n",
    "    val_prop    = VAL_PROP,\n",
    "    train_batch = TRAIN_BATCH\n",
    ")\n",
    "\n",
    "# 1) Print shapes of all tensors\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_tr        =\", X_tr.shape)\n",
    "print(\"  y_tr        =\", y_tr.shape)\n",
    "print(\"  raw_close_te=\", raw_close_te.shape)\n",
    "print(\"  raw_bid_te  =\", raw_bid_te.shape)\n",
    "print(\"  raw_ask_te  =\", raw_ask_te.shape)\n",
    "\n",
    "# 2) Print number of days in each split\n",
    "n_tr_days = torch.unique(day_id_tr).numel()\n",
    "n_val_days= torch.unique(day_id_val).numel()\n",
    "n_te_days = torch.unique(day_id_te).numel()\n",
    "print(f\"\\nDays: train={n_tr_days}, val={n_val_days}, test={n_te_days}\")\n",
    "\n",
    "# 3) Print number of windows in each split\n",
    "print(f\"Windows: train={X_tr.shape[0]}, val={X_val.shape[0]}, test={X_te.shape[0]}\")\n",
    "\n",
    "# 4) List the first few window‐end timestamps\n",
    "ends = []\n",
    "for day, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "    ts = day_df.index[look_back:]\n",
    "    ends.extend(ts[ts.time >= regular_start_pred])\n",
    "first_ends = ends[:5]\n",
    "print(\"\\nFirst 5 window‐end times:\", first_ends)\n",
    "\n",
    "# 5) Show exactly which minutes X_tr[0] covers, and where y_tr[0] sits\n",
    "first_end   = first_ends[0]\n",
    "first_start = first_end - pd.Timedelta(minutes=look_back)\n",
    "# input bars = [first_start … first_end − 1min]\n",
    "print(f\"\\nX_tr[0] covers bars from {first_start} to {first_end - pd.Timedelta(minutes=1)}\")\n",
    "print(\"Those timestamps:\")\n",
    "print(pd.date_range(first_start, first_end - pd.Timedelta(minutes=1), freq=\"1min\"))\n",
    "print(f\"y_tr[0] (and raw_close_te[0]) is the bar at {first_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9b6c4-3d79-45c0-b2c0-c4f46f1ad866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Build DataLoaders over calendar‐days\n",
    "# -----------------------------------------------------------------------------\n",
    "train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "    # Training split arrays (from chronological_split)\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    # Validation split arrays\n",
    "    X_val, y_val, day_id_val,\n",
    "    # Test split arrays + raw prices for post‐tracking\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    # Original minute‐bar DataFrame for weekday mapping\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    train_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c2ca7-ca1e-4336-9f80-3a339b8184cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer_and_scheduler(\n",
    "    model: nn.Module,\n",
    "    initial_lr: float       = INITIAL_LR,\n",
    "    weight_decay: float     = WEIGHT_DECAY,\n",
    "    clipnorm: float         = CLIPNORM,\n",
    "    plateau_factor: float   = PLATEAU_FACTOR,\n",
    "    plateau_patience: int   = PLATEAU_PATIENCE,\n",
    "    plateau_min_lr: float   = MIN_LR,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) AdamW with decoupled weight decay.\n",
    "    2) ReduceLROnPlateau: reduces LR when val‐RMSE stops improving.\n",
    "    3) CosineAnnealingWarmRestarts: per‐batch cosine schedule.\n",
    "    4) GradScaler for mixed precision.\n",
    "    \"\"\"\n",
    "    # AdamW optimizer with L2 regularization\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # LR ↓ when validation RMSE plateaus\n",
    "    plateau_sched = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=plateau_factor,\n",
    "        patience=plateau_patience,\n",
    "        min_lr=plateau_min_lr,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Cosine warm‐restarts scheduler (batch-level stepping)\n",
    "    cosine_sched = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=T_0, \n",
    "        T_mult=T_MULT, \n",
    "        eta_min=ETA_MIN\n",
    "    )\n",
    "\n",
    "    # AMP scaler for fp16 stability\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    return optimizer, plateau_sched, cosine_sched, scaler, clipnorm\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model:     nn.Module,\n",
    "    x_day:     torch.Tensor,    # (W, look_back, F), on device already\n",
    "    y_day:     torch.Tensor,    # (W,), on device already\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler:    GradScaler,\n",
    "    clipnorm:  float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Single‐day step:\n",
    "      1) zero grads\n",
    "      2) fp16 forward+loss\n",
    "      3) backward with scaler → unscale → clip → step → update scaler\n",
    "      4) return scalar loss\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    model.train()\n",
    "\n",
    "    device_type = x_day.device.type\n",
    "    # Mixed‐precision forward\n",
    "    with autocast(device_type=device_type):\n",
    "        out  = model(x_day)         # → (W, seq_len, 1)\n",
    "        last = out[:, -1, 0]        # → (W,)\n",
    "        loss = Funct.mse_loss(last, y_day, reduction='mean')\n",
    "\n",
    "    # Backward + clip + optimizer step\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)     # bring grads to fp32 for clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be13563-aee4-43e7-ac9c-064c0e38b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stateful_training_loop(\n",
    "    model:         torch.nn.Module,\n",
    "    optimizer:     torch.optim.Optimizer,\n",
    "    cosine_sched:  CosineAnnealingWarmRestarts,\n",
    "    plateau_sched: ReduceLROnPlateau,\n",
    "    scaler:        GradScaler,\n",
    "    train_loader:  torch.utils.data.DataLoader,\n",
    "    val_loader:    torch.utils.data.DataLoader,\n",
    "    *,\n",
    "    max_epochs:          int     = MAX_EPOCHS,\n",
    "    early_stop_patience: int     = EARLY_STOP_PATIENCE,\n",
    "    baseline_val_rmse:   float   = None,\n",
    "    clipnorm:            float   = CLIPNORM,\n",
    "    plateau_warmup:      int     = PLAT_EPOCHS_WARMUP,\n",
    "    device:              torch.device = torch.device(\"cpu\"),\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Full training loop:\n",
    "      • CosineAnnealingWarmRestarts stepping per batch, with restart-print\n",
    "      • ReduceLROnPlateau stepping per epoch after warmup, with reduction-print\n",
    "      • Mixed precision + gradient clipping\n",
    "      • Per-day & per-week LSTM state resets\n",
    "      • Early stopping + best-model checkpoint\n",
    "      • When plateau cuts LR, cosine scheduler is reset to continue from new LR\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state    = None\n",
    "    patience_ctr  = 0\n",
    "    live_plot     = plots.LiveRMSEPlot()\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        gc.collect()\n",
    "        # print(f\"[Epoch {epoch}] GPU alloc = {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n",
    "\n",
    "        # ── TRAIN ─────────────────────────────────────────────────────────\n",
    "        model.train()\n",
    "        model.h_short = model.h_long = None\n",
    "        train_losses = []\n",
    "        pbar = tqdm(enumerate(train_loader),\n",
    "                    total=len(train_loader),\n",
    "                    desc=f\"Epoch {epoch}\",\n",
    "                    unit=\"bundle\")\n",
    "        for batch_idx, (xb_days, yb_days, wd_days) in pbar:\n",
    "            xb_days, yb_days = xb_days.to(device), yb_days.to(device)\n",
    "            wd_days          = wd_days.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            prev_wd = None\n",
    "\n",
    "            for di in range(xb_days.size(0)):\n",
    "                wd = int(wd_days[di].item())\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                # Mixed-precision forward/backward\n",
    "                with autocast(device_type=device.type):\n",
    "                    out  = model(xb_days[di])\n",
    "                    last = out[..., -1, 0]\n",
    "                    loss = Funct.mse_loss(last, yb_days[di], reduction='mean')\n",
    "                scaler.scale(loss).backward()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                # Detach hidden states to truncate graph\n",
    "                if isinstance(model.h_short, tuple):\n",
    "                    model.h_short = tuple(h.detach() for h in model.h_short)\n",
    "                if isinstance(model.h_long, tuple):\n",
    "                    model.h_long  = tuple(h.detach() for h in model.h_long)\n",
    "                del out, last, loss\n",
    "\n",
    "            # Unscale → clip → step → update scaler\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Cosine scheduler (fractional epoch) + restart print\n",
    "            prev_lr_cos = optimizer.param_groups[0]['lr']\n",
    "            frac_epoch  = epoch - 1 + batch_idx / len(train_loader)\n",
    "            cosine_sched.step(frac_epoch)\n",
    "            new_lr_cos  = optimizer.param_groups[0]['lr']\n",
    "            if new_lr_cos > prev_lr_cos:\n",
    "                print(f\"  [Cosine restart] LR {prev_lr_cos:.2e} → {new_lr_cos:.2e}\"\n",
    "                      f\" at epoch {epoch}, batch {batch_idx}\")\n",
    "\n",
    "            # Logging\n",
    "            rmse = math.sqrt(sum(train_losses) / len(train_losses))\n",
    "            lr   = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix(train_rmse=rmse, lr=lr, refresh=False)\n",
    "            pbar.update(0)\n",
    "            gc.collect()\n",
    "        pbar.close()\n",
    "\n",
    "        # ── VALIDATE ───────────────────────────────────────────────────────\n",
    "        model.eval()\n",
    "        model.h_short = model.h_long = None\n",
    "        val_losses = []\n",
    "        prev_wd    = None\n",
    "        with torch.no_grad():\n",
    "            for xb_day, yb_day, wd in val_loader:\n",
    "                wd = int(wd.item())\n",
    "                x  = xb_day[0].to(device)\n",
    "                y  = yb_day.view(-1).to(device)\n",
    "\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                out  = model(x)\n",
    "                last = out[..., -1, 0]\n",
    "                val_losses.append(Funct.mse_loss(last, y, reduction='mean').item())\n",
    "                del xb_day, yb_day, x, y, out, last\n",
    "\n",
    "        val_rmse = math.sqrt(sum(val_losses) / len(val_losses))\n",
    "\n",
    "        # Live plot & print\n",
    "        live_plot.update(rmse, val_rmse)\n",
    "        print(f\"Epoch {epoch:03d} • train={rmse:.4f} • val={val_rmse:.4f}\"\n",
    "              f\" • lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # ── ReduceLROnPlateau (after warmup) + reduction print ──────────\n",
    "        pre_lr = optimizer.param_groups[0]['lr']\n",
    "        if epoch > plateau_warmup:\n",
    "            plateau_sched.step(val_rmse)\n",
    "        post_lr = optimizer.param_groups[0]['lr']\n",
    "        if post_lr < pre_lr:\n",
    "            print(f\"  [Plateau cut] LR {pre_lr:.1e} → {post_lr:.1e}\"\n",
    "                  f\" at epoch {epoch}\")\n",
    "            # — update cosine scheduler to continue from new LR —\n",
    "            cosine_sched.base_lrs = [post_lr for _ in cosine_sched.base_lrs]\n",
    "            cosine_sched.last_epoch = epoch - 1\n",
    "\n",
    "        # ── Early stopping ────────────────────────────────────────────────\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_state    = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr  = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping at epoch\", epoch)\n",
    "                break\n",
    "\n",
    "    # ── Save best model weights ──────────────────────────────────────────\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    ckpt_file = params.save_path / f\"{params.ticker}_{best_val_rmse:.4f}.pth\"\n",
    "    torch.save(model.state_dict(), ckpt_file)\n",
    "    print(f\"Saved best model to {ckpt_file}\")\n",
    "\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATS = len(features_cols)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the stateful DualMemoryLSTM & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "model = models.DualMemoryLSTM(\n",
    "    n_feats        = N_FEATS,        # number of input features per minute\n",
    "    short_units    = SHORT_UNITS,    # hidden size of daily LSTM\n",
    "    long_units     = LONG_UNITS,     # hidden size of weekly LSTM\n",
    "    dropout_short  = DROPOUT_SHORT,  # dropout after daily LSTM\n",
    "    dropout_long   = DROPOUT_LONG,    # dropout after weekly LSTM\n",
    "    att_heads      = ATT_HEADS,\n",
    "    att_drop       = ATT_DROPOUT\n",
    ")\n",
    "model.to(device)   # place model parameters on GPU or CPU as specified\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512a0dd-d2c8-418e-bfca-4580fb4be995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute plateau_sched timing parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# Total training samples = total windows in X_tr (one window per row)\n",
    "n_train_samples = X_tr.shape[0]\n",
    "\n",
    "# How many optimizer steps (day‐bundles) constitute one epoch?\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build optimizer, LR scheduler, AMP scaler, and gradient‐clip norm\n",
    "# -----------------------------------------------------------------------------\n",
    "optimizer, plateau_sched, cosine_sched, scaler, clipnorm = make_optimizer_and_scheduler(\n",
    "    model,\n",
    "    initial_lr        = INITIAL_LR,       \n",
    "    weight_decay      = WEIGHT_DECAY,     \n",
    "    clipnorm          = CLIPNORM,        \n",
    "    plateau_factor    = PLATEAU_FACTOR,  \n",
    "    plateau_patience  = PLATEAU_PATIENCE, \n",
    "    plateau_min_lr    = MIN_LR           \n",
    ")\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70105f-bbe5-4ce0-aabe-acf9193ef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1) Move model to CPU and build a fresh optimizer (no scheduler metadata)\n",
    "# model_cpu = model.cpu()\n",
    "# optimizer_cpu = torch.optim.AdamW(\n",
    "#     model_cpu.parameters(),\n",
    "#     lr=1e-3,        # placeholder; the finder will override this\n",
    "#     weight_decay=5e-4\n",
    "# )\n",
    "\n",
    "# # 2) Create a tiny DataLoader (batch_size=1) to save memory\n",
    "# small_loader = DataLoader(\n",
    "#     train_loader.dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# # 3) Define an aligned MSE that permutes/expands your [1,1,D] or [D,1,1]\n",
    "# #    target → [D, T, 1] to match output shape exactly.\n",
    "# def aligned_mse(output, target):\n",
    "#     # output: [D, T, 1]\n",
    "#     # target might come in as [D,1,1] or [1,1,D]\n",
    "#     tgt = target\n",
    "\n",
    "#     # Case A: target == [D, 1, 1] → expand middle dim to T\n",
    "#     if tgt.dim() == 3 and tgt.shape[0] == output.shape[0] \\\n",
    "#        and tgt.shape[1] == 1 and tgt.shape[2] == 1:\n",
    "#         tgt = tgt.expand(-1, output.size(1), -1)\n",
    "\n",
    "#     # Case B: target == [1, 1, D] → permute to [D,1,1] then expand\n",
    "#     elif tgt.dim() == 3 and tgt.shape[0] == 1 \\\n",
    "#          and tgt.shape[1] == 1 and tgt.shape[2] == output.shape[0]:\n",
    "#         # permute (0,1,2) → (2,1,0) to get [D,1,1]\n",
    "#         tgt = tgt.permute(2, 1, 0)\n",
    "#         tgt = tgt.expand(-1, output.size(1), -1)\n",
    "\n",
    "#     else:\n",
    "#         # fallback: broadcast to exactly output.shape\n",
    "#         tgt = tgt.expand(output.shape)\n",
    "\n",
    "#     return Funct.mse_loss(output, tgt)\n",
    "\n",
    "# # 4) Free any lingering GPU memory\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # 5) Run the LR‐Finder on CPU for just 30 mini‐batches\n",
    "# lr_finder = LRFinder(\n",
    "#     model_cpu,\n",
    "#     optimizer_cpu,\n",
    "#     aligned_mse,\n",
    "#     device=\"cpu\"\n",
    "# )\n",
    "# lr_finder.range_test(\n",
    "#     small_loader,\n",
    "#     end_lr=1,     # maximum LR to try\n",
    "#     num_iter=30   # number of batches\n",
    "# )\n",
    "# lr_finder.plot()   # examine loss vs. LR curve\n",
    "# lr_finder.reset()  # restore original model & optimizer states\n",
    "\n",
    "# # 6) Move model back to GPU for your main training\n",
    "# model = model_cpu.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Count how many calendar days we see each epoch\n",
    "# -----------------------------------------------------------------------------\n",
    "n_train_days = len(train_loader.dataset)  # dataset length = # unique days\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute baseline RMSE on validation (zero forecast)\n",
    "# -----------------------------------------------------------------------------\n",
    "baseline_val_rmse = models.naive_rmse(val_loader)\n",
    "print(f\"Baseline (zero‐forecast) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    cosine_sched        = cosine_sched,\n",
    "    plateau_sched       = plateau_sched,\n",
    "    scaler              = scaler,\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = MAX_EPOCHS,\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "    baseline_val_rmse   = baseline_val_rmse,\n",
    "    clipnorm            = clipnorm,\n",
    "    plateau_warmup      = PLAT_EPOCHS_WARMUP,   \n",
    "    device              = device\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final reporting: best RMSE and relative improvement\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "\n",
    "improvement = 100.0 * (1.0 - best_val_rmse / baseline_val_rmse)\n",
    "print(f\"Improvement over zero‐baseline = {improvement:5.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e0cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8660dd-d2db-434a-aa59-17814d343fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
