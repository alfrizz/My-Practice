{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 18:45:20.388156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750178720.407894   64822 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750178720.414657   64822 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750178720.430045   64822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750178720.430084   64822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750178720.430087   64822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750178720.430089   64822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:30:00</th>\n",
       "      <td>250.5906</td>\n",
       "      <td>250.6435</td>\n",
       "      <td>250.5244</td>\n",
       "      <td>250.5753</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>250.5001</td>\n",
       "      <td>250.6505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:31:00</th>\n",
       "      <td>250.5806</td>\n",
       "      <td>250.6317</td>\n",
       "      <td>250.5121</td>\n",
       "      <td>250.5606</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>250.4854</td>\n",
       "      <td>250.6358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:32:00</th>\n",
       "      <td>250.5712</td>\n",
       "      <td>250.6200</td>\n",
       "      <td>250.4938</td>\n",
       "      <td>250.5453</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>250.4701</td>\n",
       "      <td>250.6205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:33:00</th>\n",
       "      <td>250.5580</td>\n",
       "      <td>250.6094</td>\n",
       "      <td>250.4762</td>\n",
       "      <td>250.5347</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>250.4595</td>\n",
       "      <td>250.6099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:34:00</th>\n",
       "      <td>250.5491</td>\n",
       "      <td>250.5994</td>\n",
       "      <td>250.4600</td>\n",
       "      <td>250.5168</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>250.4416</td>\n",
       "      <td>250.5919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:56:00</th>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3500</td>\n",
       "      <td>203.2450</td>\n",
       "      <td>203.3200</td>\n",
       "      <td>189023.0</td>\n",
       "      <td>203.2590</td>\n",
       "      <td>203.3810</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:57:00</th>\n",
       "      <td>203.3200</td>\n",
       "      <td>203.4200</td>\n",
       "      <td>203.3050</td>\n",
       "      <td>203.3800</td>\n",
       "      <td>222383.0</td>\n",
       "      <td>203.3190</td>\n",
       "      <td>203.4410</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:58:00</th>\n",
       "      <td>203.3800</td>\n",
       "      <td>203.4300</td>\n",
       "      <td>203.3322</td>\n",
       "      <td>203.3750</td>\n",
       "      <td>279702.0</td>\n",
       "      <td>203.3140</td>\n",
       "      <td>203.4360</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:59:00</th>\n",
       "      <td>203.3700</td>\n",
       "      <td>203.4100</td>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>724307.0</td>\n",
       "      <td>203.2790</td>\n",
       "      <td>203.4010</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 21:00:00</th>\n",
       "      <td>203.3288</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>202.8400</td>\n",
       "      <td>203.1993</td>\n",
       "      <td>11076221.0</td>\n",
       "      <td>203.1383</td>\n",
       "      <td>203.2603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46904 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close      volume  \\\n",
       "2025-01-02 13:30:00  250.5906  250.6435  250.5244  250.5753      2259.0   \n",
       "2025-01-02 13:31:00  250.5806  250.6317  250.5121  250.5606      2351.0   \n",
       "2025-01-02 13:32:00  250.5712  250.6200  250.4938  250.5453      2455.0   \n",
       "2025-01-02 13:33:00  250.5580  250.6094  250.4762  250.5347      2474.0   \n",
       "2025-01-02 13:34:00  250.5491  250.5994  250.4600  250.5168      2792.0   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2025-06-03 20:56:00  203.2500  203.3500  203.2450  203.3200    189023.0   \n",
       "2025-06-03 20:57:00  203.3200  203.4200  203.3050  203.3800    222383.0   \n",
       "2025-06-03 20:58:00  203.3800  203.4300  203.3322  203.3750    279702.0   \n",
       "2025-06-03 20:59:00  203.3700  203.4100  203.2500  203.3400    724307.0   \n",
       "2025-06-03 21:00:00  203.3288  203.3400  202.8400  203.1993  11076221.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2025-01-02 13:30:00  250.5001  250.6505             0             0.00   \n",
       "2025-01-02 13:31:00  250.4854  250.6358             0             0.00   \n",
       "2025-01-02 13:32:00  250.4701  250.6205             0             0.00   \n",
       "2025-01-02 13:33:00  250.4595  250.6099             0             0.00   \n",
       "2025-01-02 13:34:00  250.4416  250.5919             0             0.00   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-03 20:56:00  203.2590  203.3810             0             1.99   \n",
       "2025-06-03 20:57:00  203.3190  203.4410             0             1.99   \n",
       "2025-06-03 20:58:00  203.3140  203.4360             0             1.99   \n",
       "2025-06-03 20:59:00  203.2790  203.4010             0             1.99   \n",
       "2025-06-03 21:00:00  203.1383  203.2603             0             1.99   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2025-01-02 13:30:00        0.000                 0.0  \n",
       "2025-01-02 13:31:00        0.000                 0.0  \n",
       "2025-01-02 13:32:00        0.000                 0.0  \n",
       "2025-01-02 13:33:00        0.000                 0.0  \n",
       "2025-01-02 13:34:00        0.000                 0.0  \n",
       "...                          ...                 ...  \n",
       "2025-06-03 20:56:00        0.942                 0.0  \n",
       "2025-06-03 20:57:00        0.882                 0.0  \n",
       "2025-06-03 20:58:00        0.887                 0.0  \n",
       "2025-06-03 20:59:00        0.922                 0.0  \n",
       "2025-06-03 21:00:00        1.062                 0.0  \n",
       "\n",
       "[46904 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/merged_{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = \"signal_smooth_norm\"\n",
    "\n",
    "feature_cols   = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "LOOK_BACK      = 60                                # minutes of history\n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "import datetime as dt\n",
    "rth_start      = dt.time(14, 30)                   # US cash-open in CET/CEST\n",
    "\n",
    "from pathlib import Path\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"best_{ticker}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1 ·  MODEL HYPER-PARAMETERS (tuned defaults)                                 #\n",
    "###############################################################################\n",
    "# — architecture —\n",
    "SHORT_UNITS        = 32\n",
    "LONG_UNITS         = 96\n",
    "DROPOUT_SHORT      = 0.15     # keep modest; >0.2 harms memory\n",
    "DROPOUT_LONG       = 0.10     # slightly lower on long tier\n",
    "REC_DROP_SHORT     = 0.05\n",
    "REC_DROP_LONG      = 0.05\n",
    "\n",
    "# — optimiser : cosine-decay-restarts —\n",
    "INITIAL_LR         = 5e-4     # cooler start; prevents early blow-ups\n",
    "FIRST_DECAY_EPOCHS = 3        # first valley after epoch-3\n",
    "T_MUL              = 2.0      # cycle length doubles each restart\n",
    "M_MUL              = 1.0\n",
    "ALPHA              = 0.05     # 5 % of initial_lr is the floor\n",
    "LOSS_FN            = \"mse\"\n",
    "CLIPNORM           = 1\n",
    "\n",
    "# — training control —\n",
    "TRAIN_BATCH        = 32       # fat batches maximise GPU utilisation\n",
    "VAL_BATCH          = 1        # 1 day for stateful inference\n",
    "MAX_EPOCHS         = 90\n",
    "EARLY_STOP_PATIENCE= 15       # epochs without val-improvement\n",
    "USE_FP16           = True     # mixed precision for speed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1-A. per-day standard-scaling of *features*\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(\n",
    "                                   day_df[feature_cols])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col]     .to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40664, 300)\n",
      "(40664,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 64  (multiple of 32)\n",
      "Validation days    : 15\n",
      "Test days          : 16\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (32 , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int = 32\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750178726.044976   64822 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (32 units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (96 units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "\n",
    "    x = layers.LSTM(short_units,\n",
    "                    stateful=True,\n",
    "                    return_sequences=True,\n",
    "                    dropout=dropout_short,\n",
    "                    recurrent_dropout=rec_drop_short,\n",
    "                    kernel_initializer=\"orthogonal\",\n",
    "                    name=\"short_lstm\")(inp)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "\n",
    "    x = layers.LSTM(long_units,\n",
    "                    stateful=True,\n",
    "                    return_sequences=True,\n",
    "                    dropout=dropout_long,\n",
    "                    recurrent_dropout=rec_drop_long,\n",
    "                    kernel_initializer=\"orthogonal\",\n",
    "                    name=\"long_lstm\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    model = models.Model(inp, out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE                       #\n",
    "###############################################################################\n",
    "\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    Blue  = train RMSE   (line + latest-dot)\n",
    "    Orange= val   RMSE   (line + latest-dot)\n",
    "    Works with `%matplotlib inline`, `%matplotlib widget`, `%matplotlib notebook`\n",
    "    without spawning a new image every epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        self._build_figure()\n",
    "        # display once and keep display_id so we can overwrite instead of spawn\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "\n",
    "        self.e, self.tr, self.va = [], [], []      # epoch → metric history\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        # 1. append data\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "\n",
    "        # 2. update lines\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "\n",
    "        # 3. update latest dots\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            self.va_dot.set_offsets([])            # hide dot if NaN\n",
    "\n",
    "        # 4. rescale axes\n",
    "        self.ax.relim();  self.ax.autoscale_view()\n",
    "\n",
    "        # 5. redraw WITHOUT spawning new figure\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:                                       # inline / notebook png\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)           # overwrite same output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# custom_stateful_training_loop                                               #\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  WHAT THIS ROUTINE DOES END-TO-END                                         #\n",
    "#  • Trains a **dual-memory, stateful** LSTM that ingests entire trading     #\n",
    "#    days (or fat-batches of 32 days) in one go.                             #\n",
    "#  • Two stacked layers:                                                     #\n",
    "#        short_lstm – intraday memory  → reset at the **start of every day** #\n",
    "#        long_lstm  – multi-day memory → reset only once per **week-end**    #\n",
    "#  • Works with three tf.data streams:                                       #\n",
    "#        – TRAIN  : (32 , T_max , F)  padded batches,     FP16, CuDNN        #\n",
    "#        – VAL/TE : (1  , T      , F)  unbatched per day, FP32               #\n",
    "#  • Displays one persistent tqdm bar that counts *calendar days* so users   #\n",
    "#    see meaningful progress.                                                #\n",
    "#  • LiveRMSEPlot() drops a train / val RMSE dot after every epoch.          #\n",
    "#  • **Early stopping** on stagnant val-RMSE; the learning-rate is governed  #\n",
    "#    entirely by the **Cosine-Decay-Restarts** schedule baked into the model #\n",
    "#                                                                             #\n",
    "#  SPEED / STABILITY DECISIONS                                               #\n",
    "#  ────────────────────────────────────────────────────────────────────────── #\n",
    "#  • Uses CuDNN-fused LSTM kernels (fastest path)  ➜  therefore the loop is  #\n",
    "#    compiled in graph mode **without** `jit_compile=True` (XLA + CuDNN      #\n",
    "#    RNNs remains unsupported).                                              #\n",
    "#  • Mixed precision ON: float16 activations + float32 weights deliver about #\n",
    "#    2× lower GPU memory use and ~1.5× speed-up during back-prop.            #\n",
    "#  • Gradient clipping (`clipnorm = 1.0`) avoids FP16 blow-ups.              #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  low-level helper  (unchanged)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.cast(model(xb, training=True), tf.float32)\n",
    "        loss   = loss_fn(yb, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return tf.sqrt(loss)           # RMSE\n",
    "\n",
    "\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,                    # (32 day , T_max , F)\n",
    "    ds_val,                              # (1  day , T     , F)\n",
    "    *,\n",
    "    n_train_days: int,                   # concrete # calendar days / epoch\n",
    "    max_epochs: int,\n",
    "    early_stop_patience: int,\n",
    "    ckpt_path\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fast GPU-optimal loop:\n",
    "      • CuDNN fused LSTM kernels (mixed-precision, no XLA)\n",
    "      • One fat-batch = 32 calendar days\n",
    "      • Outer tqdm bar counts days; no inner bars, no fancy prints\n",
    "    Returns the best validation RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    loss_fn   = tf.keras.losses.MeanSquaredError()\n",
    "    opt       = model_train.optimizer\n",
    "    opt.clipnorm = 1.0                                   # FP16 stability\n",
    "    live_plot = LiveRMSEPlot()                           # tiny matplotlib helper\n",
    "\n",
    "    # quick handles to stateful layers for manual resets\n",
    "    short_tr  = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr   = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers   if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers   if l.name == \"long_lstm\"]\n",
    "\n",
    "    best_val_rmse, patience_ctr = math.inf, 0\n",
    "\n",
    "    # ============================== EPOCH LOOP ============================== #\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "        # ── progress bar : one tick == one calendar day ────────────────────\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80)\n",
    "\n",
    "        # ── TRAIN PHASE ────────────────────────────────────────────────────\n",
    "        batch_rmses   = []\n",
    "        prev_wd_train = None                              # weekday tracker\n",
    "\n",
    "        for xb, yb, wd_batch in ds_train_batched:         # xb.shape[0] == 32\n",
    "            # 1) state resets\n",
    "            for l in short_tr: l.reset_states()\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:   # Fri→Mon\n",
    "                for l in long_tr: l.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "\n",
    "            # 2) forward + backward (single CuDNN kernel)\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "\n",
    "            # 3) bar update (+32 calendar days)\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "\n",
    "        pbar.close()                                       # no lingering 100 %\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "\n",
    "        # ── VALIDATION PHASE ───────────────────────────────────────────────\n",
    "        model_val.set_weights(model_train.get_weights())   # sync FP32 copy\n",
    "        val_rmses, prev_wd_val = [], None\n",
    "\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "            for l in short_val: l.reset_states()\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for l in long_val: l.reset_states()\n",
    "            prev_wd_val = wd\n",
    "\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]),      tf.float32)\n",
    "            val_rmses.append(\n",
    "                float(tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))))\n",
    "\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "\n",
    "        # ── LOG + LIVE PLOT ────────────────────────────────────────────────\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f}\")\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "\n",
    "        # ── EARLY-STOP CHECK ──────────────────────────────────────────────\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse, patience_ctr = epoch_val, 0\n",
    "            model_train.save_weights(ckpt_path)\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    # ============================ END EPOCH LOOP ============================ #\n",
    "\n",
    "    # restore champion weights and return score\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.295175\n",
      "Training sees 64 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd78e10971aa44a6b57e6aabaa314bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=1.153093 • val=0.857211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494eeebb350c43158a63405e335f388a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=1.059160 • val=0.736670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43267c083e941c8998697bf8eff453f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                                | 0/64 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "import math, numpy as np, tensorflow as tf\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "today     = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "file_path = save_dir / f\"model_{ticker}_{today}.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ea52-a04f-4fff-bb69-73c1717dc459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
