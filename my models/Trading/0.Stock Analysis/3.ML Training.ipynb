{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 23:56:23.912969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750456583.947775   37779 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750456583.959173   37779 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750456583.988482   37779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750456583.988576   37779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750456583.988580   37779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750456583.988583   37779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import stockanalibs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:00:00</th>\n",
       "      <td>570.600</td>\n",
       "      <td>570.6000</td>\n",
       "      <td>570.600</td>\n",
       "      <td>570.6000</td>\n",
       "      <td>199.0</td>\n",
       "      <td>570.4288</td>\n",
       "      <td>570.7712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:01:00</th>\n",
       "      <td>570.800</td>\n",
       "      <td>570.8000</td>\n",
       "      <td>570.800</td>\n",
       "      <td>570.8000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>570.6288</td>\n",
       "      <td>570.9712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.764662e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:02:00</th>\n",
       "      <td>571.000</td>\n",
       "      <td>571.0000</td>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>255.0</td>\n",
       "      <td>570.5788</td>\n",
       "      <td>570.9212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.864650e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:03:00</th>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>261.0</td>\n",
       "      <td>570.5788</td>\n",
       "      <td>570.9212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.087675e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:04:00</th>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>261.0</td>\n",
       "      <td>570.5788</td>\n",
       "      <td>570.9212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.219857e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.375</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.215</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.5129</td>\n",
       "      <td>173.6171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>3.876</td>\n",
       "      <td>4.528500e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.565</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.240</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.3280</td>\n",
       "      <td>173.4320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.061</td>\n",
       "      <td>3.019000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.390</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.200</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.2580</td>\n",
       "      <td>173.3620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.131</td>\n",
       "      <td>2.012667e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.315</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.230</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.2280</td>\n",
       "      <td>173.3320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.161</td>\n",
       "      <td>1.341778e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.300</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.170</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.5576</td>\n",
       "      <td>173.6618</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>3.831</td>\n",
       "      <td>8.945186e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1327440 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open      high      low     close     volume  \\\n",
       "2014-04-03 13:00:00  570.600  570.6000  570.600  570.6000      199.0   \n",
       "2014-04-03 13:01:00  570.800  570.8000  570.800  570.8000      135.0   \n",
       "2014-04-03 13:02:00  571.000  571.0000  570.750  570.7500      255.0   \n",
       "2014-04-03 13:03:00  570.750  570.7500  570.750  570.7500      261.0   \n",
       "2014-04-03 13:04:00  570.750  570.7500  570.750  570.7500      261.0   \n",
       "...                      ...       ...      ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.375  173.6771  173.215  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.565  173.5900  173.240  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.390  173.4100  173.200  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.315  173.4000  173.230  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.300  174.0500  173.170  173.6097  7649838.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:00:00  570.4288  570.7712             0            0.000   \n",
       "2014-04-03 13:01:00  570.6288  570.9712             0            0.000   \n",
       "2014-04-03 13:02:00  570.5788  570.9212             0            0.000   \n",
       "2014-04-03 13:03:00  570.5788  570.9212             0            0.000   \n",
       "2014-04-03 13:04:00  570.5788  570.9212             0            0.000   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.5129  173.6171             0            1.166   \n",
       "2025-06-18 20:57:00  173.3280  173.4320             0            1.166   \n",
       "2025-06-18 20:58:00  173.2580  173.3620             0            1.166   \n",
       "2025-06-18 20:59:00  173.2280  173.3320             0            1.166   \n",
       "2025-06-18 21:00:00  173.5576  173.6618             0            1.166   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2014-04-03 13:00:00        0.000        0.000000e+00  \n",
       "2014-04-03 13:01:00        0.000        4.764662e-13  \n",
       "2014-04-03 13:02:00        0.000        1.864650e-12  \n",
       "2014-04-03 13:03:00        0.000        5.087675e-12  \n",
       "2014-04-03 13:04:00        0.000        1.219857e-11  \n",
       "...                          ...                 ...  \n",
       "2025-06-18 20:56:00        3.876        4.528500e-09  \n",
       "2025-06-18 20:57:00        4.061        3.019000e-09  \n",
       "2025-06-18 20:58:00        4.131        2.012667e-09  \n",
       "2025-06-18 20:59:00        4.161        1.341778e-09  \n",
       "2025-06-18 21:00:00        3.831        8.945186e-10  \n",
       "\n",
       "[1327440 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = \"signal_smooth_norm\"\n",
    "feature_cols   = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.25     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.20     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-4     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 3e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.05     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 64       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a eg 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1-A. per-day standard-scaling of *features*\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(\n",
    "                                   day_df[feature_cols])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col]     .to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074090, 450)\n",
      "(1074090,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 1920  (multiple of 64)\n",
      "Validation days    : 422\n",
      "Test days          : 423\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (TRAIN_BATCH , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750456617.034535   37779 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=TRAIN_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,              # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,   # Total calendar days in the training epoch\n",
    "    max_epochs: int,     # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    ckpt_path           # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(ckpt_path)  # Save checkpoint of the best model.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.244102\n",
      "Training sees 1920 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00558500eb4742d699d0d7173b9883ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750456641.364579   37852 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.891950 • val=0.412121 • impr=-68.8% • lr=5.67e-05 • g≈15720.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e54bed4440d462092afec93e96cb758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.725592 • val=0.330896 • impr=-35.6% • lr=1.03e-04 • g≈7046.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac16c2255c9149d39934e59d2af9559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.669483 • val=0.296837 • impr=-21.6% • lr=2.76e-04 • g≈2425.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b043e9ab0d7d4717a3869ef62c2124de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.612976 • val=0.271308 • impr=-11.1% • lr=1.30e-04 • g≈4726.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b2c9af1aad4c2e87349062d7896571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.584351 • val=0.265934 • impr= -8.9% • lr=1.77e-05 • g≈32943.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08735387beb4d5890f489b25ee8ea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.562129 • val=0.249811 • impr= -2.3% • lr=2.83e-04 • g≈1985.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5a27deac934963b1c91a4a77451ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.513935 • val=0.240304 • impr=  1.6% • lr=2.25e-04 • g≈2287.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf07511a488464ab6cbd847c667ec83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 • train=0.483462 • val=0.236101 • impr=  3.3% • lr=1.44e-04 • g≈3368.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7f33e8ddf44d1892f8e5aeec14c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 • train=0.465263 • val=0.237536 • impr=  2.7% • lr=6.71e-05 • g≈6933.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4b4b5178e44bbebcbab5f478d22629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 • train=0.457477 • val=0.231805 • impr=  5.0% • lr=2.11e-05 • g≈21644.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e692ac7f2ed646d8aef3b0076d0e3501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 • train=0.450915 • val=0.229270 • impr=  6.1% • lr=2.98e-04 • g≈1510.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccdbdf9cd03405aafa1bb722607f014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 • train=0.423289 • val=0.224880 • impr=  7.9% • lr=2.86e-04 • g≈1478.38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cf2957078f43919c00cef50e7ad3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 • train=0.398055 • val=0.222673 • impr=  8.8% • lr=2.63e-04 • g≈1513.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1d852ad5354b64a693d1ea7c8d05d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 • train=0.378458 • val=0.218956 • impr= 10.3% • lr=2.31e-04 • g≈1640.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516851c6efed435c8a2fd43682f2d053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 • train=0.362561 • val=0.217196 • impr= 11.0% • lr=1.92e-04 • g≈1887.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2432a2974c94dc1954a31d82220c97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 • train=0.351152 • val=0.217267 • impr= 11.0% • lr=1.51e-04 • g≈2333.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb3ac78df6d4c92b9d20cbc6f675f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 • train=0.342871 • val=0.219288 • impr= 10.2% • lr=1.09e-04 • g≈3131.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c591948ef8f342c08e17bebfa5d94893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 • train=0.337307 • val=0.222294 • impr=  8.9% • lr=7.26e-05 • g≈4645.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a7facd8cac483cb15221917700099e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 • train=0.333114 • val=0.220374 • impr=  9.7% • lr=4.30e-05 • g≈7739.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c45e09e3054a0cb5e46f832b0be2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 • train=0.331934 • val=0.215493 • impr= 11.7% • lr=2.33e-05 • g≈14227.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f72634f217449e88548bf288e7bcd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 • train=0.329928 • val=0.215911 • impr= 11.5% • lr=1.52e-05 • g≈21746.37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2b05023e164c83a77eeaf4a84898f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 • train=0.325542 • val=0.214553 • impr= 12.1% • lr=2.99e-04 • g≈1089.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03bb4a654664328a843149da7638efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 • train=0.313083 • val=0.214225 • impr= 12.2% • lr=2.95e-04 • g≈1061.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513cba26cc994bc99e1bbc5f18e066aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 • train=0.301252 • val=0.212652 • impr= 12.9% • lr=2.88e-04 • g≈1046.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e519d30c3340bdb3e6ac6dfa43a51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 • train=0.291919 • val=0.211604 • impr= 13.3% • lr=2.78e-04 • g≈1050.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be48d2740844700aefea4dcace47c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 • train=0.283952 • val=0.211643 • impr= 13.3% • lr=2.65e-04 • g≈1069.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e4a243c1fb4ac88ac2bf00c6fca962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 • train=0.277774 • val=0.210830 • impr= 13.6% • lr=2.51e-04 • g≈1108.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4a71ce27434ce79c6dc3881f2be241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 • train=0.272614 • val=0.210019 • impr= 14.0% • lr=2.34e-04 • g≈1166.33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd599d9015fa4644a84a2518764c8bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 • train=0.267830 • val=0.209531 • impr= 14.2% • lr=2.15e-04 • g≈1244.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04d8185e086463c92c5036bd4526730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 • train=0.264263 • val=0.209059 • impr= 14.4% • lr=1.96e-04 • g≈1351.68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683afe3af43a4b5bbd17afb5b777ad16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 • train=0.261403 • val=0.209592 • impr= 14.1% • lr=1.75e-04 • g≈1494.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fb5d7d70814229a2e0854f7d8becc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 • train=0.258415 • val=0.210145 • impr= 13.9% • lr=1.54e-04 • g≈1677.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d9d2e3684344339f5a3dc9eb2effe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 • train=0.256549 • val=0.211180 • impr= 13.5% • lr=1.33e-04 • g≈1926.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605de832b87c40cca8bde74efe3ccc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 • train=0.255139 • val=0.212175 • impr= 13.1% • lr=1.13e-04 • g≈2261.87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefffd07099640aeab1ab2a3ee760ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 • train=0.253534 • val=0.214717 • impr= 12.0% • lr=9.34e-05 • g≈2713.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c29cb073c1240d68c6bd55fc6dc36c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 036:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 • train=0.252998 • val=0.217729 • impr= 10.8% • lr=7.54e-05 • g≈3353.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c04bb253be74e7788fd624d8869b114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 037:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 • train=0.252281 • val=0.217371 • impr= 11.0% • lr=5.92e-05 • g≈4258.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cab2919ec9f41da88f953e3a6a4b1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 038:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 • train=0.251816 • val=0.213463 • impr= 12.6% • lr=4.52e-05 • g≈5576.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f1d173161e4435a77b9673f5e47acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 039:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 • train=0.251242 • val=0.209674 • impr= 14.1% • lr=3.35e-05 • g≈7496.93\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00441afa2c9b45e1b65dc6b48285b27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 040:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 • train=0.250339 • val=0.208768 • impr= 14.5% • lr=2.45e-05 • g≈10197.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc6deb18fad4d049bd18fe9bf305c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 041:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 • train=0.249835 • val=0.208881 • impr= 14.4% • lr=1.85e-05 • g≈13532.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7800a75775ae49e98503f8079a9b24d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 042:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 • train=0.249493 • val=0.208668 • impr= 14.5% • lr=1.54e-05 • g≈16215.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213fe779578a4a2ba5af46b66fb39a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 043:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 • train=0.249430 • val=0.208014 • impr= 14.8% • lr=3.00e-04 • g≈831.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec0e7324bfb46999c45ceb75f731578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 044:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 • train=0.247122 • val=0.208999 • impr= 14.4% • lr=2.99e-04 • g≈826.13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba0a1a6eb9e4af1a47c7e35eab57d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 045:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 • train=0.243816 • val=0.208914 • impr= 14.4% • lr=2.98e-04 • g≈819.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5676f5151611484a8f223f20eb44d00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 046:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 • train=0.241473 • val=0.208398 • impr= 14.6% • lr=2.95e-04 • g≈817.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0968935368b4b40b1be362b5dc63ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 047:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 • train=0.238914 • val=0.208571 • impr= 14.6% • lr=2.92e-04 • g≈817.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114b9593c43548c6a3c707b4dc89cb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 048:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 • train=0.237215 • val=0.208095 • impr= 14.8% • lr=2.88e-04 • g≈822.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75a4de5315460d896ec84cf834d182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 049:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 • train=0.235540 • val=0.207612 • impr= 14.9% • lr=2.84e-04 • g≈829.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873afb6478094cc58b2a51efcc0a5861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 050:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 • train=0.233882 • val=0.207344 • impr= 15.1% • lr=2.79e-04 • g≈838.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b9a9cbc00449d1a085fe23d70bafe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 051:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 • train=0.232603 • val=0.206669 • impr= 15.3% • lr=2.73e-04 • g≈852.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3685598e957641128eae488df5bd5a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 052:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 052 • train=0.231715 • val=0.206468 • impr= 15.4% • lr=2.67e-04 • g≈869.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b452bafbc814d10ae44cbc9d23500db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 053:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053 • train=0.230237 • val=0.206311 • impr= 15.5% • lr=2.59e-04 • g≈887.26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3292f747da40e6bb50661b6b29d051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 054:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 054 • train=0.229438 • val=0.206121 • impr= 15.6% • lr=2.52e-04 • g≈910.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601880bae11140178cdfed890df6eade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 055:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 055 • train=0.228206 • val=0.205472 • impr= 15.8% • lr=2.44e-04 • g≈936.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eb644690b140beac82d4cab91d81af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 056:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 056 • train=0.227648 • val=0.205262 • impr= 15.9% • lr=2.35e-04 • g≈967.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f693aa2e2d4dc69f0ad1a23de03e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 057:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 057 • train=0.226657 • val=0.205330 • impr= 15.9% • lr=2.26e-04 • g≈1001.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e09794f095746069935128021264e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 058:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 058 • train=0.225893 • val=0.205724 • impr= 15.7% • lr=2.17e-04 • g≈1041.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d993be72a84fff80c3b0bd63495551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 059:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 059 • train=0.225226 • val=0.205579 • impr= 15.8% • lr=2.07e-04 • g≈1087.26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ada03115c6946f59d78f0364a9ab2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 060:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 060 • train=0.224793 • val=0.205871 • impr= 15.7% • lr=1.97e-04 • g≈1139.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17929f0cdfde40429b479ee9fe9d2e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 061:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 061 • train=0.224139 • val=0.206130 • impr= 15.6% • lr=1.87e-04 • g≈1198.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5e826855e849178f7abf7dfd1bfe71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 062:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 062 • train=0.223575 • val=0.206037 • impr= 15.6% • lr=1.77e-04 • g≈1265.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0981d72b462d44809fe0682f393e2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 063:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 063 • train=0.223394 • val=0.206584 • impr= 15.4% • lr=1.66e-04 • g≈1343.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da076247a95c45faa4ce5674f1b37bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 064:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 • train=0.222867 • val=0.206740 • impr= 15.3% • lr=1.56e-04 • g≈1430.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe4d94c4c81427c9788ef4d4949b19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 065:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 065 • train=0.222674 • val=0.206953 • impr= 15.2% • lr=1.45e-04 • g≈1532.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7c6c1f1c9045df873515798a671646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 066:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 066 • train=0.222195 • val=0.208004 • impr= 14.8% • lr=1.35e-04 • g≈1647.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1a65668c864cac99b07081c082b342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 067:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 067 • train=0.222289 • val=0.213917 • impr= 12.4% • lr=1.25e-04 • g≈1784.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1692caca786649d9bdbb8ded99e7608d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 068:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068 • train=0.222785 • val=0.211219 • impr= 13.5% • lr=1.14e-04 • g≈1946.33\n",
      "Early stopping triggered.\n",
      "\n",
      "Champion validation RMSE = 0.205262\n",
      "Improvement vs baseline   =  15.9 %\n",
      "\n",
      "Model & weights saved to dfs training/GOOGL_2025-06-20_model.keras\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "file_path = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
