{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f06a4f-691a-4a84-a305-e7212eb879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python’s garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "# 3) If you’re using PyTorch + CUDA, free any lingering GPU memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "from libs import models, plots, params\n",
    "importlib.reload(models)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "ticker         = params.ticker\n",
    "look_back      = params.look_back \n",
    "features_cols  = params.features_cols\n",
    "label_col      = params.label_col\n",
    "save_path      = params.save_path\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "regular_start_pred  = params.regular_start_pred\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = params.train_prop, params.val_prop\n",
    "\n",
    "# USE GPU if available, otherwise fallback to CPU\n",
    "device = params.device\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (attention-augmented tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 32           # hidden size of each daily LSTM layer\n",
    "LONG_UNITS          = 64           # hidden size of the weekly LSTM\n",
    "DROPOUT_SHORT       = 0.3          # dropout after residual+attention block\n",
    "DROPOUT_LONG        = 0.4          # dropout after weekly LSTM outputs\n",
    "ATT_HEADS           = 4            # number of self-attention heads\n",
    "ATT_DROPOUT         = 0.2          # dropout rate inside attention\n",
    "WEIGHT_DECAY        = 5e-4         # L2 weight decay on all model weights\n",
    "\n",
    "# ── Training Control Parameters ────────────────────────────────────────────\n",
    "TRAIN_BATCH         = params.train_batch           \n",
    "VAL_BATCH           = params.val_batch  \n",
    "NUM_WORKERS         = params.num_workers\n",
    "MAX_EPOCHS          = 120          # upper limit on training epochs\n",
    "EARLY_STOP_PATIENCE = 12           # stop if no val-improve for this many epochs\n",
    "\n",
    "# ── Optimizer Settings ─────────────────────────────────────────────────────\n",
    "LR_EPOCHS_WARMUP    = 5            # epochs to wait before decreasing the LR  \n",
    "INITIAL_LR          = 1e-3         # AdamW initial learning rate\n",
    "CLIPNORM            = 0.5          # max-norm gradient clipping\n",
    "\n",
    "# ── CosineAnnealingWarmRestarts Scheduler ─────────────────────────────────\n",
    "T_0                 = MAX_EPOCHS   # epochs before first cosine restart\n",
    "T_MULT              = 1            # cycle length multiplier after each restart\n",
    "ETA_MIN             = 1e-6         # floor LR in each cosine cycle\n",
    "\n",
    "# ── ReduceLROnPlateau Scheduler ───────────────────────────────────────────\n",
    "PLATEAU_FACTOR      = 0.5          # multiply LR by this factor on plateau\n",
    "PLATEAU_PATIENCE    = 3            # epochs with no val-improve before LR cut\n",
    "MIN_LR              = 1e-6         # lower bound on LR after reductions\n",
    "PLAT_EPOCHS_WARMUP  = 20           # epochs to wait before triggering plateau logic      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08b80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step D: saving final CSV …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>r_1</th>\n",
       "      <th>r_5</th>\n",
       "      <th>r_15</th>\n",
       "      <th>vol_15</th>\n",
       "      <th>volume_spike</th>\n",
       "      <th>vwap_dev</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:06:00</th>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>4580.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.568641</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.636251</td>\n",
       "      <td>28.653438</td>\n",
       "      <td>0.327384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:07:00</th>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>4540.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.570338</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.631098</td>\n",
       "      <td>28.648282</td>\n",
       "      <td>0.328556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:08:00</th>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.574408</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.625944</td>\n",
       "      <td>28.643125</td>\n",
       "      <td>0.329833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:09:00</th>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>4460.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.581017</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.620791</td>\n",
       "      <td>28.637968</td>\n",
       "      <td>0.331213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:10:00</th>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>4420.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.590413</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.615637</td>\n",
       "      <td>28.632811</td>\n",
       "      <td>0.332697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.375000</td>\n",
       "      <td>173.677100</td>\n",
       "      <td>173.215000</td>\n",
       "      <td>173.565000</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>2.462713</td>\n",
       "      <td>1.248428</td>\n",
       "      <td>17.019768</td>\n",
       "      <td>173.512900</td>\n",
       "      <td>173.617100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.565000</td>\n",
       "      <td>173.590000</td>\n",
       "      <td>173.240000</td>\n",
       "      <td>173.380000</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.005063</td>\n",
       "      <td>-0.010671</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>2.154838</td>\n",
       "      <td>1.246015</td>\n",
       "      <td>11.648165</td>\n",
       "      <td>173.328000</td>\n",
       "      <td>173.432000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.390000</td>\n",
       "      <td>173.410000</td>\n",
       "      <td>173.200000</td>\n",
       "      <td>173.310000</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.011816</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>1.439161</td>\n",
       "      <td>1.245096</td>\n",
       "      <td>11.384870</td>\n",
       "      <td>173.258000</td>\n",
       "      <td>173.362000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.315000</td>\n",
       "      <td>173.400000</td>\n",
       "      <td>173.230000</td>\n",
       "      <td>173.280000</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>-0.011932</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>2.836382</td>\n",
       "      <td>1.244678</td>\n",
       "      <td>11.830567</td>\n",
       "      <td>173.228000</td>\n",
       "      <td>173.332000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.300000</td>\n",
       "      <td>174.050000</td>\n",
       "      <td>173.170000</td>\n",
       "      <td>173.609700</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>8.568493</td>\n",
       "      <td>1.248745</td>\n",
       "      <td>22.962317</td>\n",
       "      <td>173.557600</td>\n",
       "      <td>173.661800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1651679 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           open        high         low       close  \\\n",
       "2014-04-03 12:06:00   28.644845   28.644845   28.644845   28.644845   \n",
       "2014-04-03 12:07:00   28.639690   28.639690   28.639690   28.639690   \n",
       "2014-04-03 12:08:00   28.634534   28.634534   28.634534   28.634534   \n",
       "2014-04-03 12:09:00   28.629379   28.629379   28.629379   28.629379   \n",
       "2014-04-03 12:10:00   28.624224   28.624224   28.624224   28.624224   \n",
       "...                         ...         ...         ...         ...   \n",
       "2025-06-18 20:56:00  173.375000  173.677100  173.215000  173.565000   \n",
       "2025-06-18 20:57:00  173.565000  173.590000  173.240000  173.380000   \n",
       "2025-06-18 20:58:00  173.390000  173.410000  173.200000  173.310000   \n",
       "2025-06-18 20:59:00  173.315000  173.400000  173.230000  173.280000   \n",
       "2025-06-18 21:00:00  173.300000  174.050000  173.170000  173.609700   \n",
       "\n",
       "                        volume       r_1       r_5      r_15    vol_15  \\\n",
       "2014-04-03 12:06:00     4580.0 -0.000180 -0.000180 -0.000180  0.000046   \n",
       "2014-04-03 12:07:00     4540.0 -0.000180 -0.000360 -0.000360  0.000063   \n",
       "2014-04-03 12:08:00     4500.0 -0.000180 -0.000540 -0.000540  0.000075   \n",
       "2014-04-03 12:09:00     4460.0 -0.000180 -0.000720 -0.000720  0.000082   \n",
       "2014-04-03 12:10:00     4420.0 -0.000180 -0.000900 -0.000900  0.000088   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "2025-06-18 20:56:00   621199.0  0.001124 -0.004226 -0.009661  0.001493   \n",
       "2025-06-18 20:57:00   624198.0 -0.001066 -0.005063 -0.010671  0.001487   \n",
       "2025-06-18 20:58:00   454542.0 -0.000404 -0.005811 -0.011816  0.001436   \n",
       "2025-06-18 20:59:00  1094746.0 -0.000173 -0.004434 -0.011932  0.001432   \n",
       "2025-06-18 21:00:00  7649838.0  0.001901  0.001382 -0.009290  0.001592   \n",
       "\n",
       "                     volume_spike  vwap_dev     rsi_14         bid  \\\n",
       "2014-04-03 12:06:00      0.568641 -0.000177   0.000000   28.636251   \n",
       "2014-04-03 12:07:00      0.570338 -0.000352   0.000000   28.631098   \n",
       "2014-04-03 12:08:00      0.574408 -0.000524   0.000000   28.625944   \n",
       "2014-04-03 12:09:00      0.581017 -0.000694   0.000000   28.620791   \n",
       "2014-04-03 12:10:00      0.590413 -0.000862   0.000000   28.615637   \n",
       "...                           ...       ...        ...         ...   \n",
       "2025-06-18 20:56:00      2.462713  1.248428  17.019768  173.512900   \n",
       "2025-06-18 20:57:00      2.154838  1.246015  11.648165  173.328000   \n",
       "2025-06-18 20:58:00      1.439161  1.245096  11.384870  173.258000   \n",
       "2025-06-18 20:59:00      2.836382  1.244678  11.830567  173.228000   \n",
       "2025-06-18 21:00:00      8.568493  1.248745  22.962317  173.557600   \n",
       "\n",
       "                            ask  signal_smooth  \n",
       "2014-04-03 12:06:00   28.653438       0.327384  \n",
       "2014-04-03 12:07:00   28.648282       0.328556  \n",
       "2014-04-03 12:08:00   28.643125       0.329833  \n",
       "2014-04-03 12:09:00   28.637968       0.331213  \n",
       "2014-04-03 12:10:00   28.632811       0.332697  \n",
       "...                         ...            ...  \n",
       "2025-06-18 20:56:00  173.617100       0.000000  \n",
       "2025-06-18 20:57:00  173.432000       0.000000  \n",
       "2025-06-18 20:58:00  173.362000       0.000000  \n",
       "2025-06-18 20:59:00  173.332000       0.000000  \n",
       "2025-06-18 21:00:00  173.661800       0.000000  \n",
       "\n",
       "[1651679 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURES ENGINEERING\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_ready.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# 1) engineer top intraday features on your 1-min bars\n",
    "for lag in (1, 5, 15):\n",
    "    df[f\"r_{lag}\"] = np.log(df[\"close\"] / df[\"close\"].shift(lag))\n",
    "\n",
    "df[\"vol_15\"]        = df[\"r_1\"].rolling(15).std()\n",
    "df[\"volume_spike\"]  = df[\"volume\"] / df[\"volume\"].rolling(15).mean()\n",
    "\n",
    "typ_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "vwap      = (typ_price * df[\"volume\"]).cumsum() / df[\"volume\"].cumsum()\n",
    "df[\"vwap_dev\"]      = (df[\"close\"] - vwap) / vwap\n",
    "\n",
    "delta     = df[\"close\"].diff()\n",
    "gain      = delta.clip(lower=0)\n",
    "loss      = -delta.clip(upper=0)\n",
    "avg_gain  = gain.rolling(14).mean()\n",
    "avg_loss  = loss.rolling(14).mean()\n",
    "rs        = avg_gain / avg_loss\n",
    "df[\"rsi_14\"]        = 100 - (100 / (1 + rs))\n",
    "\n",
    "# 2) keep only your label plus the most predictive features\n",
    "\n",
    "df = df[features_cols  + ['bid', 'ask'] + [label_col]].dropna()\n",
    "\n",
    "print(\"\\n Step D: saving final CSV …\")\n",
    "out_path = save_path / f\"{ticker}_final.csv\"\n",
    "df.to_csv(out_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1313759, 12, 120])\n",
      "torch.Size([1313759])\n",
      "Shapes:\n",
      "  X         = torch.Size([1313759, 12, 120]) (samples, features, look_back)\n",
      "  y         = torch.Size([1313759]) (samples,)\n",
      "  raw_close = torch.Size([1313759])\n",
      "  raw_bid   = torch.Size([1313759])\n",
      "  raw_ask   = torch.Size([1313759])\n",
      "\n",
      "First 5 window‐end timestamps:\n",
      "  2014-04-03 14:06:00\n",
      "  2014-04-03 14:07:00\n",
      "  2014-04-03 14:08:00\n",
      "  2014-04-03 14:09:00\n",
      "  2014-04-03 14:10:00\n",
      "\n",
      "First window covers 120 bars from\n",
      "  2014-04-03 12:06:00  →  2014-04-03 14:05:00\n",
      "and predicts the bar at 2014-04-03 14:06:00\n",
      "\n",
      "Those bars (timestamps):\n",
      "DatetimeIndex(['2014-04-03 12:06:00', '2014-04-03 12:07:00',\n",
      "               '2014-04-03 12:08:00', '2014-04-03 12:09:00',\n",
      "               '2014-04-03 12:10:00', '2014-04-03 12:11:00',\n",
      "               '2014-04-03 12:12:00', '2014-04-03 12:13:00',\n",
      "               '2014-04-03 12:14:00', '2014-04-03 12:15:00',\n",
      "               ...\n",
      "               '2014-04-03 13:56:00', '2014-04-03 13:57:00',\n",
      "               '2014-04-03 13:58:00', '2014-04-03 13:59:00',\n",
      "               '2014-04-03 14:00:00', '2014-04-03 14:01:00',\n",
      "               '2014-04-03 14:02:00', '2014-04-03 14:03:00',\n",
      "               '2014-04-03 14:04:00', '2014-04-03 14:05:00'],\n",
      "              dtype='datetime64[ns]', length=120, freq='min')\n",
      "First 5 values y: [0.3880619  0.38829955 0.38863376 0.38904947 0.38930276]\n",
      "First 5 signal_smooth values, on regular trade time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2014-04-03 14:06:00    0.388062\n",
       "2014-04-03 14:07:00    0.388300\n",
       "2014-04-03 14:08:00    0.388634\n",
       "2014-04-03 14:09:00    0.389049\n",
       "2014-04-03 14:10:00    0.389303\n",
       "Name: signal_smooth, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = models.build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=look_back,\n",
    "    features_cols=features_cols,\n",
    "    label_col=label_col,\n",
    "    regular_start=regular_start_pred\n",
    ")\n",
    "\n",
    "# 1)\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) \n",
    "\n",
    "\n",
    "# 2) quick shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"  X         =\", X.shape,    \"(samples, features, look_back)\")\n",
    "print(\"  y         =\", y.shape,    \"(samples,)\")\n",
    "print(\"  raw_close =\", raw_close.shape)\n",
    "print(\"  raw_bid   =\", raw_bid.shape)\n",
    "print(\"  raw_ask   =\", raw_ask.shape)\n",
    "\n",
    "# 3) rebuild the list of label‐timestamps (window‐ends)\n",
    "ends = []\n",
    "for day, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "    day_df = day_df.sort_index()\n",
    "    # candidate ends at positions look_back .. end\n",
    "    idxs = day_df.index[look_back:]\n",
    "    # only keep those at/after regular_start_pred\n",
    "    mask = [t >= regular_start_pred for t in idxs.time]\n",
    "    ends.extend(idxs[mask])\n",
    "\n",
    "# 4) show first few ends\n",
    "print(\"\\nFirst 5 window‐end timestamps:\")\n",
    "for ts in ends[:5]:\n",
    "    print(\" \", ts)\n",
    "\n",
    "# 5) show exactly which minutes X[0] contains\n",
    "first_end   = ends[0]\n",
    "first_start = first_end - pd.Timedelta(minutes=look_back)\n",
    "print(f\"\\nFirst window covers {look_back} bars from\")\n",
    "print(f\"  {first_start}  →  {first_end - pd.Timedelta(minutes=1)}\")\n",
    "print(f\"and predicts the bar at {first_end}\")\n",
    "\n",
    "print(\"\\nThose bars (timestamps):\")\n",
    "print(pd.date_range(first_start, first_end - pd.Timedelta(minutes=1), freq=\"1min\"))\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "y_np         = y.cpu().numpy()\n",
    "print(\"First 5 values y:\",         y_np[:5])\n",
    "print(\"First 5 signal_smooth values, on regular trade time:\")\n",
    "df.signal_smooth.iloc[look_back:look_back+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_tr        = torch.Size([891515, 12, 120])\n",
      "  y_tr        = torch.Size([891515])\n",
      "  raw_close_te= torch.Size([215633])\n",
      "  raw_bid_te  = torch.Size([215633])\n",
      "  raw_ask_te  = torch.Size([215633])\n",
      "\n",
      "Days: train=1984, val=410, test=422\n",
      "Windows: train=891515, val=206611, test=215633\n",
      "\n",
      "First 5 window‐end times: [Timestamp('2014-04-03 14:06:00'), Timestamp('2014-04-03 14:07:00'), Timestamp('2014-04-03 14:08:00'), Timestamp('2014-04-03 14:09:00'), Timestamp('2014-04-03 14:10:00')]\n",
      "\n",
      "X_tr[0] covers bars from 2014-04-03 12:06:00 to 2014-04-03 14:05:00\n",
      "Those timestamps:\n",
      "DatetimeIndex(['2014-04-03 12:06:00', '2014-04-03 12:07:00',\n",
      "               '2014-04-03 12:08:00', '2014-04-03 12:09:00',\n",
      "               '2014-04-03 12:10:00', '2014-04-03 12:11:00',\n",
      "               '2014-04-03 12:12:00', '2014-04-03 12:13:00',\n",
      "               '2014-04-03 12:14:00', '2014-04-03 12:15:00',\n",
      "               ...\n",
      "               '2014-04-03 13:56:00', '2014-04-03 13:57:00',\n",
      "               '2014-04-03 13:58:00', '2014-04-03 13:59:00',\n",
      "               '2014-04-03 14:00:00', '2014-04-03 14:01:00',\n",
      "               '2014-04-03 14:02:00', '2014-04-03 14:03:00',\n",
      "               '2014-04-03 14:04:00', '2014-04-03 14:05:00'],\n",
      "              dtype='datetime64[ns]', length=120, freq='min')\n",
      "y_tr[0] (and raw_close_te[0]) is the bar at 2014-04-03 14:06:00\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val/test by calendar day\n",
    "(X_tr, y_tr), \\\n",
    "(X_val, y_val), \\\n",
    "(X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = models.chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back   = look_back,\n",
    "    regular_start   = regular_start_pred,\n",
    "    train_prop  = TRAIN_PROP,\n",
    "    val_prop    = VAL_PROP,\n",
    "    train_batch = TRAIN_BATCH\n",
    ")\n",
    "\n",
    "# 1) Print shapes of all tensors\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_tr        =\", X_tr.shape)\n",
    "print(\"  y_tr        =\", y_tr.shape)\n",
    "print(\"  raw_close_te=\", raw_close_te.shape)\n",
    "print(\"  raw_bid_te  =\", raw_bid_te.shape)\n",
    "print(\"  raw_ask_te  =\", raw_ask_te.shape)\n",
    "\n",
    "# 2) Print number of days in each split\n",
    "n_tr_days = torch.unique(day_id_tr).numel()\n",
    "n_val_days= torch.unique(day_id_val).numel()\n",
    "n_te_days = torch.unique(day_id_te).numel()\n",
    "print(f\"\\nDays: train={n_tr_days}, val={n_val_days}, test={n_te_days}\")\n",
    "\n",
    "# 3) Print number of windows in each split\n",
    "print(f\"Windows: train={X_tr.shape[0]}, val={X_val.shape[0]}, test={X_te.shape[0]}\")\n",
    "\n",
    "# 4) List the first few window‐end timestamps\n",
    "ends = []\n",
    "for day, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "    ts = day_df.index[look_back:]\n",
    "    ends.extend(ts[ts.time >= regular_start_pred])\n",
    "first_ends = ends[:5]\n",
    "print(\"\\nFirst 5 window‐end times:\", first_ends)\n",
    "\n",
    "# 5) Show exactly which minutes X_tr[0] covers, and where y_tr[0] sits\n",
    "first_end   = first_ends[0]\n",
    "first_start = first_end - pd.Timedelta(minutes=look_back)\n",
    "# input bars = [first_start … first_end − 1min]\n",
    "print(f\"\\nX_tr[0] covers bars from {first_start} to {first_end - pd.Timedelta(minutes=1)}\")\n",
    "print(\"Those timestamps:\")\n",
    "print(pd.date_range(first_start, first_end - pd.Timedelta(minutes=1), freq=\"1min\"))\n",
    "print(f\"y_tr[0] (and raw_close_te[0]) is the bar at {first_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad9b6c4-3d79-45c0-b2c0-c4f46f1ad866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Entered split_to_day_datasets\n",
      "1) building weekday arrays\n",
      "   Weekdays counts → tr=891515, val=206611, te=215633\n",
      "2) moving all splits to CPU\n",
      "   CPU casts done\n",
      "3) zero-bas­ing day_id for val & test\n",
      "   val_day_id ∈ [0..409], total days=410\n",
      "   te_day_id  ∈ [0..421], total days=422\n",
      "4) instantiating DayWindowDatasets\n",
      "   ds_tr days: 1984\n",
      "   ds_val days: 410\n",
      "   ds_te days: 422\n",
      "5) building DataLoaders\n",
      "   train_loader ready\n",
      "   val_loader ready\n",
      "   test_loader ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Build DataLoaders over calendar‐days\n",
    "# -----------------------------------------------------------------------------\n",
    "train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "    # Training split arrays (from chronological_split)\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    # Validation split arrays\n",
    "    X_val, y_val, day_id_val,\n",
    "    # Test split arrays + raw prices for post‐tracking\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    # Original minute‐bar DataFrame for weekday mapping\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    train_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320c2ca7-ca1e-4336-9f80-3a339b8184cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer_and_scheduler(\n",
    "    model: nn.Module,\n",
    "    initial_lr: float       = INITIAL_LR,\n",
    "    weight_decay: float     = WEIGHT_DECAY,\n",
    "    clipnorm: float         = CLIPNORM,\n",
    "    plateau_factor: float   = PLATEAU_FACTOR,\n",
    "    plateau_patience: int   = PLATEAU_PATIENCE,\n",
    "    plateau_min_lr: float   = MIN_LR,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) AdamW with decoupled weight decay.\n",
    "    2) ReduceLROnPlateau: reduces LR when val‐RMSE stops improving.\n",
    "    3) CosineAnnealingWarmRestarts: per‐batch cosine schedule.\n",
    "    4) GradScaler for mixed precision.\n",
    "    \"\"\"\n",
    "    # AdamW optimizer with L2 regularization\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # LR ↓ when validation RMSE plateaus\n",
    "    plateau_sched = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=plateau_factor,\n",
    "        patience=plateau_patience,\n",
    "        min_lr=plateau_min_lr,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Cosine warm‐restarts scheduler (batch-level stepping)\n",
    "    cosine_sched = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=T_0, \n",
    "        T_mult=T_MULT, \n",
    "        eta_min=ETA_MIN\n",
    "    )\n",
    "\n",
    "    # AMP scaler for fp16 stability\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    return optimizer, plateau_sched, cosine_sched, scaler, clipnorm\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model:     nn.Module,\n",
    "    x_day:     torch.Tensor,    # (W, look_back, F), on device already\n",
    "    y_day:     torch.Tensor,    # (W,), on device already\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler:    GradScaler,\n",
    "    clipnorm:  float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Single‐day step:\n",
    "      1) zero grads\n",
    "      2) fp16 forward+loss\n",
    "      3) backward with scaler → unscale → clip → step → update scaler\n",
    "      4) return scalar loss\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    model.train()\n",
    "\n",
    "    device_type = x_day.device.type\n",
    "    # Mixed‐precision forward\n",
    "    with autocast(device_type=device_type):\n",
    "        out  = model(x_day)         # → (W, seq_len, 1)\n",
    "        last = out[:, -1, 0]        # → (W,)\n",
    "        loss = Funct.mse_loss(last, y_day, reduction='mean')\n",
    "\n",
    "    # Backward + clip + optimizer step\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)     # bring grads to fp32 for clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be13563-aee4-43e7-ac9c-064c0e38b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stateful_training_loop(\n",
    "    model:         torch.nn.Module,\n",
    "    optimizer:     torch.optim.Optimizer,\n",
    "    cosine_sched:  CosineAnnealingWarmRestarts,\n",
    "    plateau_sched: ReduceLROnPlateau,\n",
    "    scaler:        GradScaler,\n",
    "    train_loader:  torch.utils.data.DataLoader,\n",
    "    val_loader:    torch.utils.data.DataLoader,\n",
    "    *,\n",
    "    max_epochs:          int     = MAX_EPOCHS,\n",
    "    early_stop_patience: int     = EARLY_STOP_PATIENCE,\n",
    "    baseline_val_rmse:   float   = None,\n",
    "    clipnorm:            float   = CLIPNORM,\n",
    "    plateau_warmup:      int     = PLAT_EPOCHS_WARMUP,\n",
    "    device:              torch.device = torch.device(\"cpu\"),\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Full training loop:\n",
    "      • CosineAnnealingWarmRestarts stepping per batch, with restart-print\n",
    "      • ReduceLROnPlateau stepping per epoch after warmup, with reduction-print\n",
    "      • Mixed precision + gradient clipping\n",
    "      • Per-day & per-week LSTM state resets\n",
    "      • Early stopping + best-model checkpoint\n",
    "      • When plateau cuts LR, cosine scheduler is reset to continue from new LR\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state    = None\n",
    "    patience_ctr  = 0\n",
    "    live_plot     = plots.LiveRMSEPlot()\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        gc.collect()\n",
    "        print(f\"[Epoch {epoch}] GPU alloc = {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n",
    "\n",
    "        # ── TRAIN ─────────────────────────────────────────────────────────\n",
    "        model.train()\n",
    "        model.h_short = model.h_long = None\n",
    "        train_losses = []\n",
    "        pbar = tqdm(enumerate(train_loader),\n",
    "                    total=len(train_loader),\n",
    "                    desc=f\"Epoch {epoch}\",\n",
    "                    unit=\"bundle\")\n",
    "        for batch_idx, (xb_days, yb_days, wd_days) in pbar:\n",
    "            xb_days, yb_days = xb_days.to(device), yb_days.to(device)\n",
    "            wd_days          = wd_days.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            prev_wd = None\n",
    "\n",
    "            for di in range(xb_days.size(0)):\n",
    "                wd = int(wd_days[di].item())\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                # Mixed-precision forward/backward\n",
    "                with autocast(device_type=device.type):\n",
    "                    out  = model(xb_days[di])\n",
    "                    last = out[..., -1, 0]\n",
    "                    loss = Funct.mse_loss(last, yb_days[di], reduction='mean')\n",
    "                scaler.scale(loss).backward()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                # Detach hidden states to truncate graph\n",
    "                if isinstance(model.h_short, tuple):\n",
    "                    model.h_short = tuple(h.detach() for h in model.h_short)\n",
    "                if isinstance(model.h_long, tuple):\n",
    "                    model.h_long  = tuple(h.detach() for h in model.h_long)\n",
    "                del out, last, loss\n",
    "\n",
    "            # Unscale → clip → step → update scaler\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Cosine scheduler (fractional epoch) + restart print\n",
    "            prev_lr_cos = optimizer.param_groups[0]['lr']\n",
    "            frac_epoch  = epoch - 1 + batch_idx / len(train_loader)\n",
    "            cosine_sched.step(frac_epoch)\n",
    "            new_lr_cos  = optimizer.param_groups[0]['lr']\n",
    "            if new_lr_cos > prev_lr_cos:\n",
    "                print(f\"  [Cosine restart] LR {prev_lr_cos:.2e} → {new_lr_cos:.2e}\"\n",
    "                      f\" at epoch {epoch}, batch {batch_idx}\")\n",
    "\n",
    "            # Logging\n",
    "            rmse = math.sqrt(sum(train_losses) / len(train_losses))\n",
    "            lr   = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix(train_rmse=rmse, lr=lr, refresh=False)\n",
    "            pbar.update(0)\n",
    "            gc.collect()\n",
    "        pbar.close()\n",
    "\n",
    "        # ── VALIDATE ───────────────────────────────────────────────────────\n",
    "        model.eval()\n",
    "        model.h_short = model.h_long = None\n",
    "        val_losses = []\n",
    "        prev_wd    = None\n",
    "        with torch.no_grad():\n",
    "            for xb_day, yb_day, wd in val_loader:\n",
    "                wd = int(wd.item())\n",
    "                x  = xb_day[0].to(device)\n",
    "                y  = yb_day.view(-1).to(device)\n",
    "\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                out  = model(x)\n",
    "                last = out[..., -1, 0]\n",
    "                val_losses.append(Funct.mse_loss(last, y, reduction='mean').item())\n",
    "                del xb_day, yb_day, x, y, out, last\n",
    "\n",
    "        val_rmse = math.sqrt(sum(val_losses) / len(val_losses))\n",
    "\n",
    "        # Live plot & print\n",
    "        live_plot.update(rmse, val_rmse)\n",
    "        print(f\"Epoch {epoch:03d} • train={rmse:.4f} • val={val_rmse:.4f}\"\n",
    "              f\" • lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # ── ReduceLROnPlateau (after warmup) + reduction print ──────────\n",
    "        pre_lr = optimizer.param_groups[0]['lr']\n",
    "        if epoch > plateau_warmup:\n",
    "            plateau_sched.step(val_rmse)\n",
    "        post_lr = optimizer.param_groups[0]['lr']\n",
    "        if post_lr < pre_lr:\n",
    "            print(f\"  [Plateau cut] LR {pre_lr:.1e} → {post_lr:.1e}\"\n",
    "                  f\" at epoch {epoch}\")\n",
    "            # — update cosine scheduler to continue from new LR —\n",
    "            cosine_sched.base_lrs = [post_lr for _ in cosine_sched.base_lrs]\n",
    "            cosine_sched.last_epoch = epoch - 1\n",
    "\n",
    "        # ── Early stopping ────────────────────────────────────────────────\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_state    = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr  = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping at epoch\", epoch)\n",
    "                break\n",
    "\n",
    "    # ── Save best model weights ──────────────────────────────────────────\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    ckpt_file = params.save_path / f\"{params.ticker}_{best_val_rmse:.4f}.pth\"\n",
    "    torch.save(model.state_dict(), ckpt_file)\n",
    "    print(f\"Saved best model to {ckpt_file}\")\n",
    "\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualMemoryLSTM(\n",
       "  (short_lstm): LSTM(12, 32, batch_first=True)\n",
       "  (attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (do_short): Dropout(p=0.3, inplace=False)\n",
       "  (ln_short): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (long_lstm): LSTM(32, 64, batch_first=True)\n",
       "  (do_long): Dropout(p=0.4, inplace=False)\n",
       "  (ln_long): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (pred): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATS = len(features_cols)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the stateful DualMemoryLSTM & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "model = models.DualMemoryLSTM(\n",
    "    n_feats        = N_FEATS,        # number of input features per minute\n",
    "    short_units    = SHORT_UNITS,    # hidden size of daily LSTM\n",
    "    long_units     = LONG_UNITS,     # hidden size of weekly LSTM\n",
    "    dropout_short  = DROPOUT_SHORT,  # dropout after daily LSTM\n",
    "    dropout_long   = DROPOUT_LONG,    # dropout after weekly LSTM\n",
    "    att_heads      = ATT_HEADS,\n",
    "    att_drop       = ATT_DROPOUT\n",
    ")\n",
    "model.to(device)   # place model parameters on GPU or CPU as specified\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5512a0dd-d2c8-418e-bfca-4580fb4be995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.001\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0005\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute plateau_sched timing parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# Total training samples = total windows in X_tr (one window per row)\n",
    "n_train_samples = X_tr.shape[0]\n",
    "\n",
    "# How many optimizer steps (day‐bundles) constitute one epoch?\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build optimizer, LR scheduler, AMP scaler, and gradient‐clip norm\n",
    "# -----------------------------------------------------------------------------\n",
    "optimizer, plateau_sched, cosine_sched, scaler, clipnorm = make_optimizer_and_scheduler(\n",
    "    model,\n",
    "    initial_lr        = INITIAL_LR,       \n",
    "    weight_decay      = WEIGHT_DECAY,     \n",
    "    clipnorm          = CLIPNORM,        \n",
    "    plateau_factor    = PLATEAU_FACTOR,  \n",
    "    plateau_patience  = PLATEAU_PATIENCE, \n",
    "    plateau_min_lr    = MIN_LR           \n",
    ")\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e70105f-bbe5-4ce0-aabe-acf9193ef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1) Move model to CPU and build a fresh optimizer (no scheduler metadata)\n",
    "# model_cpu = model.cpu()\n",
    "# optimizer_cpu = torch.optim.AdamW(\n",
    "#     model_cpu.parameters(),\n",
    "#     lr=1e-3,        # placeholder; the finder will override this\n",
    "#     weight_decay=5e-4\n",
    "# )\n",
    "\n",
    "# # 2) Create a tiny DataLoader (batch_size=1) to save memory\n",
    "# small_loader = DataLoader(\n",
    "#     train_loader.dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# # 3) Define an aligned MSE that permutes/expands your [1,1,D] or [D,1,1]\n",
    "# #    target → [D, T, 1] to match output shape exactly.\n",
    "# def aligned_mse(output, target):\n",
    "#     # output: [D, T, 1]\n",
    "#     # target might come in as [D,1,1] or [1,1,D]\n",
    "#     tgt = target\n",
    "\n",
    "#     # Case A: target == [D, 1, 1] → expand middle dim to T\n",
    "#     if tgt.dim() == 3 and tgt.shape[0] == output.shape[0] \\\n",
    "#        and tgt.shape[1] == 1 and tgt.shape[2] == 1:\n",
    "#         tgt = tgt.expand(-1, output.size(1), -1)\n",
    "\n",
    "#     # Case B: target == [1, 1, D] → permute to [D,1,1] then expand\n",
    "#     elif tgt.dim() == 3 and tgt.shape[0] == 1 \\\n",
    "#          and tgt.shape[1] == 1 and tgt.shape[2] == output.shape[0]:\n",
    "#         # permute (0,1,2) → (2,1,0) to get [D,1,1]\n",
    "#         tgt = tgt.permute(2, 1, 0)\n",
    "#         tgt = tgt.expand(-1, output.size(1), -1)\n",
    "\n",
    "#     else:\n",
    "#         # fallback: broadcast to exactly output.shape\n",
    "#         tgt = tgt.expand(output.shape)\n",
    "\n",
    "#     return Funct.mse_loss(output, tgt)\n",
    "\n",
    "# # 4) Free any lingering GPU memory\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # 5) Run the LR‐Finder on CPU for just 30 mini‐batches\n",
    "# lr_finder = LRFinder(\n",
    "#     model_cpu,\n",
    "#     optimizer_cpu,\n",
    "#     aligned_mse,\n",
    "#     device=\"cpu\"\n",
    "# )\n",
    "# lr_finder.range_test(\n",
    "#     small_loader,\n",
    "#     end_lr=1,     # maximum LR to try\n",
    "#     num_iter=30   # number of batches\n",
    "# )\n",
    "# lr_finder.plot()   # examine loss vs. LR curve\n",
    "# lr_finder.reset()  # restore original model & optimizer states\n",
    "\n",
    "# # 6) Move model back to GPU for your main training\n",
    "# model = model_cpu.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sees 1984 calendar days per epoch\n",
      "\n",
      "Baseline (zero‐forecast) RMSE on validation = 0.535586\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] GPU alloc = 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 62/62 [00:45<00:00,  1.37bundle/s, lr=0.001, train_rmse=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.2650 • val=0.3149 • lr=1.00e-03\n",
      "[Epoch 2] GPU alloc = 0.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 62/62 [00:45<00:00,  1.38bundle/s, lr=0.000999, train_rmse=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.2184 • val=0.3007 • lr=9.99e-04\n",
      "[Epoch 3] GPU alloc = 0.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 62/62 [00:42<00:00,  1.44bundle/s, lr=0.000998, train_rmse=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.2044 • val=0.2964 • lr=9.98e-04\n",
      "[Epoch 4] GPU alloc = 0.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  94%|█████████▎| 58/62 [00:40<00:02,  1.52bundle/s, lr=0.000997, train_rmse=0.204]"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Count how many calendar days we see each epoch\n",
    "# -----------------------------------------------------------------------------\n",
    "n_train_days = len(train_loader.dataset)  # dataset length = # unique days\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute baseline RMSE on validation (zero forecast)\n",
    "# -----------------------------------------------------------------------------\n",
    "baseline_val_rmse = models.naive_rmse(val_loader)\n",
    "print(f\"Baseline (zero‐forecast) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    cosine_sched        = cosine_sched,\n",
    "    plateau_sched       = plateau_sched,\n",
    "    scaler              = scaler,\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = MAX_EPOCHS,\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "    baseline_val_rmse   = baseline_val_rmse,\n",
    "    clipnorm            = clipnorm,\n",
    "    plateau_warmup      = PLAT_EPOCHS_WARMUP,   \n",
    "    device              = device\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final reporting: best RMSE and relative improvement\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "\n",
    "improvement = 100.0 * (1.0 - best_val_rmse / baseline_val_rmse)\n",
    "print(f\"Improvement over zero‐baseline = {improvement:5.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e0cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8660dd-d2db-434a-aa59-17814d343fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
