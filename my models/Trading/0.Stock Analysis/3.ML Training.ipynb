{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 23:39:21.533531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750714761.560315   72177 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750714761.568662   72177 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750714761.596724   72177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750714761.596760   72177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750714761.596763   72177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750714761.596765   72177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'stockanalibs' from '/mnt/g/My Drive/Ingegneria/Data Science GD/My-Practice/my models/Trading/0.Stock Analysis/stockanalibs.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import importlib\n",
    "import stockanalibs\n",
    "importlib.reload(stockanalibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:07:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:08:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:09:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:10:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:11:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:49:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>174.02</td>\n",
       "      <td>173.8900</td>\n",
       "      <td>173.99</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>173.9378</td>\n",
       "      <td>174.0422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:50:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>173.98</td>\n",
       "      <td>163.4137</td>\n",
       "      <td>173.92</td>\n",
       "      <td>524.0</td>\n",
       "      <td>173.8678</td>\n",
       "      <td>173.9722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:51:00</th>\n",
       "      <td>173.970</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8933</td>\n",
       "      <td>173.90</td>\n",
       "      <td>3898.0</td>\n",
       "      <td>173.8478</td>\n",
       "      <td>173.9522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:52:00</th>\n",
       "      <td>173.908</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8600</td>\n",
       "      <td>173.97</td>\n",
       "      <td>779.0</td>\n",
       "      <td>173.9178</td>\n",
       "      <td>174.0222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:53:00</th>\n",
       "      <td>173.960</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8911</td>\n",
       "      <td>174.00</td>\n",
       "      <td>939.0</td>\n",
       "      <td>173.9478</td>\n",
       "      <td>174.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296484 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open    high       low   close  volume       bid  \\\n",
       "2014-04-03 13:07:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:08:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:09:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:10:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:11:00  570.500  570.50  570.5000  570.50   100.0  570.3288   \n",
       "...                      ...     ...       ...     ...     ...       ...   \n",
       "2025-06-18 20:49:00  173.980  174.02  173.8900  173.99  1604.0  173.9378   \n",
       "2025-06-18 20:50:00  173.980  173.98  163.4137  173.92   524.0  173.8678   \n",
       "2025-06-18 20:51:00  173.970  174.00  173.8933  173.90  3898.0  173.8478   \n",
       "2025-06-18 20:52:00  173.908  174.00  173.8600  173.97   779.0  173.9178   \n",
       "2025-06-18 20:53:00  173.960  174.00  173.8911  174.00   939.0  173.9478   \n",
       "\n",
       "                          ask  trade_action  StrategyEarning  EarningDiff  \\\n",
       "2014-04-03 13:07:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:08:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:09:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:10:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:11:00  570.6712             0              0.0        0.000   \n",
       "...                       ...           ...              ...          ...   \n",
       "2025-06-18 20:49:00  174.0422             0              0.0        1.395   \n",
       "2025-06-18 20:50:00  173.9722             0              0.0        1.465   \n",
       "2025-06-18 20:51:00  173.9522             0              0.0        1.485   \n",
       "2025-06-18 20:52:00  174.0222             0              0.0        1.415   \n",
       "2025-06-18 20:53:00  174.0522             0              0.0        1.385   \n",
       "\n",
       "                     signal_smooth_adjusted  \n",
       "2014-04-03 13:07:00                0.071363  \n",
       "2014-04-03 13:08:00                0.078887  \n",
       "2014-04-03 13:09:00                0.087231  \n",
       "2014-04-03 13:10:00                0.096444  \n",
       "2014-04-03 13:11:00                0.106627  \n",
       "...                                     ...  \n",
       "2025-06-18 20:49:00                0.000000  \n",
       "2025-06-18 20:50:00                0.000000  \n",
       "2025-06-18 20:51:00                0.000000  \n",
       "2025-06-18 20:52:00                0.000000  \n",
       "2025-06-18 20:53:00                0.000000  \n",
       "\n",
       "[1296484 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.25     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.20     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-4     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 3e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.05     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 64       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,\n",
    "    flatten: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts one big minute-bar DataFrame (many days) into NumPy arrays ready for the stateful LSTM.\n",
    "    \n",
    "    RULES ENFORCED:\n",
    "      • Windows never cross midnight.\n",
    "      • Features and labels are standardized per day (to avoid leakage).\n",
    "    \n",
    "    Returns:\n",
    "      X         : Design matrix; every row is a sliding window (flattened if flatten is True).\n",
    "      y         : One-step-ahead targets corresponding to each window.\n",
    "      raw_close : Raw (unstandardized) close prices for each target.\n",
    "      raw_bid   : Raw bid prices.\n",
    "      raw_ask   : Raw ask prices.\n",
    "    \"\"\"\n",
    "    X_rows, y_rows, close_rows, bid_rows, ask_rows = [], [], [], [], []\n",
    "    \n",
    "    # Process one calendar day at a time.\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "        day_df = day_df.sort_index()\n",
    "        \n",
    "        # Extract raw price columns before scaling.\n",
    "        raw_close_vals = day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        raw_bid_vals   = day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        raw_ask_vals   = day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Standardize features and target per day.\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols])\n",
    "        day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]])\n",
    "        \n",
    "        feats_np = day_df[feature_cols].to_numpy(dtype=np.float32)  # shape: (T, n_feats)\n",
    "        label_np = day_df[label_col].to_numpy(dtype=np.float32)       # shape: (T,)\n",
    "        \n",
    "        # Create mask for Regular Trading Hours (RTH).\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():\n",
    "            continue\n",
    "        \n",
    "        T, _ = feats_np.shape\n",
    "        \n",
    "        # Build sliding windows using vectorized approach.\n",
    "        win_3d = sliding_window_view(feats_np, (look_back, feats_np.shape[1]), axis=(0, 1))\n",
    "        win_3d = win_3d[:, 0, :, :]  # (T - look_back + 1, look_back, n_feats)\n",
    "        \n",
    "        # Alignment fix: drop the last window so that targets align.\n",
    "        win_3d = win_3d[:-1]\n",
    "        y_aligned = label_np[look_back:]            # (T - look_back,)\n",
    "        close_aligned = raw_close_vals[look_back:]    # (T - look_back,)\n",
    "        bid_aligned   = raw_bid_vals[look_back:]      # (T - look_back,)\n",
    "        ask_aligned   = raw_ask_vals[look_back:]      # (T - look_back,)\n",
    "        \n",
    "        # Trim by RTH (apply mask to the target indices).\n",
    "        rth_mask_shifted = rth_mask[look_back:]\n",
    "        win_3d       = win_3d[rth_mask_shifted]\n",
    "        y_aligned    = y_aligned[rth_mask_shifted]\n",
    "        close_aligned = close_aligned[rth_mask_shifted]\n",
    "        bid_aligned   = bid_aligned[rth_mask_shifted]\n",
    "        ask_aligned   = ask_aligned[rth_mask_shifted]\n",
    "        \n",
    "        # Flatten each window if desired.\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))\n",
    "        else:\n",
    "            X_rows.append(win_3d)\n",
    "        y_rows.append(y_aligned)\n",
    "        close_rows.append(close_aligned)\n",
    "        bid_rows.append(bid_aligned)\n",
    "        ask_rows.append(ask_aligned)\n",
    "    \n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No valid RTH windows found; check rth_start or data gaps.\")\n",
    "    \n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "    raw_close = np.concatenate(close_rows).astype(np.float32)\n",
    "    raw_bid   = np.concatenate(bid_rows).astype(np.float32)\n",
    "    raw_ask   = np.concatenate(ask_rows).astype(np.float32)\n",
    "    \n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1077116, 300)\n",
      "(1077116,)\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    raw_close: np.ndarray,\n",
    "    raw_bid: np.ndarray,\n",
    "    raw_ask: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    "    TRAIN_BATCH: int  # added parameter for training batch size\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_train, y_train)\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_val, y_val)\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],    # (X_test, y_test, raw_close_test, raw_bid_test, raw_ask_test)\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id_tr, day_id_val, day_id_te\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits X, y (and the raw signals) into chronological train/val/test partitions by whole days.\n",
    "    \n",
    "    It uses the following logic:\n",
    "      1. Count the number of usable windows per calendar day.\n",
    "      2. Create a day_id vector that tags each sample with its day.\n",
    "      3. Compute the total number of calendar days, D.\n",
    "      4. Compute the \"original\" intended training set size as D * train_prop.\n",
    "      5. Round this count up (in day units) to the next multiple of TRAIN_BATCH.\n",
    "         (This ensures that the training set always contains full training batches.)\n",
    "      6. The validation split starts immediately after training ends. \n",
    "      7. The test split then follows, with no data dropped.\n",
    "    \n",
    "    Returns:\n",
    "      ((X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "       samples_per_day, day_id_tr, day_id_val, day_id_te)\n",
    "       \n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each calendar day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "    \"\"\"\n",
    "    # 1. Count usable windows per day (vectorized)\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                # minute rows today\n",
    "        idx = np.arange(T)\n",
    "        mask_window_ready = idx >= look_back\n",
    "        mask_rth_target   = day_df.index.time >= rth_start\n",
    "        usable_today = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "        samples_per_day.append(usable_today)\n",
    "    \n",
    "    # Verify that summed count equals len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "    \n",
    "    # 2. Build the day_id vector: each window gets the day index (0-based)\n",
    "    day_id_vec = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "    \n",
    "    # 3. Total number of days\n",
    "    D = len(samples_per_day)\n",
    "    \n",
    "    # 4. Compute the original intended training count (in days)\n",
    "    original_train_count = int(D * train_prop)\n",
    "    \n",
    "    # 5. Round up the training days to the next multiple of TRAIN_BATCH\n",
    "    # (We want the training portion to include full batches.)\n",
    "    new_train_count = int(np.ceil(original_train_count / TRAIN_BATCH) * TRAIN_BATCH)\n",
    "    # Make sure we don't exceed the total number of days.\n",
    "    new_train_count = min(new_train_count, D)\n",
    "    # In day_id_vec, days are 0-indexed, so the training cut is:\n",
    "    cut_train = new_train_count - 1\n",
    "    \n",
    "    # 6. Determine the validation cut-point using the original proportion.\n",
    "    # Validation ends at the day_index given by:\n",
    "    cut_val = int(D * (train_prop + val_prop))\n",
    "    \n",
    "    # 7. Create masks for the splits.\n",
    "    mask_tr = day_id_vec <= cut_train\n",
    "    mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te = day_id_vec > cut_val\n",
    "    \n",
    "    # 8. Slice X, y, and the raw arrays.\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_val, y_val = X[mask_val], y[mask_val]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "    \n",
    "    raw_close_te = raw_close[mask_te]\n",
    "    raw_bid_te   = raw_bid[mask_te]\n",
    "    raw_ask_te   = raw_ask[mask_te]\n",
    "    \n",
    "    # Also slice the day_id vector.\n",
    "    day_id_tr = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te = day_id_vec[mask_te]\n",
    "    \n",
    "    return (X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "           samples_per_day, day_id_tr, day_id_val, day_id_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1984 distinct calendar days retained, organized into batches of 64 days each (no partial batches).\n",
      "Validation: 409 distinct calendar days available for model validation.\n",
      "Test: 422 distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    TRAIN_BATCH=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(np.unique(day_id_tr))} distinct calendar days retained, organized into batches of {TRAIN_BATCH} days each (no partial batches).\")\n",
    "print(f\"Validation: {len(np.unique(day_id_val))} distinct calendar days available for model validation.\")\n",
    "print(f\"Test: {len(np.unique(day_id_te))} distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_dataset(\n",
    "    X           : np.ndarray,    # (N, …)\n",
    "    y           : np.ndarray,    # (N,)\n",
    "    day_id      : np.ndarray,    # (N,)\n",
    "    weekday_vec : np.ndarray,    # (N,)\n",
    "    raw_close   : np.ndarray = None,  # (N,) optional\n",
    "    raw_bid     : np.ndarray = None,  # (N,) optional\n",
    "    raw_ask     : np.ndarray = None   # (N,) optional\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset where each element corresponds to one calendar day.\n",
    "    \n",
    "    If raw price arrays are provided, yields a 6-tuple:\n",
    "      (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)\n",
    "    Otherwise, yields a 3-tuple:\n",
    "      (x_day, y_day, weekday)\n",
    "    \n",
    "    - x_day: (1, T, n_feats) float32\n",
    "    - y_day: (1, T)         float32\n",
    "    - weekday: ()           int32\n",
    "    \"\"\"\n",
    "    # Sort inputs in chronological order.\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "    if raw_close is not None and raw_bid is not None and raw_ask is not None:\n",
    "        raw_close, raw_bid, raw_ask = [a[idx] for a in (raw_close, raw_bid, raw_ask)]\n",
    "    \n",
    "    # Determine boundaries for each day.\n",
    "    change = np.where(np.diff(day_id) != 0)[0] + 1\n",
    "    day_slices = np.split(np.arange(len(day_id)), change)\n",
    "    \n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]      # (T, …)\n",
    "            y_block = y[sl]      # (T,)\n",
    "            weekday = int(weekday_vec[sl[0]])\n",
    "            if raw_close is None:\n",
    "                # Yield the original 3-tuple.\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "            else:\n",
    "                # Extract raw price slices.\n",
    "                close_block = raw_close[sl]  # (T,)\n",
    "                bid_block   = raw_bid[sl]    # (T,)\n",
    "                ask_block   = raw_ask[sl]    # (T,)\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),    # (1, T, …)\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),      # (1, T)\n",
    "                    np.expand_dims(close_block, 0).astype(np.float32),  # (1, T)\n",
    "                    np.expand_dims(bid_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.expand_dims(ask_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "    \n",
    "    feat_shape = X.shape[1:]\n",
    "    if raw_close is None:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    else:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_close_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_bid_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_ask_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(gen, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,           # training arrays\n",
    "        X_val, y_val, day_id_val,          # validation arrays\n",
    "        X_te, y_te, day_id_te,             # test arrays\n",
    "        raw_close_te, raw_bid_te, raw_ask_te,  # test raw price arrays\n",
    "        *,\n",
    "        df,                              # original DataFrame (for weekday vector)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits training, validation, and test arrays into day-level tf.data.Datasets.\n",
    "    \n",
    "    For training and validation, raw signals are not saved (3-tuple).\n",
    "    For testing, the raw price arrays (raw_close, raw_bid, raw_ask) are provided, yielding a 6-tuple.\n",
    "    \n",
    "    Returns:\n",
    "      ds_train_batched, ds_val_unbatched, ds_test_unbatched\n",
    "    \"\"\"\n",
    "    # Build one weekday vector covering all rows from the original DataFrame.\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)\n",
    "    \n",
    "    # Determine split lengths.\n",
    "    n_tr = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te = len(X_te)\n",
    "    \n",
    "    # Create weekday vectors for each split.\n",
    "    weekday_vec_tr = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr:n_tr+n_val]\n",
    "    weekday_vec_te = weekday_all[n_tr+n_val:n_tr+n_val+n_te]\n",
    "    \n",
    "    # Build training and validation datasets (3-tuple).\n",
    "    ds_tr = make_day_dataset(X_tr, y_tr, day_id_tr, weekday_vec_tr)\n",
    "    ds_val = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    \n",
    "    # Build test dataset with raw price arrays (6-tuple).\n",
    "    ds_test = make_day_dataset(X_te, y_te, day_id_te, weekday_vec_te,\n",
    "                               raw_close=raw_close_te, raw_bid=raw_bid_te, raw_ask=raw_ask_te)\n",
    "    \n",
    "    # For training, strip the extra outer batch dimension.\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd\n",
    "\n",
    "    ds_train_batched = (ds_tr\n",
    "                         .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                         .padded_batch(train_batch, drop_remainder=True)\n",
    "                         .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    # (Optional) Save the test dataset if needed.\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "    \n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750714795.205574   72177 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    X_val, y_val, day_id_val,\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,              # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,   # Total calendar days in the training epoch\n",
    "    max_epochs: int,     # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    ckpt_path           # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(ckpt_path)  # Save checkpoint of the best model.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.913413\n",
      "Training sees 1984 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c8678f7db94ba7ac589c3388e0042b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750714818.233243   72246 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=1.124836 • val=0.825934 • impr=  9.6% • lr=4.22e-05 • g≈26645.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107415b3d64649e68049c4ca0b28ee3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=1.021585 • val=0.786076 • impr= 13.9% • lr=8.30e-05 • g≈12301.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2187775787e34803bd237966cb831304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.986752 • val=0.770751 • impr= 15.6% • lr=2.66e-04 • g≈3711.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57f099e6c7640ce9f2b1244a99bbbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.953197 • val=0.759019 • impr= 16.9% • lr=1.08e-04 • g≈8811.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e999948575b4be4b0c1b0cfeab5e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.939105 • val=0.754801 • impr= 17.4% • lr=3.00e-04 • g≈3130.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e84b974c8f4ddaa0ee2cd6ce666287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.924878 • val=0.746606 • impr= 18.3% • lr=2.74e-04 • g≈3370.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6961e920c57047899b29498a08157ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.901597 • val=0.743203 • impr= 18.6% • lr=2.07e-04 • g≈4359.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202ff2b1098948b9968f5a7668b00e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 • train=0.887784 • val=0.741260 • impr= 18.8% • lr=1.22e-04 • g≈7305.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480c04c598654b3dad426c2ccbc5fa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 • train=0.880015 • val=0.740062 • impr= 19.0% • lr=4.91e-05 • g≈17907.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a216c02b4240adb4f0a44e88b12c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 • train=0.876316 • val=0.737554 • impr= 19.3% • lr=1.57e-05 • g≈55865.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538e2595913b4c32842e53a9ca6a96b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "file_path = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
