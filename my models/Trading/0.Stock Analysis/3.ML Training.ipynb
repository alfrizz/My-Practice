{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 20:37:42.452029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750963062.587770    5914 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750963062.629584    5914 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750963062.962076    5914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750963062.962137    5914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750963062.962140    5914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750963062.962143    5914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'stockanalibs' from '/mnt/g/My Drive/Ingegneria/Data Science GD/My-Practice/my models/Trading/0.Stock Analysis/stockanalibs.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm             # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import importlib\n",
    "import stockanalibs\n",
    "importlib.reload(stockanalibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:33:00</th>\n",
       "      <td>28.5959</td>\n",
       "      <td>28.5959</td>\n",
       "      <td>28.5932</td>\n",
       "      <td>28.5932</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>28.5846</td>\n",
       "      <td>28.6018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.625132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:34:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:35:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.670647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:36:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.689432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:37:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.706628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.3750</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.2150</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.5129</td>\n",
       "      <td>173.6171</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>2.331</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.5650</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.2400</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.3280</td>\n",
       "      <td>173.4320</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>2.516</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.3900</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.2000</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.2580</td>\n",
       "      <td>173.3620</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>2.586</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.3150</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.2300</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.2280</td>\n",
       "      <td>173.3320</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>2.616</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.3000</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.1700</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.5576</td>\n",
       "      <td>173.6618</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>2.286</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1235302 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close     volume  \\\n",
       "2014-04-03 13:33:00   28.5959   28.5959   28.5932   28.5932     5540.0   \n",
       "2014-04-03 13:34:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:35:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:36:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:37:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "...                       ...       ...       ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.3750  173.6771  173.2150  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.5650  173.5900  173.2400  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.3900  173.4100  173.2000  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.3150  173.4000  173.2300  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.3000  174.0500  173.1700  173.6097  7649838.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:33:00   28.5846   28.6018             0            0.000   \n",
       "2014-04-03 13:34:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:35:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:36:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:37:00   28.5724   28.5895             0            0.000   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.5129  173.6171             0           -0.379   \n",
       "2025-06-18 20:57:00  173.3280  173.4320             0           -0.379   \n",
       "2025-06-18 20:58:00  173.2580  173.3620             0           -0.379   \n",
       "2025-06-18 20:59:00  173.2280  173.3320             0           -0.379   \n",
       "2025-06-18 21:00:00  173.5576  173.6618             0           -0.379   \n",
       "\n",
       "                     EarningDiff  signal_smooth_adjusted  \n",
       "2014-04-03 13:33:00        0.000                0.625132  \n",
       "2014-04-03 13:34:00        0.000                0.649119  \n",
       "2014-04-03 13:35:00        0.000                0.670647  \n",
       "2014-04-03 13:36:00        0.000                0.689432  \n",
       "2014-04-03 13:37:00        0.000                0.706628  \n",
       "...                          ...                     ...  \n",
       "2025-06-18 20:56:00        2.331                0.000000  \n",
       "2025-06-18 20:57:00        2.516                0.000000  \n",
       "2025-06-18 20:58:00        2.586                0.000000  \n",
       "2025-06-18 20:59:00        2.616                0.000000  \n",
       "2025-06-18 21:00:00        2.286                0.000000  \n",
       "\n",
       "[1235302 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_final.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "weights_path   = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "model_path     = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 48       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.30     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.25     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-3     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 5e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.03     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 32       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts one big minute-bar DataFrame (many days) into NumPy arrays ready for the stateful LSTM.\n",
    "    \n",
    "    RULES ENFORCED:\n",
    "      • Windows never cross midnight.\n",
    "      • Features and labels are standardized per day (to avoid leakage).\n",
    "    \n",
    "    Returns:\n",
    "      X         : Design matrix; every row is a sliding window (flattened).\n",
    "      y         : One-step-ahead targets corresponding to each window.\n",
    "      raw_close : Raw (unstandardized) close prices for each target.\n",
    "      raw_bid   : Raw bid prices.\n",
    "      raw_ask   : Raw ask prices.\n",
    "    \"\"\"\n",
    "    X_rows, y_rows, close_rows, bid_rows, ask_rows = [], [], [], [], []\n",
    "    \n",
    "    # Process one calendar day at a time.\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "        day_df = day_df.sort_index()\n",
    "        \n",
    "        # Extract raw price columns before scaling.\n",
    "        raw_close = day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        raw_bid   = day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        raw_ask   = day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Standardize features and target per day.\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols]) ####################################################################\n",
    "        # day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]]) ###################################################################\n",
    "        \n",
    "        feats_np = day_df[feature_cols].to_numpy(dtype=np.float32)  # shape: (T, n_feats)\n",
    "        label_np = day_df[label_col].to_numpy(dtype=np.float32)       # shape: (T,)\n",
    "        \n",
    "        # Create mask for Regular Trading Hours (RTH).\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():\n",
    "            continue\n",
    "        \n",
    "        T, _ = feats_np.shape\n",
    "        \n",
    "        # Build sliding windows using vectorized approach.\n",
    "        win_3d = sliding_window_view(feats_np, (look_back, feats_np.shape[1]), axis=(0, 1))\n",
    "        # After the sliding window operation, we get an array whose shape is roughly (T - look_back + 1, 1, look_back, n_feats).\n",
    "        win_3d = win_3d[:, 0, :, :]  # removes the extra dimension: (T - look_back + 1, look_back, n_feats)\n",
    "        \n",
    "        # Alignment fix: drop the last window so that targets align.\n",
    "        win_3d = win_3d[:-1] # drops the very last window. This makes the number of windows exactly equal to the number of available targets.\n",
    "        y_aligned = label_np[look_back:]              # (T - look_back,)\n",
    "        close_aligned = raw_close[look_back:]    # (T - look_back,)\n",
    "        bid_aligned   = raw_bid[look_back:]      # (T - look_back,)\n",
    "        ask_aligned   = raw_ask[look_back:]      # (T - look_back,)\n",
    "        \n",
    "        # Trim by RTH (apply mask to the target indices).\n",
    "        rth_mask_shifted = rth_mask[look_back:]\n",
    "        win_3d       = win_3d[rth_mask_shifted]\n",
    "        y_aligned    = y_aligned[rth_mask_shifted]\n",
    "        close_aligned = close_aligned[rth_mask_shifted]\n",
    "        bid_aligned   = bid_aligned[rth_mask_shifted]\n",
    "        ask_aligned   = ask_aligned[rth_mask_shifted]\n",
    "        \n",
    "        # Flatten each window\n",
    "        X_rows.append(win_3d.reshape(win_3d.shape[0], -1))\n",
    "        y_rows.append(y_aligned)\n",
    "        close_rows.append(close_aligned)\n",
    "        bid_rows.append(bid_aligned)\n",
    "        ask_rows.append(ask_aligned)\n",
    "    \n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No valid RTH windows found; check rth_start or data gaps.\")\n",
    "    \n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "    raw_close = np.concatenate(close_rows).astype(np.float32)\n",
    "    raw_bid   = np.concatenate(bid_rows).astype(np.float32)\n",
    "    raw_ask   = np.concatenate(ask_rows).astype(np.float32)\n",
    "    \n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1066222, 300)\n",
      "(1066222,)\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    raw_close: np.ndarray,\n",
    "    raw_bid: np.ndarray,\n",
    "    raw_ask: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    "    TRAIN_BATCH: int  # added parameter for training batch size\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_train, y_train)\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_val, y_val)\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],    # (X_test, y_test, raw_close_test, raw_bid_test, raw_ask_test)\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id_tr, day_id_val, day_id_te\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits X, y (and the raw signals) into chronological train/val/test partitions by whole days.\n",
    "    The code creates a day-based vector (day_id_vec) and then applies masks to split all windows into training, validation, and test groups. \n",
    "    Each group is consecutive in terms of days, so no day is dropped or skipped—the splits follow in order. \n",
    "    This ensures that if we later recombine the splits, they cover all days from 0 to D–1.\n",
    "\n",
    "            [ Training ]         [ Validation ]            [ Test ]\n",
    "    Days: 0   ...   cut_train | cut_train+1 ... cut_val | cut_val+1 ... D-1\n",
    "    \n",
    "    It uses the following logic:\n",
    "      1. Count the number of usable windows per calendar day.\n",
    "      2. Create a day_id vector that tags each sample with its day.\n",
    "      3. Compute the total number of calendar days, D.\n",
    "      4. Compute the \"original\" intended training set size as D * train_prop.\n",
    "      5. Round this count up (in day units) to the next multiple of TRAIN_BATCH.\n",
    "         (This ensures that the training set always contains full training batches.)\n",
    "      6. The validation split starts immediately after training ends. \n",
    "      7. The test split then follows, with no data dropped.\n",
    "    \n",
    "    Returns:\n",
    "      ((X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "       samples_per_day, day_id_tr, day_id_val, day_id_te)\n",
    "       \n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each calendar day.\n",
    "        +-----------------------------------------------------------+\n",
    "        |  Total minute rows (T)                                    |\n",
    "        |                                                           |\n",
    "        |   0, 1, 2, ... , look_back-1  | look_back, ..., T-1         |\n",
    "        |      (Not ready)            |  (Potential windows)         |\n",
    "        |                              |   ┌──────────┐               |\n",
    "        |                              |   │ rth_start│ <-- Only count rows with timestamp >= rth_start\n",
    "        |                              |   └──────────┘               |\n",
    "        |                              |    Usable windows = count of rows satisfying both conditions\n",
    "        +-----------------------------------------------------------+\n",
    "\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "    \"\"\"\n",
    "    # 1. Count usable windows per day (vectorized)\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                # minute rows today\n",
    "        idx = np.arange(T)\n",
    "        mask_window_ready = idx >= look_back\n",
    "        mask_rth_target   = day_df.index.time >= rth_start\n",
    "        usable_today = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "        samples_per_day.append(usable_today)\n",
    "    \n",
    "    # Verify that summed count equals len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "    \n",
    "    # 2. Build the day_id vector: each window gets the day index (0-based)\n",
    "    day_id_vec = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "    \n",
    "    # 3. Total number of days\n",
    "    D = len(samples_per_day)\n",
    "    \n",
    "    # 4. Compute the original intended training count (in days)\n",
    "    original_train_count = int(D * train_prop)\n",
    "    \n",
    "    # 5. Round up the training days to the next multiple of TRAIN_BATCH\n",
    "    # (We want the training portion to include full batches.)\n",
    "    new_train_count = int(np.ceil(original_train_count / TRAIN_BATCH) * TRAIN_BATCH)\n",
    "    # Make sure we don't exceed the total number of days.\n",
    "    new_train_count = min(new_train_count, D)\n",
    "    # In day_id_vec, days are 0-indexed, so the training cut is:\n",
    "    cut_train = new_train_count - 1\n",
    "    \n",
    "    # 6. Determine the validation cut-point using the original proportion.\n",
    "    # Validation ends at the day_index given by:\n",
    "    cut_val = int(D * (train_prop + val_prop))\n",
    "\n",
    "    # 7. Create masks for the splits.\n",
    "    mask_tr = day_id_vec <= cut_train\n",
    "    mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te = day_id_vec > cut_val\n",
    "    \n",
    "    # 8. Slice X, y, and the raw arrays.\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_val, y_val = X[mask_val], y[mask_val]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "    \n",
    "    raw_close_te = raw_close[mask_te]\n",
    "    raw_bid_te   = raw_bid[mask_te]\n",
    "    raw_ask_te   = raw_ask[mask_te]\n",
    "    \n",
    "    # Also slice the day_id vector.\n",
    "    day_id_tr = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te = day_id_vec[mask_te]\n",
    "    \n",
    "    return (X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "           samples_per_day, day_id_tr, day_id_val, day_id_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1984 distinct calendar days retained, organized into batches of 32 days each (no partial batches).\n",
      "Validation: 412 distinct calendar days available for model validation.\n",
      "Test: 422 distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    TRAIN_BATCH=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(np.unique(day_id_tr))} distinct calendar days retained, organized into batches of {TRAIN_BATCH} days each (no partial batches).\")\n",
    "print(f\"Validation: {len(np.unique(day_id_val))} distinct calendar days available for model validation.\")\n",
    "print(f\"Test: {len(np.unique(day_id_te))} distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_dataset(\n",
    "    X           : np.ndarray,    # (N, …)\n",
    "    y           : np.ndarray,    # (N,)\n",
    "    day_id      : np.ndarray,    # (N,)\n",
    "    weekday_vec : np.ndarray,    # (N,)\n",
    "    raw_close   : np.ndarray = None,  # (N,) optional\n",
    "    raw_bid     : np.ndarray = None,  # (N,) optional\n",
    "    raw_ask     : np.ndarray = None   # (N,) optional\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset where each element corresponds to one calendar day.\n",
    "    \n",
    "    If raw price arrays are provided, yields a 6-tuple:\n",
    "      (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)\n",
    "    Otherwise, yields a 3-tuple:\n",
    "      (x_day, y_day, weekday)\n",
    "    \n",
    "    - x_day: (1, T, n_feats) float32\n",
    "    - y_day: (1, T)         float32\n",
    "    - weekday: ()           int32\n",
    "    \"\"\"\n",
    "    # Sort inputs in chronological order.\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "    if raw_close is not None and raw_bid is not None and raw_ask is not None:\n",
    "        raw_close, raw_bid, raw_ask = [a[idx] for a in (raw_close, raw_bid, raw_ask)]\n",
    "    \n",
    "    # Determine boundaries for each day.\n",
    "    # the code splits the dataset into daily blocks based on the day_id, \n",
    "    # then produces a TensorFlow dataset where each element represents one complete day’s data \n",
    "    # (with features, targets, and possibly raw prices) along with the corresponding weekday\n",
    "    change = np.where(np.diff(day_id) != 0)[0] + 1\n",
    "    day_slices = np.split(np.arange(len(day_id)), change)\n",
    "\n",
    "    # the generator function walks through the dataset day by day and “packs” each day’s data into one neat bundle. \n",
    "    # For each day it: Selects the day's data, Formats the data, Yields the bundle \n",
    "    # Using a generator is both memory efficient and flexible. It creates each day's data on the fly rather than building a huge list all at once.\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]      # (T, …)\n",
    "            y_block = y[sl]      # (T,)\n",
    "            weekday = int(weekday_vec[sl[0]])\n",
    "            if raw_close is None:\n",
    "                # Yield the original 3-tuple.\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "            else:\n",
    "                # Extract raw price slices.\n",
    "                close_block = raw_close[sl]  # (T,)\n",
    "                bid_block   = raw_bid[sl]    # (T,)\n",
    "                ask_block   = raw_ask[sl]    # (T,)\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),    # (1, T, …)\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),      # (1, T)\n",
    "                    np.expand_dims(close_block, 0).astype(np.float32),  # (1, T)\n",
    "                    np.expand_dims(bid_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.expand_dims(ask_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "    \n",
    "    feat_shape = X.shape[1:]\n",
    "    if raw_close is None:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    else:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_close_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_bid_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_ask_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(gen, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,                 # training arrays\n",
    "        X_val, y_val, day_id_val,              # validation arrays\n",
    "        X_te, y_te, day_id_te,                 # test arrays\n",
    "        raw_close_te, raw_bid_te, raw_ask_te,  # test raw price arrays\n",
    "        *,\n",
    "        df,                                    # original DataFrame (for weekday vector)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits training, validation, and test arrays into day-level tf.data.Datasets.\n",
    "    \n",
    "    For training and validation, raw signals are not saved (3-tuple).\n",
    "    For testing, the raw price arrays (raw_close, raw_bid, raw_ask) are provided, yielding a 6-tuple.\n",
    "    \n",
    "    Returns:\n",
    "      ds_train_batched, ds_val_unbatched, ds_test_unbatched\n",
    "    \"\"\"\n",
    "    # Build one weekday vector covering all rows from the original DataFrame.\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)\n",
    "    \n",
    "    # Determine split lengths.\n",
    "    n_tr = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te = len(X_te)\n",
    "    \n",
    "    # Create weekday vectors for each split.\n",
    "    # they will be used to identify the end of the week and reset the long-term state of the LSTM layers\n",
    "    weekday_vec_tr = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr:n_tr+n_val]\n",
    "    weekday_vec_te = weekday_all[n_tr+n_val:n_tr+n_val+n_te]\n",
    "    \n",
    "    # Build training and validation datasets (3-tuple).\n",
    "    ds_tr = make_day_dataset(X_tr, y_tr, day_id_tr, weekday_vec_tr)\n",
    "    ds_val = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    \n",
    "    # Build test dataset with raw price arrays (6-tuple).\n",
    "    ds_test = make_day_dataset(X_te, y_te, day_id_te, weekday_vec_te,\n",
    "                               raw_close=raw_close_te, raw_bid=raw_bid_te, raw_ask=raw_ask_te)\n",
    "    \n",
    "    # For training, strip the extra outer batch dimension.\n",
    "    # the _strip function only on the training dataset to remove that extra outer dimension \n",
    "    # In the validation or test datasets we don't need to strip this dimension because those pipelines expect one sample at a time \n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd\n",
    "\n",
    "    ds_train_batched = (ds_tr\n",
    "                         .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                         .padded_batch(train_batch, drop_remainder=True)\n",
    "                         .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    # Save the test dataset.\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "    \n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750963091.863029    5914 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    X_val, y_val, day_id_val,\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n",
    "\n",
    "# save validation model, to reuse for inference\n",
    "model_val.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,          # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,                    # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,         # Total calendar days in the training epoch\n",
    "    max_epochs: int,           # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    weights_path               # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(weights_path)  # Save checkpoint of the best weights.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(weights_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.422795\n",
      "Training sees 1984 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e4c5e5cefe46d08bdccc647442ee8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750963115.096843    5982 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.812090 • val=0.309239 • impr= 26.9% • lr=1.31e-04 • g≈6208.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f15822ab554c28871a962271c2d9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.598902 • val=0.272110 • impr= 35.6% • lr=1.74e-04 • g≈3450.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e6a1b0841c479bacf730f5069b5097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.525628 • val=0.297252 • impr= 29.7% • lr=4.56e-04 • g≈1151.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d5fb7a66834c6a99bacdb758bbb133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.449128 • val=0.248953 • impr= 41.1% • lr=1.96e-04 • g≈2288.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fe683958e5410390891fd389b450ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.413580 • val=0.245274 • impr= 42.0% • lr=1.62e-05 • g≈25580.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58060b7c9b4f494aba3a480027dd2ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.396486 • val=0.277266 • impr= 34.4% • lr=4.63e-04 • g≈856.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee83860893041debfbf281e7c293c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.350196 • val=0.232865 • impr= 44.9% • lr=3.52e-04 • g≈993.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c73c5611424754be5ad1a8f6564f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 • train=0.323756 • val=0.234430 • impr= 44.6% • lr=2.08e-04 • g≈1557.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3b8dd677144ebcb70331f934a9b53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 • train=0.306936 • val=0.243749 • impr= 42.3% • lr=8.11e-05 • g≈3786.93\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3bf7d1d75c4d07aef83c4d8566b5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 • train=0.301226 • val=0.231132 • impr= 45.3% • lr=1.76e-05 • g≈17091.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce538e18104402cba9ec98c9b449a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 • train=0.303060 • val=0.256403 • impr= 39.4% • lr=4.94e-04 • g≈614.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22846e63a33432aa3c0fa0766a6700f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 • train=0.287834 • val=0.232182 • impr= 45.1% • lr=4.66e-04 • g≈617.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e54aef084434fafad42827ce5c85f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 • train=0.273594 • val=0.231263 • impr= 45.3% • lr=4.19e-04 • g≈652.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8ab7e6fbb84cac93874eb438c5a1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 • train=0.262022 • val=0.230475 • impr= 45.5% • lr=3.58e-04 • g≈732.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e0fd0896aa45eebd45af00e8f02311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 • train=0.255113 • val=0.226959 • impr= 46.3% • lr=2.87e-04 • g≈888.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8ac253ab2449ee84a616577fcd3150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 • train=0.249638 • val=0.230077 • impr= 45.6% • lr=2.14e-04 • g≈1168.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2f1e8e67724390a494de7003f63b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 • train=0.244001 • val=0.235671 • impr= 44.3% • lr=1.44e-04 • g≈1691.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9840a08e513a42269737ecfb383c27ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 • train=0.240863 • val=0.237345 • impr= 43.9% • lr=8.52e-05 • g≈2827.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3056ff4e2e4215be3679c39291e727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 • train=0.240838 • val=0.230464 • impr= 45.5% • lr=4.20e-05 • g≈5737.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40df18cb10004805a53f1c5db44e09f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 • train=0.240347 • val=0.226520 • impr= 46.4% • lr=1.86e-05 • g≈12942.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17811a7c92874f38988c4ba28cb7244c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 • train=0.243039 • val=0.245116 • impr= 42.0% • lr=4.99e-04 • g≈486.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc93044ac9024b33aea7a0c4dd97bcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 • train=0.246986 • val=0.229706 • impr= 45.7% • lr=4.94e-04 • g≈499.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841d24ebf5014320a0e206f3993bb6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 • train=0.241713 • val=0.229266 • impr= 45.8% • lr=4.84e-04 • g≈499.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d5c4e4d9f34874b8b5ce016dd8e9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 • train=0.236780 • val=0.229184 • impr= 45.8% • lr=4.68e-04 • g≈506.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf0c84c3b0743ef945a6ffe29641a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 • train=0.233209 • val=0.228944 • impr= 45.8% • lr=4.47e-04 • g≈521.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55007f02c8441a5ba544833bbd95aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 • train=0.230474 • val=0.227527 • impr= 46.2% • lr=4.22e-04 • g≈546.57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f183bc4f81842c2a59ea3eb5ac1b0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 • train=0.228689 • val=0.225042 • impr= 46.8% • lr=3.93e-04 • g≈582.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94945ca49b5e4871b375a21166a93fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 • train=0.226489 • val=0.224441 • impr= 46.9% • lr=3.61e-04 • g≈628.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a00196220c4e31a3e7cc850f8df6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 • train=0.223807 • val=0.225243 • impr= 46.7% • lr=3.26e-04 • g≈686.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120882fb933447c28257003b1241248d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 • train=0.221344 • val=0.225948 • impr= 46.6% • lr=2.90e-04 • g≈762.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dad762c7ab34a719cde9686ff362d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 • train=0.219698 • val=0.227163 • impr= 46.3% • lr=2.53e-04 • g≈867.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7993f785e50e47ac847329445a063ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 • train=0.217898 • val=0.228726 • impr= 45.9% • lr=2.17e-04 • g≈1005.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4420e5670b4768a9139d3ec3b6d0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 • train=0.215496 • val=0.227975 • impr= 46.1% • lr=1.81e-04 • g≈1191.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd04f46319641488a2a6be44572a06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 • train=0.216938 • val=0.226250 • impr= 46.5% • lr=1.47e-04 • g≈1477.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1273d44c491743b885ac7d4715eaed58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 • train=0.216778 • val=0.224601 • impr= 46.9% • lr=1.15e-04 • g≈1877.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bea17d78bda4ff8ada7841c38e9781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 036:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 • train=0.216219 • val=0.223593 • impr= 47.1% • lr=8.73e-05 • g≈2476.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efef1d22421140728d884dd212aa4b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 037:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 • train=0.215530 • val=0.223092 • impr= 47.2% • lr=6.31e-05 • g≈3416.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e03d1eb9ab94c85b250030219a938aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 038:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 • train=0.214893 • val=0.222887 • impr= 47.3% • lr=4.34e-05 • g≈4956.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4f5c321fd24bdb9a8b78b665e6c63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 039:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 • train=0.214712 • val=0.222680 • impr= 47.3% • lr=2.86e-05 • g≈7512.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf2455ea725477681425a04a2107c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 040:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 • train=0.214274 • val=0.222503 • impr= 47.4% • lr=1.91e-05 • g≈11220.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139b78e42e5b4eb9acfb137d3ef62130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 041:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 • train=0.214107 • val=0.222439 • impr= 47.4% • lr=1.51e-05 • g≈14157.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1637d0ed9044f4bf0b3df5c5cc8105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 042:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 • train=0.217813 • val=0.224082 • impr= 47.0% • lr=5.00e-04 • g≈436.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bc6e3e12fa4feaa8cc55dc1ce2cd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 043:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 • train=0.217200 • val=0.223928 • impr= 47.0% • lr=4.98e-04 • g≈436.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86682374df5a464fb5753d7b4fe44106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 044:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 • train=0.216290 • val=0.223609 • impr= 47.1% • lr=4.95e-04 • g≈437.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c529f9cb8baf409f9bbd2ac1afdf058a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 045:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 • train=0.215740 • val=0.223474 • impr= 47.1% • lr=4.90e-04 • g≈440.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bd4ca65abb40eab51c5e0cbdc5cf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 046:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 • train=0.214863 • val=0.223296 • impr= 47.2% • lr=4.84e-04 • g≈443.87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0d51a6fc614a0eaa737788ca9f40de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 047:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 • train=0.214308 • val=0.223152 • impr= 47.2% • lr=4.77e-04 • g≈449.43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a847f4870da54f7a9be667ba475a111a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 048:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 • train=0.213414 • val=0.222950 • impr= 47.3% • lr=4.68e-04 • g≈455.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1373baa550b41f9b6b55157f88e30c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 049:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 • train=0.212904 • val=0.222820 • impr= 47.3% • lr=4.59e-04 • g≈464.21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3248c3e676f442149aee6afbc421ca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 050:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 • train=0.212244 • val=0.222750 • impr= 47.3% • lr=4.48e-04 • g≈474.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a138f1b549b64e43a49ae86e3dbd24d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 051:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 • train=0.211687 • val=0.222959 • impr= 47.3% • lr=4.36e-04 • g≈485.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75721d355484e4086bea585d960efc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 052:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 052 • train=0.211071 • val=0.222977 • impr= 47.3% • lr=4.23e-04 • g≈499.26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01799eb7d31a48e5964cd9eaecd9e44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 053:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053 • train=0.210326 • val=0.222704 • impr= 47.3% • lr=4.09e-04 • g≈514.50\n",
      "Early stopping triggered.\n",
      "\n",
      "Champion validation RMSE = 0.222439\n",
      "Improvement vs baseline   =  47.4 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        weights_path        = weights_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
