{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f06a4f-691a-4a84-a305-e7212eb879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3b57ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:11:23.411162: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-02 14:11:23.411580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-02 14:11:23.440170: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'stockanalibs' from '/workspace/stockanalibs.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations               # allow postponed evaluation of annotations\n",
    "\n",
    "import os                                          # for setting environment variables\n",
    "# — Silence noisy logs —\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"           # only show warning+errors\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # disable XLA memory prealloc\n",
    "# ensure we load cuDNN9 before any TF bindings\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "import ctypes                                      # to force-load cudnn.so.9\n",
    "# Force-load cuDNN from /usr/local/cuda/lib64/libcudnn.so (v9.x)\n",
    "ctypes.CDLL(\"libcudnn.so\", mode=ctypes.RTLD_GLOBAL)\n",
    "\n",
    "import logging                                     # for controlling TF log level\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import tensorflow as tf                            # core TF\n",
    "# — Configure TensorFlow —\n",
    "tf.config.optimizer.set_jit(True)                   # enable XLA JIT compilation\n",
    "# allow GPU memory to grow instead of pre-allocating\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# — Keras & TF utilities —\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer  # FP16 loss scaling\n",
    "from tensorflow.keras.optimizers import Adam                    # optimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts  # LR schedule\n",
    "from tensorflow.keras import layers, models, metrics, regularizers, optimizers, initializers   # NN building blocks\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# — Data & plotting libs —\n",
    "import numpy as np                                  # numerical arrays\n",
    "from numpy.lib.stride_tricks import sliding_window_view  # sliding windows\n",
    "import pandas as pd                                 # dataframes\n",
    "from sklearn.preprocessing import StandardScaler    # scaling features\n",
    "\n",
    "import matplotlib.pyplot as plt                      # for plotting training curves\n",
    "from IPython.display import display, update_display, clear_output  # Jupyter helpers\n",
    "\n",
    "from tqdm.auto import tqdm                          # progress bars in notebooks\n",
    "\n",
    "# — Misc utilities —\n",
    "import time                                        # timing\n",
    "import math                                        # math functions\n",
    "import pickle                                      # serialization\n",
    "import platform                                    # platform info\n",
    "import sys                                         # system-specific parameters\n",
    "from datetime import datetime\n",
    "from pathlib import Path                            # filesystem paths\n",
    "from typing import Sequence, Tuple, List           # type hints\n",
    "\n",
    "# — Custom libs —\n",
    "import importlib\n",
    "import stockanalibs                                # user utilities\n",
    "importlib.reload(stockanalibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b88c5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:    3.10.12\n",
      "TensorFlow: 2.15.0\n",
      "Built with CUDA: 12.4\n",
      "cuDNN version: 9\n",
      "\n",
      "Physical GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Logical GPUs:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "All devices:\n",
      " - /device:CPU:0        CPU     memory_limit=268435456\n",
      " - /device:GPU:0        GPU     memory_limit=13798211584\n"
     ]
    }
   ],
   "source": [
    "# 1) Basic TF info\n",
    "print(f\"Python:    {platform.python_version()}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "build_info = tf.sysconfig.get_build_info()\n",
    "print(f\"Built with CUDA: {build_info['cuda_version']}\")\n",
    "print(f\"cuDNN version: {build_info['cudnn_version']}\\n\")\n",
    "\n",
    "# 2) GPU summary\n",
    "phys = tf.config.list_physical_devices(\"GPU\")\n",
    "logi = tf.config.list_logical_devices(\"GPU\")\n",
    "print(f\"Physical GPUs: {phys}\")\n",
    "print(f\"Logical GPUs:  {logi}\\n\")\n",
    "\n",
    "# 3) List all local devices via device_lib\n",
    "print(\"All devices:\")\n",
    "for d in device_lib.list_local_devices():\n",
    "    print(f\" - {d.name:20s} {d.device_type:6s}  memory_limit={d.memory_limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08b80ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:00:00</th>\n",
       "      <td>28.6251</td>\n",
       "      <td>28.6251</td>\n",
       "      <td>28.62100</td>\n",
       "      <td>28.62100</td>\n",
       "      <td>4900.0</td>\n",
       "      <td>28.61240</td>\n",
       "      <td>28.62960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.018362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:01:00</th>\n",
       "      <td>28.6258</td>\n",
       "      <td>28.6258</td>\n",
       "      <td>28.62182</td>\n",
       "      <td>28.62182</td>\n",
       "      <td>5056.6</td>\n",
       "      <td>28.61324</td>\n",
       "      <td>28.63042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.020963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:02:00</th>\n",
       "      <td>28.6265</td>\n",
       "      <td>28.6265</td>\n",
       "      <td>28.62264</td>\n",
       "      <td>28.62264</td>\n",
       "      <td>5213.2</td>\n",
       "      <td>28.61408</td>\n",
       "      <td>28.63124</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.023603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:03:00</th>\n",
       "      <td>28.6272</td>\n",
       "      <td>28.6272</td>\n",
       "      <td>28.62346</td>\n",
       "      <td>28.62346</td>\n",
       "      <td>5369.8</td>\n",
       "      <td>28.61492</td>\n",
       "      <td>28.63206</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.026278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:04:00</th>\n",
       "      <td>28.6279</td>\n",
       "      <td>28.6279</td>\n",
       "      <td>28.62428</td>\n",
       "      <td>28.62428</td>\n",
       "      <td>5526.4</td>\n",
       "      <td>28.61576</td>\n",
       "      <td>28.63288</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.029433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.3750</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.21500</td>\n",
       "      <td>173.56500</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.51290</td>\n",
       "      <td>173.61710</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>1.729</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.5650</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.24000</td>\n",
       "      <td>173.38000</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.32800</td>\n",
       "      <td>173.43200</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>1.914</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.3900</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.20000</td>\n",
       "      <td>173.31000</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.25800</td>\n",
       "      <td>173.36200</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>1.984</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.3150</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.23000</td>\n",
       "      <td>173.28000</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.22800</td>\n",
       "      <td>173.33200</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>2.014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.3000</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.17000</td>\n",
       "      <td>173.60970</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.55760</td>\n",
       "      <td>173.66180</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>1.684</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354977 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high        low      close     volume  \\\n",
       "2014-04-03 13:00:00   28.6251   28.6251   28.62100   28.62100     4900.0   \n",
       "2014-04-03 13:01:00   28.6258   28.6258   28.62182   28.62182     5056.6   \n",
       "2014-04-03 13:02:00   28.6265   28.6265   28.62264   28.62264     5213.2   \n",
       "2014-04-03 13:03:00   28.6272   28.6272   28.62346   28.62346     5369.8   \n",
       "2014-04-03 13:04:00   28.6279   28.6279   28.62428   28.62428     5526.4   \n",
       "...                       ...       ...        ...        ...        ...   \n",
       "2025-06-18 20:56:00  173.3750  173.6771  173.21500  173.56500   621199.0   \n",
       "2025-06-18 20:57:00  173.5650  173.5900  173.24000  173.38000   624198.0   \n",
       "2025-06-18 20:58:00  173.3900  173.4100  173.20000  173.31000   454542.0   \n",
       "2025-06-18 20:59:00  173.3150  173.4000  173.23000  173.28000  1094746.0   \n",
       "2025-06-18 21:00:00  173.3000  174.0500  173.17000  173.60970  7649838.0   \n",
       "\n",
       "                           bid        ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:00:00   28.61240   28.62960             0            0.000   \n",
       "2014-04-03 13:01:00   28.61324   28.63042             0            0.000   \n",
       "2014-04-03 13:02:00   28.61408   28.63124             0            0.000   \n",
       "2014-04-03 13:03:00   28.61492   28.63206             0            0.000   \n",
       "2014-04-03 13:04:00   28.61576   28.63288             0            0.000   \n",
       "...                        ...        ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.51290  173.61710             0           -0.981   \n",
       "2025-06-18 20:57:00  173.32800  173.43200             0           -0.981   \n",
       "2025-06-18 20:58:00  173.25800  173.36200             0           -0.981   \n",
       "2025-06-18 20:59:00  173.22800  173.33200             0           -0.981   \n",
       "2025-06-18 21:00:00  173.55760  173.66180             0           -0.981   \n",
       "\n",
       "                     EarningDiff  signal_smooth  \n",
       "2014-04-03 13:00:00        0.000       1.018362  \n",
       "2014-04-03 13:01:00        0.000       1.020963  \n",
       "2014-04-03 13:02:00        0.000       1.023603  \n",
       "2014-04-03 13:03:00        0.000       1.026278  \n",
       "2014-04-03 13:04:00        0.000       1.029433  \n",
       "...                          ...            ...  \n",
       "2025-06-18 20:56:00        1.729       0.000000  \n",
       "2025-06-18 20:57:00        1.914       0.000000  \n",
       "2025-06-18 20:58:00        1.984       0.000000  \n",
       "2025-06-18 20:59:00        2.014       0.000000  \n",
       "2025-06-18 21:00:00        1.684       0.000000  \n",
       "\n",
       "[1354977 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_final.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fb5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) \n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "weights_path   = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "model_path     = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 192      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.25     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.20     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "KERNEL_REG          = 1e-4     # L2 regularization factor for LSTM layers\n",
    "    \n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 3e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.01     # Minimum LR factor relative to INITIAL_LR\n",
    "CLIPNORM            = 2.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 64       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 100      # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 10       # Early stopping patience (10–20 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85dc7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts a minute-bar DataFrame into arrays for a stateful LSTM with fixed\n",
    "    window length look_back.  Returns:\n",
    "\n",
    "      X         : shape (n_samples, look_back, n_feats)\n",
    "      y         : shape (n_samples,)\n",
    "      raw_close : shape (n_samples,)\n",
    "      raw_bid   : shape (n_samples,)\n",
    "      raw_ask   : shape (n_samples,)\n",
    "    \"\"\"\n",
    "    X_windows, y_list, close_list, bid_list, ask_list = [], [], [], [], []\n",
    "\n",
    "    for date, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # raw prices before scaling\n",
    "        raw_close = day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        raw_bid   = day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        raw_ask   = day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # per-day standardization of features\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols])\n",
    "        feats_np = day_df[feature_cols].to_numpy(dtype=np.float32)  # (T, n_feats)\n",
    "        label_np = day_df[label_col].to_numpy(dtype=np.float32)     # (T,)\n",
    "\n",
    "        # only windows whose final timestamp ≥ rth_start\n",
    "        mask_rth = day_df.index.time >= rth_start\n",
    "        if not mask_rth.any():\n",
    "            continue\n",
    "\n",
    "        # build all sliding windows of shape (T - look_back + 1, look_back, n_feats)\n",
    "        win3d = sliding_window_view(feats_np, (look_back, feats_np.shape[1]), axis=(0,1))\n",
    "        win3d = win3d[:,0]           # drop extra dim → (n_wins, look_back, n_feats)\n",
    "\n",
    "        # align targets: drop last window so y has same count\n",
    "        win3d = win3d[:-1]           # now (T - look_back, look_back, n_feats)\n",
    "        y_aligned     = label_np[look_back:]     # (T - look_back,)\n",
    "        close_aligned = raw_close[look_back:]\n",
    "        bid_aligned   = raw_bid[look_back:]\n",
    "        ask_aligned   = raw_ask[look_back:]\n",
    "\n",
    "        # only keep windows within RTH\n",
    "        rth_win_mask  = mask_rth[look_back:]\n",
    "        win3d         = win3d[rth_win_mask]\n",
    "        y_aligned     = y_aligned[rth_win_mask]\n",
    "        close_aligned = close_aligned[rth_win_mask]\n",
    "        bid_aligned   = bid_aligned[rth_win_mask]\n",
    "        ask_aligned   = ask_aligned[rth_win_mask]\n",
    "\n",
    "        # collect without flattening\n",
    "        X_windows.append(win3d)  \n",
    "        y_list   .append(y_aligned)\n",
    "        close_list.append(close_aligned)\n",
    "        bid_list .append(bid_aligned)\n",
    "        ask_list .append(ask_aligned)\n",
    "\n",
    "    if not X_windows:\n",
    "        raise ValueError(\"No RTH windows found; check rth_start or data.\")\n",
    "\n",
    "    # concatenate along sample axis\n",
    "    X         = np.concatenate(X_windows, axis=0)  # (n_samples, look_back, n_feats)\n",
    "    y         = np.concatenate(y_list,    axis=0)\n",
    "    raw_close = np.concatenate(close_list,axis=0)\n",
    "    raw_bid   = np.concatenate(bid_list,  axis=0)\n",
    "    raw_ask   = np.concatenate(ask_list,  axis=0)\n",
    "\n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101447, 90, 5)\n",
      "(1101447,)\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed58229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,             # shape (n_samples, look_back, n_feats)\n",
    "    y: np.ndarray,             # shape (n_samples,)\n",
    "    raw_close: np.ndarray,     \n",
    "    raw_bid: np.ndarray,\n",
    "    raw_ask: np.ndarray,\n",
    "    df: pd.DataFrame,           # same minute‐bar DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,            # window length in timesteps\n",
    "    rth_start: dt.time,        # “regular” start time mask\n",
    "    train_prop: float,         # fraction of days → train\n",
    "    val_prop: float,           # fraction of days → validation\n",
    "    TRAIN_BATCH: int           # batch size (must divide # days after rounding)\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    \n",
    "        Tuple[np.ndarray, np.ndarray],    \n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n",
    "        List[int],\n",
    "        np.ndarray, np.ndarray, np.ndarray\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits X, y, and the raw price arrays into chronological train/val/test sets\n",
    "    by whole calendar days.  Each split is a contiguous block of days, and the\n",
    "    train portion is rounded up to a multiple of TRAIN_BATCH days to guarantee\n",
    "    full batches.\n",
    "\n",
    "    Args:\n",
    "      X            (n_samples, look_back, n_feats)\n",
    "      y            (n_samples,)\n",
    "      raw_*        (n_samples,)\n",
    "      df           original minute‐bar DataFrame (for day boundaries)\n",
    "      look_back    window length in timesteps\n",
    "      rth_start    time-of-day to start counting samples each day\n",
    "      train_prop   fraction of days to use for training\n",
    "      val_prop     fraction of days (after train) to use for validation\n",
    "      TRAIN_BATCH  number of days per “batch” (rounds train days up to multiple)\n",
    "\n",
    "    Returns:\n",
    "      (X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "      samples_per_day, day_id_tr, day_id_val, day_id_te\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) count usable windows per calendar day\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)\n",
    "        idx = np.arange(T)\n",
    "        mask_ready = idx >= look_back\n",
    "        mask_rth   = day_df.index.time >= rth_start\n",
    "        samples_per_day.append(int((mask_ready & mask_rth).sum()))\n",
    "\n",
    "    # sanity check\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\"X length mismatch: check look_back & rth_start vs. build_lstm_tensors().\")\n",
    "\n",
    "    # 2) tag each sample by its day index\n",
    "    day_id_vec = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # 3) total days\n",
    "    D = len(samples_per_day)\n",
    "\n",
    "    # 4) compute training days → round up to full TRAIN_BATCH days\n",
    "    orig_train_days = int(D * train_prop)\n",
    "    train_days = int(np.ceil(orig_train_days / TRAIN_BATCH) * TRAIN_BATCH)\n",
    "    train_days = min(train_days, D)\n",
    "    cut_train = train_days - 1\n",
    "\n",
    "    # 5) validation cut\n",
    "    cut_val = int(D * (train_prop + val_prop))\n",
    "\n",
    "    # 6) build boolean masks\n",
    "    mask_tr  = day_id_vec <= cut_train\n",
    "    mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te  = day_id_vec > cut_val\n",
    "\n",
    "    # 7) slice arrays (X remains 3-D, masks work on axis-0)\n",
    "    X_tr,    y_tr    = X[mask_tr],    y[mask_tr]\n",
    "    X_val,  y_val   = X[mask_val],  y[mask_val]\n",
    "    X_te,   y_te    = X[mask_te],   y[mask_te]\n",
    "    raw_close_te     = raw_close[mask_te]\n",
    "    raw_bid_te       = raw_bid[mask_te]\n",
    "    raw_ask_te       = raw_ask[mask_te]\n",
    "\n",
    "    # 8) per-split day-IDs (useful for state reset logic)\n",
    "    day_id_tr  = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te  = day_id_vec[mask_te]\n",
    "\n",
    "    return (\n",
    "        (X_tr,    y_tr),\n",
    "        (X_val,  y_val),\n",
    "        (X_te,    y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "        samples_per_day,\n",
    "        day_id_tr, day_id_val, day_id_te\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1984 distinct calendar days retained, organized into batches of 64 days each (no partial batches).\n",
      "Validation: 411 distinct calendar days available for model validation.\n",
      "Test: 422 distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    TRAIN_BATCH=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(np.unique(day_id_tr))} distinct calendar days retained, organized into batches of {TRAIN_BATCH} days each (no partial batches).\")\n",
    "print(f\"Validation: {len(np.unique(day_id_val))} distinct calendar days available for model validation.\")\n",
    "print(f\"Test: {len(np.unique(day_id_te))} distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57469293-3f60-45d3-af16-81466e9977b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps/epoch: 12121 First decay steps: 60605\n"
     ]
    }
   ],
   "source": [
    "# Compute steps and decay‐steps\n",
    "n_train_samples = X_tr.shape[0]\n",
    "steps_per_epoch = n_train_samples // TRAIN_BATCH\n",
    "FIRST_DECAY_STEPS = FIRST_DECAY_EPOCHS * steps_per_epoch\n",
    "print(\"Steps/epoch:\", steps_per_epoch, \"First decay steps:\", FIRST_DECAY_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc12fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_dataset(\n",
    "    X           : np.ndarray,    # (n_samples, look_back, n_feats)\n",
    "    y           : np.ndarray,    # (n_samples,)\n",
    "    day_id      : np.ndarray,    # (n_samples,)\n",
    "    weekday_vec : np.ndarray,    # (n_samples,)\n",
    "    raw_close   : np.ndarray = None,  # (n_samples,) optional\n",
    "    raw_bid     : np.ndarray = None,  # (n_samples,) optional\n",
    "    raw_ask     : np.ndarray = None   # (n_samples,) optional\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset where each element is one calendar day’s worth of\n",
    "    sliding windows.  Yields either:\n",
    "\n",
    "      • (x_day, y_day, weekday)  \n",
    "        x_day: (1, n_windows, look_back, n_feats)  \n",
    "        y_day: (1, n_windows)  \n",
    "        weekday: scalar  \n",
    "\n",
    "    or, if raw prices provided:\n",
    "\n",
    "      • (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)  \n",
    "        raw_*_day: (1, n_windows)\n",
    "\n",
    "    where `n_windows` is the number of look_back-length windows for that day.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) sort by day, group into slices\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "    if raw_close is not None:\n",
    "        raw_close, raw_bid, raw_ask = [a[idx] for a in (raw_close, raw_bid, raw_ask)]\n",
    "\n",
    "    change = np.where(np.diff(day_id) != 0)[0] + 1\n",
    "    day_slices = np.split(np.arange(len(day_id)), change)\n",
    "\n",
    "    # 2) generator that yields one day at a time\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]        # (n_windows, look_back, n_feats)\n",
    "            y_block = y[sl]        # (n_windows,)\n",
    "            weekday = int(weekday_vec[sl[0]])\n",
    "\n",
    "            xb = np.expand_dims(x_block, 0).astype(np.float32)  # (1, n_windows, look_back, n_feats)\n",
    "            yb = np.expand_dims(y_block, 0).astype(np.float32)  # (1, n_windows)\n",
    "            if raw_close is None:\n",
    "                yield xb, yb, np.int32(weekday)\n",
    "            else:\n",
    "                cb = np.expand_dims(raw_close[sl], 0).astype(np.float32)\n",
    "                bb = np.expand_dims(raw_bid[sl],   0).astype(np.float32)\n",
    "                ab = np.expand_dims(raw_ask[sl],   0).astype(np.float32)\n",
    "                yield xb, yb, cb, bb, ab, np.int32(weekday)\n",
    "\n",
    "    feat_shape = X.shape[1:]  # (look_back, n_feats)\n",
    "    if raw_close is None:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),\n",
    "            tf.TensorSpec((1, None),          tf.float32),\n",
    "            tf.TensorSpec((),                 tf.int32),\n",
    "        )\n",
    "    else:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),\n",
    "            tf.TensorSpec((1, None),             tf.float32),\n",
    "            tf.TensorSpec((1, None),             tf.float32),\n",
    "            tf.TensorSpec((1, None),             tf.float32),\n",
    "            tf.TensorSpec((1, None),             tf.float32),\n",
    "            tf.TensorSpec((),                    tf.int32),\n",
    "        )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, output_signature=output_signature) \\\n",
    "                         .prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf4949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,                 # training arrays: (n_train, look_back, n_feats), (n_train,), (n_train,)\n",
    "    X_val, y_val, day_id_val,              # validation arrays\n",
    "    X_te, y_te, day_id_te,                 # test arrays\n",
    "    raw_close_te, raw_bid_te, raw_ask_te,  # test raw price arrays: each (n_test,)\n",
    "    *,\n",
    "    df,                                    # original minute‐bar DataFrame\n",
    "    train_batch: int                       # stateful training batch size\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits train/val/test arrays into day‐level tf.data.Datasets.\n",
    "    \n",
    "    - Training & validation datasets yield 3‐tuples:\n",
    "        (x_day, y_day, weekday)\n",
    "      where x_day has shape (n_windows, look_back, n_feats).\n",
    "    \n",
    "    - Test dataset yields 6‐tuples:\n",
    "        (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)\n",
    "    \"\"\"\n",
    "\n",
    "    # Build a single weekday vector over all timestamps.\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)\n",
    "    \n",
    "    # Compute split lengths.\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "    \n",
    "    # Slice out each split's weekday vector.\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr:n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val:n_tr + n_val + n_te]\n",
    "    \n",
    "    # Create day‐level datasets.\n",
    "    ds_tr  = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test= make_day_dataset(\n",
    "        X_te, y_te, day_id_te, weekday_vec_te,\n",
    "        raw_close=raw_close_te, raw_bid=raw_bid_te, raw_ask=raw_ask_te\n",
    "    )\n",
    "    \n",
    "    # For training: strip the extra leading batch dimension\n",
    "    # then padded‐batch by train_batch for stateful training.\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        # x_day: (1, n_windows, look_back, n_feats)\n",
    "        # squeeze removes the outer 1\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "        .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .padded_batch(train_batch, drop_remainder=True)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    # Optionally save the test dataset for later inspection\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "    \n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35e0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    X_val, y_val, day_id_val,\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32da4126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9cb23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) YOUR ORIGINAL STATEFUL DUAL-LSTM BUILDER (pure Keras LSTM)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, metrics\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "    *, time_steps:int, n_feats:int, batch_size:int,\n",
    "      short_units:int, long_units:int,\n",
    "      dropout_short:float, dropout_long:float,\n",
    "      initial_lr:float, first_decay_steps:int,\n",
    "      t_mul:float, m_mul:float, alpha:float,\n",
    "      clipnorm:float\n",
    ") -> models.Model:\n",
    "    inp = layers.Input(\n",
    "        batch_shape=(batch_size, time_steps, n_feats),\n",
    "        dtype=\"float32\", name=\"inp\"\n",
    "    )\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        short_units,\n",
    "        stateful=True, return_sequences=True,\n",
    "        dtype=\"float32\",\n",
    "        kernel_initializer=\"orthogonal\",\n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "        name=\"short_lstm\"\n",
    "    )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        long_units,\n",
    "        stateful=True, return_sequences=True,\n",
    "        dtype=\"float32\",\n",
    "        kernel_initializer=\"orthogonal\",\n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "        name=\"long_lstm\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "    lr_schedule = CosineDecayRestarts(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        first_decay_steps=first_decay_steps,\n",
    "        t_mul=t_mul, m_mul=m_mul, alpha=alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_schedule, clipnorm=clipnorm)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=\"mse\",\n",
    "        metrics=[metrics.RootMeanSquaredError(name=\"rmse\")]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79b8700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                               #\n",
    "###############################################################################\n",
    "\n",
    "# fast mixed-precision (FP16) train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    # sequence dimensions\n",
    "    time_steps        = LOOK_BACK,      # fixed window length\n",
    "    n_feats           = N_FEATS,        # features per timestep\n",
    "    batch_size        = TRAIN_BATCH,    # stateful batch size for training\n",
    "\n",
    "    # architecture\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "\n",
    "    # optimizer & schedule\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_STEPS,  # absolute gradient steps before first decay\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "\n",
    "    # misc\n",
    "    clipnorm          = CLIPNORM\n",
    ")\n",
    "\n",
    "# pure FP32 validation/inference graph (day-by-day; batch_size = 1)\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    time_steps        = LOOK_BACK,\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,       # typically 1 for per-day eval\n",
    "\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_STEPS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "\n",
    "    clipnorm          = CLIPNORM\n",
    ")\n",
    "\n",
    "# save the val/inference model to disk\n",
    "model_val.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e9e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c6f336-225d-4afb-88c9-7171798079c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2) STATEFUL TRAINING STEP (_train_step)\n",
    "# =============================================================================\n",
    "@tf.function(jit_compile=True)\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    One mixed-precision update on (B, time_steps, n_feats) windows.\n",
    "    - Uses LossScaleOptimizer for FP16 scaling.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1) forward pass\n",
    "        y_pred = model(xb, training=True)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        tf.debugging.check_numerics(y_pred, \"y_pred has NaN/Inf\")\n",
    "\n",
    "        # 2) compute raw MSE loss\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        tf.debugging.check_numerics(loss, \"loss has NaN/Inf\")\n",
    "\n",
    "        # 3) scale the loss for FP16\n",
    "        scaled_loss = opt.get_scaled_loss(loss)\n",
    "\n",
    "    # 4) compute scaled gradients\n",
    "    scaled_grads = tape.gradient(scaled_loss, model.trainable_weights)\n",
    "    # 4a) check each scaled grad individually\n",
    "    for g in scaled_grads:\n",
    "        if g is not None:\n",
    "            tf.debugging.check_numerics(g, \"scaled_grad has NaN/Inf\")\n",
    "\n",
    "    # 5) unscale gradients\n",
    "    grads = opt.get_unscaled_gradients(scaled_grads)\n",
    "    # 5a) check each unscaled grad individually\n",
    "    for g in grads:\n",
    "        if g is not None:\n",
    "            tf.debugging.check_numerics(g, \"unscaled_grad has NaN/Inf\")\n",
    "\n",
    "    # 6) apply\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # 7) return true RMSE\n",
    "    return tf.sqrt(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d215f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # yields (xb_days, yb_days, wd_days)\n",
    "    ds_val,              # yields (x_day, y_day, wd)\n",
    "    *,\n",
    "    n_train_days: int,\n",
    "    max_epochs: int,\n",
    "    early_stop_patience: int,\n",
    "    baseline_val_rmse: float,\n",
    "    weights_path\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a stateful LSTM training regime with two internal memories:\n",
    "      - short_lstm: reset every day\n",
    "      - long_lstm:  reset once per week (on weekend rollover)\n",
    "\n",
    "    Training bundles TRAIN_BATCH days for prefetching but still\n",
    "    processes them day-by-day to preserve chronological resets.\n",
    "    Validation runs one day at a time (VAL_BATCH=1).\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 1) Setup: loss, optimizer, visualization, and locate stateful layers\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    opt     = model_train.optimizer           # your LossScaleOptimizer from step 1\n",
    "    live_plot = LiveRMSEPlot()                # optional live plotting helper\n",
    "\n",
    "    # Extract TRAIN_BATCH (days bundled per ds_train_batched element)\n",
    "    TRAIN_BATCH = model_train.input_shape[0]\n",
    "\n",
    "    # Find your two stateful LSTM layers by name for resetting\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr  = 0\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 2) Epoch Loop\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        pbar = tqdm(\n",
    "            total=n_train_days,\n",
    "            desc=f\"Epoch {epoch:03d}\",\n",
    "            unit=\"day\",\n",
    "            dynamic_ncols=True,\n",
    "            leave=False\n",
    "        )\n",
    "        batch_rmses = []\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2A) TRAINING PHASE\n",
    "        # -----------------------------\n",
    "        for xb_days, yb_days, wd_days in ds_train_batched:\n",
    "            # xb_days: (TRAIN_BATCH, windows_per_day, LOOK_BACK, n_feats)\n",
    "            # yb_days: (TRAIN_BATCH, windows_per_day)\n",
    "            prev_wd = None\n",
    "            collected_x, collected_y = [], []\n",
    "\n",
    "            # 1) Iterate each of the bundled days in date order\n",
    "            for day_i in range(xb_days.shape[0]):\n",
    "                x_day = xb_days[day_i]     # (W_i, LOOK_BACK, n_feats)\n",
    "                y_day = yb_days[day_i]     # (W_i,)\n",
    "                wd    = int(wd_days[day_i])\n",
    "\n",
    "                # -- reset short-term memory each new day\n",
    "                for layer in short_tr:\n",
    "                    layer.reset_states()\n",
    "\n",
    "                # -- reset long-term memory if weekday rolled over\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    for layer in long_tr:\n",
    "                        layer.reset_states()\n",
    "                prev_wd = wd\n",
    "\n",
    "                # collect this day's windows & labels\n",
    "                collected_x.append(x_day)\n",
    "                collected_y.append(y_day)\n",
    "\n",
    "            # 2) Concatenate into one big batch of windows\n",
    "            xb_flat = tf.concat(collected_x, axis=0)  # (sum W_i, LOOK_BACK, n_feats)\n",
    "            yb_flat = tf.concat(collected_y, axis=0)  # (sum W_i,)\n",
    "\n",
    "            # 3) Slice the flat windows into TRAIN_BATCH-sized chunks\n",
    "            total_windows = xb_flat.shape[0]\n",
    "            for start in range(0, total_windows, TRAIN_BATCH):\n",
    "                xb_batch = xb_flat[start:start+TRAIN_BATCH]\n",
    "                yb_batch = yb_flat[start:start+TRAIN_BATCH]\n",
    "                if xb_batch.shape[0] < TRAIN_BATCH:\n",
    "                    break  # drop incomplete batch at end\n",
    "\n",
    "                # run one training step on this mini-batch\n",
    "                rmse = _train_step(xb_batch, yb_batch, model_train, loss_fn, opt)\n",
    "                batch_rmses.append(float(rmse))\n",
    "\n",
    "            # advance progress bar by the number of days in this bundle\n",
    "            pbar.update(xb_days.shape[0])\n",
    "\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2B) VALIDATION PHASE\n",
    "        # -----------------------------\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "        val_rmses = []\n",
    "        prev_wd_val = None\n",
    "\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "            # reset daily memory\n",
    "            for layer in short_val:\n",
    "                layer.reset_states()\n",
    "            # reset weekly memory on rollover\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for layer in long_val:\n",
    "                    layer.reset_states()\n",
    "            prev_wd_val = wd\n",
    "\n",
    "            # forward pass on one day\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]),        tf.float32)\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean((y_true - y_pred) ** 2))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "        impr_pct  = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "        current_lr = current_lr_from(opt)\n",
    "        grad_norm  = np.mean(batch_rmses) / current_lr\n",
    "\n",
    "        # Log & plot\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f}\"\n",
    "            f\" • impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\"\n",
    "        )\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "        pbar.close()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2C) Early Stopping & Checkpoint\n",
    "        # -----------------------------\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr  = 0\n",
    "            model_train.save_weights(weights_path)\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # End of epoch loop: restore best weights and return RMSE\n",
    "    model_train.load_weights(weights_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615e2ced-17a4-46b3-b6e1-a7521ca7da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible fused RNN ops: [('dual_mem_lstm/ln_short/FusedBatchNormV3', 'FusedBatchNormV3'), ('dual_mem_lstm/ln_long/FusedBatchNormV3', 'FusedBatchNormV3')]\n"
     ]
    }
   ],
   "source": [
    "cf = tf.function(lambda x: model_train(x, training=False), jit_compile=False) \\\n",
    "       .get_concrete_function(\n",
    "         tf.TensorSpec((TRAIN_BATCH, LOOK_BACK, N_FEATS), tf.float32)\n",
    "       )\n",
    "\n",
    "fused_ops = [\n",
    "  (op.name, op.type)\n",
    "  for op in cf.graph.get_operations()\n",
    "  if \"Fused\" in op.type or \"RNN\" in op.type\n",
    "]\n",
    "print(\"Possible fused RNN ops:\", fused_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2032b8d9-e4f4-4124-b0e5-cd57ed2a073f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:   12.4\n",
      "cuDNN:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751460084.634446    6279 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-07-02 12:41:24.647483: E external/local_xla/xla/stream_executor/dnn.cc:1140] Sequence lengths for RNN are required from CUDNN 9.0+\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 5, 64, 1, 90, 64, 64] \n\t [[{{node CudnnRNN}}]]\n\t [[dual_mem_lstm/short_lstm/PartitionedCall]] [Op:__inference_infer_4407]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# warm up & get concrete function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m spec \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorSpec((TRAIN_BATCH, LOOK_BACK, N_FEATS), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 15\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_BATCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOOK_BACK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FEATS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m cf \u001b[38;5;241m=\u001b[39m infer\u001b[38;5;241m.\u001b[39mget_concrete_function(spec)\n\u001b[1;32m     18\u001b[0m ops \u001b[38;5;241m=\u001b[39m [op\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m cf\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mget_operations()]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 5, 64, 1, 90, 64, 64] \n\t [[{{node CudnnRNN}}]]\n\t [[dual_mem_lstm/short_lstm/PartitionedCall]] [Op:__inference_infer_4407]"
     ]
    }
   ],
   "source": [
    "import time, numpy as np, tensorflow as tf\n",
    "\n",
    "# 1) Versions\n",
    "info = tf.sysconfig.get_build_info()\n",
    "print(f\"CUDA:   {info['cuda_version']}\")\n",
    "print(f\"cuDNN:  {info['cudnn_version']}\")\n",
    "\n",
    "# 2) Graph inspection\n",
    "@tf.function\n",
    "def infer(x):\n",
    "    return model_train(x, training=False)\n",
    "\n",
    "# warm up & get concrete function\n",
    "spec = tf.TensorSpec((TRAIN_BATCH, LOOK_BACK, N_FEATS), tf.float32)\n",
    "_ = infer(tf.zeros((TRAIN_BATCH, LOOK_BACK, N_FEATS)))\n",
    "cf = infer.get_concrete_function(spec)\n",
    "\n",
    "ops = [op.type for op in cf.graph.get_operations()]\n",
    "found = [o for o in ops if \"CudnnRNN\" in o]\n",
    "print(\"Fused CuDNN ops:\", found)\n",
    "\n",
    "# 3) Micro-benchmark\n",
    "optimizer = model_train.optimizer\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model_train(x, training=True)\n",
    "        loss   = tf.reduce_mean((y - y_pred)**2)\n",
    "    grads = tape.gradient(loss, model_train.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model_train.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "xb = tf.random.uniform((TRAIN_BATCH, LOOK_BACK, N_FEATS), dtype=tf.float32)\n",
    "yb = tf.random.uniform((TRAIN_BATCH, LOOK_BACK, 1),       dtype=tf.float32)\n",
    "\n",
    "# warm up\n",
    "_ = train_step(xb, yb)\n",
    "\n",
    "# GPU timing\n",
    "tf.config.set_visible_devices(tf.config.list_physical_devices(\"GPU\"), \"GPU\")\n",
    "tf.config.optimizer.set_jit(True)\n",
    "gpu_times = [time.perf_counter() for _ in range(1)]  # single-shot\n",
    "_ = train_step(xb, yb)\n",
    "gpu_time = (time.perf_counter() - gpu_times[0]) * 1000\n",
    "print(f\"GPU step time: {gpu_time:.1f} ms\")\n",
    "\n",
    "# CPU timing\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "_ = train_step(xb, yb)  # rebuild on CPU\n",
    "cpu_times = [time.perf_counter() for _ in range(1)]\n",
    "_ = train_step(xb, yb)\n",
    "cpu_time = (time.perf_counter() - cpu_times[0]) * 1000\n",
    "print(f\"CPU step time: {cpu_time:.1f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba549d-a11e-435c-9cce-f82d46ba3e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        weights_path        = weights_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40272296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e0cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
