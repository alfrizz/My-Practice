{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 18:08:26.184370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751040506.263258    3534 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751040506.288071    3534 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751040506.382767    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751040506.382899    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751040506.382905    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751040506.382907    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'stockanalibs' from '/mnt/g/My Drive/Ingegneria/Data Science GD/My-Practice/my models/Trading/0.Stock Analysis/stockanalibs.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm             # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import importlib\n",
    "import stockanalibs\n",
    "importlib.reload(stockanalibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:33:00</th>\n",
       "      <td>28.5959</td>\n",
       "      <td>28.5959</td>\n",
       "      <td>28.5932</td>\n",
       "      <td>28.5932</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>28.5846</td>\n",
       "      <td>28.6018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:34:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.760984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:35:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.770777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:36:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.779774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:37:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.788084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.3750</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.2150</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.5129</td>\n",
       "      <td>173.6171</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.049</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.5650</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.2400</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.3280</td>\n",
       "      <td>173.4320</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.3900</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.2000</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.2580</td>\n",
       "      <td>173.3620</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.304</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.3150</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.2300</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.2280</td>\n",
       "      <td>173.3320</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.334</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.3000</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.1700</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.5576</td>\n",
       "      <td>173.6618</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1235302 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close     volume  \\\n",
       "2014-04-03 13:33:00   28.5959   28.5959   28.5932   28.5932     5540.0   \n",
       "2014-04-03 13:34:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:35:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:36:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:37:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "...                       ...       ...       ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.3750  173.6771  173.2150  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.5650  173.5900  173.2400  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.3900  173.4100  173.2000  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.3150  173.4000  173.2300  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.3000  174.0500  173.1700  173.6097  7649838.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:33:00   28.5846   28.6018             0            0.000   \n",
       "2014-04-03 13:34:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:35:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:36:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:37:00   28.5724   28.5895             0            0.000   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.5129  173.6171             0           -0.661   \n",
       "2025-06-18 20:57:00  173.3280  173.4320             0           -0.661   \n",
       "2025-06-18 20:58:00  173.2580  173.3620             0           -0.661   \n",
       "2025-06-18 20:59:00  173.2280  173.3320             0           -0.661   \n",
       "2025-06-18 21:00:00  173.5576  173.6618             0           -0.661   \n",
       "\n",
       "                     EarningDiff  signal_smooth  \n",
       "2014-04-03 13:33:00        0.000       0.750262  \n",
       "2014-04-03 13:34:00        0.000       0.760984  \n",
       "2014-04-03 13:35:00        0.000       0.770777  \n",
       "2014-04-03 13:36:00        0.000       0.779774  \n",
       "2014-04-03 13:37:00        0.000       0.788084  \n",
       "...                          ...            ...  \n",
       "2025-06-18 20:56:00        2.049       0.000000  \n",
       "2025-06-18 20:57:00        2.234       0.000000  \n",
       "2025-06-18 20:58:00        2.304       0.000000  \n",
       "2025-06-18 20:59:00        2.334       0.000000  \n",
       "2025-06-18 21:00:00        2.004       0.000000  \n",
       "\n",
       "[1235302 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_final.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "weights_path   = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "model_path     = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 48       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.30     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.25     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-3     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 5e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.03     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 32       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 100      # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 10       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts one big minute-bar DataFrame (many days) into NumPy arrays ready for the stateful LSTM.\n",
    "    \n",
    "    RULES ENFORCED:\n",
    "      • Windows never cross midnight.\n",
    "      • Features and labels are standardized per day (to avoid leakage).\n",
    "    \n",
    "    Returns:\n",
    "      X         : Design matrix; every row is a sliding window (flattened).\n",
    "      y         : One-step-ahead targets corresponding to each window.\n",
    "      raw_close : Raw (unstandardized) close prices for each target.\n",
    "      raw_bid   : Raw bid prices.\n",
    "      raw_ask   : Raw ask prices.\n",
    "    \"\"\"\n",
    "    X_rows, y_rows, close_rows, bid_rows, ask_rows = [], [], [], [], []\n",
    "    \n",
    "    # Process one calendar day at a time.\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "        day_df = day_df.sort_index()\n",
    "        \n",
    "        # Extract raw price columns before scaling.\n",
    "        raw_close = day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        raw_bid   = day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        raw_ask   = day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Standardize features and target per day.\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols]) ####################################################################\n",
    "        # day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]]) ###################################################################\n",
    "        \n",
    "        feats_np = day_df[feature_cols].to_numpy(dtype=np.float32)  # shape: (T, n_feats)\n",
    "        label_np = day_df[label_col].to_numpy(dtype=np.float32)       # shape: (T,)\n",
    "        \n",
    "        # Create mask for Regular Trading Hours (RTH).\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():\n",
    "            continue\n",
    "        \n",
    "        T, _ = feats_np.shape\n",
    "        \n",
    "        # Build sliding windows using vectorized approach.\n",
    "        win_3d = sliding_window_view(feats_np, (look_back, feats_np.shape[1]), axis=(0, 1))\n",
    "        # After the sliding window operation, we get an array whose shape is roughly (T - look_back + 1, 1, look_back, n_feats).\n",
    "        win_3d = win_3d[:, 0, :, :]  # removes the extra dimension: (T - look_back + 1, look_back, n_feats)\n",
    "        \n",
    "        # Alignment fix: drop the last window so that targets align.\n",
    "        win_3d = win_3d[:-1] # drops the very last window. This makes the number of windows exactly equal to the number of available targets.\n",
    "        y_aligned = label_np[look_back:]              # (T - look_back,)\n",
    "        close_aligned = raw_close[look_back:]    # (T - look_back,)\n",
    "        bid_aligned   = raw_bid[look_back:]      # (T - look_back,)\n",
    "        ask_aligned   = raw_ask[look_back:]      # (T - look_back,)\n",
    "        \n",
    "        # Trim by RTH (apply mask to the target indices).\n",
    "        rth_mask_shifted = rth_mask[look_back:]\n",
    "        win_3d       = win_3d[rth_mask_shifted]\n",
    "        y_aligned    = y_aligned[rth_mask_shifted]\n",
    "        close_aligned = close_aligned[rth_mask_shifted]\n",
    "        bid_aligned   = bid_aligned[rth_mask_shifted]\n",
    "        ask_aligned   = ask_aligned[rth_mask_shifted]\n",
    "        \n",
    "        # Flatten each window\n",
    "        X_rows.append(win_3d.reshape(win_3d.shape[0], -1))\n",
    "        y_rows.append(y_aligned)\n",
    "        close_rows.append(close_aligned)\n",
    "        bid_rows.append(bid_aligned)\n",
    "        ask_rows.append(ask_aligned)\n",
    "    \n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No valid RTH windows found; check rth_start or data gaps.\")\n",
    "    \n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "    raw_close = np.concatenate(close_rows).astype(np.float32)\n",
    "    raw_bid   = np.concatenate(bid_rows).astype(np.float32)\n",
    "    raw_ask   = np.concatenate(ask_rows).astype(np.float32)\n",
    "    \n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1066222, 300)\n",
      "(1066222,)\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    raw_close: np.ndarray,\n",
    "    raw_bid: np.ndarray,\n",
    "    raw_ask: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    "    TRAIN_BATCH: int  # added parameter for training batch size\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_train, y_train)\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_val, y_val)\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],    # (X_test, y_test, raw_close_test, raw_bid_test, raw_ask_test)\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id_tr, day_id_val, day_id_te\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits X, y (and the raw signals) into chronological train/val/test partitions by whole days.\n",
    "    The code creates a day-based vector (day_id_vec) and then applies masks to split all windows into training, validation, and test groups. \n",
    "    Each group is consecutive in terms of days, so no day is dropped or skipped—the splits follow in order. \n",
    "    This ensures that if we later recombine the splits, they cover all days from 0 to D–1.\n",
    "\n",
    "            [ Training ]         [ Validation ]            [ Test ]\n",
    "    Days: 0   ...   cut_train | cut_train+1 ... cut_val | cut_val+1 ... D-1\n",
    "    \n",
    "    It uses the following logic:\n",
    "      1. Count the number of usable windows per calendar day.\n",
    "      2. Create a day_id vector that tags each sample with its day.\n",
    "      3. Compute the total number of calendar days, D.\n",
    "      4. Compute the \"original\" intended training set size as D * train_prop.\n",
    "      5. Round this count up (in day units) to the next multiple of TRAIN_BATCH.\n",
    "         (This ensures that the training set always contains full training batches.)\n",
    "      6. The validation split starts immediately after training ends. \n",
    "      7. The test split then follows, with no data dropped.\n",
    "    \n",
    "    Returns:\n",
    "      ((X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "       samples_per_day, day_id_tr, day_id_val, day_id_te)\n",
    "       \n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each calendar day.\n",
    "        +-----------------------------------------------------------+\n",
    "        |  Total minute rows (T)                                    |\n",
    "        |                                                           |\n",
    "        |   0, 1, 2, ... , look_back-1  | look_back, ..., T-1         |\n",
    "        |      (Not ready)            |  (Potential windows)         |\n",
    "        |                              |   ┌──────────┐               |\n",
    "        |                              |   │ rth_start│ <-- Only count rows with timestamp >= rth_start\n",
    "        |                              |   └──────────┘               |\n",
    "        |                              |    Usable windows = count of rows satisfying both conditions\n",
    "        +-----------------------------------------------------------+\n",
    "\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "    \"\"\"\n",
    "    # 1. Count usable windows per day (vectorized)\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                # minute rows today\n",
    "        idx = np.arange(T)\n",
    "        mask_window_ready = idx >= look_back\n",
    "        mask_rth_target   = day_df.index.time >= rth_start\n",
    "        usable_today = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "        samples_per_day.append(usable_today)\n",
    "    \n",
    "    # Verify that summed count equals len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "    \n",
    "    # 2. Build the day_id vector: each window gets the day index (0-based)\n",
    "    day_id_vec = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "    \n",
    "    # 3. Total number of days\n",
    "    D = len(samples_per_day)\n",
    "    \n",
    "    # 4. Compute the original intended training count (in days)\n",
    "    original_train_count = int(D * train_prop)\n",
    "    \n",
    "    # 5. Round up the training days to the next multiple of TRAIN_BATCH\n",
    "    # (We want the training portion to include full batches.)\n",
    "    new_train_count = int(np.ceil(original_train_count / TRAIN_BATCH) * TRAIN_BATCH)\n",
    "    # Make sure we don't exceed the total number of days.\n",
    "    new_train_count = min(new_train_count, D)\n",
    "    # In day_id_vec, days are 0-indexed, so the training cut is:\n",
    "    cut_train = new_train_count - 1\n",
    "    \n",
    "    # 6. Determine the validation cut-point using the original proportion.\n",
    "    # Validation ends at the day_index given by:\n",
    "    cut_val = int(D * (train_prop + val_prop))\n",
    "\n",
    "    # 7. Create masks for the splits.\n",
    "    mask_tr = day_id_vec <= cut_train\n",
    "    mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te = day_id_vec > cut_val\n",
    "    \n",
    "    # 8. Slice X, y, and the raw arrays.\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_val, y_val = X[mask_val], y[mask_val]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "    \n",
    "    raw_close_te = raw_close[mask_te]\n",
    "    raw_bid_te   = raw_bid[mask_te]\n",
    "    raw_ask_te   = raw_ask[mask_te]\n",
    "    \n",
    "    # Also slice the day_id vector.\n",
    "    day_id_tr = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te = day_id_vec[mask_te]\n",
    "    \n",
    "    return (X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "           samples_per_day, day_id_tr, day_id_val, day_id_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1984 distinct calendar days retained, organized into batches of 32 days each (no partial batches).\n",
      "Validation: 412 distinct calendar days available for model validation.\n",
      "Test: 422 distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    TRAIN_BATCH=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(np.unique(day_id_tr))} distinct calendar days retained, organized into batches of {TRAIN_BATCH} days each (no partial batches).\")\n",
    "print(f\"Validation: {len(np.unique(day_id_val))} distinct calendar days available for model validation.\")\n",
    "print(f\"Test: {len(np.unique(day_id_te))} distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_dataset(\n",
    "    X           : np.ndarray,    # (N, …)\n",
    "    y           : np.ndarray,    # (N,)\n",
    "    day_id      : np.ndarray,    # (N,)\n",
    "    weekday_vec : np.ndarray,    # (N,)\n",
    "    raw_close   : np.ndarray = None,  # (N,) optional\n",
    "    raw_bid     : np.ndarray = None,  # (N,) optional\n",
    "    raw_ask     : np.ndarray = None   # (N,) optional\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset where each element corresponds to one calendar day.\n",
    "    \n",
    "    If raw price arrays are provided, yields a 6-tuple:\n",
    "      (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)\n",
    "    Otherwise, yields a 3-tuple:\n",
    "      (x_day, y_day, weekday)\n",
    "    \n",
    "    - x_day: (1, T, n_feats) float32\n",
    "    - y_day: (1, T)         float32\n",
    "    - weekday: ()           int32\n",
    "    \"\"\"\n",
    "    # Sort inputs in chronological order.\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "    if raw_close is not None and raw_bid is not None and raw_ask is not None:\n",
    "        raw_close, raw_bid, raw_ask = [a[idx] for a in (raw_close, raw_bid, raw_ask)]\n",
    "    \n",
    "    # Determine boundaries for each day.\n",
    "    # the code splits the dataset into daily blocks based on the day_id, \n",
    "    # then produces a TensorFlow dataset where each element represents one complete day’s data \n",
    "    # (with features, targets, and possibly raw prices) along with the corresponding weekday\n",
    "    change = np.where(np.diff(day_id) != 0)[0] + 1\n",
    "    day_slices = np.split(np.arange(len(day_id)), change)\n",
    "\n",
    "    # the generator function walks through the dataset day by day and “packs” each day’s data into one neat bundle. \n",
    "    # For each day it: Selects the day's data, Formats the data, Yields the bundle \n",
    "    # Using a generator is both memory efficient and flexible. It creates each day's data on the fly rather than building a huge list all at once.\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]      # (T, …)\n",
    "            y_block = y[sl]      # (T,)\n",
    "            weekday = int(weekday_vec[sl[0]])\n",
    "            if raw_close is None:\n",
    "                # Yield the original 3-tuple.\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "            else:\n",
    "                # Extract raw price slices.\n",
    "                close_block = raw_close[sl]  # (T,)\n",
    "                bid_block   = raw_bid[sl]    # (T,)\n",
    "                ask_block   = raw_ask[sl]    # (T,)\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),    # (1, T, …)\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),      # (1, T)\n",
    "                    np.expand_dims(close_block, 0).astype(np.float32),  # (1, T)\n",
    "                    np.expand_dims(bid_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.expand_dims(ask_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "    \n",
    "    feat_shape = X.shape[1:]\n",
    "    if raw_close is None:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    else:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_close_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_bid_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_ask_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(gen, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,                 # training arrays\n",
    "        X_val, y_val, day_id_val,              # validation arrays\n",
    "        X_te, y_te, day_id_te,                 # test arrays\n",
    "        raw_close_te, raw_bid_te, raw_ask_te,  # test raw price arrays\n",
    "        *,\n",
    "        df,                                    # original DataFrame (for weekday vector)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits training, validation, and test arrays into day-level tf.data.Datasets.\n",
    "    \n",
    "    For training and validation, raw signals are not saved (3-tuple).\n",
    "    For testing, the raw price arrays (raw_close, raw_bid, raw_ask) are provided, yielding a 6-tuple.\n",
    "    \n",
    "    Returns:\n",
    "      ds_train_batched, ds_val_unbatched, ds_test_unbatched\n",
    "    \"\"\"\n",
    "    # Build one weekday vector covering all rows from the original DataFrame.\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)\n",
    "    \n",
    "    # Determine split lengths.\n",
    "    n_tr = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te = len(X_te)\n",
    "    \n",
    "    # Create weekday vectors for each split.\n",
    "    # they will be used to identify the end of the week and reset the long-term state of the LSTM layers\n",
    "    weekday_vec_tr = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr:n_tr+n_val]\n",
    "    weekday_vec_te = weekday_all[n_tr+n_val:n_tr+n_val+n_te]\n",
    "    \n",
    "    # Build training and validation datasets (3-tuple).\n",
    "    ds_tr = make_day_dataset(X_tr, y_tr, day_id_tr, weekday_vec_tr)\n",
    "    ds_val = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    \n",
    "    # Build test dataset with raw price arrays (6-tuple).\n",
    "    ds_test = make_day_dataset(X_te, y_te, day_id_te, weekday_vec_te,\n",
    "                               raw_close=raw_close_te, raw_bid=raw_bid_te, raw_ask=raw_ask_te)\n",
    "    \n",
    "    # For training, strip the extra outer batch dimension.\n",
    "    # the _strip function only on the training dataset to remove that extra outer dimension \n",
    "    # In the validation or test datasets we don't need to strip this dimension because those pipelines expect one sample at a time \n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd\n",
    "\n",
    "    ds_train_batched = (ds_tr\n",
    "                         .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                         .padded_batch(train_batch, drop_remainder=True)\n",
    "                         .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    # Save the test dataset.\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "    \n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751040534.814365    3534 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    X_val, y_val, day_id_val,\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n",
    "\n",
    "# save validation model, to reuse for inference\n",
    "model_val.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,          # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,                    # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,         # Total calendar days in the training epoch\n",
    "    max_epochs: int,           # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    weights_path               # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(weights_path)  # Save checkpoint of the best weights.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(weights_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.370207\n",
      "Training sees 1984 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b47994642894e1bafeb8b9b1bdc466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751040558.654291    3601 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.794588 • val=0.282886 • impr= 23.6% • lr=1.31e-04 • g≈6075.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f292e128a5b47b1b7f27f7be1666db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.562277 • val=0.237295 • impr= 35.9% • lr=1.74e-04 • g≈3239.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2940fbf6f1f4efbaa84b3efc7712094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.474810 • val=0.283220 • impr= 23.5% • lr=4.56e-04 • g≈1040.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e78d8173d242a19ccf18d64a27f829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.395296 • val=0.215038 • impr= 41.9% • lr=1.96e-04 • g≈2014.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae6e2adb0244e05be203ddbd3469798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.361792 • val=0.215156 • impr= 41.9% • lr=1.62e-05 • g≈22377.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7dfe110ca4480db438ac9848abfa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.347144 • val=0.249048 • impr= 32.7% • lr=4.63e-04 • g≈749.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71942aa71f14740ba2911cfb9a13e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.304233 • val=0.200223 • impr= 45.9% • lr=3.52e-04 • g≈863.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516a37ee88c74d5c8ea750d8c0968541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1984 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        weights_path        = weights_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
