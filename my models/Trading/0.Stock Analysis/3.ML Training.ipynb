{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f06a4f-691a-4a84-a305-e7212eb879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python’s garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "# 3) If you’re using PyTorch + CUDA, free any lingering GPU memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "from libs import models, plots, params\n",
    "importlib.reload(models)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "ticker         = params.ticker\n",
    "LOOK_BACK      = params.look_back \n",
    "features_cols  = params.features_cols\n",
    "label_col      = params.label_col\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "regular_start      = params.regular_start_pred\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = params.train_prop, params.val_prop\n",
    "\n",
    "# USE GPU if available, otherwise fallback to CPU\n",
    "device = params.device\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (attention-augmented tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 32           # hidden size of each daily LSTM layer\n",
    "LONG_UNITS          = 64          # hidden size of the weekly LSTM\n",
    "DROPOUT_SHORT       = 0.3          # dropout after residual+attention block\n",
    "DROPOUT_LONG        = 0.5          # dropout after weekly LSTM outputs\n",
    "ATT_HEADS           = 4            # number of self-attention heads\n",
    "ATT_DROPOUT         = 0.2          # dropout rate inside attention\n",
    "WEIGHT_DECAY        = 1e-3         # L2 weight decay on all model weights\n",
    "\n",
    "# ── Training Control Parameters ────────────────────────────────────────────\n",
    "TRAIN_BATCH         = params.train_batch           \n",
    "VAL_BATCH           = params.val_batch  \n",
    "NUM_WORKERS         = params.num_workers\n",
    "MAX_EPOCHS          = 200          # upper limit on training epochs\n",
    "EARLY_STOP_PATIENCE = 20           # stop if no val-improve for this many epochs\n",
    "\n",
    "# ── Optimizer Settings ─────────────────────────────────────────────────────\n",
    "INITIAL_LR          = 1e-3         # AdamW initial learning rate\n",
    "CLIPNORM            = 0.3          # max-norm gradient clipping\n",
    "\n",
    "# ── ReduceLROnPlateau Scheduler ───────────────────────────────────────────\n",
    "PLATEAU_FACTOR      = 0.5          # multiply LR by this factor on plateau\n",
    "PLATEAU_PATIENCE    = 2            # epochs with no val-improve before LR cut\n",
    "MIN_LR              = 1e-5         # lower bound on LR after reductions\n",
    "\n",
    "# ── CosineAnnealingWarmRestarts Scheduler ─────────────────────────────────\n",
    "T_0                 = 2            # epochs before first cosine restart\n",
    "T_MULT              = 2            # cycle length multiplier after each restart\n",
    "ETA_MIN             = MIN_LR       # floor LR in each cosine cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08b80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step D: saving final CSV …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>r_1</th>\n",
       "      <th>r_5</th>\n",
       "      <th>r_15</th>\n",
       "      <th>vol_15</th>\n",
       "      <th>volume_spike</th>\n",
       "      <th>vwap_dev</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:06:00</th>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>28.644845</td>\n",
       "      <td>4580.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.568641</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.636251</td>\n",
       "      <td>28.653438</td>\n",
       "      <td>0.327384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:07:00</th>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>28.639690</td>\n",
       "      <td>4540.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.570338</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.631098</td>\n",
       "      <td>28.648282</td>\n",
       "      <td>0.328556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:08:00</th>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>28.634534</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.574408</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.625944</td>\n",
       "      <td>28.643125</td>\n",
       "      <td>0.329833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:09:00</th>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>28.629379</td>\n",
       "      <td>4460.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.581017</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.620791</td>\n",
       "      <td>28.637968</td>\n",
       "      <td>0.331213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 12:10:00</th>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>28.624224</td>\n",
       "      <td>4420.0</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.590413</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.615637</td>\n",
       "      <td>28.632811</td>\n",
       "      <td>0.332697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.375000</td>\n",
       "      <td>173.677100</td>\n",
       "      <td>173.215000</td>\n",
       "      <td>173.565000</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>2.462713</td>\n",
       "      <td>1.248428</td>\n",
       "      <td>17.019768</td>\n",
       "      <td>173.512900</td>\n",
       "      <td>173.617100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.565000</td>\n",
       "      <td>173.590000</td>\n",
       "      <td>173.240000</td>\n",
       "      <td>173.380000</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.005063</td>\n",
       "      <td>-0.010671</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>2.154838</td>\n",
       "      <td>1.246015</td>\n",
       "      <td>11.648165</td>\n",
       "      <td>173.328000</td>\n",
       "      <td>173.432000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.390000</td>\n",
       "      <td>173.410000</td>\n",
       "      <td>173.200000</td>\n",
       "      <td>173.310000</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-0.011816</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>1.439161</td>\n",
       "      <td>1.245096</td>\n",
       "      <td>11.384870</td>\n",
       "      <td>173.258000</td>\n",
       "      <td>173.362000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.315000</td>\n",
       "      <td>173.400000</td>\n",
       "      <td>173.230000</td>\n",
       "      <td>173.280000</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>-0.011932</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>2.836382</td>\n",
       "      <td>1.244678</td>\n",
       "      <td>11.830567</td>\n",
       "      <td>173.228000</td>\n",
       "      <td>173.332000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.300000</td>\n",
       "      <td>174.050000</td>\n",
       "      <td>173.170000</td>\n",
       "      <td>173.609700</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>-0.009290</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>8.568493</td>\n",
       "      <td>1.248745</td>\n",
       "      <td>22.962317</td>\n",
       "      <td>173.557600</td>\n",
       "      <td>173.661800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1651679 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           open        high         low       close  \\\n",
       "2014-04-03 12:06:00   28.644845   28.644845   28.644845   28.644845   \n",
       "2014-04-03 12:07:00   28.639690   28.639690   28.639690   28.639690   \n",
       "2014-04-03 12:08:00   28.634534   28.634534   28.634534   28.634534   \n",
       "2014-04-03 12:09:00   28.629379   28.629379   28.629379   28.629379   \n",
       "2014-04-03 12:10:00   28.624224   28.624224   28.624224   28.624224   \n",
       "...                         ...         ...         ...         ...   \n",
       "2025-06-18 20:56:00  173.375000  173.677100  173.215000  173.565000   \n",
       "2025-06-18 20:57:00  173.565000  173.590000  173.240000  173.380000   \n",
       "2025-06-18 20:58:00  173.390000  173.410000  173.200000  173.310000   \n",
       "2025-06-18 20:59:00  173.315000  173.400000  173.230000  173.280000   \n",
       "2025-06-18 21:00:00  173.300000  174.050000  173.170000  173.609700   \n",
       "\n",
       "                        volume       r_1       r_5      r_15    vol_15  \\\n",
       "2014-04-03 12:06:00     4580.0 -0.000180 -0.000180 -0.000180  0.000046   \n",
       "2014-04-03 12:07:00     4540.0 -0.000180 -0.000360 -0.000360  0.000063   \n",
       "2014-04-03 12:08:00     4500.0 -0.000180 -0.000540 -0.000540  0.000075   \n",
       "2014-04-03 12:09:00     4460.0 -0.000180 -0.000720 -0.000720  0.000082   \n",
       "2014-04-03 12:10:00     4420.0 -0.000180 -0.000900 -0.000900  0.000088   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "2025-06-18 20:56:00   621199.0  0.001124 -0.004226 -0.009661  0.001493   \n",
       "2025-06-18 20:57:00   624198.0 -0.001066 -0.005063 -0.010671  0.001487   \n",
       "2025-06-18 20:58:00   454542.0 -0.000404 -0.005811 -0.011816  0.001436   \n",
       "2025-06-18 20:59:00  1094746.0 -0.000173 -0.004434 -0.011932  0.001432   \n",
       "2025-06-18 21:00:00  7649838.0  0.001901  0.001382 -0.009290  0.001592   \n",
       "\n",
       "                     volume_spike  vwap_dev     rsi_14         bid  \\\n",
       "2014-04-03 12:06:00      0.568641 -0.000177   0.000000   28.636251   \n",
       "2014-04-03 12:07:00      0.570338 -0.000352   0.000000   28.631098   \n",
       "2014-04-03 12:08:00      0.574408 -0.000524   0.000000   28.625944   \n",
       "2014-04-03 12:09:00      0.581017 -0.000694   0.000000   28.620791   \n",
       "2014-04-03 12:10:00      0.590413 -0.000862   0.000000   28.615637   \n",
       "...                           ...       ...        ...         ...   \n",
       "2025-06-18 20:56:00      2.462713  1.248428  17.019768  173.512900   \n",
       "2025-06-18 20:57:00      2.154838  1.246015  11.648165  173.328000   \n",
       "2025-06-18 20:58:00      1.439161  1.245096  11.384870  173.258000   \n",
       "2025-06-18 20:59:00      2.836382  1.244678  11.830567  173.228000   \n",
       "2025-06-18 21:00:00      8.568493  1.248745  22.962317  173.557600   \n",
       "\n",
       "                            ask  signal_smooth  \n",
       "2014-04-03 12:06:00   28.653438       0.327384  \n",
       "2014-04-03 12:07:00   28.648282       0.328556  \n",
       "2014-04-03 12:08:00   28.643125       0.329833  \n",
       "2014-04-03 12:09:00   28.637968       0.331213  \n",
       "2014-04-03 12:10:00   28.632811       0.332697  \n",
       "...                         ...            ...  \n",
       "2025-06-18 20:56:00  173.617100       0.000000  \n",
       "2025-06-18 20:57:00  173.432000       0.000000  \n",
       "2025-06-18 20:58:00  173.362000       0.000000  \n",
       "2025-06-18 20:59:00  173.332000       0.000000  \n",
       "2025-06-18 21:00:00  173.661800       0.000000  \n",
       "\n",
       "[1651679 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURES ENGINEERING\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_ready.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# 1) engineer top intraday features on your 1-min bars\n",
    "for lag in (1, 5, 15):\n",
    "    df[f\"r_{lag}\"] = np.log(df[\"close\"] / df[\"close\"].shift(lag))\n",
    "\n",
    "df[\"vol_15\"]        = df[\"r_1\"].rolling(15).std()\n",
    "df[\"volume_spike\"]  = df[\"volume\"] / df[\"volume\"].rolling(15).mean()\n",
    "\n",
    "typ_price = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "vwap      = (typ_price * df[\"volume\"]).cumsum() / df[\"volume\"].cumsum()\n",
    "df[\"vwap_dev\"]      = (df[\"close\"] - vwap) / vwap\n",
    "\n",
    "delta     = df[\"close\"].diff()\n",
    "gain      = delta.clip(lower=0)\n",
    "loss      = -delta.clip(upper=0)\n",
    "avg_gain  = gain.rolling(14).mean()\n",
    "avg_loss  = loss.rolling(14).mean()\n",
    "rs        = avg_gain / avg_loss\n",
    "df[\"rsi_14\"]        = 100 - (100 / (1 + rs))\n",
    "\n",
    "# 2) keep only your label plus the most predictive features\n",
    "\n",
    "df = df[features_cols  + ['bid', 'ask'] + [label_col]].dropna()\n",
    "\n",
    "print(\"\\n Step D: saving final CSV …\")\n",
    "out_path = f\"dfs training/{ticker}_final.csv\"\n",
    "df.to_csv(out_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1313759, 12, 120])\n",
      "torch.Size([1313759])\n",
      "Shapes:\n",
      "  X         = torch.Size([1313759, 12, 120]) (samples, features, look_back)\n",
      "  y         = torch.Size([1313759]) (samples,)\n",
      "  raw_close = torch.Size([1313759])\n",
      "  raw_bid   = torch.Size([1313759])\n",
      "  raw_ask   = torch.Size([1313759])\n",
      "\n",
      "First 5 window‐end timestamps:\n",
      "  2014-04-03 14:06:00\n",
      "  2014-04-03 14:07:00\n",
      "  2014-04-03 14:08:00\n",
      "  2014-04-03 14:09:00\n",
      "  2014-04-03 14:10:00\n",
      "\n",
      "First window covers 120 bars from\n",
      "  2014-04-03 12:06:00  →  2014-04-03 14:05:00\n",
      "and predicts the bar at 2014-04-03 14:06:00\n",
      "\n",
      "Those bars (timestamps):\n",
      "DatetimeIndex(['2014-04-03 12:06:00', '2014-04-03 12:07:00',\n",
      "               '2014-04-03 12:08:00', '2014-04-03 12:09:00',\n",
      "               '2014-04-03 12:10:00', '2014-04-03 12:11:00',\n",
      "               '2014-04-03 12:12:00', '2014-04-03 12:13:00',\n",
      "               '2014-04-03 12:14:00', '2014-04-03 12:15:00',\n",
      "               ...\n",
      "               '2014-04-03 13:56:00', '2014-04-03 13:57:00',\n",
      "               '2014-04-03 13:58:00', '2014-04-03 13:59:00',\n",
      "               '2014-04-03 14:00:00', '2014-04-03 14:01:00',\n",
      "               '2014-04-03 14:02:00', '2014-04-03 14:03:00',\n",
      "               '2014-04-03 14:04:00', '2014-04-03 14:05:00'],\n",
      "              dtype='datetime64[ns]', length=120, freq='min')\n",
      "First 5 values y: [0.3880619  0.38829955 0.38863376 0.38904947 0.38930276]\n",
      "First 5 signal_smooth values, on regular trade time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2014-04-03 14:06:00    0.388062\n",
       "2014-04-03 14:07:00    0.388300\n",
       "2014-04-03 14:08:00    0.388634\n",
       "2014-04-03 14:09:00    0.389049\n",
       "2014-04-03 14:10:00    0.389303\n",
       "Name: signal_smooth, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = models.build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    features_cols=features_cols,\n",
    "    label_col=label_col,\n",
    "    regular_start=regular_start\n",
    ")\n",
    "\n",
    "# 1)\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) \n",
    "\n",
    "\n",
    "# 2) quick shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"  X         =\", X.shape,    \"(samples, features, look_back)\")\n",
    "print(\"  y         =\", y.shape,    \"(samples,)\")\n",
    "print(\"  raw_close =\", raw_close.shape)\n",
    "print(\"  raw_bid   =\", raw_bid.shape)\n",
    "print(\"  raw_ask   =\", raw_ask.shape)\n",
    "\n",
    "# 3) rebuild the list of label‐timestamps (window‐ends)\n",
    "ends = []\n",
    "for day, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "    day_df = day_df.sort_index()\n",
    "    # candidate ends at positions look_back .. end\n",
    "    idxs = day_df.index[LOOK_BACK:]\n",
    "    # only keep those at/after regular_start\n",
    "    mask = [t >= regular_start for t in idxs.time]\n",
    "    ends.extend(idxs[mask])\n",
    "\n",
    "# 4) show first few ends\n",
    "print(\"\\nFirst 5 window‐end timestamps:\")\n",
    "for ts in ends[:5]:\n",
    "    print(\" \", ts)\n",
    "\n",
    "# 5) show exactly which minutes X[0] contains\n",
    "first_end   = ends[0]\n",
    "first_start = first_end - pd.Timedelta(minutes=LOOK_BACK)\n",
    "print(f\"\\nFirst window covers {LOOK_BACK} bars from\")\n",
    "print(f\"  {first_start}  →  {first_end - pd.Timedelta(minutes=1)}\")\n",
    "print(f\"and predicts the bar at {first_end}\")\n",
    "\n",
    "print(\"\\nThose bars (timestamps):\")\n",
    "print(pd.date_range(first_start, first_end - pd.Timedelta(minutes=1), freq=\"1min\"))\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "y_np         = y.cpu().numpy()\n",
    "print(\"First 5 values y:\",         y_np[:5])\n",
    "print(\"First 5 signal_smooth values, on regular trade time:\")\n",
    "df.signal_smooth.iloc[LOOK_BACK:LOOK_BACK+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_tr        = torch.Size([891515, 12, 120])\n",
      "  y_tr        = torch.Size([891515])\n",
      "  raw_close_te= torch.Size([215633])\n",
      "  raw_bid_te  = torch.Size([215633])\n",
      "  raw_ask_te  = torch.Size([215633])\n",
      "\n",
      "Days: train=1984, val=410, test=422\n",
      "Windows: train=891515, val=206611, test=215633\n",
      "\n",
      "First 5 window‐end times: [Timestamp('2014-04-03 14:06:00'), Timestamp('2014-04-03 14:07:00'), Timestamp('2014-04-03 14:08:00'), Timestamp('2014-04-03 14:09:00'), Timestamp('2014-04-03 14:10:00')]\n",
      "\n",
      "X_tr[0] covers bars from 2014-04-03 12:06:00 to 2014-04-03 14:05:00\n",
      "Those timestamps:\n",
      "DatetimeIndex(['2014-04-03 12:06:00', '2014-04-03 12:07:00',\n",
      "               '2014-04-03 12:08:00', '2014-04-03 12:09:00',\n",
      "               '2014-04-03 12:10:00', '2014-04-03 12:11:00',\n",
      "               '2014-04-03 12:12:00', '2014-04-03 12:13:00',\n",
      "               '2014-04-03 12:14:00', '2014-04-03 12:15:00',\n",
      "               ...\n",
      "               '2014-04-03 13:56:00', '2014-04-03 13:57:00',\n",
      "               '2014-04-03 13:58:00', '2014-04-03 13:59:00',\n",
      "               '2014-04-03 14:00:00', '2014-04-03 14:01:00',\n",
      "               '2014-04-03 14:02:00', '2014-04-03 14:03:00',\n",
      "               '2014-04-03 14:04:00', '2014-04-03 14:05:00'],\n",
      "              dtype='datetime64[ns]', length=120, freq='min')\n",
      "y_tr[0] (and raw_close_te[0]) is the bar at 2014-04-03 14:06:00\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val/test by calendar day\n",
    "(X_tr, y_tr), \\\n",
    "(X_val, y_val), \\\n",
    "(X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = models.chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back   = LOOK_BACK,\n",
    "    regular_start   = regular_start,\n",
    "    train_prop  = TRAIN_PROP,\n",
    "    val_prop    = VAL_PROP,\n",
    "    train_batch = TRAIN_BATCH\n",
    ")\n",
    "\n",
    "# 1) Print shapes of all tensors\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_tr        =\", X_tr.shape)\n",
    "print(\"  y_tr        =\", y_tr.shape)\n",
    "print(\"  raw_close_te=\", raw_close_te.shape)\n",
    "print(\"  raw_bid_te  =\", raw_bid_te.shape)\n",
    "print(\"  raw_ask_te  =\", raw_ask_te.shape)\n",
    "\n",
    "# 2) Print number of days in each split\n",
    "n_tr_days = torch.unique(day_id_tr).numel()\n",
    "n_val_days= torch.unique(day_id_val).numel()\n",
    "n_te_days = torch.unique(day_id_te).numel()\n",
    "print(f\"\\nDays: train={n_tr_days}, val={n_val_days}, test={n_te_days}\")\n",
    "\n",
    "# 3) Print number of windows in each split\n",
    "print(f\"Windows: train={X_tr.shape[0]}, val={X_val.shape[0]}, test={X_te.shape[0]}\")\n",
    "\n",
    "# 4) List the first few window‐end timestamps\n",
    "ends = []\n",
    "for day, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "    ts = day_df.index[LOOK_BACK:]\n",
    "    ends.extend(ts[ts.time >= regular_start])\n",
    "first_ends = ends[:5]\n",
    "print(\"\\nFirst 5 window‐end times:\", first_ends)\n",
    "\n",
    "# 5) Show exactly which minutes X_tr[0] covers, and where y_tr[0] sits\n",
    "first_end   = first_ends[0]\n",
    "first_start = first_end - pd.Timedelta(minutes=LOOK_BACK)\n",
    "# input bars = [first_start … first_end − 1min]\n",
    "print(f\"\\nX_tr[0] covers bars from {first_start} to {first_end - pd.Timedelta(minutes=1)}\")\n",
    "print(\"Those timestamps:\")\n",
    "print(pd.date_range(first_start, first_end - pd.Timedelta(minutes=1), freq=\"1min\"))\n",
    "print(f\"y_tr[0] (and raw_close_te[0]) is the bar at {first_end}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad9b6c4-3d79-45c0-b2c0-c4f46f1ad866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Entered split_to_day_datasets\n",
      "1) building weekday arrays\n",
      "   Weekdays counts → tr=891515, val=206611, te=215633\n",
      "2) moving all splits to CPU\n",
      "   CPU casts done\n",
      "3) zero-bas­ing day_id for val & test\n",
      "   val_day_id ∈ [0..409], total days=410\n",
      "   te_day_id  ∈ [0..421], total days=422\n",
      "4) instantiating DayWindowDatasets\n",
      "   ds_tr days: 1984\n",
      "   ds_val days: 410\n",
      "   ds_te days: 422\n",
      "5) building DataLoaders\n",
      "   train_loader ready\n",
      "   val_loader ready\n",
      "   test_loader ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Build DataLoaders over calendar‐days\n",
    "# -----------------------------------------------------------------------------\n",
    "train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "    # Training split arrays (from chronological_split)\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    # Validation split arrays\n",
    "    X_val, y_val, day_id_val,\n",
    "    # Test split arrays + raw prices for post‐tracking\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    # Original minute‐bar DataFrame for weekday mapping\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH,\n",
    "    train_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ed9892-6d63-44d6-8d1e-619cb2485841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_optimizer_and_scheduler(\n",
    "    model: nn.Module,\n",
    "    initial_lr: float,\n",
    "    weight_decay: float = 1e-3,\n",
    "    lr_reduce_factor: float = 0.5,\n",
    "    lr_patience: int = 3,\n",
    "    lr_min: float = 1e-6,\n",
    "    clipnorm: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build optimizer, LR scheduler, AMP scaler, and clipping threshold.\n",
    "\n",
    "    • optimizer: AdamW with L2 weight_decay on all non‐bias weights.\n",
    "    • plateau_sched: ReduceLROnPlateau – watches val‐RMSE, cuts LR when plateau.\n",
    "      - mode='min'       → lower-is-better metric (we minimize RMSE).\n",
    "      - factor=0.5       → multiply LR by 0.5 on plateau.\n",
    "      - patience=3       → wait 3 epochs with no improvement before cutting.\n",
    "      - min_lr=1e-6      → never drop below this learning rate.\n",
    "      - verbose=True     → print a message whenever LR is reduced.\n",
    "    • scaler: GradScaler for mixed‐precision (automatically handles device).\n",
    "    • clipnorm: float threshold for gradient clipping in train_step.\n",
    "\n",
    "    Returns:\n",
    "      optimizer, plateau_sched, scaler, clipnorm\n",
    "    \"\"\"\n",
    "    # 1) AdamW: adaptive moment optimizer with decoupled weight decay\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # 2) ReduceLROnPlateau: only reduces LR when val metric plateaus\n",
    "    plateau_sched = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=lr_reduce_factor,\n",
    "        patience=lr_patience,\n",
    "        min_lr=lr_min,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    cosine_sched  = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=T_0, \n",
    "        T_mult=T_MULT, \n",
    "        eta_min=ETA_MIN\n",
    "    )\n",
    "\n",
    "    # 3) GradScaler: handles loss scaling for mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Return all four objects; clipnorm passed through to train_step\n",
    "    return optimizer, plateau_sched, cosine_sched, scaler, clipnorm\n",
    "\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model:     nn.Module,\n",
    "    x_day:     torch.Tensor,            # shape (W, look_back, F), already on device\n",
    "    y_day:     torch.Tensor,            # shape (W,),            already on device\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler:    GradScaler,\n",
    "    clipnorm:  float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform one mixed‐precision training update for a single “day” of data.\n",
    "\n",
    "    Steps:\n",
    "      1. Zero gradients (`optimizer.zero_grad(set_to_none=True)`).\n",
    "      2. Forward in fp16 context:\n",
    "         - autocast on x_day.device\n",
    "         - model(x_day) → out shape (W, seq_len, 1)\n",
    "         - extract last step: out[:, -1, 0] → (W,)\n",
    "         - compute MSE loss against y_day\n",
    "      3. Backward:\n",
    "         - scaler.scale(loss).backward()\n",
    "         - scaler.unscale_(optimizer) to prepare for clipping\n",
    "         - clip gradients to `clipnorm`\n",
    "         - scaler.step(optimizer) + scaler.update()\n",
    "      4. Return float(loss)\n",
    "\n",
    "    Returns:\n",
    "      The scalar loss value (Python float) for logging.\n",
    "    \"\"\"\n",
    "    # 1) Reset gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    model.train()\n",
    "\n",
    "    # 2) Mixed‐precision forward\n",
    "    device = x_day.device\n",
    "    with autocast(device_type=device.type):\n",
    "        out = model(x_day)            # → (W, seq_len, 1)\n",
    "        last = out[:, -1, 0]          # → (W,)\n",
    "        loss = Funct.mse_loss(last, y_day, reduction='mean')\n",
    "\n",
    "    # 3) Backward with gradient scaling and clipping\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)        # bring gradients back to fp32 for clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def get_current_lr(optimizer: torch.optim.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Retrieve the current learning rate from the first parameter group.\n",
    "    \"\"\"\n",
    "    return float(optimizer.param_groups[0]['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e0ad3f-d804-4d03-a348-760baa264ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_stateful_training_loop(\n",
    "    model:         torch.nn.Module,\n",
    "    optimizer:     torch.optim.Optimizer,\n",
    "    cosine_sched:  torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    plateau_sched: torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scaler:        torch.cuda.amp.GradScaler,\n",
    "    train_loader:  torch.utils.data.DataLoader,\n",
    "    val_loader:    torch.utils.data.DataLoader,\n",
    "    *,\n",
    "    max_epochs:          int,\n",
    "    early_stop_patience: int,\n",
    "    baseline_val_rmse:   float,\n",
    "    clipnorm:            float,\n",
    "    device:              torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Stateful LSTM Training Loop with:\n",
    "      - Mixed‐precision (AMP)\n",
    "      - Per‐day hidden‐state resets\n",
    "      - CosineWarmRestarts (per‐batch) + ReduceLROnPlateau (per‐epoch)\n",
    "      - Early stopping + live RMSE plot\n",
    "      - Extra GPU‐cache clears & memory logs to prevent silent OOMs\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state    = None\n",
    "    patience_ctr  = 0\n",
    "\n",
    "    save_path = params.save_path\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    live_plot = plots.LiveRMSEPlot()\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # ─── PRE‐EPOCH: clear and log memory ─────────────────────────────\n",
    "        # torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"[Epoch {epoch}] GPU alloc = {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n",
    "\n",
    "        # ─── TRAIN PHASE ─────────────────────────────────────────────────\n",
    "        model.train()\n",
    "        model.h_short = model.h_long = None\n",
    "        train_losses = []\n",
    "\n",
    "        pbar = tqdm(\n",
    "            enumerate(train_loader),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch {epoch}\",\n",
    "            unit=\"bundle\"\n",
    "        )\n",
    "\n",
    "        for batch_idx, (xb_days, yb_days, wd_days) in pbar:\n",
    "            # Move one bundle of days to GPU\n",
    "            xb_days = xb_days.to(device, non_blocking=True)\n",
    "            yb_days = yb_days.to(device, non_blocking=True)\n",
    "            wd_days = wd_days.to(device)\n",
    "\n",
    "            # Zero grads once per bundle\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            prev_wd = None\n",
    "\n",
    "            for di in range(xb_days.size(0)):\n",
    "                wd = int(wd_days[di].item())\n",
    "\n",
    "                # Reset states\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                # Autocast forward/backward\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    out  = model(xb_days[di])       # -> (W, look_back, 1)\n",
    "                    last = out[..., -1, 0]          # -> (W,)\n",
    "                    loss = Funct.mse_loss(last, yb_days[di], reduction='mean')\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                # Detach hidden‐state to trim graph\n",
    "                if isinstance(model.h_short, tuple):\n",
    "                    model.h_short = tuple(h.detach() for h in model.h_short)\n",
    "                if isinstance(model.h_long, tuple):\n",
    "                    model.h_long = tuple(h.detach() for h in model.h_long)\n",
    "\n",
    "                # free per‐day intermediates\n",
    "                del out, last, loss\n",
    "\n",
    "            # Unscale, clip, step & update scaler\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Cosine scheduler step (fractional epoch)\n",
    "            frac_epoch = epoch - 1 + batch_idx / len(train_loader)\n",
    "            cosine_sched.step(frac_epoch)\n",
    "\n",
    "            # Log bundle RMSE & LR\n",
    "            rmse = math.sqrt(sum(train_losses) / len(train_losses))\n",
    "            lr   = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix(train_rmse=rmse, lr=lr, refresh=False)\n",
    "            pbar.update(0)\n",
    "\n",
    "            # Clear bundle tensors & cache\n",
    "            del xb_days, yb_days, wd_days\n",
    "            # torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        # ─── VALIDATION PHASE ─────────────────────────────────────────────\n",
    "        model.eval()\n",
    "        model.h_short = model.h_long = None\n",
    "        val_losses = []\n",
    "        prev_wd    = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb_day, yb_day, wd in val_loader:\n",
    "                wd = int(wd.item())\n",
    "                x  = xb_day[0].to(device, non_blocking=True)\n",
    "                y  = yb_day.view(-1).to(device, non_blocking=True)\n",
    "\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                out  = model(x)\n",
    "                last = out[..., -1, 0]\n",
    "                val_losses.append(Funct.mse_loss(last, y, reduction='mean').item())\n",
    "\n",
    "                del xb_day, yb_day, x, y, out, last\n",
    "                # torch.cuda.empty_cache()\n",
    "\n",
    "        val_rmse = math.sqrt(sum(val_losses) / len(val_losses))\n",
    "\n",
    "        # ─── LOG & SCHEDULERS ─────────────────────────────────────────────\n",
    "        live_plot.update(rmse, val_rmse)\n",
    "        print(f\"Epoch {epoch:03d} • train={rmse:.4f} • val={val_rmse:.4f} • lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # ReduceLROnPlateau\n",
    "        pre_lr = optimizer.param_groups[0]['lr']\n",
    "        plateau_sched.step(val_rmse)\n",
    "        post_lr = optimizer.param_groups[0]['lr']\n",
    "        if post_lr < pre_lr:\n",
    "            print(f\"Plateau reduced LR: {pre_lr:.1e} → {post_lr:.1e} at epoch {epoch}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_state    = copy.deepcopy(model.state_dict())\n",
    "            patience_ctr  = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered at epoch\", epoch)\n",
    "                break\n",
    "\n",
    "    # ─── SAVE BEST MODEL ─────────────────────────────────────────────────\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    ckpt_file = save_path / f\"{ticker}_{best_val_rmse:.4f}.pth\"\n",
    "    torch.save(model, ckpt_file)\n",
    "    print(f\"Saved best model (val RMSE={best_val_rmse:.4f}) to {ckpt_file}\")\n",
    "\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sees 1984 calendar days per epoch\n",
      "\n",
      "Baseline (zero‐forecast) RMSE on validation = 0.535586\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] GPU alloc = 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_807/4215409556.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|██████████| 31/31 [00:33<00:00,  1.08s/bundle, lr=0.00053, train_rmse=0.384] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.3842 • val=0.3221 • lr=5.30e-04\n",
      "[Epoch 2] GPU alloc = 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 31/31 [00:29<00:00,  1.04bundle/s, lr=1.06e-5, train_rmse=0.29]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.2895 • val=0.4264 • lr=1.06e-05\n",
      "[Epoch 3] GPU alloc = 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 31/31 [00:29<00:00,  1.05bundle/s, lr=0.000864, train_rmse=0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.2194 • val=0.3379 • lr=8.64e-04\n",
      "[Epoch 4] GPU alloc = 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 31/31 [00:30<00:00,  1.03bundle/s, lr=0.000518, train_rmse=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.2790 • val=0.3482 • lr=5.18e-04\n",
      "Plateau reduced LR: 5.2e-04 → 2.6e-04 at epoch 4\n",
      "[Epoch 5] GPU alloc = 0.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  65%|██████▍   | 20/31 [00:20<00:10,  1.04bundle/s, lr=0.000276, train_rmse=0.19] "
     ]
    }
   ],
   "source": [
    "N_FEATS = len(features_cols)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the stateful DualMemoryLSTM & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "model = models.DualMemoryLSTM(\n",
    "    n_feats        = N_FEATS,        # number of input features per minute\n",
    "    short_units    = SHORT_UNITS,    # hidden size of daily LSTM\n",
    "    long_units     = LONG_UNITS,     # hidden size of weekly LSTM\n",
    "    dropout_short  = DROPOUT_SHORT,  # dropout after daily LSTM\n",
    "    dropout_long   = DROPOUT_LONG,    # dropout after weekly LSTM\n",
    "    att_heads      = ATT_HEADS,\n",
    "    att_drop       = ATT_DROPOUT\n",
    ")\n",
    "model.to(device)   # place model parameters on GPU or CPU as specified\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute plateau_sched timing parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# Total training samples = total windows in X_tr (one window per row)\n",
    "n_train_samples = X_tr.shape[0]\n",
    "\n",
    "# How many optimizer steps (day‐bundles) constitute one epoch?\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build optimizer, LR scheduler, AMP scaler, and gradient‐clip norm\n",
    "# -----------------------------------------------------------------------------\n",
    "optimizer, plateau_sched, cosine_sched, scaler, clipnorm = make_optimizer_and_scheduler(\n",
    "    model,\n",
    "    initial_lr         = INITIAL_LR,         # starting learning rate, e.g. 3e-4\n",
    "    weight_decay       = WEIGHT_DECAY,       # L2 penalty on all weights\n",
    "    lr_reduce_factor   = PLATEAU_FACTOR,     # multiply LR by IT on plateau\n",
    "    lr_patience        = PLATEAU_PATIENCE,   # epochs with no val‐improve before reduce\n",
    "    lr_min             = MIN_LR,             # lower bound on LR\n",
    "    clipnorm           = CLIPNORM            # max gradient‐norm before clipping\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Count how many calendar days we see each epoch\n",
    "# -----------------------------------------------------------------------------\n",
    "n_train_days = len(train_loader.dataset)  # dataset length = # unique days\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute baseline RMSE on validation (zero forecast)\n",
    "# -----------------------------------------------------------------------------\n",
    "baseline_val_rmse = models.naive_rmse(val_loader)\n",
    "print(f\"Baseline (zero‐forecast) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    cosine_sched        = cosine_sched,\n",
    "    plateau_sched       = plateau_sched,\n",
    "    scaler              = scaler,\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = MAX_EPOCHS,\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "    baseline_val_rmse   = baseline_val_rmse,\n",
    "    clipnorm            = clipnorm,\n",
    "    device              = device      # torch.device('cuda') or 'cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final reporting: best RMSE and relative improvement\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "\n",
    "improvement = 100.0 * (1.0 - best_val_rmse / baseline_val_rmse)\n",
    "print(f\"Improvement over zero‐baseline = {improvement:5.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11a096-4db7-493b-a3bb-4ae171c7df6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e0cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8660dd-d2db-434a-aa59-17814d343fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
