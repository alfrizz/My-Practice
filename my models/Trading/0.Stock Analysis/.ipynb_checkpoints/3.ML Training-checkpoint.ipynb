{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 19:12:43.231308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750093963.252103   37342 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750093963.259716   37342 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750093963.277295   37342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750093963.277318   37342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750093963.277320   37342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750093963.277322   37342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import platform \n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:30:00</th>\n",
       "      <td>250.5906</td>\n",
       "      <td>250.6435</td>\n",
       "      <td>250.5244</td>\n",
       "      <td>250.5753</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>250.5001</td>\n",
       "      <td>250.6505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:31:00</th>\n",
       "      <td>250.5806</td>\n",
       "      <td>250.6317</td>\n",
       "      <td>250.5121</td>\n",
       "      <td>250.5606</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>250.4854</td>\n",
       "      <td>250.6358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:32:00</th>\n",
       "      <td>250.5712</td>\n",
       "      <td>250.6200</td>\n",
       "      <td>250.4938</td>\n",
       "      <td>250.5453</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>250.4701</td>\n",
       "      <td>250.6205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:33:00</th>\n",
       "      <td>250.5580</td>\n",
       "      <td>250.6094</td>\n",
       "      <td>250.4762</td>\n",
       "      <td>250.5347</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>250.4595</td>\n",
       "      <td>250.6099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 13:34:00</th>\n",
       "      <td>250.5491</td>\n",
       "      <td>250.5994</td>\n",
       "      <td>250.4600</td>\n",
       "      <td>250.5168</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>250.4416</td>\n",
       "      <td>250.5919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:56:00</th>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3500</td>\n",
       "      <td>203.2450</td>\n",
       "      <td>203.3200</td>\n",
       "      <td>189023.0</td>\n",
       "      <td>203.2590</td>\n",
       "      <td>203.3810</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:57:00</th>\n",
       "      <td>203.3200</td>\n",
       "      <td>203.4200</td>\n",
       "      <td>203.3050</td>\n",
       "      <td>203.3800</td>\n",
       "      <td>222383.0</td>\n",
       "      <td>203.3190</td>\n",
       "      <td>203.4410</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:58:00</th>\n",
       "      <td>203.3800</td>\n",
       "      <td>203.4300</td>\n",
       "      <td>203.3322</td>\n",
       "      <td>203.3750</td>\n",
       "      <td>279702.0</td>\n",
       "      <td>203.3140</td>\n",
       "      <td>203.4360</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 20:59:00</th>\n",
       "      <td>203.3700</td>\n",
       "      <td>203.4100</td>\n",
       "      <td>203.2500</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>724307.0</td>\n",
       "      <td>203.2790</td>\n",
       "      <td>203.4010</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-03 21:00:00</th>\n",
       "      <td>203.3288</td>\n",
       "      <td>203.3400</td>\n",
       "      <td>202.8400</td>\n",
       "      <td>203.1993</td>\n",
       "      <td>11076221.0</td>\n",
       "      <td>203.1383</td>\n",
       "      <td>203.2603</td>\n",
       "      <td>0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46904 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close      volume  \\\n",
       "2025-01-02 13:30:00  250.5906  250.6435  250.5244  250.5753      2259.0   \n",
       "2025-01-02 13:31:00  250.5806  250.6317  250.5121  250.5606      2351.0   \n",
       "2025-01-02 13:32:00  250.5712  250.6200  250.4938  250.5453      2455.0   \n",
       "2025-01-02 13:33:00  250.5580  250.6094  250.4762  250.5347      2474.0   \n",
       "2025-01-02 13:34:00  250.5491  250.5994  250.4600  250.5168      2792.0   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2025-06-03 20:56:00  203.2500  203.3500  203.2450  203.3200    189023.0   \n",
       "2025-06-03 20:57:00  203.3200  203.4200  203.3050  203.3800    222383.0   \n",
       "2025-06-03 20:58:00  203.3800  203.4300  203.3322  203.3750    279702.0   \n",
       "2025-06-03 20:59:00  203.3700  203.4100  203.2500  203.3400    724307.0   \n",
       "2025-06-03 21:00:00  203.3288  203.3400  202.8400  203.1993  11076221.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2025-01-02 13:30:00  250.5001  250.6505             0             0.00   \n",
       "2025-01-02 13:31:00  250.4854  250.6358             0             0.00   \n",
       "2025-01-02 13:32:00  250.4701  250.6205             0             0.00   \n",
       "2025-01-02 13:33:00  250.4595  250.6099             0             0.00   \n",
       "2025-01-02 13:34:00  250.4416  250.5919             0             0.00   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-03 20:56:00  203.2590  203.3810             0             1.99   \n",
       "2025-06-03 20:57:00  203.3190  203.4410             0             1.99   \n",
       "2025-06-03 20:58:00  203.3140  203.4360             0             1.99   \n",
       "2025-06-03 20:59:00  203.2790  203.4010             0             1.99   \n",
       "2025-06-03 21:00:00  203.1383  203.2603             0             1.99   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2025-01-02 13:30:00        0.000                 0.0  \n",
       "2025-01-02 13:31:00        0.000                 0.0  \n",
       "2025-01-02 13:32:00        0.000                 0.0  \n",
       "2025-01-02 13:33:00        0.000                 0.0  \n",
       "2025-01-02 13:34:00        0.000                 0.0  \n",
       "...                          ...                 ...  \n",
       "2025-06-03 20:56:00        0.942                 0.0  \n",
       "2025-06-03 20:57:00        0.882                 0.0  \n",
       "2025-06-03 20:58:00        0.887                 0.0  \n",
       "2025-06-03 20:59:00        0.922                 0.0  \n",
       "2025-06-03 21:00:00        1.062                 0.0  \n",
       "\n",
       "[46904 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/merged_{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"signal_smooth_norm\"\n",
    "\n",
    "feature_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "raw_F = len(feature_cols)\n",
    "look_back = 60\n",
    "n_feats = raw_F * look_back \n",
    "\n",
    "rth_start = dt.time(14, 30) \n",
    "save_dir   = Path(\"dfs training\") \n",
    "ckpt_path = save_dir / f\"best_{ticker}.weights.h5\" \n",
    "\n",
    "train_prop = 0.7\n",
    "val_prop = 0.15\n",
    "units = 128\n",
    "\n",
    "dropout = 0.10\n",
    "recurrent_dropout = 0.05\n",
    "initial_lr = 1e-3\n",
    "min_lr     = 1e-5   # two orders of magnitude lower\n",
    "loss = \"mse\"\n",
    "\n",
    "max_epochs             = 60\n",
    "early_stop_patience    = 10    # epochs without improvement\n",
    "lr_reduce_patience     = 4    # epochs without improvement → halve LR\n",
    "\n",
    "STEPS_PER_PLOT = 1              # refresh tqdm postfix every n trading sessions\n",
    "\n",
    "TRAIN_BATCH   = 32                      # 8-64 if VRAM allows, 1 keeps CPU parity\n",
    "VAL_BATCH   = 1\n",
    "USE_FP16      = True                    # set False if GPU < Turing / Ampere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# build_lstm_tensors                                                   #\n",
    "# ---------------------------------------------------------------------\n",
    "# • Converts a multi-day minute-bar DataFrame into leakage-free NumPy   #\n",
    "#   tensors suitable for our stateful LSTM.                            #\n",
    "# • One *sample*  = last `look_back` candles ➜ predict current candle. #\n",
    "# • Windows NEVER cross midnight.                                      #\n",
    "# • Features are standard-scaled *within each day* to kill day-to-day  #\n",
    "#   level shifts without leaking information between sessions.         #\n",
    "# • Output dtype = float32 (saves RAM & plays nice with GPUs).         #\n",
    "# =====================================================================\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,                         # ← force keyword args for safety\n",
    "    look_back: int,            # length of sliding window (minutes)\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,        # e.g. 14:30 CET for US stocks\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray, shape (samples , look_back·n_feats)\n",
    "        Flattened window for every RTH bar.\n",
    "    y : np.ndarray, shape (samples ,)\n",
    "        One-step-ahead target (same length as samples).\n",
    "    \"\"\"\n",
    "\n",
    "    X_windows, y_targets = [], []             # collectors\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 1) Iterate one calendar day at a time  –– main leak stopper\n",
    "    # -------------------------------------------------------------\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "\n",
    "        # --- chronological order is critical for windows ----------\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # --- per-day standardisation of *features* -----------------\n",
    "        scaler = StandardScaler()\n",
    "        day_df[feature_cols] = scaler.fit_transform(day_df[feature_cols])\n",
    "\n",
    "        # --- pull NumPy views (cheap) ------------------------------\n",
    "        feat_np  = day_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "        label_np = day_df[label_col]      .to_numpy(dtype=np.float32)\n",
    "\n",
    "        # --- locate bars inside Regular Trading Hours --------------\n",
    "        mask_rth = day_df.index.time >= rth_start\n",
    "        rth_idx  = np.flatnonzero(mask_rth)   # integer positions\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 2) Build sliding windows that stay *within the same day*\n",
    "        # -----------------------------------------------------------\n",
    "        for i in rth_idx:\n",
    "            if i < look_back:            # not enough history yet\n",
    "                continue\n",
    "\n",
    "            # rows t-look_back … t-1 become the window\n",
    "            window_3d = feat_np[i - look_back : i]         # (L , F)\n",
    "\n",
    "            # ── choose representation ───────────────────────────\n",
    "            # OPTION A: keep 3-D window  (uncomment next two lines\n",
    "            #          and comment the \"flatten\" line if you prefer)\n",
    "            # X_windows.append(window_3d)\n",
    "            # ----------------------------------------------------\n",
    "            # OPTION B: flatten to 1-D (default – lighter & matches\n",
    "            #          our make_day_dataset reshape)\n",
    "            X_windows.append(window_3d.reshape(-1))        # (L·F,)\n",
    "            # ----------------------------------------------------\n",
    "\n",
    "            y_targets.append(label_np[i])                  # scalar\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 3) Stack lists → final tensors\n",
    "    # -------------------------------------------------------------\n",
    "    X = np.stack(X_windows, dtype=np.float32)    # (N , L·F)  or (N , L , F)\n",
    "    y = np.asarray(y_targets, dtype=np.float32)  # (N ,)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40664, 300)\n",
      "(40664,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=look_back,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split                                                         #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Purpose                                                                     #\n",
    "#   Chronologically split the sliding-window tensors (X, y) into              #\n",
    "#       • train      – first `train_prop`   fraction of *days*                #\n",
    "#       • validation – next  `val_prop`     fraction of *days*                #\n",
    "#       • test       – remainder                                              #\n",
    "#                                                                            #\n",
    "# Why we have to count *windows per day* again                                #\n",
    "#   `build_lstm_tensors()` creates `X` by looping minute-by-minute **inside** #\n",
    "#   each day and it skips the first `look_back` indices (they have no full    #\n",
    "#   context).  Therefore the number of samples coming out of one day is       #\n",
    "#                                                                            #\n",
    "#         (# RTH rows in that day)  minus  (how many of those rows have       #\n",
    "#                                      global-index  < look_back)            #\n",
    "#                                                                            #\n",
    "#   We reproduce exactly that logic here so that the vector `day_id` we build #\n",
    "#   matches X *one-to-one*.                                                   #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],   # train\n",
    "        Tuple[np.ndarray, np.ndarray],   # val\n",
    "        Tuple[np.ndarray, np.ndarray],   # test\n",
    "        List[int],                       # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray   # day_id train/val/test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X , y    Sliding-window tensors returned by `build_lstm_tensors`.\n",
    "    df       Same DataFrame used to create X, y (index = DateTimeIndex).\n",
    "    look_back\n",
    "              Length of each window (minutes).\n",
    "    rth_start\n",
    "              First bar that counts as a *target* (e.g. 14:30:00 CET).\n",
    "    train_prop , val_prop\n",
    "              Fractions of *days* that go into train / val.  Remainder\n",
    "              becomes the test split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train , y_train) , (X_val , y_val) , (X_test , y_test)\n",
    "    samples_per_day      - list[int] , number of usable windows per day\n",
    "    day_id_tr , day_id_val , day_id_te\n",
    "              - arrays mapping each sample back to its calendar day\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 1) How many windows did each calendar day contribute?\n",
    "    #    We must replicate the *same* \"skip first look_back rows\" rule\n",
    "    #    that build_lstm_tensors() applied, otherwise lengths won't match.\n",
    "    # -----------------------------------------------------------------\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.date):\n",
    "        day_df = day_df.sort_index()                       # chronological\n",
    "\n",
    "        # integer row numbers of bars that are inside RTH for *this* day\n",
    "        idx_rth = np.flatnonzero(day_df.index.time >= rth_start)\n",
    "\n",
    "        # keep only those indices that have at least `look_back` rows of\n",
    "        # history *within the same day*  (identical IF-condition as before)\n",
    "        idx_valid = idx_rth[idx_rth >= look_back]\n",
    "\n",
    "        samples_per_day.append(len(idx_valid))\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2) Build `day_id` – one integer tag for every sample in X\n",
    "    # -----------------------------------------------------------------\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    if len(day_id) != len(X):\n",
    "        raise ValueError(\n",
    "            f\"Mismatch: day_id length = {len(day_id)}  but  X length = {len(X)}.\\n\"\n",
    "            \"Check that build_lstm_tensors() and chronological_split() \"\n",
    "            \"apply the *same* look_back and rth_start logic.\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 3) Determine split boundaries in *day space* (not in sample space)\n",
    "    # -----------------------------------------------------------------\n",
    "    last_day_index = len(samples_per_day) - 1        # 0-based\n",
    "    train_cut_day  = int(last_day_index * train_prop)\n",
    "    val_cut_day    = int(last_day_index * (train_prop + val_prop))\n",
    "\n",
    "    train_mask =  day_id <= train_cut_day\n",
    "    val_mask   = (day_id > train_cut_day) & (day_id <= val_cut_day)\n",
    "    test_mask  =  day_id > val_cut_day\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 4) Slice tensors\n",
    "    # -----------------------------------------------------------------\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val,   y_val   = X[val_mask],   y[val_mask]\n",
    "    X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 5) Return everything – keeping `day_id` arrays lets downstream\n",
    "    #    builders (make_day_dataset) stitch windows back into full days.\n",
    "    # -----------------------------------------------------------------\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), \\\n",
    "           samples_per_day,                                       \\\n",
    "           day_id[train_mask], day_id[val_mask], day_id[test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 64  (multiple of 32)\n",
      "Validation days    : 15\n",
      "Test days          : 16\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=look_back,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=train_prop,\n",
    "        val_prop=val_prop\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset                                                             #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *per-minute* sliding-window arrays (X, y, day_id) into a        #\n",
    "# tf.data.Dataset that streams ONE element per trading session.               #\n",
    "#                                                                              #\n",
    "# Output per element                                                           #\n",
    "#   x_day        : (batch = 1 , T , n_feats)   – full intraday feature tensor  #\n",
    "#   y_day        : (batch = 1 , T)             – matching label vector         #\n",
    "#   new_day_flag : scalar bool  (always True)  – lets the training loop call   #\n",
    "#                                              reset_states() explicitly       #\n",
    "#                                                                              #\n",
    "# Important:                                                                   \n",
    "#   • `X` already contains the *flattened* 60-bar window for every minute      #\n",
    "#     (shape per sample = look_back × raw_feats).  Therefore we NO LONGER      #\n",
    "#     slice `[:, -1, :]`; we keep the vector “as is”.                          #\n",
    "#   • If you switched back to the 3-D window variant, replace the “reshape”    #\n",
    "#     comment below accordingly.                                               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def make_day_dataset(\n",
    "        X: np.ndarray,           # (N , L·F)   – flattened 60-bar window\n",
    "        y: np.ndarray,           # (N ,)       – scalar target per sample\n",
    "        day_id: np.ndarray,      # (N ,)       – integer day tag\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Groups minute-level samples into per-day tensors and returns a\n",
    "    stateful-ready Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 1.  Chronological ordering  (just in case the caller shuffled)\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    sort_idx  = np.argsort(day_id, kind=\"stable\")\n",
    "    X_sorted  = X[sort_idx]\n",
    "    y_sorted  = y[sort_idx]\n",
    "    d_sorted  = day_id[sort_idx]\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 2.  Slice contiguous blocks that share the same day_id\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    day_change = np.where(np.diff(d_sorted) != 0)[0] + 1\n",
    "    split_ids  = np.split(np.arange(len(d_sorted)), day_change)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 3.  Generator – one yield == one full RTH session\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    def gen():\n",
    "        for idx_block in split_ids:\n",
    "            # All minutes for one calendar day\n",
    "            x_block = X_sorted[idx_block]          # (T , L·F)\n",
    "            y_block = y_sorted[idx_block]          # (T ,)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # If you kept the 3-D window (60 , F) variant in\n",
    "            # build_lstm_tensors(), replace the next two lines with:\n",
    "            #     x_day = x_block.reshape(T , look_back , raw_F)\n",
    "            #     n_feats = look_back * raw_F  <-- adjust below\n",
    "            # ---------------------------------------------------------\n",
    "            x_day = x_block                        # (T , n_feats) – already flat\n",
    "\n",
    "            # Label vector is already one-dimensional\n",
    "            y_day = y_block                        # (T ,)\n",
    "\n",
    "            # Add leading batch dim = 1 for stateful=True\n",
    "            x_day = np.expand_dims(x_day, 0)       # (1 , T , n_feats)\n",
    "            y_day = np.expand_dims(y_day, 0)       # (1 , T)\n",
    "\n",
    "            yield x_day.astype(np.float32), y_day.astype(np.float32), True\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # 4.  Wrap in tf.data.Dataset\n",
    "    #     T is variable (None) because each day has a different #bars\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    n_feats = X.shape[-1]                 # here n_feats = look_back · raw_F\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(1, None, n_feats), dtype=tf.float32),  # x_day\n",
    "        tf.TensorSpec(shape=(1, None),       dtype=tf.float32),     # y_day\n",
    "        tf.TensorSpec(shape=(),              dtype=tf.bool)         # flag\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)     # overlaps data prep & GPU work\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch shape : (32, 391, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750093967.593030   37342 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# create day-level streams\n",
    "ds_train = make_day_dataset(X_tr,  y_tr,  day_id_tr)\n",
    "ds_val   = make_day_dataset(X_val, y_val, day_id_val)\n",
    "ds_test  = make_day_dataset(X_te,  y_te,  day_id_te)\n",
    "\n",
    "# helper removes the dummy *inner* batch axis (shape (1, T, F) → (T, F))\n",
    "def _squeeze(x_day, y_day, flag):\n",
    "    # x_day : (1, T, n_feats)  →  (T, n_feats)\n",
    "    # y_day : (T, 1)  stays as-is (nothing to squeeze on axis 0)\n",
    "    return tf.squeeze(x_day, axis=0), y_day, flag\n",
    "\n",
    "# ── TRAIN ──\n",
    "# we MUST squeeze before padded_batch, otherwise the leading “1” would add\n",
    "# an unwanted dimension inside the (32, …) mega-batch.\n",
    "ds_train_batched = (ds_train\n",
    "    .map(_squeeze, num_parallel_calls=tf.data.AUTOTUNE)          # remove axis 0\n",
    "    .padded_batch(TRAIN_BATCH, drop_remainder=True)              # → (32, Tₘₐₓ, F)\n",
    "    .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# ── VALIDATION ──\n",
    "# NO squeeze!  X keeps shape (1, T, F) so the batch-1 stateful model\n",
    "# receives exactly the batch_shape it was built for: (1, None, 300).\n",
    "ds_val_unbatched = ds_val     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│                ❶  NETWORK WEIGHTS   (global knowledge)                │\\n│   • Millions of parameters learned across *all* historic days.        │\\n│   • Do **not** change during inference.                               │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│              ❷  CELL STATE  cₜ   (“within-day long-term”)             │\\n│   • Slow-changing store; integrator.                                   │\\n│   • Carries patterns from early-morning bars to late-afternoon bars.   │\\n│   • Reset to 0 at day boundary.                                        │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│              ❸  HIDDEN STATE  hₜ  (“within-day short-term”)           │\\n│   • Quickly reacts to the last few bars (momentum, spikes).            │\\n│   • Also reset to 0 overnight.                                         │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌─────────────────────────────────────────────────────────────────────────┐\\n│              ❹  CURRENT INPUT WINDOW  xₜ  (60 latest bars)            │\\n│   • Raw OHLCV features for minutes t-60 … t-1.                         │\\n│   • Injected into the gates together with hₜ to produce the output.    │\\n└─────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                       Predicted signal  ŷₜ\\n\\n\\n\\n\\nSource\\t                                                Typical influence on ŷₜ\\n\\nNetwork weights (“what patterns matter in general?”)\\t≈ 50 %\\nCell state cₜ (earlier bars this day)\\t                  20 – 30 %\\nHidden state hₜ (very recent bars)\\t                      10 – 20 %\\nCurrent 60-bar raw input\\t                              10 – 20 %\\n\\n\\n\\nDay i\\n ───────────────────────────────────────────────────────────────────────\\n t=0        t=200          t=390 (close)\\n │──────────│──────────────│\\n cₜ:   0 → growing memory → carries full-day context ─┐\\n hₜ:   0 → wobbles quickly → tracks local moves      │\\n                                                     │\\n model.reset_states()  ◄─────────────────────────────┘\\n Day i+1   (both c and h back to zero)\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                ❶  NETWORK WEIGHTS   (global knowledge)                │\n",
    "│   • Millions of parameters learned across *all* historic days.        │\n",
    "│   • Do **not** change during inference.                               │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              ❷  CELL STATE  cₜ   (“within-day long-term”)             │\n",
    "│   • Slow-changing store; integrator.                                   │\n",
    "│   • Carries patterns from early-morning bars to late-afternoon bars.   │\n",
    "│   • Reset to 0 at day boundary.                                        │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              ❸  HIDDEN STATE  hₜ  (“within-day short-term”)           │\n",
    "│   • Quickly reacts to the last few bars (momentum, spikes).            │\n",
    "│   • Also reset to 0 overnight.                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              ❹  CURRENT INPUT WINDOW  xₜ  (60 latest bars)            │\n",
    "│   • Raw OHLCV features for minutes t-60 … t-1.                         │\n",
    "│   • Injected into the gates together with hₜ to produce the output.    │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                       Predicted signal  ŷₜ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Source\t                                                Typical influence on ŷₜ\n",
    "\n",
    "Network weights (“what patterns matter in general?”)\t≈ 50 %\n",
    "Cell state cₜ (earlier bars this day)\t                  20 – 30 %\n",
    "Hidden state hₜ (very recent bars)\t                      10 – 20 %\n",
    "Current 60-bar raw input\t                              10 – 20 %\n",
    "\n",
    "\n",
    "\n",
    "Day i\n",
    " ───────────────────────────────────────────────────────────────────────\n",
    " t=0        t=200          t=390 (close)\n",
    " │──────────│──────────────│\n",
    " cₜ:   0 → growing memory → carries full-day context ─┐\n",
    " hₜ:   0 → wobbles quickly → tracks local moves      │\n",
    "                                                     │\n",
    " model.reset_states()  ◄─────────────────────────────┘\n",
    " Day i+1   (both c and h back to zero)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2678537d-791e-45a2-8d6c-d94d374887b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4. build_stateful_lstm                                                      #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "#   Create **one** compact, stateful, seq-to-seq LSTM that:                   #\n",
    "#       • receives  (B , T , n_feats)   where n_feats = look_back × F         #\n",
    "#       • emits     (B , T , 1)          → one prediction *per minute*        #\n",
    "#       • keeps its hidden state (h, c) through the entire RTH session        #\n",
    "#       • is compiled and ready for `train_on_batch` / `model.fit`            #\n",
    "#                                                                             #\n",
    "# MEMORY-RETENTION TWEAKS (explained in comments)                             #\n",
    "# ──────────────────────────────────────────────────────────────────────────── #\n",
    "#   1) `return_sequences=True`               – emit a vector each minute.     #\n",
    "#   2) `unit_forget_bias=True` (default)     – sets the forget-gate bias ≈ +1 #\n",
    "#      which empirically lets cₜ remember 2-3× longer.                        #\n",
    "#   3) `recurrent_dropout=0.05`              – low value so gates aren’t      #\n",
    "#      zeroed too often → longer usable horizon.                              #\n",
    "#   4) Optional: bump `units` ≥128 for even longer recall if GPU allows.      #\n",
    "#                                                                             #\n",
    "# NEW – PERFORMANCE-ONLY KNOBS *WITHOUT CHANGING LOGIC*                       #\n",
    "# ──────────────────────────────────────────────────────────────────────────── #\n",
    "#   • `train_batch` ─ how many **independent** day-sessions to pack in one    #\n",
    "#       GPU launch.  Inference must stay at 1; training can be 8-128.         #\n",
    "#   • `use_mixed_precision` – fp16 kernels on GPUs (identical outputs).       #\n",
    "#   • `use_xla` – enable TF-XLA JIT fusion (identical outputs).               #\n",
    "###############################################################################\n",
    "\n",
    "def build_stateful_lstm(\n",
    "        look_back: int,\n",
    "        n_feats:   int,\n",
    "        *,\n",
    "        # original hyper-params (unchanged defaults)\n",
    "        batch_size:        int,      # 1  for inference;  ≥8 for training\n",
    "        units:             int,\n",
    "        dropout:           float,\n",
    "        recurrent_dropout: float,\n",
    "        initial_lr:        float,\n",
    "        loss:              str,\n",
    "        # new purely-performance flags\n",
    "        use_mixed_precision: bool,\n",
    "        use_xla:             bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.Model\n",
    "        A compiled, stateful LSTM that outputs a scalar per minute.\n",
    "    ------------------------------------------------------------------\n",
    "    Input  : (batch=batch_size , T , n_feats)\n",
    "    Output : (batch=batch_size , T , 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # ─────────────────── optional mixed precision ─────────────────── #\n",
    "    if use_mixed_precision:\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        # Turn it off in a different process by resetting to \"float32\".\n",
    "\n",
    "    # ───────────────────────── NETWORK CORE ───────────────────────── #\n",
    "    model = models.Sequential([\n",
    "\n",
    "        # Fixed batch size for stateful=True; variable sequence length.\n",
    "        layers.Input(batch_shape=(batch_size, None, n_feats)),\n",
    "\n",
    "        layers.LSTM(\n",
    "            units             = units,\n",
    "            stateful          = True,        # keep (h, c) across minutes\n",
    "            return_sequences  = True,        # one vector EACH minute\n",
    "            dropout           = dropout,\n",
    "            recurrent_dropout = recurrent_dropout,\n",
    "            # `unit_forget_bias=True` is already the default\n",
    "        ),\n",
    "\n",
    "        # Map every time-step’s hidden vector → one scalar prediction\n",
    "        layers.TimeDistributed(layers.Dense(1, activation=\"linear\"))\n",
    "    ])\n",
    "\n",
    "    # ─────────────────────────── COMPILE ──────────────────────────── #\n",
    "    model.compile(\n",
    "        optimizer   = optimizers.Adam(learning_rate=initial_lr),\n",
    "        loss        = loss,\n",
    "        metrics     = [metrics.RootMeanSquaredError(name=\"rmse\")],\n",
    "        jit_compile = use_xla      # XLA JIT for free speed-ups\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1. BUILD TWO COPIES OF THE SAME NETWORK                                     #\n",
    "#    • model_train   – fat batch, fp16, XLA → FAST GPU training               #\n",
    "#    • model_val   – batch-1, fp32        → identical to old logic          #\n",
    "###############################################################################\n",
    "\n",
    "# ---------- fast-training copy ----------------------------------------------\n",
    "model_train = build_stateful_lstm(\n",
    "    look_back          = look_back,\n",
    "    n_feats            = n_feats,\n",
    "    batch_size         = TRAIN_BATCH,       # ← bigger batch\n",
    "    units              = units,\n",
    "    dropout            = dropout,\n",
    "    recurrent_dropout  = recurrent_dropout,\n",
    "    initial_lr         = initial_lr,\n",
    "    loss               = loss,\n",
    "    use_mixed_precision= USE_FP16,          # speed only, no logic change\n",
    "    use_xla            = False               # XLA just-in-time fusion\n",
    ")\n",
    "\n",
    "# ---------- inference copy (old behaviour) ----------------------------------\n",
    "# We do train just one set of weights. We hold them in model_train during back-prop and copy them into model_val for evaluation\n",
    "model_val = build_stateful_lstm(\n",
    "    look_back          = look_back,\n",
    "    n_feats            = n_feats,\n",
    "    batch_size         = 1,                 # ← live stream is one lane\n",
    "    units              = units,\n",
    "    dropout            = dropout,\n",
    "    recurrent_dropout  = recurrent_dropout,\n",
    "    initial_lr         = initial_lr,        # lr irrelevant in inference\n",
    "    loss               = loss,\n",
    "    use_mixed_precision= False,\n",
    "    use_xla            = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot – final, backend-safe                                           #\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    Blue =train RMSE, Orange =val RMSE.  Works with\n",
    "        %matplotlib widget   (ipympl)  -> smooth in-place update\n",
    "        %matplotlib inline   -> clear-output redraw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        from IPython.display import display, clear_output, HTML\n",
    "        import matplotlib.pyplot as plt\n",
    "        self.display, self.clear, self.HTML = display, clear_output, HTML\n",
    "\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.tr_line, = self.ax.plot([], [], c=\"#1f77b4\", label=\"train\")\n",
    "        self.va_line, = self.ax.plot([], [], c=\"#ff7f0e\", label=\"val\")\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True); self.ax.legend()\n",
    "        self.e, self.tr, self.va = [], [], []\n",
    "\n",
    "        display(self.fig)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # KEY PATCH: use matplotlib.get_backend() instead of manager method\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    def _redraw(self):\n",
    "        import matplotlib\n",
    "        backend = matplotlib.get_backend().lower()\n",
    "\n",
    "        if \"widget\" in backend or \"ipympl\" in backend:     # live canvas\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:                                              # png/inline\n",
    "            self.clear(wait=True)\n",
    "            self.display(self.fig)\n",
    "            self.display(self.HTML(\"\"))                    # forces repaint\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        import numpy as np\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "\n",
    "        # training curve\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        # validation curve (skip NaNs / infs)\n",
    "        finite = np.isfinite(self.va)\n",
    "        if finite.any():\n",
    "            self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                                  np.asarray(self.va)[finite])\n",
    "\n",
    "        self.ax.relim(); self.ax.autoscale_view()\n",
    "        self._redraw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                               custom_stateful_training_loop                #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Purpose                                                                     #\n",
    "# -------                                                                     #\n",
    "# Train a *stateful* LSTM with a **fat batch** (32 parallel day-streams) and  #\n",
    "# validate it with an identical **batch-1** copy so that:                     #\n",
    "#   • every training batch has the fixed shape (32, …) → GPU kernels happy    #\n",
    "#   • every calendar-day in the validation split is evaluated exactly once    #\n",
    "#   • no day is ever dropped or padded incorrectly                            #\n",
    "#                                                                             #\n",
    "# Live feedback:                                                              #\n",
    "#   • tqdm progress bar counts *days* processed                               #\n",
    "#   • LiveRMSEPlot shows blue (train) & orange (val) curves from epoch 1      #\n",
    "#                                                                             #\n",
    "# Early stopping & LR schedule identical to Keras callbacks but implemented   #\n",
    "# manually so we keep full control.                                           #\n",
    "###############################################################################\n",
    "\n",
    "def custom_stateful_training_loop(\n",
    "        model_train, model_val,\n",
    "        ds_train, ds_val,\n",
    "        *, max_epochs,\n",
    "        early_stop_patience, lr_reduce_patience, min_lr,\n",
    "        ckpt_path):\n",
    "\n",
    "    # ─── 0. once-per-run initialisation ────────────────────────────────────\n",
    "    plotter = LiveRMSEPlot()\n",
    "    lstm_train_layers = [l for l in model_train.layers if hasattr(l, \"reset_states\")]\n",
    "    lstm_val_layers   = [l for l in model_val.layers   if hasattr(l, \"reset_states\")]\n",
    "\n",
    "    train_batches = tf.data.experimental.cardinality(ds_train).numpy()\n",
    "    total_days    = None if train_batches <= 0 else train_batches * TRAIN_BATCH\n",
    "\n",
    "    best_val, pat_no_imp, pat_lr = np.inf, 0, 0\n",
    "\n",
    "    # ─── 1. epoch loop ─────────────────────────────────────────────────────\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "        # ── 1-A. TRAIN ────────────────────────────────────────────────────\n",
    "        batch_rmse, pbar = [], tqdm(total=total_days, unit=\"day\",\n",
    "                                    desc=f\"Epoch {epoch}\", leave=True, dynamic_ncols = True)\n",
    "\n",
    "        for Xb, yb, _ in ds_train:\n",
    "            for l in lstm_train_layers: l.reset_states()\n",
    "            logs = model_train.train_on_batch(Xb, yb, return_dict=True)\n",
    "            batch_rmse.append(logs[\"rmse\"])\n",
    "            pbar.update(Xb.shape[0]); pbar.set_postfix(rmse=f\"{logs['rmse']:.4f}\")\n",
    "        pbar.close()\n",
    "        epoch_train = float(np.mean(batch_rmse))\n",
    "\n",
    "        # ── 1-B. VALIDATE (batch-1 model, dtype-safe) ────────────────────\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "        val_rmse = []\n",
    "        \n",
    "        for X_day, y_day, _ in ds_val:                 # X_day = (1, T, F)\n",
    "            for l in lstm_val_layers: l.reset_states()\n",
    "        \n",
    "            # fp16 forward pass → (1, T, 1)  →  (T,)  cast to float32\n",
    "            y_pred = model_val(X_day, training=False)\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "        \n",
    "            # y_day might be (T,1) or (1,T).  Flatten to 1-D safely.\n",
    "            y_true = tf.reshape(y_day, [-1])           # ★ replaces squeeze(axis=1)\n",
    "        \n",
    "            rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmse.append(float(rmse))\n",
    "\n",
    "        epoch_val = float(np.mean(val_rmse))\n",
    "\n",
    "        # ── 1-C. LOG + PLOT ──────────────────────────────────────────────\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f}\")\n",
    "        plotter.update(epoch_train, epoch_val)\n",
    "\n",
    "        # ── 1-D. EARLY STOP + LR SCHEDULE ────────────────────────────────\n",
    "        if epoch_val < best_val:                     # new champion\n",
    "            best_val, pat_no_imp, pat_lr = epoch_val, 0, 0\n",
    "            model_train.save_weights(ckpt_path)\n",
    "        else:\n",
    "            pat_no_imp += 1; pat_lr += 1\n",
    "            if pat_lr >= lr_reduce_patience:         # halve LR (floored)\n",
    "                lr = max(model_train.optimizer.learning_rate.numpy() * 0.5,\n",
    "                         min_lr)\n",
    "                model_train.optimizer.learning_rate.assign(lr)\n",
    "                pat_lr = 0\n",
    "                print(f\"    ↳ LR halved to {lr:.1e}\")\n",
    "        if pat_no_imp >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\"); break\n",
    "\n",
    "    # ─── 2. restore best weights and return best metric ───────────────────\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7131591dcc45939861f9669b29eec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1: 0day [00:00, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.632459 • val=0.419498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a7522126024f95aeee90a77c7e2e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2: 0day [00:00, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.712724 • val=0.731689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e98b44384e40138d6f0aa3d21eb384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3: 0day [00:00, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_rmse = custom_stateful_training_loop(\n",
    "    model_train=model_train,\n",
    "    model_val  =model_val,\n",
    "    ds_train   =ds_train_batched,\n",
    "    ds_val     =ds_val_unbatched,\n",
    "    max_epochs =max_epochs,\n",
    "    early_stop_patience=early_stop_patience,\n",
    "    lr_reduce_patience =lr_reduce_patience,\n",
    "    min_lr=min_lr,\n",
    "    ckpt_path=ckpt_path)\n",
    "\n",
    "model_val.save(save_dir / f\"model_{ticker}.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431ea52-a04f-4fff-bb69-73c1717dc459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
