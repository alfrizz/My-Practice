{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 18:08:08.837068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750608488.857398   58346 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750608488.864341   58346 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750608488.885127   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750608488.885165   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750608488.885168   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750608488.885170   58346 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import stockanalibs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:07:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:08:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:09:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:10:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:11:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:49:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>174.02</td>\n",
       "      <td>173.8900</td>\n",
       "      <td>173.99</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>173.9378</td>\n",
       "      <td>174.0422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:50:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>173.98</td>\n",
       "      <td>163.4137</td>\n",
       "      <td>173.92</td>\n",
       "      <td>524.0</td>\n",
       "      <td>173.8678</td>\n",
       "      <td>173.9722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:51:00</th>\n",
       "      <td>173.970</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8933</td>\n",
       "      <td>173.90</td>\n",
       "      <td>3898.0</td>\n",
       "      <td>173.8478</td>\n",
       "      <td>173.9522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:52:00</th>\n",
       "      <td>173.908</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8600</td>\n",
       "      <td>173.97</td>\n",
       "      <td>779.0</td>\n",
       "      <td>173.9178</td>\n",
       "      <td>174.0222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:53:00</th>\n",
       "      <td>173.960</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8911</td>\n",
       "      <td>174.00</td>\n",
       "      <td>939.0</td>\n",
       "      <td>173.9478</td>\n",
       "      <td>174.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296484 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open    high       low   close  volume       bid  \\\n",
       "2014-04-03 13:07:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:08:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:09:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:10:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:11:00  570.500  570.50  570.5000  570.50   100.0  570.3288   \n",
       "...                      ...     ...       ...     ...     ...       ...   \n",
       "2025-06-18 20:49:00  173.980  174.02  173.8900  173.99  1604.0  173.9378   \n",
       "2025-06-18 20:50:00  173.980  173.98  163.4137  173.92   524.0  173.8678   \n",
       "2025-06-18 20:51:00  173.970  174.00  173.8933  173.90  3898.0  173.8478   \n",
       "2025-06-18 20:52:00  173.908  174.00  173.8600  173.97   779.0  173.9178   \n",
       "2025-06-18 20:53:00  173.960  174.00  173.8911  174.00   939.0  173.9478   \n",
       "\n",
       "                          ask  trade_action  StrategyEarning  EarningDiff  \\\n",
       "2014-04-03 13:07:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:08:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:09:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:10:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:11:00  570.6712             0              0.0        0.000   \n",
       "...                       ...           ...              ...          ...   \n",
       "2025-06-18 20:49:00  174.0422             0              0.0        1.395   \n",
       "2025-06-18 20:50:00  173.9722             0              0.0        1.465   \n",
       "2025-06-18 20:51:00  173.9522             0              0.0        1.485   \n",
       "2025-06-18 20:52:00  174.0222             0              0.0        1.415   \n",
       "2025-06-18 20:53:00  174.0522             0              0.0        1.385   \n",
       "\n",
       "                     signal_smooth_adjusted  \n",
       "2014-04-03 13:07:00                0.071363  \n",
       "2014-04-03 13:08:00                0.078887  \n",
       "2014-04-03 13:09:00                0.087231  \n",
       "2014-04-03 13:10:00                0.096444  \n",
       "2014-04-03 13:11:00                0.106627  \n",
       "...                                     ...  \n",
       "2025-06-18 20:49:00                0.000000  \n",
       "2025-06-18 20:50:00                0.000000  \n",
       "2025-06-18 20:51:00                0.000000  \n",
       "2025-06-18 20:52:00                0.000000  \n",
       "2025-06-18 20:53:00                0.000000  \n",
       "\n",
       "[1296484 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibsfeature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.25     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.20     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-4     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 3e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.05     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 64       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a eg 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1. per-day standard-scaling\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols])\n",
    "        day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col].to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1043134, 450)\n",
      "(1043134,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 1920  (multiple of 64)\n",
      "Validation days    : 422\n",
      "Test days          : 423\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (TRAIN_BATCH , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750608522.179188   58346 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=TRAIN_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,              # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,   # Total calendar days in the training epoch\n",
    "    max_epochs: int,     # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    ckpt_path           # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(ckpt_path)  # Save checkpoint of the best model.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.907639\n",
      "Training sees 1920 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d622bf387e6e4e3fb7f0a1fdb439dfd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750608545.197639   58421 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=1.176972 • val=0.825945 • impr=  9.0% • lr=5.67e-05 • g≈20744.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3377b531541450791de1f80e413ceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=1.052638 • val=0.784371 • impr= 13.6% • lr=1.03e-04 • g≈10223.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ea654acab4f05b50dec87e57b4a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=1.007424 • val=0.770674 • impr= 15.1% • lr=2.76e-04 • g≈3650.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb2dfd753fc436fa9db4d03f2b0267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.963808 • val=0.758859 • impr= 16.4% • lr=1.30e-04 • g≈7431.07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4988d3ab064b53a63bdf0f99588e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.940563 • val=0.756372 • impr= 16.7% • lr=1.77e-05 • g≈53025.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843512c0a9be4c4aae36f8776774079a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.925790 • val=0.754644 • impr= 16.9% • lr=2.83e-04 • g≈3269.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829f230634b9475299e4e3f6432edc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.897981 • val=0.742247 • impr= 18.2% • lr=2.25e-04 • g≈3996.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1862e17b33b4023bb672b475ff16aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 • train=0.877592 • val=0.741674 • impr= 18.3% • lr=1.44e-04 • g≈6114.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6334313824c568944631afc05552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 • train=0.866898 • val=0.741178 • impr= 18.3% • lr=6.71e-05 • g≈12919.69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1837811e20724a6cb54f85d54fc4f566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 • train=0.863116 • val=0.738399 • impr= 18.6% • lr=2.11e-05 • g≈40836.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f01ce70e3a426fbf0349e58c907c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 • train=0.858693 • val=0.743241 • impr= 18.1% • lr=2.98e-04 • g≈2877.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcbf3287de747918c3c1f53033098df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 • train=0.852004 • val=0.746422 • impr= 17.8% • lr=2.86e-04 • g≈2975.72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a9b289d8bc4ac896616f1bb81ffbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 • train=0.838488 • val=0.731849 • impr= 19.4% • lr=2.63e-04 • g≈3187.13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe46eedce884834bf72792fdbf9aa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 • train=0.826016 • val=0.724887 • impr= 20.1% • lr=2.31e-04 • g≈3579.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd768f1b86547dcad516f5bcf4e2e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 • train=0.816842 • val=0.723035 • impr= 20.3% • lr=1.92e-04 • g≈4251.63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b9345f861e42f99f877961b47d5b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 • train=0.811036 • val=0.722178 • impr= 20.4% • lr=1.51e-04 • g≈5388.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c8f1ee41544f65adf899eb91cab292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 • train=0.805597 • val=0.721502 • impr= 20.5% • lr=1.09e-04 • g≈7357.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abf30b0121d4188bce60ede3d7f31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 • train=0.801333 • val=0.722327 • impr= 20.4% • lr=7.26e-05 • g≈11035.69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be6122687e74db8bcaaef2d16147166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 • train=0.799972 • val=0.722249 • impr= 20.4% • lr=4.30e-05 • g≈18585.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730f47dcaeab4f1f8d43e27876c14a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 • train=0.799058 • val=0.720142 • impr= 20.7% • lr=2.33e-05 • g≈34250.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1921fa6ae48e49c997d958d958c54950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 • train=0.797729 • val=0.719837 • impr= 20.7% • lr=1.52e-05 • g≈52580.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636cf8d82a3d4082bd9a1577c17aac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 • train=0.798396 • val=0.725563 • impr= 20.1% • lr=2.99e-04 • g≈2670.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4019b2d7c7641288239553f6848d9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 • train=0.793793 • val=0.723351 • impr= 20.3% • lr=2.95e-04 • g≈2692.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53061262967241d890aabbcf1aaea0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 • train=0.789625 • val=0.726690 • impr= 19.9% • lr=2.88e-04 • g≈2743.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0aa5f7cf4e49409470512cc874d587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 • train=0.784571 • val=0.732374 • impr= 19.3% • lr=2.78e-04 • g≈2823.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea422cf7eb6b42858bd07a1be5d58aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 • train=0.779494 • val=0.720042 • impr= 20.7% • lr=2.65e-04 • g≈2937.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880c56d981f841a5a0ef4d27141f2f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 • train=0.774835 • val=0.708650 • impr= 21.9% • lr=2.51e-04 • g≈3092.20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f73cbf0502e4a0987b52d0e55289977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 • train=0.770460 • val=0.705814 • impr= 22.2% • lr=2.34e-04 • g≈3296.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3729188b0924b0290f5a9b1f8364947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 • train=0.765740 • val=0.704442 • impr= 22.4% • lr=2.15e-04 • g≈3557.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0c401e89c44a6e91f795ea6ef82af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 • train=0.762634 • val=0.701734 • impr= 22.7% • lr=1.96e-04 • g≈3900.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4beabe4600ee4cebb604c28e64993827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 • train=0.759706 • val=0.699960 • impr= 22.9% • lr=1.75e-04 • g≈4342.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ed309c9c3949589f8793519ce63117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 • train=0.756282 • val=0.697611 • impr= 23.1% • lr=1.54e-04 • g≈4910.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa4dc9e16154a8aba5dd7d5008d015c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 • train=0.753899 • val=0.699215 • impr= 23.0% • lr=1.33e-04 • g≈5662.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691b936522ee48389307e0aafb0c9205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 • train=0.752000 • val=0.698449 • impr= 23.0% • lr=1.13e-04 • g≈6666.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0479bd294e174fd397cde697afb3cee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 • train=0.752312 • val=0.698794 • impr= 23.0% • lr=9.34e-05 • g≈8052.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd516114851a4cf98b1208a87d9db430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 036:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 • train=0.749023 • val=0.698472 • impr= 23.0% • lr=7.54e-05 • g≈9927.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f61ce0bf1c048e3adc68684b78fd999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 037:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 • train=0.748065 • val=0.699762 • impr= 22.9% • lr=5.92e-05 • g≈12627.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d755265b664c4c62a095c0f5e52c8652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 038:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 • train=0.748285 • val=0.697798 • impr= 23.1% • lr=4.52e-05 • g≈16569.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed68be911b840c182813bf25f7be752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 039:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 • train=0.746164 • val=0.696252 • impr= 23.3% • lr=3.35e-05 • g≈22265.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4ac60846ee4dbfaec5cc9f1fe3a08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 040:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 • train=0.746049 • val=0.696530 • impr= 23.3% • lr=2.45e-05 • g≈30390.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51f53946b534b0bb72744e7cd1fd505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 041:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 • train=0.744640 • val=0.698057 • impr= 23.1% • lr=1.85e-05 • g≈40332.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f817234e1874b9aadca9aedccd254ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 042:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 • train=0.745280 • val=0.698442 • impr= 23.0% • lr=1.54e-05 • g≈48438.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f806b9e3746ef8643a6a7d92d9ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 043:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 • train=0.747863 • val=0.706518 • impr= 22.2% • lr=3.00e-04 • g≈2493.68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a10738cc9e4abbaae00fc03ae1799b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 044:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 • train=0.750903 • val=0.696310 • impr= 23.3% • lr=2.99e-04 • g≈2510.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b93aa1602b40b58b0dafc836dd6590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 045:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 • train=0.748400 • val=0.706275 • impr= 22.2% • lr=2.98e-04 • g≈2514.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c66cd981da4780b9e7894c19bfc67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 046:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 • train=0.744322 • val=0.696472 • impr= 23.3% • lr=2.95e-04 • g≈2520.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cb652abd0843919c4eb1fe36bef15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 047:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 • train=0.743951 • val=0.700465 • impr= 22.8% • lr=2.92e-04 • g≈2545.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a597e9947dc4a18930ca5bd1f07ce82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 048:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 • train=0.742668 • val=0.691601 • impr= 23.8% • lr=2.88e-04 • g≈2574.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3dd8e990794dffa2fd89c528a407fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 049:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 • train=0.741241 • val=0.699361 • impr= 22.9% • lr=2.84e-04 • g≈2610.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785d13c25ce44f4b9c7596a3bde70689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 050:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 • train=0.737469 • val=0.700323 • impr= 22.8% • lr=2.79e-04 • g≈2644.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a7bc9d58c48e9ab25a7d32042c77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 051:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 • train=0.735062 • val=0.694542 • impr= 23.5% • lr=2.73e-04 • g≈2692.63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dda3ead0d6b4134a7ad82803357ae03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 052:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 052 • train=0.732993 • val=0.696197 • impr= 23.3% • lr=2.67e-04 • g≈2750.07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ef8c6c883046bfa91b5af92ed337e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 053:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053 • train=0.732167 • val=0.691939 • impr= 23.8% • lr=2.59e-04 • g≈2821.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e364c905e146989c8b4a57d62859ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 054:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 054 • train=0.730704 • val=0.696233 • impr= 23.3% • lr=2.52e-04 • g≈2900.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293d78bfb9f949b491f240edc0347dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 055:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 055 • train=0.727826 • val=0.692215 • impr= 23.7% • lr=2.44e-04 • g≈2985.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e14401ce9d4ef3ab1f7aaab42b3955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 056:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 056 • train=0.727761 • val=0.693953 • impr= 23.5% • lr=2.35e-04 • g≈3094.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0dbcf91ad142c39729d8ad0566431f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 057:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 057 • train=0.727237 • val=0.693754 • impr= 23.6% • lr=2.26e-04 • g≈3214.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69219bf7be0a41c6a9c0ee845f9ed36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 058:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 058 • train=0.726757 • val=0.695250 • impr= 23.4% • lr=2.17e-04 • g≈3351.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc418b15e9b14c4e8dbf9a78bb8d35b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 059:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 059 • train=0.724504 • val=0.691207 • impr= 23.8% • lr=2.07e-04 • g≈3497.49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce26e8122c1d4e4dacbc14e0c46bd660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 060:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 060 • train=0.721925 • val=0.690837 • impr= 23.9% • lr=1.97e-04 • g≈3661.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777823faabe543b193aa420ab2d16d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 061:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 061 • train=0.721227 • val=0.690806 • impr= 23.9% • lr=1.87e-04 • g≈3856.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a5ba75dc8042029faef48a657834db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 062:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 062 • train=0.721878 • val=0.699876 • impr= 22.9% • lr=1.77e-04 • g≈4085.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b662b61d4e546bb9900da5ad36164a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 063:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 063 • train=0.723566 • val=0.693006 • impr= 23.6% • lr=1.66e-04 • g≈4352.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc81aad381024b10982c96cd08b13fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 064:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 • train=0.720095 • val=0.689959 • impr= 24.0% • lr=1.56e-04 • g≈4623.37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34fa02b199d454595dcc0082640feec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 065:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 065 • train=0.719481 • val=0.688077 • impr= 24.2% • lr=1.45e-04 • g≈4952.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c21e92a41a94439aa0b8a92714f6a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 066:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 066 • train=0.718354 • val=0.690733 • impr= 23.9% • lr=1.35e-04 • g≈5326.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4079210bc746a682dcb41abb67da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 067:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 067 • train=0.719027 • val=0.696161 • impr= 23.3% • lr=1.25e-04 • g≈5771.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd0e6b17aa049e4abbb67cf73a697ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 068:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068 • train=0.717609 • val=0.691840 • impr= 23.8% • lr=1.14e-04 • g≈6269.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e82d0bc60bd4c52b3c15fd9daf76697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 069:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 069 • train=0.717092 • val=0.691090 • impr= 23.9% • lr=1.05e-04 • g≈6856.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dee2ec8e5b44eefa658ff6b6636792b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 070:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 070 • train=0.715707 • val=0.690386 • impr= 23.9% • lr=9.50e-05 • g≈7533.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06178d98f15f4978afcbf58fc726776b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 071:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 071 • train=0.715688 • val=0.690654 • impr= 23.9% • lr=8.57e-05 • g≈8346.63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7b915972674260a0fae2bfcfb990b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 072:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 072 • train=0.714905 • val=0.691478 • impr= 23.8% • lr=7.69e-05 • g≈9298.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362c0b231c4448b1a791bfe616eec2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 073:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 073 • train=0.715197 • val=0.691550 • impr= 23.8% • lr=6.85e-05 • g≈10447.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd244989491447b1a258a9bb2d3e1171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 074:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 074 • train=0.714464 • val=0.692018 • impr= 23.8% • lr=6.05e-05 • g≈11806.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a772612142645d38d9b8ed6f54ab0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 075:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 075 • train=0.713795 • val=0.692357 • impr= 23.7% • lr=5.31e-05 • g≈13443.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d1848942fe428b912e7afe913b62db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 076:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 076 • train=0.714170 • val=0.693300 • impr= 23.6% • lr=4.62e-05 • g≈15443.20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991685af29464aec9b824df064f23c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 077:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 077 • train=0.714388 • val=0.693133 • impr= 23.6% • lr=4.00e-05 • g≈17861.46\n",
      "Early stopping triggered.\n",
      "\n",
      "Champion validation RMSE = 0.688077\n",
      "Improvement vs baseline   =  24.2 %\n",
      "\n",
      "Model & weights saved to dfs training/GOOGL_2025-06-22_model.keras\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "file_path = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
