{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 18:08:26.184370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751040506.263258    3534 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751040506.288071    3534 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751040506.382767    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751040506.382899    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751040506.382905    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751040506.382907    3534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'stockanalibs' from '/mnt/g/My Drive/Ingegneria/Data Science GD/My-Practice/my models/Trading/0.Stock Analysis/stockanalibs.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ‚Üê for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm             # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import importlib\n",
    "import stockanalibs\n",
    "importlib.reload(stockanalibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:33:00</th>\n",
       "      <td>28.5959</td>\n",
       "      <td>28.5959</td>\n",
       "      <td>28.5932</td>\n",
       "      <td>28.5932</td>\n",
       "      <td>5540.0</td>\n",
       "      <td>28.5846</td>\n",
       "      <td>28.6018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:34:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.760984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:35:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.770777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:36:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.779774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:37:00</th>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>28.5809</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>28.5724</td>\n",
       "      <td>28.5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.788084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.3750</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.2150</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.5129</td>\n",
       "      <td>173.6171</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.049</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.5650</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.2400</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.3280</td>\n",
       "      <td>173.4320</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.3900</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.2000</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.2580</td>\n",
       "      <td>173.3620</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.304</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.3150</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.2300</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.2280</td>\n",
       "      <td>173.3320</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.334</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.3000</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.1700</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.5576</td>\n",
       "      <td>173.6618</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>2.004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1235302 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close     volume  \\\n",
       "2014-04-03 13:33:00   28.5959   28.5959   28.5932   28.5932     5540.0   \n",
       "2014-04-03 13:34:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:35:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:36:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "2014-04-03 13:37:00   28.5809   28.5809   28.5809   28.5809     5084.0   \n",
       "...                       ...       ...       ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.3750  173.6771  173.2150  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.5650  173.5900  173.2400  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.3900  173.4100  173.2000  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.3150  173.4000  173.2300  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.3000  174.0500  173.1700  173.6097  7649838.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:33:00   28.5846   28.6018             0            0.000   \n",
       "2014-04-03 13:34:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:35:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:36:00   28.5724   28.5895             0            0.000   \n",
       "2014-04-03 13:37:00   28.5724   28.5895             0            0.000   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.5129  173.6171             0           -0.661   \n",
       "2025-06-18 20:57:00  173.3280  173.4320             0           -0.661   \n",
       "2025-06-18 20:58:00  173.2580  173.3620             0           -0.661   \n",
       "2025-06-18 20:59:00  173.2280  173.3320             0           -0.661   \n",
       "2025-06-18 21:00:00  173.5576  173.6618             0           -0.661   \n",
       "\n",
       "                     EarningDiff  signal_smooth  \n",
       "2014-04-03 13:33:00        0.000       0.750262  \n",
       "2014-04-03 13:34:00        0.000       0.760984  \n",
       "2014-04-03 13:35:00        0.000       0.770777  \n",
       "2014-04-03 13:36:00        0.000       0.779774  \n",
       "2014-04-03 13:37:00        0.000       0.788084  \n",
       "...                          ...            ...  \n",
       "2025-06-18 20:56:00        2.049       0.000000  \n",
       "2025-06-18 20:57:00        2.234       0.000000  \n",
       "2025-06-18 20:58:00        2.304       0.000000  \n",
       "2025-06-18 20:59:00        2.334       0.000000  \n",
       "2025-06-18 21:00:00        2.004       0.000000  \n",
       "\n",
       "[1235302 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_final.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ¬∑  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "weights_path   = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‚Äêticker\n",
    "model_path     = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # ‚Üí 0.15 test remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 ¬∑ MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Architecture Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SHORT_UNITS         = 48       # LSTM short-term units (32‚Äì128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64‚Äì256 recommended)\n",
    "DROPOUT_SHORT       = 0.30     # Dropout for short LSTM outputs (0.1‚Äì0.3)\n",
    "DROPOUT_LONG        = 0.25     # Dropout for long LSTM outputs (0.1‚Äì0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-3     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ‚îÄ‚îÄ Optimizer Settings: Cosine Decay with Restarts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "INITIAL_LR          = 5e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.03     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5‚Äì5.0)\n",
    "\n",
    "# ‚îÄ‚îÄ Training Control Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "TRAIN_BATCH         = 32       # Training batch size (32‚Äì128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 100      # Max training epochs (50‚Äì150)\n",
    "EARLY_STOP_PATIENCE = 10       # Early stopping patience (10‚Äì20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts one big minute-bar DataFrame (many days) into NumPy arrays ready for the stateful LSTM.\n",
    "    \n",
    "    RULES ENFORCED:\n",
    "      ‚Ä¢ Windows never cross midnight.\n",
    "      ‚Ä¢ Features and labels are standardized per day (to avoid leakage).\n",
    "    \n",
    "    Returns:\n",
    "      X         : Design matrix; every row is a sliding window (flattened).\n",
    "      y         : One-step-ahead targets corresponding to each window.\n",
    "      raw_close : Raw (unstandardized) close prices for each target.\n",
    "      raw_bid   : Raw bid prices.\n",
    "      raw_ask   : Raw ask prices.\n",
    "    \"\"\"\n",
    "    X_rows, y_rows, close_rows, bid_rows, ask_rows = [], [], [], [], []\n",
    "    \n",
    "    # Process one calendar day at a time.\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "        day_df = day_df.sort_index()\n",
    "        \n",
    "        # Extract raw price columns before scaling.\n",
    "        raw_close = day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        raw_bid   = day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        raw_ask   = day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Standardize features and target per day.\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols]) ####################################################################\n",
    "        # day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]]) ###################################################################\n",
    "        \n",
    "        feats_np = day_df[feature_cols].to_numpy(dtype=np.float32)  # shape: (T, n_feats)\n",
    "        label_np = day_df[label_col].to_numpy(dtype=np.float32)       # shape: (T,)\n",
    "        \n",
    "        # Create mask for Regular Trading Hours (RTH).\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():\n",
    "            continue\n",
    "        \n",
    "        T, _ = feats_np.shape\n",
    "        \n",
    "        # Build sliding windows using vectorized approach.\n",
    "        win_3d = sliding_window_view(feats_np, (look_back, feats_np.shape[1]), axis=(0, 1))\n",
    "        # After the sliding window operation, we get an array whose shape is roughly (T - look_back + 1, 1, look_back, n_feats).\n",
    "        win_3d = win_3d[:, 0, :, :]  # removes the extra dimension: (T - look_back + 1, look_back, n_feats)\n",
    "        \n",
    "        # Alignment fix: drop the last window so that targets align.\n",
    "        win_3d = win_3d[:-1] # drops the very last window. This makes the number of windows exactly equal to the number of available targets.\n",
    "        y_aligned = label_np[look_back:]              # (T - look_back,)\n",
    "        close_aligned = raw_close[look_back:]    # (T - look_back,)\n",
    "        bid_aligned   = raw_bid[look_back:]      # (T - look_back,)\n",
    "        ask_aligned   = raw_ask[look_back:]      # (T - look_back,)\n",
    "        \n",
    "        # Trim by RTH (apply mask to the target indices).\n",
    "        rth_mask_shifted = rth_mask[look_back:]\n",
    "        win_3d       = win_3d[rth_mask_shifted]\n",
    "        y_aligned    = y_aligned[rth_mask_shifted]\n",
    "        close_aligned = close_aligned[rth_mask_shifted]\n",
    "        bid_aligned   = bid_aligned[rth_mask_shifted]\n",
    "        ask_aligned   = ask_aligned[rth_mask_shifted]\n",
    "        \n",
    "        # Flatten each window\n",
    "        X_rows.append(win_3d.reshape(win_3d.shape[0], -1))\n",
    "        y_rows.append(y_aligned)\n",
    "        close_rows.append(close_aligned)\n",
    "        bid_rows.append(bid_aligned)\n",
    "        ask_rows.append(ask_aligned)\n",
    "    \n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No valid RTH windows found; check rth_start or data gaps.\")\n",
    "    \n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "    raw_close = np.concatenate(close_rows).astype(np.float32)\n",
    "    raw_bid   = np.concatenate(bid_rows).astype(np.float32)\n",
    "    raw_ask   = np.concatenate(ask_rows).astype(np.float32)\n",
    "    \n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1066222, 300)\n",
      "(1066222,)\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    raw_close: np.ndarray,\n",
    "    raw_bid: np.ndarray,\n",
    "    raw_ask: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    "    TRAIN_BATCH: int  # added parameter for training batch size\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_train, y_train)\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_val, y_val)\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],    # (X_test, y_test, raw_close_test, raw_bid_test, raw_ask_test)\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id_tr, day_id_val, day_id_te\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits X, y (and the raw signals) into chronological train/val/test partitions by whole days.\n",
    "    The code creates a day-based vector (day_id_vec) and then applies masks to split all windows into training, validation, and test groups. \n",
    "    Each group is consecutive in terms of days, so no day is dropped or skipped‚Äîthe splits follow in order. \n",
    "    This ensures that if we later recombine the splits, they cover all days from 0 to D‚Äì1.\n",
    "\n",
    "            [ Training ]         [ Validation ]            [ Test ]\n",
    "    Days: 0   ...   cut_train | cut_train+1 ... cut_val | cut_val+1 ... D-1\n",
    "    \n",
    "    It uses the following logic:\n",
    "      1. Count the number of usable windows per calendar day.\n",
    "      2. Create a day_id vector that tags each sample with its day.\n",
    "      3. Compute the total number of calendar days, D.\n",
    "      4. Compute the \"original\" intended training set size as D * train_prop.\n",
    "      5. Round this count up (in day units) to the next multiple of TRAIN_BATCH.\n",
    "         (This ensures that the training set always contains full training batches.)\n",
    "      6. The validation split starts immediately after training ends. \n",
    "      7. The test split then follows, with no data dropped.\n",
    "    \n",
    "    Returns:\n",
    "      ((X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "       samples_per_day, day_id_tr, day_id_val, day_id_te)\n",
    "       \n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each calendar day.\n",
    "        +-----------------------------------------------------------+\n",
    "        |  Total minute rows (T)                                    |\n",
    "        |                                                           |\n",
    "        |   0, 1, 2, ... , look_back-1  | look_back, ..., T-1         |\n",
    "        |      (Not ready)            |  (Potential windows)         |\n",
    "        |                              |   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               |\n",
    "        |                              |   ‚îÇ rth_start‚îÇ <-- Only count rows with timestamp >= rth_start\n",
    "        |                              |   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               |\n",
    "        |                              |    Usable windows = count of rows satisfying both conditions\n",
    "        +-----------------------------------------------------------+\n",
    "\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "    \"\"\"\n",
    "    # 1. Count usable windows per day (vectorized)\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                # minute rows today\n",
    "        idx = np.arange(T)\n",
    "        mask_window_ready = idx >= look_back\n",
    "        mask_rth_target   = day_df.index.time >= rth_start\n",
    "        usable_today = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "        samples_per_day.append(usable_today)\n",
    "    \n",
    "    # Verify that summed count equals len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "    \n",
    "    # 2. Build the day_id vector: each window gets the day index (0-based)\n",
    "    day_id_vec = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "    \n",
    "    # 3. Total number of days\n",
    "    D = len(samples_per_day)\n",
    "    \n",
    "    # 4. Compute the original intended training count (in days)\n",
    "    original_train_count = int(D * train_prop)\n",
    "    \n",
    "    # 5. Round up the training days to the next multiple of TRAIN_BATCH\n",
    "    # (We want the training portion to include full batches.)\n",
    "    new_train_count = int(np.ceil(original_train_count / TRAIN_BATCH) * TRAIN_BATCH)\n",
    "    # Make sure we don't exceed the total number of days.\n",
    "    new_train_count = min(new_train_count, D)\n",
    "    # In day_id_vec, days are 0-indexed, so the training cut is:\n",
    "    cut_train = new_train_count - 1\n",
    "    \n",
    "    # 6. Determine the validation cut-point using the original proportion.\n",
    "    # Validation ends at the day_index given by:\n",
    "    cut_val = int(D * (train_prop + val_prop))\n",
    "\n",
    "    # 7. Create masks for the splits.\n",
    "    mask_tr = day_id_vec <= cut_train\n",
    "    mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te = day_id_vec > cut_val\n",
    "    \n",
    "    # 8. Slice X, y, and the raw arrays.\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_val, y_val = X[mask_val], y[mask_val]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "    \n",
    "    raw_close_te = raw_close[mask_te]\n",
    "    raw_bid_te   = raw_bid[mask_te]\n",
    "    raw_ask_te   = raw_ask[mask_te]\n",
    "    \n",
    "    # Also slice the day_id vector.\n",
    "    day_id_tr = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te = day_id_vec[mask_te]\n",
    "    \n",
    "    return (X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "           samples_per_day, day_id_tr, day_id_val, day_id_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1984 distinct calendar days retained, organized into batches of 32 days each (no partial batches).\n",
      "Validation: 412 distinct calendar days available for model validation.\n",
      "Test: 422 distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    TRAIN_BATCH=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(np.unique(day_id_tr))} distinct calendar days retained, organized into batches of {TRAIN_BATCH} days each (no partial batches).\")\n",
    "print(f\"Validation: {len(np.unique(day_id_val))} distinct calendar days available for model validation.\")\n",
    "print(f\"Test: {len(np.unique(day_id_te))} distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_dataset(\n",
    "    X           : np.ndarray,    # (N, ‚Ä¶)\n",
    "    y           : np.ndarray,    # (N,)\n",
    "    day_id      : np.ndarray,    # (N,)\n",
    "    weekday_vec : np.ndarray,    # (N,)\n",
    "    raw_close   : np.ndarray = None,  # (N,) optional\n",
    "    raw_bid     : np.ndarray = None,  # (N,) optional\n",
    "    raw_ask     : np.ndarray = None   # (N,) optional\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset where each element corresponds to one calendar day.\n",
    "    \n",
    "    If raw price arrays are provided, yields a 6-tuple:\n",
    "      (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)\n",
    "    Otherwise, yields a 3-tuple:\n",
    "      (x_day, y_day, weekday)\n",
    "    \n",
    "    - x_day: (1, T, n_feats) float32\n",
    "    - y_day: (1, T)         float32\n",
    "    - weekday: ()           int32\n",
    "    \"\"\"\n",
    "    # Sort inputs in chronological order.\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "    if raw_close is not None and raw_bid is not None and raw_ask is not None:\n",
    "        raw_close, raw_bid, raw_ask = [a[idx] for a in (raw_close, raw_bid, raw_ask)]\n",
    "    \n",
    "    # Determine boundaries for each day.\n",
    "    # the code splits the dataset into daily blocks based on the day_id, \n",
    "    # then produces a TensorFlow dataset where each element represents one complete day‚Äôs data \n",
    "    # (with features, targets, and possibly raw prices) along with the corresponding weekday\n",
    "    change = np.where(np.diff(day_id) != 0)[0] + 1\n",
    "    day_slices = np.split(np.arange(len(day_id)), change)\n",
    "\n",
    "    # the generator function walks through the dataset day by day and ‚Äúpacks‚Äù each day‚Äôs data into one neat bundle. \n",
    "    # For each day it: Selects the day's data, Formats the data, Yields the bundle \n",
    "    # Using a generator is both memory efficient and flexible. It creates each day's data on the fly rather than building a huge list all at once.\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]      # (T, ‚Ä¶)\n",
    "            y_block = y[sl]      # (T,)\n",
    "            weekday = int(weekday_vec[sl[0]])\n",
    "            if raw_close is None:\n",
    "                # Yield the original 3-tuple.\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "            else:\n",
    "                # Extract raw price slices.\n",
    "                close_block = raw_close[sl]  # (T,)\n",
    "                bid_block   = raw_bid[sl]    # (T,)\n",
    "                ask_block   = raw_ask[sl]    # (T,)\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),    # (1, T, ‚Ä¶)\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),      # (1, T)\n",
    "                    np.expand_dims(close_block, 0).astype(np.float32),  # (1, T)\n",
    "                    np.expand_dims(bid_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.expand_dims(ask_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "    \n",
    "    feat_shape = X.shape[1:]\n",
    "    if raw_close is None:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    else:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_close_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_bid_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_ask_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(gen, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,                 # training arrays\n",
    "        X_val, y_val, day_id_val,              # validation arrays\n",
    "        X_te, y_te, day_id_te,                 # test arrays\n",
    "        raw_close_te, raw_bid_te, raw_ask_te,  # test raw price arrays\n",
    "        *,\n",
    "        df,                                    # original DataFrame (for weekday vector)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits training, validation, and test arrays into day-level tf.data.Datasets.\n",
    "    \n",
    "    For training and validation, raw signals are not saved (3-tuple).\n",
    "    For testing, the raw price arrays (raw_close, raw_bid, raw_ask) are provided, yielding a 6-tuple.\n",
    "    \n",
    "    Returns:\n",
    "      ds_train_batched, ds_val_unbatched, ds_test_unbatched\n",
    "    \"\"\"\n",
    "    # Build one weekday vector covering all rows from the original DataFrame.\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)\n",
    "    \n",
    "    # Determine split lengths.\n",
    "    n_tr = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te = len(X_te)\n",
    "    \n",
    "    # Create weekday vectors for each split.\n",
    "    # they will be used to identify the end of the week and reset the long-term state of the LSTM layers\n",
    "    weekday_vec_tr = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr:n_tr+n_val]\n",
    "    weekday_vec_te = weekday_all[n_tr+n_val:n_tr+n_val+n_te]\n",
    "    \n",
    "    # Build training and validation datasets (3-tuple).\n",
    "    ds_tr = make_day_dataset(X_tr, y_tr, day_id_tr, weekday_vec_tr)\n",
    "    ds_val = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    \n",
    "    # Build test dataset with raw price arrays (6-tuple).\n",
    "    ds_test = make_day_dataset(X_te, y_te, day_id_te, weekday_vec_te,\n",
    "                               raw_close=raw_close_te, raw_bid=raw_bid_te, raw_ask=raw_ask_te)\n",
    "    \n",
    "    # For training, strip the extra outer batch dimension.\n",
    "    # the _strip function only on the training dataset to remove that extra outer dimension \n",
    "    # In the validation or test datasets we don't need to strip this dimension because those pipelines expect one sample at a time \n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd\n",
    "\n",
    "    ds_train_batched = (ds_tr\n",
    "                         .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                         .padded_batch(train_batch, drop_remainder=True)\n",
    "                         .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    # Save the test dataset.\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "    \n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751040534.814365    3534 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    X_val, y_val, day_id_val,\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ù∂  NETWORK  WEIGHTS  Œ∏  ‚Äì learned across all history, fixed at runtime   ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ù∑  CELL STATE  c‚Çú  ‚Äì slow integrator covering the *whole* current day    ‚îÇ\\n‚îÇ    ‚Ä¢ retains early-morning context                                       ‚îÇ\\n‚îÇ    ‚Ä¢ reset_states()  at every midnight ‚Üí zero on next session            ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ù∏  HIDDEN STATE  h‚Çú  ‚Äì fast dynamics (a few bars)                        ‚îÇ\\n‚îÇ    ‚Ä¢ captures spikes / micro-structure                                   ‚îÇ\\n‚îÇ    ‚Ä¢ reset together with c‚Çú midnight                                     ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ùπ  INPUT WINDOW  x‚Çú  ‚Äì last 60 minutes of raw features                   ‚îÇ\\n‚îÇ    ‚Ä¢ first RTH prediction uses 60 *pre-trade* minutes only               ‚îÇ\\n‚îÇ    ‚Ä¢ later predictions mix pre-trade + today‚Äôs RTH, never yesterday RTH  ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n                        Predicted signal ≈∑‚Çú\\n\\n\\nDay i                               Day i+1\\n|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚Ä¶‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\nc‚Çú,h‚Çú: 0 ‚Üí accumulate ‚Üí reset_states() ‚Üí 0 ‚Üí accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ù∂  NETWORK  WEIGHTS  Œ∏  ‚Äì learned across all history, fixed at runtime   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ù∑  CELL STATE  c‚Çú  ‚Äì slow integrator covering the *whole* current day    ‚îÇ\n",
    "‚îÇ    ‚Ä¢ retains early-morning context                                       ‚îÇ\n",
    "‚îÇ    ‚Ä¢ reset_states()  at every midnight ‚Üí zero on next session            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ù∏  HIDDEN STATE  h‚Çú  ‚Äì fast dynamics (a few bars)                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ captures spikes / micro-structure                                   ‚îÇ\n",
    "‚îÇ    ‚Ä¢ reset together with c‚Çú midnight                                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ùπ  INPUT WINDOW  x‚Çú  ‚Äì last 60 minutes of raw features                   ‚îÇ\n",
    "‚îÇ    ‚Ä¢ first RTH prediction uses 60 *pre-trade* minutes only               ‚îÇ\n",
    "‚îÇ    ‚Ä¢ later predictions mix pre-trade + today‚Äôs RTH, never yesterday RTH  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "                        Predicted signal ≈∑‚Çú\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚Ä¶‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "c‚Çú,h‚Çú: 0 ‚Üí accumulate ‚Üí reset_states() ‚Üí 0 ‚Üí accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   ‚Ä¢ Layer 0  ‚Äúshort_lstm‚Äù  (SHORT_UNITS units)  ‚Üí quick dynamics, daily reset        #\n",
    "#   ‚Ä¢ Layer 1  ‚Äúlong_lstm‚Äù   (LONG_UNITS units)  ‚Üí slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what ‚Äúweek‚Äù means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚îÄ‚îÄ 1 ¬∑ optional mixed-precision context ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ 2 ¬∑ network definition ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ‚îÄ‚îÄ 3 ¬∑ optimiser & schedule ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ‚îÄ‚îÄ 4 ¬∑ restore dtype policy outside mixed-FP16 scope ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object ‚Üí harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n",
    "\n",
    "# save validation model, to reuse for inference\n",
    "model_val.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ‚ñ∏  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      ‚Ä¢ The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      ‚Ä¢ The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,          # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,                    # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,         # Total calendar days in the training epoch\n",
    "    max_epochs: int,           # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    weights_path               # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models‚Äîsuch as LSTMs‚Äîwith separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} ‚Ä¢ train={epoch_train:.6f} ‚Ä¢ val={epoch_val:.6f} \"\n",
    "              f\"‚Ä¢ impr={impr_pct:5.1f}% ‚Ä¢ lr={current_lr:.2e} ‚Ä¢ g‚âà{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(weights_path)  # Save checkpoint of the best weights.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(weights_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.370207\n",
      "Training sees 1984 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b47994642894e1bafeb8b9b1bdc466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751040558.654291    3601 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 ‚Ä¢ train=0.794588 ‚Ä¢ val=0.282886 ‚Ä¢ impr= 23.6% ‚Ä¢ lr=1.31e-04 ‚Ä¢ g‚âà6075.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f292e128a5b47b1b7f27f7be1666db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 ‚Ä¢ train=0.562277 ‚Ä¢ val=0.237295 ‚Ä¢ impr= 35.9% ‚Ä¢ lr=1.74e-04 ‚Ä¢ g‚âà3239.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2940fbf6f1f4efbaa84b3efc7712094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 ‚Ä¢ train=0.474810 ‚Ä¢ val=0.283220 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=4.56e-04 ‚Ä¢ g‚âà1040.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e78d8173d242a19ccf18d64a27f829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 ‚Ä¢ train=0.395296 ‚Ä¢ val=0.215038 ‚Ä¢ impr= 41.9% ‚Ä¢ lr=1.96e-04 ‚Ä¢ g‚âà2014.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae6e2adb0244e05be203ddbd3469798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 ‚Ä¢ train=0.361792 ‚Ä¢ val=0.215156 ‚Ä¢ impr= 41.9% ‚Ä¢ lr=1.62e-05 ‚Ä¢ g‚âà22377.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7dfe110ca4480db438ac9848abfa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 ‚Ä¢ train=0.347144 ‚Ä¢ val=0.249048 ‚Ä¢ impr= 32.7% ‚Ä¢ lr=4.63e-04 ‚Ä¢ g‚âà749.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71942aa71f14740ba2911cfb9a13e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 ‚Ä¢ train=0.304233 ‚Ä¢ val=0.200223 ‚Ä¢ impr= 45.9% ‚Ä¢ lr=3.52e-04 ‚Ä¢ g‚âà863.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516a37ee88c74d5c8ea750d8c0968541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 ‚Ä¢ train=0.281741 ‚Ä¢ val=0.197782 ‚Ä¢ impr= 46.6% ‚Ä¢ lr=2.08e-04 ‚Ä¢ g‚âà1355.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c22d9709524d039a3c8b1b66638d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 ‚Ä¢ train=0.269823 ‚Ä¢ val=0.209323 ‚Ä¢ impr= 43.5% ‚Ä¢ lr=8.11e-05 ‚Ä¢ g‚âà3329.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30413067971b4f8681d4db00f8257a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 ‚Ä¢ train=0.263485 ‚Ä¢ val=0.195090 ‚Ä¢ impr= 47.3% ‚Ä¢ lr=1.76e-05 ‚Ä¢ g‚âà14949.76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc03ba2e926430bb2c52f08d5765e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 ‚Ä¢ train=0.265460 ‚Ä¢ val=0.221453 ‚Ä¢ impr= 40.2% ‚Ä¢ lr=4.94e-04 ‚Ä¢ g‚âà537.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f00e61607a946df8675a47e22655c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 ‚Ä¢ train=0.252504 ‚Ä¢ val=0.194008 ‚Ä¢ impr= 47.6% ‚Ä¢ lr=4.66e-04 ‚Ä¢ g‚âà541.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673402f261864c1a9b0a79cc1c1d16a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 ‚Ä¢ train=0.237913 ‚Ä¢ val=0.191319 ‚Ä¢ impr= 48.3% ‚Ä¢ lr=4.19e-04 ‚Ä¢ g‚âà567.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12bfa9316ea447cb4185b79be56cd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 ‚Ä¢ train=0.228252 ‚Ä¢ val=0.193318 ‚Ä¢ impr= 47.8% ‚Ä¢ lr=3.58e-04 ‚Ä¢ g‚âà637.68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2214a80c2335418babd53e5fabbcbde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 ‚Ä¢ train=0.221770 ‚Ä¢ val=0.192227 ‚Ä¢ impr= 48.1% ‚Ä¢ lr=2.87e-04 ‚Ä¢ g‚âà772.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ed7f4e7c764bd7b618248b83243ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 ‚Ä¢ train=0.218898 ‚Ä¢ val=0.189274 ‚Ä¢ impr= 48.9% ‚Ä¢ lr=2.14e-04 ‚Ä¢ g‚âà1024.33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ff6a66d8d94c149533d0ebd2757a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 ‚Ä¢ train=0.214824 ‚Ä¢ val=0.198331 ‚Ä¢ impr= 46.4% ‚Ä¢ lr=1.44e-04 ‚Ä¢ g‚âà1489.38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e682e477fb0430685732780efa15d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 ‚Ä¢ train=0.210637 ‚Ä¢ val=0.196201 ‚Ä¢ impr= 47.0% ‚Ä¢ lr=8.52e-05 ‚Ä¢ g‚âà2472.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2993cd49069349cbb9edb32d602a4c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 ‚Ä¢ train=0.210445 ‚Ä¢ val=0.187671 ‚Ä¢ impr= 49.3% ‚Ä¢ lr=4.20e-05 ‚Ä¢ g‚âà5013.72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862e1bf3a3a74450943969c0ab633967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 ‚Ä¢ train=0.209528 ‚Ä¢ val=0.185582 ‚Ä¢ impr= 49.9% ‚Ä¢ lr=1.86e-05 ‚Ä¢ g‚âà11283.07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2b5ade83b24d22a7edd9f754b14910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 ‚Ä¢ train=0.212045 ‚Ä¢ val=0.203527 ‚Ä¢ impr= 45.0% ‚Ä¢ lr=4.99e-04 ‚Ä¢ g‚âà424.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4d997a319d41e48fdd9275f3d9c01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 ‚Ä¢ train=0.213835 ‚Ä¢ val=0.186162 ‚Ä¢ impr= 49.7% ‚Ä¢ lr=4.94e-04 ‚Ä¢ g‚âà432.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0925decc603040caa087a5ace1923c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 ‚Ä¢ train=0.206982 ‚Ä¢ val=0.187432 ‚Ä¢ impr= 49.4% ‚Ä¢ lr=4.84e-04 ‚Ä¢ g‚âà428.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8734a9653e048c594d304486ba69faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 ‚Ä¢ train=0.202235 ‚Ä¢ val=0.187387 ‚Ä¢ impr= 49.4% ‚Ä¢ lr=4.68e-04 ‚Ä¢ g‚âà432.49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea42e4f7d094ed9946266e4a5f79afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 ‚Ä¢ train=0.198550 ‚Ä¢ val=0.187820 ‚Ä¢ impr= 49.3% ‚Ä¢ lr=4.47e-04 ‚Ä¢ g‚âà444.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fadab883424c4993f2d5078384ed65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 ‚Ä¢ train=0.195696 ‚Ä¢ val=0.188153 ‚Ä¢ impr= 49.2% ‚Ä¢ lr=4.22e-04 ‚Ä¢ g‚âà464.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119391406712470181201ac924d024f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 ‚Ä¢ train=0.194050 ‚Ä¢ val=0.186325 ‚Ä¢ impr= 49.7% ‚Ä¢ lr=3.93e-04 ‚Ä¢ g‚âà494.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd1abb8275c402187f813c02baeb0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 ‚Ä¢ train=0.192431 ‚Ä¢ val=0.184568 ‚Ä¢ impr= 50.1% ‚Ä¢ lr=3.61e-04 ‚Ä¢ g‚âà533.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51c11d6bb8a41e990c0a33b867770ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 ‚Ä¢ train=0.191496 ‚Ä¢ val=0.180922 ‚Ä¢ impr= 51.1% ‚Ä¢ lr=3.26e-04 ‚Ä¢ g‚âà587.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08d8cbe8c4a4fdd883f4656bccb3ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 ‚Ä¢ train=0.189559 ‚Ä¢ val=0.184195 ‚Ä¢ impr= 50.2% ‚Ä¢ lr=2.90e-04 ‚Ä¢ g‚âà653.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb762d4bc7864f5bb21679911cc2679f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 ‚Ä¢ train=0.186205 ‚Ä¢ val=0.186707 ‚Ä¢ impr= 49.6% ‚Ä¢ lr=2.53e-04 ‚Ä¢ g‚âà735.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5716b27fdc48cc9e0417b4f40a43f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 ‚Ä¢ train=0.184632 ‚Ä¢ val=0.190892 ‚Ä¢ impr= 48.4% ‚Ä¢ lr=2.17e-04 ‚Ä¢ g‚âà852.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967921a9319c48a2947ef348ec83214d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 ‚Ä¢ train=0.181994 ‚Ä¢ val=0.182241 ‚Ä¢ impr= 50.8% ‚Ä¢ lr=1.81e-04 ‚Ä¢ g‚âà1006.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8693d9d79b4d9ca4a79733e2e0ecfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 ‚Ä¢ train=0.183127 ‚Ä¢ val=0.179569 ‚Ä¢ impr= 51.5% ‚Ä¢ lr=1.47e-04 ‚Ä¢ g‚âà1246.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76823e7857145edbba04e510bee58dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 ‚Ä¢ train=0.182457 ‚Ä¢ val=0.178463 ‚Ä¢ impr= 51.8% ‚Ä¢ lr=1.15e-04 ‚Ä¢ g‚âà1580.48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68288a76e4d64e8eab71c9073f0df64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 036:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 ‚Ä¢ train=0.181723 ‚Ä¢ val=0.178283 ‚Ä¢ impr= 51.8% ‚Ä¢ lr=8.73e-05 ‚Ä¢ g‚âà2081.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9620292d5d714d2eace8c7fd8b5c73bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 037:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 ‚Ä¢ train=0.181188 ‚Ä¢ val=0.178479 ‚Ä¢ impr= 51.8% ‚Ä¢ lr=6.31e-05 ‚Ä¢ g‚âà2872.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6101b6ae5d3f48748f878af0a7a04f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 038:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 ‚Ä¢ train=0.180553 ‚Ä¢ val=0.178575 ‚Ä¢ impr= 51.8% ‚Ä¢ lr=4.34e-05 ‚Ä¢ g‚âà4164.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fdc40c27e54dda8003df518a859f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 039:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 ‚Ä¢ train=0.180053 ‚Ä¢ val=0.178285 ‚Ä¢ impr= 51.8% ‚Ä¢ lr=2.86e-05 ‚Ä¢ g‚âà6299.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d77183629cc4c60a7e1378216df47fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 040:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 ‚Ä¢ train=0.180057 ‚Ä¢ val=0.177697 ‚Ä¢ impr= 52.0% ‚Ä¢ lr=1.91e-05 ‚Ä¢ g‚âà9428.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427bb83606504938b1571115e422c3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 041:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 ‚Ä¢ train=0.179823 ‚Ä¢ val=0.177454 ‚Ä¢ impr= 52.1% ‚Ä¢ lr=1.51e-05 ‚Ä¢ g‚âà11890.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fbe835be4443dd89ae5df886ec78ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 042:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 ‚Ä¢ train=0.183214 ‚Ä¢ val=0.181573 ‚Ä¢ impr= 51.0% ‚Ä¢ lr=5.00e-04 ‚Ä¢ g‚âà366.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae944f006f57498ab828fdb2eab66372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 043:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 ‚Ä¢ train=0.183193 ‚Ä¢ val=0.182011 ‚Ä¢ impr= 50.8% ‚Ä¢ lr=4.98e-04 ‚Ä¢ g‚âà368.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acddb1808e72470295ca56717d24e11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 044:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 ‚Ä¢ train=0.181844 ‚Ä¢ val=0.182060 ‚Ä¢ impr= 50.8% ‚Ä¢ lr=4.95e-04 ‚Ä¢ g‚âà367.69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713b5500c6eb40d6a7a14aa415e93cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 045:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 ‚Ä¢ train=0.181065 ‚Ä¢ val=0.181992 ‚Ä¢ impr= 50.8% ‚Ä¢ lr=4.90e-04 ‚Ä¢ g‚âà369.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf1e648561a46f4896043f77a8c2e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 046:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 ‚Ä¢ train=0.180358 ‚Ä¢ val=0.182304 ‚Ä¢ impr= 50.8% ‚Ä¢ lr=4.84e-04 ‚Ä¢ g‚âà372.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b277b72536cf479c94535e1acf50a640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 047:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 ‚Ä¢ train=0.179057 ‚Ä¢ val=0.182820 ‚Ä¢ impr= 50.6% ‚Ä¢ lr=4.77e-04 ‚Ä¢ g‚âà375.50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e14223e55be43dd8afa89e674d8f992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 048:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 ‚Ä¢ train=0.178488 ‚Ä¢ val=0.183111 ‚Ä¢ impr= 50.5% ‚Ä¢ lr=4.68e-04 ‚Ä¢ g‚âà381.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7894da021ba42aaa0120ba21e889b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 049:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 ‚Ä¢ train=0.177668 ‚Ä¢ val=0.183343 ‚Ä¢ impr= 50.5% ‚Ä¢ lr=4.59e-04 ‚Ä¢ g‚âà387.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3351eb72556460ebbac19725ccf56cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 050:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 ‚Ä¢ train=0.177072 ‚Ä¢ val=0.184419 ‚Ä¢ impr= 50.2% ‚Ä¢ lr=4.48e-04 ‚Ä¢ g‚âà395.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7408836c26d49c893e491dec9d9771e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 051:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 ‚Ä¢ train=0.176236 ‚Ä¢ val=0.184939 ‚Ä¢ impr= 50.0% ‚Ä¢ lr=4.36e-04 ‚Ä¢ g‚âà404.42\n",
      "Early stopping triggered.\n",
      "\n",
      "Champion validation RMSE = 0.177454\n",
      "Improvement vs baseline   =  52.1 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  Training loop (outer bar only ‚Üí maximum throughput)                     #\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        weights_path        = weights_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
