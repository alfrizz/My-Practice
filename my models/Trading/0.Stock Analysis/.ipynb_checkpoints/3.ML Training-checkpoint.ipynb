{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f06a4f-691a-4a84-a305-e7212eb879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import stockanalibs\n",
    "importlib.reload(stockanalibs)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08b80ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:00:00</th>\n",
       "      <td>28.650</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.6255</td>\n",
       "      <td>28.6255</td>\n",
       "      <td>6100.0</td>\n",
       "      <td>28.616910</td>\n",
       "      <td>28.634090</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.043081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:01:00</th>\n",
       "      <td>28.650</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.6304</td>\n",
       "      <td>28.6304</td>\n",
       "      <td>6956.0</td>\n",
       "      <td>28.621809</td>\n",
       "      <td>28.638991</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.048548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:02:00</th>\n",
       "      <td>28.650</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.6353</td>\n",
       "      <td>28.6353</td>\n",
       "      <td>7812.0</td>\n",
       "      <td>28.626708</td>\n",
       "      <td>28.643892</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.054214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:03:00</th>\n",
       "      <td>28.650</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.6402</td>\n",
       "      <td>28.6402</td>\n",
       "      <td>8668.0</td>\n",
       "      <td>28.631607</td>\n",
       "      <td>28.648793</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.060053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:04:00</th>\n",
       "      <td>28.650</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.6451</td>\n",
       "      <td>28.6451</td>\n",
       "      <td>9524.0</td>\n",
       "      <td>28.636506</td>\n",
       "      <td>28.653694</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.065460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.375</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.2150</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.512900</td>\n",
       "      <td>173.617100</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>1.779</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.565</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.2400</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.328000</td>\n",
       "      <td>173.432000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>1.964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.390</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.2000</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.258000</td>\n",
       "      <td>173.362000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>2.034</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.315</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.2300</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.228000</td>\n",
       "      <td>173.332000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>2.064</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.300</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.1700</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.557600</td>\n",
       "      <td>173.661800</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>1.734</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354496 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open      high       low     close     volume  \\\n",
       "2014-04-03 13:00:00   28.650   28.6500   28.6255   28.6255     6100.0   \n",
       "2014-04-03 13:01:00   28.650   28.6500   28.6304   28.6304     6956.0   \n",
       "2014-04-03 13:02:00   28.650   28.6500   28.6353   28.6353     7812.0   \n",
       "2014-04-03 13:03:00   28.650   28.6500   28.6402   28.6402     8668.0   \n",
       "2014-04-03 13:04:00   28.650   28.6500   28.6451   28.6451     9524.0   \n",
       "...                      ...       ...       ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.375  173.6771  173.2150  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.565  173.5900  173.2400  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.390  173.4100  173.2000  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.315  173.4000  173.2300  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.300  174.0500  173.1700  173.6097  7649838.0   \n",
       "\n",
       "                            bid         ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:00:00   28.616910   28.634090             0            0.000   \n",
       "2014-04-03 13:01:00   28.621809   28.638991             0            0.000   \n",
       "2014-04-03 13:02:00   28.626708   28.643892             0            0.000   \n",
       "2014-04-03 13:03:00   28.631607   28.648793             0            0.000   \n",
       "2014-04-03 13:04:00   28.636506   28.653694             0            0.000   \n",
       "...                         ...         ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.512900  173.617100             0           -0.931   \n",
       "2025-06-18 20:57:00  173.328000  173.432000             0           -0.931   \n",
       "2025-06-18 20:58:00  173.258000  173.362000             0           -0.931   \n",
       "2025-06-18 20:59:00  173.228000  173.332000             0           -0.931   \n",
       "2025-06-18 21:00:00  173.557600  173.661800             0           -0.931   \n",
       "\n",
       "                     EarningDiff  signal_smooth  \n",
       "2014-04-03 13:00:00        0.000       1.043081  \n",
       "2014-04-03 13:01:00        0.000       1.048548  \n",
       "2014-04-03 13:02:00        0.000       1.054214  \n",
       "2014-04-03 13:03:00        0.000       1.060053  \n",
       "2014-04-03 13:04:00        0.000       1.065460  \n",
       "...                          ...            ...  \n",
       "2025-06-18 20:56:00        1.779       0.000000  \n",
       "2025-06-18 20:57:00        1.964       0.000000  \n",
       "2025-06-18 20:58:00        2.034       0.000000  \n",
       "2025-06-18 20:59:00        2.064       0.000000  \n",
       "2025-06-18 21:00:00        1.734       0.000000  \n",
       "\n",
       "[1354496 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}_final.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fb5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) \n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n",
    "\n",
    "# USE GPU if available, otherwise fallback to CPU\n",
    "device = stockanalibs.device\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # hidden size of short-term LSTM (32–128)\n",
    "LONG_UNITS          = 128      # hidden size of long-term LSTM  (64–256)\n",
    "DROPOUT_SHORT       = 0.15     # dropout on short-term outputs   (0.05–0.3)\n",
    "DROPOUT_LONG        = 0.25     # dropout on long-term outputs    (0.1–0.3)\n",
    "WEIGHT_DECAY        = 5e-4     # L2 weight decay for all LSTM layers\n",
    "WEIGHT_DROPOUT      = 0.2      # to drop hidden-to-hidden weights\n",
    "\n",
    "# ── Optimizer Settings ─────────────────────────────────────────────────────\n",
    "INITIAL_LR          = 5e-4     # AdamW initial learning rate      (1e-4 to 1e-3)\n",
    "CLIPNORM            = 1.0      # max norm for gradient clipping   (0.5–5.0)\n",
    "\n",
    "# ── ReduceLROnPlateau Scheduler ───────────────────────────────────────────\n",
    "PLATEAU_FACTOR      = 0.5      # LR ← LR * factor when plateau   (0.1–0.5)\n",
    "PLATEAU_PATIENCE    = 8        # epochs with no val-improve      (3–5)\n",
    "MIN_LR              = 1e-6     # lower bound on LR after decay    (1e-6)\n",
    "\n",
    "# ── CosineAnnealingWarmRestarts Scheduler ─────────────────────────────────\n",
    "T_0                 = 20       # epochs before first cosine restart\n",
    "T_MULT              = 1        # multiplier for subsequent cycle lengths\n",
    "ETA_MIN             = MIN_LR   # floor LR in cosine decay         (same as MIN_LR)\n",
    "\n",
    "# ── Training Control Parameters ────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 32       # days per training bundle         (8–64)\n",
    "VAL_BATCH           = 1        # days per validation batch         (usually 1)\n",
    "MAX_EPOCHS          = 150      # max number of training epochs     (50–150)\n",
    "EARLY_STOP_PATIENCE = 20       # epochs w/o val-improve before stop (8–15)\n",
    "NUM_WORKERS         = 8        # DataLoader workers for parallel I/O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2723445e-c4ea-4974-8b81-50b353918934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,                # number of past minutes used to form each sample\n",
    "    feature_cols: Sequence[str],   # names of DataFrame columns fed into the LSTM\n",
    "    label_col: str,                # name of the column to predict (next‐step target)\n",
    "    rth_start: dt.time,            # only keep windows whose end‐time ≥ this “regular” market open\n",
    "    device: Optional[torch.device] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert a minute‐bar DataFrame into ready‐to‐train PyTorch tensors for a stateful LSTM.\n",
    "    \n",
    "    Steps & outputs:\n",
    "    \n",
    "      0) Pick your target device (GPU if available, else CPU).\n",
    "      1) Slice the DataFrame into calendar‐day chunks (so we standardize & state‐reset per day).\n",
    "      2) For each day:\n",
    "         a) Sort by timestamp.\n",
    "         b) Extract raw price series (close, bid, ask) as CPU tensors.\n",
    "         c) Standardize your chosen feature columns with a fresh StandardScaler.\n",
    "         d) Build sliding windows of length `look_back` via `Tensor.unfold`.\n",
    "         e) Align next‐minute labels (`label_col`) and raw‐price points by dropping the first `look_back` rows.\n",
    "         f) Mask out any windows whose *end* timestamp is before `rth_start`.\n",
    "      3) Concatenate all days’ windows, labels, and raw‐price arrays along the sample dimension.\n",
    "      4) Move everything onto the target device exactly once.\n",
    "\n",
    "    Returns:\n",
    "      X         – float32 Tensor of shape (N, look_back, F), the standardized input windows\n",
    "      y         – float32 Tensor of shape (N,       ), the aligned single‐step targets\n",
    "      raw_close – float32 Tensor of shape (N,       ), the raw close price at prediction time\n",
    "      raw_bid   – float32 Tensor of shape (N,       ), the raw bid   price at prediction time\n",
    "      raw_ask   – float32 Tensor of shape (N,       ), the raw ask   price at prediction time\n",
    "\n",
    "    Where:\n",
    "      N = total number of windows across all days that passed the RTH filter,\n",
    "      F = number of feature columns,\n",
    "      look_back = number of minutes in each input window.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare per-day lists (CPU‐resident) to collect windows & labels\n",
    "    X_days, y_days = [], []\n",
    "    c_days, b_days, a_days = [], [], []  # raw close/bid/ask\n",
    "\n",
    "    # 1) Loop over each calendar day chunk\n",
    "    #    df.index.normalize() groups by midnight‐normalized date\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        # Ensure chronological order\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1a) Raw price series as 1D CPU Tensors (length T = # minutes in the day)\n",
    "        close_t = torch.from_numpy(\n",
    "            day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        )  # shape: (T,)\n",
    "        bid_t = torch.from_numpy(\n",
    "            day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        )\n",
    "        ask_t = torch.from_numpy(\n",
    "            day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        # 1b) Standardize features *per day* on CPU\n",
    "        #     - fit_transform ensures zero‐mean, unit‐variance per-day\n",
    "        feats_np = StandardScaler().fit_transform(\n",
    "            day_df[feature_cols].to_numpy()\n",
    "        )           # shape: (T, F)\n",
    "        feats_t = torch.from_numpy(feats_np.astype(np.float32))\n",
    "\n",
    "        # 1c) Raw labels (next‐step targets) as CPU Tensor\n",
    "        labels_t = torch.from_numpy(\n",
    "            day_df[label_col].to_numpy(dtype=np.float32)\n",
    "        )  # shape: (T,)\n",
    "\n",
    "        # 2) Build sliding windows (unfold on the time axis)\n",
    "        #    Tensor.unfold(dim=0, size=look_back, step=1) \n",
    "        #    produces shape (T - look_back + 1, look_back, F)\n",
    "        windows = feats_t.unfold(0, look_back, 1)\n",
    "\n",
    "        # 3) Align targets one step *after* each window:\n",
    "        #    - Drop the final window so that `targets = labels_t[look_back:]` aligns 1:1\n",
    "        windows = windows[:-1]               # new shape: (T - look_back, look_back, F)\n",
    "        targets = labels_t[look_back:]       # shape: (T - look_back,)\n",
    "        c_pts = close_t[look_back:]          # raw close aligned\n",
    "        b_pts = bid_t[look_back:]            # raw bid aligned\n",
    "        a_pts = ask_t[look_back:]            # raw ask aligned\n",
    "\n",
    "        # 4) RTH filtering: only keep windows whose *end* timestamp is ≥ rth_start\n",
    "        #    day_df.index.time[look_back:] is a numpy array of length (T - look_back)\n",
    "        end_times = day_df.index.time[look_back:]\n",
    "        mask = (end_times >= rth_start)     # boolean mask, shape=(T - look_back,)\n",
    "        if not np.any(mask):\n",
    "            # no windows qualify for this day → skip\n",
    "            continue\n",
    "\n",
    "        # Convert numpy bool mask into a torch.BoolTensor\n",
    "        mask_t = torch.from_numpy(mask)\n",
    "\n",
    "        # 5) Apply mask to CPU tensors (window‐batch dim is dimension 0)\n",
    "        windows = windows[mask_t]            # (n_i, look_back, F)\n",
    "        targets = targets[mask_t]            # (n_i,)\n",
    "        c_pts = c_pts[mask_t]                # (n_i,)\n",
    "        b_pts = b_pts[mask_t]                # (n_i,)\n",
    "        a_pts = a_pts[mask_t]                # (n_i,)\n",
    "\n",
    "        # 6) Collect this day’s filtered windows & labels\n",
    "        X_days.append(windows)\n",
    "        y_days.append(targets)\n",
    "        c_days.append(c_pts)\n",
    "        b_days.append(b_pts)\n",
    "        a_days.append(a_pts)\n",
    "\n",
    "    # If after filtering no windows remain, alert the user\n",
    "    if not X_days:\n",
    "        raise ValueError(\n",
    "            \"No windows passed RTH filter; check your rth_start or input DataFrame.\"\n",
    "        )\n",
    "\n",
    "    # 7) Concatenate all days along the sample dimension (dim=0) on CPU\n",
    "    X_cpu = torch.cat(X_days, dim=0)   # final shape: (N, look_back, F)\n",
    "    y_cpu = torch.cat(y_days, dim=0)   # final shape: (N,)\n",
    "    c_cpu = torch.cat(c_days, dim=0)   # final shape: (N,)\n",
    "    b_cpu = torch.cat(b_days, dim=0)   # final shape: (N,)\n",
    "    a_cpu = torch.cat(a_days, dim=0)   # final shape: (N,)\n",
    "\n",
    "    # 8) One‐shot transfer to the chosen device for fastest GPU memory copy\n",
    "    #    non_blocking=True overlaps host→device copies with compute when possible.\n",
    "    X         = X_cpu.to(device, non_blocking=True)\n",
    "    y         = y_cpu.to(device, non_blocking=True)\n",
    "    raw_close = c_cpu.to(device, non_blocking=True)\n",
    "    raw_bid   = b_cpu.to(device, non_blocking=True)\n",
    "    raw_ask   = a_cpu.to(device, non_blocking=True)\n",
    "\n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1101056, 5, 90])\n",
      "torch.Size([1101056])\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b409879-a447-440c-ab7d-d69a81b1dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: torch.Tensor,            # full input tensor, shape (N, look_back, F)\n",
    "    y: torch.Tensor,            # full target tensor, shape (N,)\n",
    "    raw_close: torch.Tensor,    # raw close prices, shape (N,)\n",
    "    raw_bid: torch.Tensor,      # raw bid   prices, shape (N,)\n",
    "    raw_ask: torch.Tensor,      # raw ask   prices, shape (N,)\n",
    "    df: pd.DataFrame,           # original minute‐bar DataFrame (with a DatetimeIndex)\n",
    "    *,\n",
    "    look_back: int,             # length of each input window (must match X’s second dim)\n",
    "    rth_start: dt.time,         # only keep windows whose end‐time ≥ this “regular” market open\n",
    "    train_prop: float,          # fraction of calendar days assigned to training (0.0–1.0)\n",
    "    val_prop: float,            # fraction of calendar days assigned to validation\n",
    "    train_batch: int,           # group train days into multiples of this (for batching)\n",
    "    device: Optional[torch.device] = None\n",
    ") -> Tuple[\n",
    "    Tuple[torch.Tensor, torch.Tensor],                                   # (X_tr, y_tr)\n",
    "    Tuple[torch.Tensor, torch.Tensor],                                   # (X_val, y_val)\n",
    "    Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],  # (X_te, y_te, close_te, bid_te, ask_te)\n",
    "    List[int],                                                          # samples_per_day\n",
    "    torch.Tensor,                                                       # day_id_tr\n",
    "    torch.Tensor,                                                       # day_id_val\n",
    "    torch.Tensor                                                        # day_id_te\n",
    "]:\n",
    "    \"\"\"\n",
    "    Split a monolithic sliding‐window dataset into chronological train/val/test subsets,\n",
    "    ensuring that no day’s windows are split across sets.\n",
    "\n",
    "    Workflow:\n",
    "\n",
    "      0) Choose a device for any new tensors (default: same as X).\n",
    "      1) Group the original minute‐bar DataFrame by calendar day.\n",
    "      2) For each day:\n",
    "         a) Sort by timestamp to preserve chronology.\n",
    "         b) Determine how many windows (of length `look_back`) fall entirely within\n",
    "            that day AND whose *end* timestamp is ≥ `rth_start`.\n",
    "         c) Append that count to `samples_per_day`.\n",
    "      3) Verify the sum of per‐day window counts equals N = X.shape[0].\n",
    "      4) Build a `day_id_vec` of length N: each window is tagged by its day index [0..D-1].\n",
    "      5) Compute how many days go to train/val/test:\n",
    "         - `train_days_orig = floor(D*train_prop)`\n",
    "         - Round `train_days_orig` *up* to a multiple of `train_batch` → `train_days`\n",
    "         - `cut_train = train_days - 1` (inclusive index)\n",
    "         - `cut_val   = floor(D*(train_prop+val_prop))` (inclusive index)\n",
    "      6) Create boolean masks on `day_id_vec`:\n",
    "         - `mask_tr = day_id_vec <= cut_train`\n",
    "         - `mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)`\n",
    "         - `mask_te = day_id_vec > cut_val`\n",
    "      7) Slice X, y, raw_close, raw_bid, raw_ask with these masks.\n",
    "      8) Also slice out `day_id_vec` per split for bookkeeping.\n",
    "      9) Return:\n",
    "         - `(X_tr, y_tr)`,\n",
    "         - `(X_val, y_val)`,\n",
    "         - `(X_te, y_te, close_te, bid_te, ask_te)`,\n",
    "         - `samples_per_day` list of length D,\n",
    "         - per-split `day_id_tr`, `day_id_val`, `day_id_te` tensors.\n",
    "\n",
    "    All returned tensors remain on `device`.\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) Default device for any new tensors\n",
    "    device = device or X.device\n",
    "\n",
    "    # 1) Count valid windows for each calendar day\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        # a) ensure time‐ordered\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # b) total minutes in this day\n",
    "        T = len(day_df)\n",
    "\n",
    "        # c) a window can only end at index i if i >= look_back\n",
    "        #    so we’ll consider positions look_back .. T-1\n",
    "        #    mask_ready[i - look_back] == True for those indices\n",
    "        mask_ready = np.ones(T - look_back, dtype=bool)\n",
    "\n",
    "        # d) compute end timestamps for each candidate window\n",
    "        end_times = day_df.index.time[look_back:]  # length (T - look_back)\n",
    "\n",
    "        # e) build RTH mask: only keep windows ending on/after rth_start\n",
    "        mask_rth = np.array([t >= rth_start for t in end_times], dtype=bool)\n",
    "\n",
    "        # f) combine readiness & RTH filters\n",
    "        valid_mask = mask_ready & mask_rth\n",
    "\n",
    "        # g) count how many windows survived\n",
    "        count = int(valid_mask.sum())\n",
    "        samples_per_day.append(count)\n",
    "\n",
    "    # 2) Sanity check: sum(sample counts) must match N\n",
    "    total_samples = sum(samples_per_day)\n",
    "    if total_samples != X.size(0):\n",
    "        raise ValueError(\n",
    "            f\"Total windows mismatch: sum(samples_per_day)={total_samples} \"\n",
    "            f\"but X.shape[0]={X.size(0)}\"\n",
    "        )\n",
    "\n",
    "    # 3) Build a day‐ID vector (length N) tagging each window by its day index\n",
    "    D = len(samples_per_day)  # total number of days\n",
    "    # Create a tensor of counts per day on `device`\n",
    "    day_counts = torch.tensor(samples_per_day, dtype=torch.long, device=device)\n",
    "    # Repeat each day index `day_counts[i]` times\n",
    "    day_id_vec = torch.repeat_interleave(\n",
    "        torch.arange(D, device=device, dtype=torch.long),\n",
    "        day_counts\n",
    "    )  # shape: (N,)\n",
    "\n",
    "    # 4) Determine how many days go to train / val / test\n",
    "    # a) nominal train-days before batching\n",
    "    train_days_orig = int(D * train_prop)\n",
    "    # b) round up to a multiple of train_batch\n",
    "    batches_needed = (train_days_orig + train_batch - 1) // train_batch\n",
    "    train_days = min(D, batches_needed * train_batch)\n",
    "    # c) inclusive index cutoff for training days\n",
    "    cut_train = train_days - 1\n",
    "\n",
    "    # d) inclusive index cutoff for validation days\n",
    "    cut_val = int(D * (train_prop + val_prop))  # floors automatically\n",
    "\n",
    "    # 5) Build boolean masks over the sample‐axis\n",
    "    mask_tr  = (day_id_vec <= cut_train)\n",
    "    mask_val = (day_id_vec >  cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te  = (day_id_vec >  cut_val)\n",
    "\n",
    "    # 6) Slice out each split\n",
    "    X_tr, y_tr       = X[mask_tr],       y[mask_tr]\n",
    "    X_val, y_val     = X[mask_val],     y[mask_val]\n",
    "    X_te, y_te       = X[mask_te],      y[mask_te]\n",
    "    close_te         = raw_close[mask_te]\n",
    "    bid_te           = raw_bid[mask_te]\n",
    "    ask_te           = raw_ask[mask_te]\n",
    "\n",
    "    # 7) Also slice day‐ID vectors for each split\n",
    "    day_id_tr  = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te  = day_id_vec[mask_te]\n",
    "\n",
    "    # Final return: structured splits + metadata\n",
    "    return (\n",
    "        (X_tr, y_tr),\n",
    "        (X_val, y_val),\n",
    "        (X_te, y_te, close_te, bid_te, ask_te),\n",
    "        samples_per_day,\n",
    "        day_id_tr, day_id_val, day_id_te\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train days: 1984\n",
      " Val days: 410\n",
      "Test days: 422\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "# make sure these are Tensors\n",
    "day_id_tr_t  = torch.as_tensor(day_id_tr,  dtype=torch.int64, device='cpu')\n",
    "day_id_val_t = torch.as_tensor(day_id_val, dtype=torch.int64, device='cpu')\n",
    "day_id_te_t  = torch.as_tensor(day_id_te,  dtype=torch.int64, device='cpu')\n",
    "\n",
    "train_days_count = torch.unique(day_id_tr_t).numel()\n",
    "val_days_count   = torch.unique(day_id_val_t).numel()\n",
    "test_days_count  = torch.unique(day_id_te_t).numel()\n",
    "\n",
    "print(f\"Train days: {train_days_count}\")\n",
    "print(f\" Val days: {val_days_count}\")\n",
    "print(f\"Test days: {test_days_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ed9892-6d63-44d6-8d1e-619cb2485841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_optimizer_and_scheduler(\n",
    "    model: nn.Module,\n",
    "    initial_lr: float,\n",
    "    weight_decay: float = 1e-3,\n",
    "    lr_reduce_factor: float = 0.5,\n",
    "    lr_patience: int = 3,\n",
    "    lr_min: float = 1e-6,\n",
    "    clipnorm: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build optimizer, LR scheduler, AMP scaler, and clipping threshold.\n",
    "\n",
    "    • optimizer: AdamW with L2 weight_decay on all non‐bias weights.\n",
    "    • plateau_sched: ReduceLROnPlateau – watches val‐RMSE, cuts LR when plateau.\n",
    "      - mode='min'       → lower-is-better metric (we minimize RMSE).\n",
    "      - factor=0.5       → multiply LR by 0.5 on plateau.\n",
    "      - patience=3       → wait 3 epochs with no improvement before cutting.\n",
    "      - min_lr=1e-6      → never drop below this learning rate.\n",
    "      - verbose=True     → print a message whenever LR is reduced.\n",
    "    • scaler: GradScaler for mixed‐precision (automatically handles device).\n",
    "    • clipnorm: float threshold for gradient clipping in train_step.\n",
    "\n",
    "    Returns:\n",
    "      optimizer, plateau_sched, scaler, clipnorm\n",
    "    \"\"\"\n",
    "    # 1) AdamW: adaptive moment optimizer with decoupled weight decay\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=initial_lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # 2) ReduceLROnPlateau: only reduces LR when val metric plateaus\n",
    "    plateau_sched = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=lr_reduce_factor,\n",
    "        patience=lr_patience,\n",
    "        min_lr=lr_min,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    cosine_sched  = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=T_0, \n",
    "        T_mult=T_MULT, \n",
    "        eta_min=ETA_MIN\n",
    "    )\n",
    "\n",
    "    # 3) GradScaler: handles loss scaling for mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Return all four objects; clipnorm passed through to train_step\n",
    "    return optimizer, plateau_sched, cosine_sched, scaler, clipnorm\n",
    "\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model:     nn.Module,\n",
    "    x_day:     torch.Tensor,            # shape (W, look_back, F), already on device\n",
    "    y_day:     torch.Tensor,            # shape (W,),            already on device\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler:    GradScaler,\n",
    "    clipnorm:  float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform one mixed‐precision training update for a single “day” of data.\n",
    "\n",
    "    Steps:\n",
    "      1. Zero gradients (`optimizer.zero_grad(set_to_none=True)`).\n",
    "      2. Forward in fp16 context:\n",
    "         - autocast on x_day.device\n",
    "         - model(x_day) → out shape (W, seq_len, 1)\n",
    "         - extract last step: out[:, -1, 0] → (W,)\n",
    "         - compute MSE loss against y_day\n",
    "      3. Backward:\n",
    "         - scaler.scale(loss).backward()\n",
    "         - scaler.unscale_(optimizer) to prepare for clipping\n",
    "         - clip gradients to `clipnorm`\n",
    "         - scaler.step(optimizer) + scaler.update()\n",
    "      4. Return float(loss)\n",
    "\n",
    "    Returns:\n",
    "      The scalar loss value (Python float) for logging.\n",
    "    \"\"\"\n",
    "    # 1) Reset gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    model.train()\n",
    "\n",
    "    # 2) Mixed‐precision forward\n",
    "    device = x_day.device\n",
    "    with autocast(device_type=device.type):\n",
    "        out = model(x_day)            # → (W, seq_len, 1)\n",
    "        last = out[:, -1, 0]          # → (W,)\n",
    "        loss = Funct.mse_loss(last, y_day, reduction='mean')\n",
    "\n",
    "    # 3) Backward with gradient scaling and clipping\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)        # bring gradients back to fp32 for clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clipnorm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def get_current_lr(optimizer: torch.optim.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Retrieve the current learning rate from the first parameter group.\n",
    "    \"\"\"\n",
    "    return float(optimizer.param_groups[0]['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30eb786b-f535-4b29-ae1b-e575bfe6d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_stateful_training_loop(\n",
    "    model:       torch.nn.Module,\n",
    "    optimizer:   torch.optim.Optimizer,\n",
    "    cosine_sched: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    plateau_sched: torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scaler:      torch.cuda.amp.GradScaler,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader:   torch.utils.data.DataLoader,\n",
    "    *,\n",
    "    max_epochs:         int,\n",
    "    early_stop_patience:int,\n",
    "    baseline_val_rmse:  float,\n",
    "    clipnorm:           float,\n",
    "    device:             torch.device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Training loop with:\n",
    "      - mixed precision\n",
    "      - per-day / per-week LSTM resets\n",
    "      - CosineAnnealingWarmRestarts (per-batch)\n",
    "      - ReduceLROnPlateau (per-epoch, on val RMSE)\n",
    "      - live RMSE plotting & early stop\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_ctr  = 0\n",
    "\n",
    "    save_path = stockanalibs.save_path\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    live_plot = stockanalibs.LiveRMSEPlot()\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # ─── TRAIN ───────────────────────────────────────────\n",
    "        model.train()\n",
    "        model.h_short = model.h_long = None\n",
    "        train_losses = []\n",
    "\n",
    "        # enumerate so we can step cosine_sched per-batch\n",
    "        pbar = tqdm(\n",
    "            enumerate(train_loader, start=0),\n",
    "            total = len(train_loader),\n",
    "            desc  = f\"Epoch {epoch}\",\n",
    "            unit  = \"bundle\"\n",
    "        )\n",
    "\n",
    "        for batch_idx, (xb_days, yb_days, wd_days) in pbar:\n",
    "            xb_days = xb_days.to(device, non_blocking=True)\n",
    "            yb_days = yb_days.to(device, non_blocking=True)\n",
    "            wd_days = wd_days.to(device)\n",
    "\n",
    "            prev_wd = None\n",
    "            # loop over each day in this bundle\n",
    "            for di in range(xb_days.size(0)):\n",
    "                wd = int(wd_days[di].item())\n",
    "\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                loss_val = train_step(\n",
    "                    model,\n",
    "                    xb_days[di],   # (W, look_back, F)\n",
    "                    yb_days[di],   # (W,)\n",
    "                    optimizer,\n",
    "                    scaler,\n",
    "                    clipnorm\n",
    "                )\n",
    "                train_losses.append(loss_val)\n",
    "\n",
    "            # step the cosine‐warm‐restart scheduler\n",
    "            frac_epoch = epoch - 1 + batch_idx / len(train_loader)\n",
    "            cosine_sched.step(frac_epoch)\n",
    "\n",
    "            # update progress bar\n",
    "            rmse = math.sqrt(sum(train_losses) / len(train_losses))\n",
    "            lr   = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix(train_rmse=rmse, lr=lr, refresh=False)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        # ─── VALIDATE ─────────────────────────────────────────\n",
    "        model.eval()\n",
    "        model.h_short = model.h_long = None\n",
    "        val_losses = []\n",
    "        prev_wd    = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb_day, yb_day, wd in val_loader:\n",
    "                wd = int(wd.item())\n",
    "                x  = xb_day[0].to(device, non_blocking=True)  # (W, look_back, F)\n",
    "                y  = yb_day.view(-1).to(device, non_blocking=True)  # (W,)\n",
    "\n",
    "                model.reset_short()\n",
    "                if prev_wd is not None and wd < prev_wd:\n",
    "                    model.reset_long()\n",
    "                prev_wd = wd\n",
    "\n",
    "                out  = model(x)               # (W, look_back, 1)\n",
    "                last = out[:, -1, 0]          # (W,)\n",
    "                val_losses.append(Funct.mse_loss(last, y, reduction='mean').item())\n",
    "\n",
    "        val_rmse = math.sqrt(sum(val_losses) / len(val_losses))\n",
    "\n",
    "        # ─── LOG & PLOT ───────────────────────────────────────\n",
    "        live_plot.update(rmse, val_rmse)\n",
    "        print(f\"Epoch {epoch:03d} • train={rmse:.4f} • val={val_rmse:.4f} • lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # ─── PLATEAU STEP ─────────────────────────────────────\n",
    "        plateau_sched.step(val_rmse)\n",
    "\n",
    "        # ─── EARLY STOPPING ───────────────────────────────────\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_ctr  = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # ─── SAVE BEST ─────────────────────────────────────────\n",
    "    ckpt_file = save_path / f\"best_{best_val_rmse:.4f}.pth\"\n",
    "    torch.save(model, ckpt_file)\n",
    "    print(f\"Saved best model (val RMSE={best_val_rmse:.4f}) to {ckpt_file}\")\n",
    "\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sees 1984 calendar days per epoch\n",
      "\n",
      "Baseline (zero‐forecast) RMSE on validation = 0.487780\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/62 [00:00<?, ?bundle/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 2, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 106\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline (zero‐forecast) RMSE on validation = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_val_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 8) Run the custom stateful training loop\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m best_val_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_stateful_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcosine_sched\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcosine_sched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplateau_sched\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplateau_sched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEARLY_STOP_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaseline_val_rmse\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbaseline_val_rmse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# torch.device('cuda') or 'cpu'\u001b[39;49;00m\n\u001b[1;32m    119\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# 9) Final reporting: best RMSE and relative improvement\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChampion validation RMSE = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 63\u001b[0m, in \u001b[0;36mcustom_stateful_training_loop\u001b[0;34m(model, optimizer, cosine_sched, plateau_sched, scaler, train_loader, val_loader, max_epochs, early_stop_patience, baseline_val_rmse, clipnorm, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m         model\u001b[38;5;241m.\u001b[39mreset_long()\n\u001b[1;32m     61\u001b[0m     prev_wd \u001b[38;5;241m=\u001b[39m wd\n\u001b[0;32m---> 63\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxb_days\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# (W, look_back, F)\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43myb_days\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# (W,)\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# step the cosine‐warm‐restart scheduler\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 93\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, x_day, y_day, optimizer, scaler, clipnorm)\u001b[0m\n\u001b[1;32m     91\u001b[0m device \u001b[38;5;241m=\u001b[39m x_day\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype):\n\u001b[0;32m---> 93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_day\u001b[49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# → (W, seq_len, 1)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     last \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]          \u001b[38;5;66;03m# → (W,)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m Funct\u001b[38;5;241m.\u001b[39mmse_loss(last, y_day, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/my models/Trading/0.Stock Analysis/stockanalibs.py:1909\u001b[0m, in \u001b[0;36mDualMemoryLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_lstm\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[1;32m   1908\u001b[0m \u001b[38;5;66;03m# daily LSTM\u001b[39;00m\n\u001b[0;32m-> 1909\u001b[0m out_s, (h_s, c_s) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshort_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_short\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_short\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_short, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_short \u001b[38;5;241m=\u001b[39m h_s\u001b[38;5;241m.\u001b[39mdetach_(), c_s\u001b[38;5;241m.\u001b[39mdetach_()\n\u001b[1;32m   1911\u001b[0m out_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_short(out_s); out_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_short(out_s)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/my models/Trading/0.Stock Analysis/stockanalibs.py:1825\u001b[0m, in \u001b[0;36mWeightDrop.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setweights()\n\u001b[0;32m-> 1825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1138\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1146\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 2, but got NoneType"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Baseline RMSE: zero‐forecast on validation set\n",
    "# -----------------------------------------------------------------------------\n",
    "def naive_rmse(val_loader: torch.utils.data.DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Compute the RMSE you’d get if you always forecast “0” for every time step.\n",
    "\n",
    "    This tells you how much a trivial model (predicting zero change) would err,\n",
    "    giving you a baseline to beat.\n",
    "\n",
    "    Args:\n",
    "        val_loader: yields tuples (x_day, y_day, weekday) where\n",
    "            x_day : Tensor shape (1, W, look_back, F)\n",
    "            y_day : Tensor shape (1, W)\n",
    "            weekday: Python int (0=Mon … 6=Sun)\n",
    "\n",
    "    Returns:\n",
    "        rmse: float, the root‐mean‐square‐error of predicting zero for all steps\n",
    "    \"\"\"\n",
    "    total_se = 0.0  # sum of squared errors across all validation samples\n",
    "    total_n  = 0    # total number of samples\n",
    "\n",
    "    for x_day, y_day, _weekday in val_loader:\n",
    "        # y_day coming in shape (1, W); squeeze to (W,)\n",
    "        y = y_day.squeeze(0).view(-1)  \n",
    "        # accumulate sum of squares ( (0 - y)^2 == y^2 )\n",
    "        total_se += float((y ** 2).sum().item())\n",
    "        # accumulate sample count\n",
    "        total_n  += y.numel()\n",
    "\n",
    "    # RMSE = sqrt( sum(errors^2) / N )\n",
    "    return math.sqrt(total_se / total_n)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Build DataLoaders over calendar‐days\n",
    "# -----------------------------------------------------------------------------\n",
    "train_loader, val_loader, test_loader = stockanalibs.split_to_day_datasets(\n",
    "    # Training split arrays (from chronological_split)\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    # Validation split arrays\n",
    "    X_val, y_val, day_id_val,\n",
    "    # Test split arrays + raw prices for post‐tracking\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    # Original minute‐bar DataFrame for weekday mapping\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Instantiate the stateful DualMemoryLSTM & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "model = stockanalibs.DualMemoryLSTM(\n",
    "    n_feats       = N_FEATS,        # number of input features per minute\n",
    "    short_units   = SHORT_UNITS,    # hidden size of daily LSTM\n",
    "    long_units    = LONG_UNITS,     # hidden size of weekly LSTM\n",
    "    dropout_short = DROPOUT_SHORT,  # dropout after daily LSTM\n",
    "    dropout_long  = DROPOUT_LONG,   # dropout after weekly LSTM\n",
    "    weight_dropout= WEIGHT_DROPOUT # dropping their hidden-to-hidden weights\n",
    ")\n",
    "model.to(device)   # place model parameters on GPU or CPU as specified\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Compute plateau_sched timing parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# Total training samples = total windows in X_tr (one window per row)\n",
    "n_train_samples = X_tr.shape[0]\n",
    "\n",
    "# How many optimizer steps (day‐bundles) constitute one epoch?\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Build optimizer, LR scheduler, AMP scaler, and gradient‐clip norm\n",
    "# -----------------------------------------------------------------------------\n",
    "optimizer, plateau_sched, cosine_sched, scaler, clipnorm = make_optimizer_and_scheduler(\n",
    "    model,\n",
    "    initial_lr         = INITIAL_LR,         # starting learning rate, e.g. 3e-4\n",
    "    weight_decay       = WEIGHT_DECAY,       # L2 penalty on all weights\n",
    "    lr_reduce_factor   = PLATEAU_FACTOR,     # multiply LR by IT on plateau\n",
    "    lr_patience        = PLATEAU_PATIENCE,   # epochs with no val‐improve before reduce\n",
    "    lr_min             = MIN_LR,             # lower bound on LR\n",
    "    clipnorm           = CLIPNORM            # max gradient‐norm before clipping\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Count how many calendar days we see each epoch\n",
    "# -----------------------------------------------------------------------------\n",
    "n_train_days = len(train_loader.dataset)  # dataset length = # unique days\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Compute baseline RMSE on validation (zero forecast)\n",
    "# -----------------------------------------------------------------------------\n",
    "baseline_val_rmse = naive_rmse(val_loader)\n",
    "print(f\"Baseline (zero‐forecast) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    cosine_sched        = cosine_sched,\n",
    "    plateau_sched       = plateau_sched,\n",
    "    scaler              = scaler,\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = MAX_EPOCHS,\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "    baseline_val_rmse   = baseline_val_rmse,\n",
    "    clipnorm            = clipnorm,\n",
    "    device              = device      # torch.device('cuda') or 'cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) Final reporting: best RMSE and relative improvement\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "\n",
    "improvement = 100.0 * (1.0 - best_val_rmse / baseline_val_rmse)\n",
    "print(f\"Improvement over zero‐baseline = {improvement:5.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40272296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e0cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8660dd-d2db-434a-aa59-17814d343fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
