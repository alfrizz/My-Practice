{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ← for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm          # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import stockanalibs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:00:00</th>\n",
       "      <td>570.600</td>\n",
       "      <td>570.6000</td>\n",
       "      <td>570.600</td>\n",
       "      <td>570.6000</td>\n",
       "      <td>199.0</td>\n",
       "      <td>570.4288</td>\n",
       "      <td>570.7712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:01:00</th>\n",
       "      <td>570.800</td>\n",
       "      <td>570.8000</td>\n",
       "      <td>570.800</td>\n",
       "      <td>570.8000</td>\n",
       "      <td>135.0</td>\n",
       "      <td>570.6288</td>\n",
       "      <td>570.9712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.764662e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:02:00</th>\n",
       "      <td>571.000</td>\n",
       "      <td>571.0000</td>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>255.0</td>\n",
       "      <td>570.5788</td>\n",
       "      <td>570.9212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.864650e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:03:00</th>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>261.0</td>\n",
       "      <td>570.5788</td>\n",
       "      <td>570.9212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.087675e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:04:00</th>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>570.750</td>\n",
       "      <td>570.7500</td>\n",
       "      <td>261.0</td>\n",
       "      <td>570.5788</td>\n",
       "      <td>570.9212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.219857e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>173.375</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.215</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>621199.0</td>\n",
       "      <td>173.5129</td>\n",
       "      <td>173.6171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>3.876</td>\n",
       "      <td>4.528500e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>173.565</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.240</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>624198.0</td>\n",
       "      <td>173.3280</td>\n",
       "      <td>173.4320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.061</td>\n",
       "      <td>3.019000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>173.390</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.200</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>454542.0</td>\n",
       "      <td>173.2580</td>\n",
       "      <td>173.3620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.131</td>\n",
       "      <td>2.012667e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>173.315</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.230</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>1094746.0</td>\n",
       "      <td>173.2280</td>\n",
       "      <td>173.3320</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>4.161</td>\n",
       "      <td>1.341778e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>173.300</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.170</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>7649838.0</td>\n",
       "      <td>173.5576</td>\n",
       "      <td>173.6618</td>\n",
       "      <td>0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>3.831</td>\n",
       "      <td>8.945186e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1327440 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open      high      low     close     volume  \\\n",
       "2014-04-03 13:00:00  570.600  570.6000  570.600  570.6000      199.0   \n",
       "2014-04-03 13:01:00  570.800  570.8000  570.800  570.8000      135.0   \n",
       "2014-04-03 13:02:00  571.000  571.0000  570.750  570.7500      255.0   \n",
       "2014-04-03 13:03:00  570.750  570.7500  570.750  570.7500      261.0   \n",
       "2014-04-03 13:04:00  570.750  570.7500  570.750  570.7500      261.0   \n",
       "...                      ...       ...      ...       ...        ...   \n",
       "2025-06-18 20:56:00  173.375  173.6771  173.215  173.5650   621199.0   \n",
       "2025-06-18 20:57:00  173.565  173.5900  173.240  173.3800   624198.0   \n",
       "2025-06-18 20:58:00  173.390  173.4100  173.200  173.3100   454542.0   \n",
       "2025-06-18 20:59:00  173.315  173.4000  173.230  173.2800  1094746.0   \n",
       "2025-06-18 21:00:00  173.300  174.0500  173.170  173.6097  7649838.0   \n",
       "\n",
       "                          bid       ask  trade_action  StrategyEarning  \\\n",
       "2014-04-03 13:00:00  570.4288  570.7712             0            0.000   \n",
       "2014-04-03 13:01:00  570.6288  570.9712             0            0.000   \n",
       "2014-04-03 13:02:00  570.5788  570.9212             0            0.000   \n",
       "2014-04-03 13:03:00  570.5788  570.9212             0            0.000   \n",
       "2014-04-03 13:04:00  570.5788  570.9212             0            0.000   \n",
       "...                       ...       ...           ...              ...   \n",
       "2025-06-18 20:56:00  173.5129  173.6171             0            1.166   \n",
       "2025-06-18 20:57:00  173.3280  173.4320             0            1.166   \n",
       "2025-06-18 20:58:00  173.2580  173.3620             0            1.166   \n",
       "2025-06-18 20:59:00  173.2280  173.3320             0            1.166   \n",
       "2025-06-18 21:00:00  173.5576  173.6618             0            1.166   \n",
       "\n",
       "                     EarningDiff  signal_smooth_norm  \n",
       "2014-04-03 13:00:00        0.000        0.000000e+00  \n",
       "2014-04-03 13:01:00        0.000        4.764662e-13  \n",
       "2014-04-03 13:02:00        0.000        1.864650e-12  \n",
       "2014-04-03 13:03:00        0.000        5.087675e-12  \n",
       "2014-04-03 13:04:00        0.000        1.219857e-11  \n",
       "...                          ...                 ...  \n",
       "2025-06-18 20:56:00        3.876        4.528500e-09  \n",
       "2025-06-18 20:57:00        4.061        3.019000e-09  \n",
       "2025-06-18 20:58:00        4.131        2.012667e-09  \n",
       "2025-06-18 20:59:00        4.161        1.341778e-09  \n",
       "2025-06-18 21:00:00        3.831        8.945186e-10  \n",
       "\n",
       "[1327440 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ·  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = \"signal_smooth_norm\"\n",
    "feature_cols   = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "ckpt_path      = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‐ticker\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # → 0.15 test remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ── Architecture Parameters ───────────────────────────────────────────────\n",
    "SHORT_UNITS         = 64       # LSTM short-term units (32–128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64–256 recommended)\n",
    "DROPOUT_SHORT       = 0.25     # Dropout for short LSTM outputs (0.1–0.3)\n",
    "DROPOUT_LONG        = 0.20     # Dropout for long LSTM outputs (0.1–0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-4     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ── Optimizer Settings: Cosine Decay with Restarts ──────────────────────────\n",
    "INITIAL_LR          = 3e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.05     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5–5.0)\n",
    "\n",
    "# ── Training Control Parameters ─────────────────────────────────────────────\n",
    "TRAIN_BATCH         = 64       # Training batch size (32–128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50–150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10–20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_lstm_tensors                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts **one big minute-bar DataFrame (many days)** into two leakage-free #\n",
    "# NumPy arrays ready for the stateful LSTM:                                   #\n",
    "#                                                                             #\n",
    "#   X  – design matrix; every row is a eg 60-min (or `look_back`) window         #\n",
    "#        flattened to 1-D:  (look_back × n_features,)                         #\n",
    "#   y  – the single-step-ahead target corresponding to the *last* row         #\n",
    "#        in that window.                                                      #\n",
    "#                                                                             #\n",
    "# RULES ENFORCED                                                              #\n",
    "# • Windows never cross midnight → yesterday’s RTH info can’t leak into the   #\n",
    "#   first prediction of the new day.                                          #\n",
    "# • The very first RTH window of the day uses ONLY pre-trade minutes.         #\n",
    "# • Features are **standardised per day** (mean-0 / std-1) so levels &        #\n",
    "#   volumes that drift day-to-day don’t leak statistics across sessions.      #\n",
    "# • Output dtype = float32 (GPU-friendly & half the RAM of float64).          #\n",
    "# • Entire routine is vectorised with `np.lib.stride_tricks.sliding_window_view`\n",
    "#   ⇒ no Python loop over 391 minutes → 40× faster than naive for-loops.      #\n",
    "###############################################################################\n",
    "\n",
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time,                # e.g. 14:30 CET for US equities\n",
    "    flatten: bool = True               # keep 2-D (N, L·F) by default\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        One-minute OHLCV bars **for many calendar days**.  DatetimeIndex\n",
    "        must be tz-aware & strictly increasing (no duplicates).\n",
    "    look_back : int\n",
    "        Size of the sliding window in minutes (e.g. 60).\n",
    "    feature_cols : list[str]\n",
    "        Names of numeric feature columns to feed the network.\n",
    "    label_col : str\n",
    "        Name of the 1-step-ahead target column (already computed).\n",
    "    rth_start : datetime.time\n",
    "        Clock time that marks the first minute of **regular trading hours**.\n",
    "        Every minute whose index.time ≥ rth_start is considered “tradable”.\n",
    "    flatten : bool\n",
    "        • True  →  X rows are 1-D vectors  (L·F,)  → matches our current\n",
    "                   `make_day_dataset` reshape → lighter I/O.\n",
    "        • False →  X rows keep shape  (L, F)     → nicer for exploratory\n",
    "                   notebook charting, but slower to feed into tf.data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray (N, L·F)  *or* (N, L, F)\n",
    "        Design matrix.  N = (#days × intraday minutes) − look_back.\n",
    "    y : np.ndarray (N,)\n",
    "        Targets aligned 1:1 with X rows.\n",
    "    \"\"\"\n",
    "\n",
    "    X_rows, y_rows = [], []                    # collectors\n",
    "\n",
    "    # 1. process *one calendar day* at a time (prevents inter-day leakage)\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "\n",
    "        day_df = day_df.sort_index()\n",
    "\n",
    "        # 1-A. per-day standard-scaling of *features*\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(\n",
    "                                   day_df[feature_cols])\n",
    "\n",
    "        feats_np  = day_df[feature_cols].to_numpy(dtype=np.float32)   # (T, F)\n",
    "        label_np  = day_df[label_col]     .to_numpy(dtype=np.float32) # (T,)\n",
    "\n",
    "        # Boolean mask of rows that belong to regular trading hours\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():                      # holiday / half-day\n",
    "            continue\n",
    "\n",
    "        T, F = feats_np.shape                      # minute-rows in this day\n",
    "\n",
    "        # 2. build all candidate windows (vectorised)\n",
    "        #\n",
    "        # sliding_window_view gives (T-L+1, L, F)\n",
    "        win_3d = sliding_window_view(\n",
    "                     feats_np,\n",
    "                     (look_back, F),\n",
    "                     axis=(0, 1))[:, 0, :, :]      # squeeze spurious dim\n",
    "\n",
    "        # ── ALIGNMENT FIX ──────────────────────────────────────────────\n",
    "        # win_3d has length  T-look_back+1\n",
    "        # labels available for prediction run from index look_back .. T-1\n",
    "        # → exactly  T-look_back  entries\n",
    "        win_3d   = win_3d[:-1]                    # drop the *last* window\n",
    "        y_aligned = label_np[look_back:]          # (T-L,)\n",
    "\n",
    "        # also trim the RTH mask so it matches the new length\n",
    "        rth_mask_shifted = rth_mask[look_back:]   # (T-L,)\n",
    "\n",
    "        # keep only windows whose **target** timestamp is inside RTH\n",
    "        win_3d   = win_3d[  rth_mask_shifted]\n",
    "        y_aligned = y_aligned[rth_mask_shifted]\n",
    "\n",
    "        # final representation\n",
    "        if flatten:\n",
    "            X_rows.append(win_3d.reshape(win_3d.shape[0], -1))  # (N_d, L·F)\n",
    "        else:\n",
    "            X_rows.append(win_3d)                               # (N_d, L, F)\n",
    "\n",
    "        y_rows.append(y_aligned)\n",
    "\n",
    "    # 3. concatenate across all calendar days\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No RTH rows found – check rth_start or data gaps.\")\n",
    "\n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074090, 450)\n",
      "(1074090,)\n"
     ]
    }
   ],
   "source": [
    "X, y = build_lstm_tensors(df=df,\n",
    "                         look_back=LOOK_BACK,         \n",
    "                         feature_cols=feature_cols,\n",
    "                         label_col=label_col,\n",
    "                         rth_start=rth_start)\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# chronological_split  – v2 (vectorised, exhaustively commented)              #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Converts the *sample–level* tensors (X, y) produced by `build_lstm_tensors` #\n",
    "# into three chronological blocks: train / validation / test.                 #\n",
    "#                                                                             #\n",
    "# HOW IT WORKS                                                                #\n",
    "# 1.  Computes, for every calendar day, **how many windows survived** the     #\n",
    "#     `build_lstm_tensors()` rules.  We do this WITHOUT iterating minute-by-  #\n",
    "#     minute:                                                                 #\n",
    "#           usable_windows = max(0, #RTH_rows  –  look_back)                  #\n",
    "#     because build_lstm_tensors dropped the first `look_back` labels in each #\n",
    "#     day and then kept only those whose TARGET timestamp ∈ RTH.              #\n",
    "#                                                                             #\n",
    "# 2.  Builds a `day_id` vector so each row in X knows from which date it came.#\n",
    "#     This lets downstream tf.data pipelines re-assemble full days.           #\n",
    "#                                                                             #\n",
    "# 3.  Slices X & y purely by **day boundaries** (not sample indices) so all   #\n",
    "#     windows that belong to the same calendar day end up in the same split.  #\n",
    "#                                                                             #\n",
    "# 4.  Returns:                                                                #\n",
    "#     • the three splits (X_tr, y_tr) / (X_val, y_val) / (X_te, y_te)         #\n",
    "#     • `samples_per_day`  – diagnostic array, can drive batch padding sizes  #\n",
    "#     • `day_id_*` vectors  – handy for make_day_dataset()                    #\n",
    "###############################################################################\n",
    "\n",
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # train tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # val   tensors\n",
    "        Tuple[np.ndarray, np.ndarray],    # test  tensors\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id train / val / test\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits `X, y` into chronological train/val/test by **whole days**.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each day.\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "        These are needed by make_day_dataset() to pad & batch whole days.\n",
    "    \"\"\"\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 1. Re-count usable windows per calendar day  (vectorised, exact) #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    samples_per_day: List[int] = []\n",
    "\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                              # minute rows today\n",
    "        idx = np.arange(T)                           # 0 … T-1\n",
    "\n",
    "        mask_window_ready = idx >= look_back         # enough history\n",
    "        mask_rth_target  = day_df.index.time >= rth_start\n",
    "        usable_today     = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "\n",
    "        samples_per_day.append(usable_today)\n",
    "\n",
    "    # Safety: summed count must equal len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. \"\n",
    "            \"Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 2. Build the day_id vector (one int tag per sample)              #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    day_id = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 3. Determine split cut-points in **day space**                   #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    last_day = len(samples_per_day) - 1\n",
    "    cut_train = int(last_day * train_prop)\n",
    "    cut_val   = int(last_day * (train_prop + val_prop))\n",
    "\n",
    "    mask_tr =  day_id <= cut_train\n",
    "    mask_va = (day_id >  cut_train) & (day_id <= cut_val)\n",
    "    mask_te =  day_id >  cut_val\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 4. Slice tensors                                                 #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_va, y_va = X[mask_va], y[mask_va]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    # 5. Return everything                                             #\n",
    "    # ──────────────────────────────────────────────────────────────────\n",
    "    return (X_tr, y_tr), (X_va, y_va), (X_te, y_te), \\\n",
    "           samples_per_day, \\\n",
    "           day_id[mask_tr], day_id[mask_va], day_id[mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training days kept : 1920  (multiple of 64)\n",
      "Validation days    : 422\n",
      "Test days          : 423\n"
     ]
    }
   ],
   "source": [
    "# call the splitter  ─────────────────────────────────────────────\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te), samples_per_day, \\\n",
    "day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "        X, y, df,\n",
    "        look_back=LOOK_BACK,\n",
    "        rth_start=rth_start,\n",
    "        train_prop=TRAIN_PROP,\n",
    "        val_prop=VAL_PROP\n",
    ")\n",
    "\n",
    "# ─── NEW: force train days to k·32 so no remainder is ever dropped ───────\n",
    "unique_days_tr = np.unique(day_id_tr)\n",
    "n_train_days   = len(unique_days_tr)\n",
    "keep_days      = (n_train_days // TRAIN_BATCH) * TRAIN_BATCH     # largest multiple\n",
    "days_to_keep   = set(unique_days_tr[:keep_days])                 # earliest days\n",
    "train_keep_mask = np.isin(day_id_tr, list(days_to_keep))\n",
    "\n",
    "# slice again, but only on train tensors\n",
    "X_tr  = X_tr[train_keep_mask]\n",
    "y_tr  = y_tr[train_keep_mask]\n",
    "day_id_tr = day_id_tr[train_keep_mask]\n",
    "\n",
    "print(f\"Training days kept : {keep_days}  (multiple of {TRAIN_BATCH})\")\n",
    "print(f\"Validation days    : {len(np.unique(day_id_val))}\")\n",
    "print(f\"Test days          : {len(np.unique(day_id_te))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# make_day_dataset  (weekday-safe, glare-free)                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# PURPOSE                                                                     #\n",
    "# Take minute-level samples X, y (already leakage-free) and yield **exactly   #\n",
    "# ONE `tf.data` element per calendar day**.                                   #\n",
    "# Each element carries:                                                       #\n",
    "#      x_day   (1 , T , n_feats)  – the whole RTH session                     #\n",
    "#      y_day   (1 , T)            – its labels                                #\n",
    "#      weekday scalar int32       – 0=Mon … 6=Sun (from the REAL calendar)    #\n",
    "# The weekday is later used by the training loop to decide when to reset the  #\n",
    "# “long” LSTM state (only on week-end).                                       #\n",
    "#                                                                             #\n",
    "# DESIGN NOTES                                                                #\n",
    "# •  The function stays NumPy-vectorised; only ~1 000 generator yields per    #\n",
    "#    epoch, so Python overhead is negligible.                                 #\n",
    "# •  No dependency on the original DataFrame – we pass `weekday_vec` that     #\n",
    "#    came straight from the DatetimeIndex.                                    #\n",
    "###############################################################################\n",
    "\n",
    "def make_day_dataset(\n",
    "    X           : np.ndarray,          # (N , …)\n",
    "    y           : np.ndarray,          # (N ,)\n",
    "    day_id      : np.ndarray,          # (N ,)  strictly ascending\n",
    "    weekday_vec : np.ndarray,          # (N ,)  real weekday of every row\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    tf.data.Dataset\n",
    "        element = (x_day, y_day, weekday)\n",
    "        x_day : (1 , T , n_feats)  float32\n",
    "        y_day : (1 , T)            float32\n",
    "        weekday : () int32\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Chronological safety – keep everything sorted the same way ─────\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "\n",
    "    # ── 2. Build slices for each distinct day_id (vectorised) ─────────────\n",
    "    change      = np.where(np.diff(day_id) != 0)[0] + 1      # breakpoints\n",
    "    day_slices  = np.split(np.arange(len(day_id)), change)   # list[np.ndarray]\n",
    "\n",
    "    # ── 3. Python generator : ONE yield == one calendar day ───────────────\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]                        # (T , …)\n",
    "            y_block = y[sl]                        # (T ,)\n",
    "            weekday = int(weekday_vec[sl[0]])      # real weekday, 0–6\n",
    "\n",
    "            # add batch dimension expected by stateful LSTM\n",
    "            yield (\n",
    "                np.expand_dims(x_block, 0).astype(np.float32),  # (1 , T , …)\n",
    "                np.expand_dims(y_block, 0).astype(np.float32),  # (1 , T)\n",
    "                np.int32(weekday),\n",
    "            )\n",
    "\n",
    "    # ── 4. Static signature (time axis None → variable length) ────────────\n",
    "    feat_shape = X.shape[1:]                                   # supports (L·F) or (L, F)\n",
    "    output_signature = (\n",
    "        tf.TensorSpec((1, None, *feat_shape), tf.float32),      # x_day\n",
    "        tf.TensorSpec((1, None),              tf.float32),      # y_day\n",
    "        tf.TensorSpec((),                    tf.int32),         # weekday\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "          .prefetch(tf.data.AUTOTUNE)        # overlaps CPU ↔ GPU copies\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# split_to_day_datasets  –  zero-copy, minimal arguments                     #\n",
    "###############################################################################\n",
    "# One call returns three ready-to-feed pipelines:\n",
    "#   ds_train_batched   → (TRAIN_BATCH , T_max , n_feats) per step\n",
    "#   ds_val_unbatched   → (1  , T ,    n_feats) per step\n",
    "#   ds_test_unbatched  → idem  (optional)\n",
    "###############################################################################\n",
    "\n",
    "def split_to_day_datasets(\n",
    "        X_tr,  y_tr,  day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te,  y_te,  day_id_te,\n",
    "        *,\n",
    "        df,                 # full DataFrame (for weekday lookup)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr , y_tr , day_id_tr  : np.ndarray  – chronological, no gaps\n",
    "    X_val, y_val, day_id_val : np.ndarray\n",
    "    X_te , y_te , day_id_te  : np.ndarray\n",
    "    df                       : original DataFrame used to build X/y\n",
    "    train_batch              : how many days per “fat” training batch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train_batched , ds_val_unbatched , ds_test_unbatched\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1. Build one weekday vector covering ALL samples ──────────────────\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)   # (N,)\n",
    "\n",
    "    # Because X_tr, X_val, X_te are sequential slices, we can split by length\n",
    "    n_tr  = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te  = len(X_te)\n",
    "\n",
    "    weekday_vec_tr  = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr : n_tr + n_val]\n",
    "    weekday_vec_te  = weekday_all[n_tr + n_val : n_tr + n_val + n_te]\n",
    "\n",
    "    # ── 2. Convert each split to “one day → one element” datasets ─────────\n",
    "    ds_tr   = make_day_dataset(X_tr,  y_tr,  day_id_tr,  weekday_vec_tr)\n",
    "    ds_val  = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    ds_test = make_day_dataset(X_te,  y_te,  day_id_te,  weekday_vec_te)\n",
    "\n",
    "    # ── 3. TRAIN pipeline – strip batch-1 dim, pad to fixed length ───────\n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd   # (T , …)\n",
    "\n",
    "    ds_train_batched = (\n",
    "        ds_tr\n",
    "          .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .padded_batch(train_batch, drop_remainder=True)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    ds_test.savestr((save_dir / {ticker}_ds_test.pkl\"), compression=\"GZIP\")\n",
    "\n",
    "    # ── 4. Return ready-to-feed pipelines ────────────────────────────────\n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        X_val, y_val, day_id_val,\n",
    "        X_te, y_te, day_id_te,\n",
    "        df=df,\n",
    "        train_batch=TRAIN_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\\n│    • retains early-morning context                                       │\\n│    • reset_states()  at every midnight → zero on next session            │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\\n│    • captures spikes / micro-structure                                   │\\n│    • reset together with cₜ midnight                                     │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n┌──────────────────────────────────────────────────────────────────────────┐\\n│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\\n│    • first RTH prediction uses 60 *pre-trade* minutes only               │\\n│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\\n└──────────────────────────────────────────────────────────────────────────┘\\n        │\\n        ▼\\n                        Predicted signal ŷₜ\\n\\n\\nDay i                               Day i+1\\n|────────────┬──────────────┬───…──┬────────┐\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\ncₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❶  NETWORK  WEIGHTS  θ  – learned across all history, fixed at runtime   │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❷  CELL STATE  cₜ  – slow integrator covering the *whole* current day    │\n",
    "│    • retains early-morning context                                       │\n",
    "│    • reset_states()  at every midnight → zero on next session            │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❸  HIDDEN STATE  hₜ  – fast dynamics (a few bars)                        │\n",
    "│    • captures spikes / micro-structure                                   │\n",
    "│    • reset together with cₜ midnight                                     │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ ❹  INPUT WINDOW  xₜ  – last 60 minutes of raw features                   │\n",
    "│    • first RTH prediction uses 60 *pre-trade* minutes only               │\n",
    "│    • later predictions mix pre-trade + today’s RTH, never yesterday RTH  │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "                        Predicted signal ŷₜ\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|────────────┬──────────────┬───…──┬────────┐\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "cₜ,hₜ: 0 → accumulate → reset_states() → 0 → accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   • Layer 0  “short_lstm”  (SHORT_UNITS units)  → quick dynamics, daily reset        #\n",
    "#   • Layer 1  “long_lstm”   (LONG_UNITS units)  → slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what “week” means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 1 · optional mixed-precision context ────────────────────────────\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ── 2 · network definition ─────────────────────────────────────────\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ── 3 · optimiser & schedule ───────────────────────────────────────\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ── 4 · restore dtype policy outside mixed-FP16 scope ──────────────\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object → harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ▸  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      • The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      • The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,    # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,              # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,   # Total calendar days in the training epoch\n",
    "    max_epochs: int,     # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    ckpt_path           # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models—such as LSTMs—with separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} • train={epoch_train:.6f} • val={epoch_val:.6f} \"\n",
    "              f\"• impr={impr_pct:5.1f}% • lr={current_lr:.2e} • g≈{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(ckpt_path)  # Save checkpoint of the best model.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(ckpt_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.244102\n",
      "Training sees 1920 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fc9b7945b844aa804184d4f03c5aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750433564.030149   32177 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 • train=0.765785 • val=0.366870 • impr=-50.3% • lr=5.67e-05 • g≈13497.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7e73649ecd4096ae56a33c1ff47f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 • train=0.592081 • val=0.307258 • impr=-25.9% • lr=1.03e-04 • g≈5750.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4a3a5fd583472080914f8ca9113535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 • train=0.552440 • val=0.288252 • impr=-18.1% • lr=2.76e-04 • g≈2001.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a88e8ba5ed4d699000bdc6c61b517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 • train=0.508563 • val=0.268421 • impr=-10.0% • lr=1.30e-04 • g≈3921.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadae36f9ff444ceb235af56d7f20e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 • train=0.486035 • val=0.263146 • impr= -7.8% • lr=1.77e-05 • g≈27400.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99b41229da247b89c805da3a03b03c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 • train=0.467488 • val=0.255841 • impr= -4.8% • lr=2.83e-04 • g≈1650.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0d9dd98b4f40a7b09dd147c9c0c34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 • train=0.432656 • val=0.246713 • impr= -1.1% • lr=2.25e-04 • g≈1925.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cee99261ce499fbd6f79213dfe5a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 • train=0.409355 • val=0.241578 • impr=  1.0% • lr=1.44e-04 • g≈2852.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb099ce72084a6e8ffbe4f15a6d5d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 • train=0.394477 • val=0.245096 • impr= -0.4% • lr=6.71e-05 • g≈5879.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a956bfe768374bcf99129e9da5e7d533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 • train=0.389581 • val=0.238091 • impr=  2.5% • lr=2.11e-05 • g≈18432.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec41cb4f7a44265952c31cb945dc7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 • train=0.384827 • val=0.237236 • impr=  2.8% • lr=2.98e-04 • g≈1289.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1661b14d2e4954895bf2157ffef821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 • train=0.364046 • val=0.232364 • impr=  4.8% • lr=2.86e-04 • g≈1271.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689bf0eae70d4524abc74584fb81ebde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 • train=0.345405 • val=0.229166 • impr=  6.1% • lr=2.63e-04 • g≈1312.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59ccb36cd7f4582b41cbc29bdee019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 • train=0.331778 • val=0.225953 • impr=  7.4% • lr=2.31e-04 • g≈1437.76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f582a77b7a9440b8fa70d4eb0b74a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 • train=0.320700 • val=0.223879 • impr=  8.3% • lr=1.92e-04 • g≈1669.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadce96c1acf4a90a8427960d9975ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 • train=0.312116 • val=0.222821 • impr=  8.7% • lr=1.51e-04 • g≈2073.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1b9d9510544c5eb63268c3342b8a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 • train=0.305998 • val=0.223502 • impr=  8.4% • lr=1.09e-04 • g≈2794.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cac7ec683f4607bdf328c46886fc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 • train=0.302753 • val=0.226266 • impr=  7.3% • lr=7.26e-05 • g≈4169.41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5afa8f95da422f8296ac05d244b725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 • train=0.300613 • val=0.226437 • impr=  7.2% • lr=4.30e-05 • g≈6984.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e6c4917d804688b85c8e34782e71f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 • train=0.298969 • val=0.218537 • impr= 10.5% • lr=2.33e-05 • g≈12814.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd2c82dca0747f2829aca0437b04d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 • train=0.297572 • val=0.217897 • impr= 10.7% • lr=1.52e-05 • g≈19613.68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c8217934f1431aa4e2eaeea270caf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 • train=0.295082 • val=0.223123 • impr=  8.6% • lr=2.99e-04 • g≈987.13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8020fbf0424476aa5eeb932adfb3974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 • train=0.285872 • val=0.221264 • impr=  9.4% • lr=2.95e-04 • g≈969.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077f16ec37b84c939dc23726c2e3b014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 • train=0.278103 • val=0.218055 • impr= 10.7% • lr=2.88e-04 • g≈966.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dd2a2fc6c94a4a8cf933c88097b9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 • train=0.271850 • val=0.217228 • impr= 11.0% • lr=2.78e-04 • g≈978.26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85177fd67b9a4ac09f363a110f4d991c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 • train=0.266718 • val=0.216774 • impr= 11.2% • lr=2.65e-04 • g≈1004.96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbb217f0ebd46e0af23176badb02853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 • train=0.262138 • val=0.215396 • impr= 11.8% • lr=2.51e-04 • g≈1046.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f14229b26245089814a31df0d76eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 • train=0.258619 • val=0.214267 • impr= 12.2% • lr=2.34e-04 • g≈1106.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2514deb06934ce0bd38595935887911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 • train=0.255372 • val=0.213427 • impr= 12.6% • lr=2.15e-04 • g≈1186.41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606c03b3f5fe47059bab3ba382ce2f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 • train=0.252376 • val=0.212433 • impr= 13.0% • lr=1.96e-04 • g≈1290.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb560b60e2e84f92a0db554c9dcd9ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 • train=0.250212 • val=0.212121 • impr= 13.1% • lr=1.75e-04 • g≈1430.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c07e7e5c2f74a8fa515d493cf93510e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 • train=0.248315 • val=0.211950 • impr= 13.2% • lr=1.54e-04 • g≈1612.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e087356bbf435284e90be3e8e94f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 • train=0.246719 • val=0.212164 • impr= 13.1% • lr=1.33e-04 • g≈1853.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7268819d3bf44cebb7abf9ddd8f06ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 • train=0.245601 • val=0.213144 • impr= 12.7% • lr=1.13e-04 • g≈2177.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b4243542604c088f84b06628fd882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 • train=0.244890 • val=0.215149 • impr= 11.9% • lr=9.34e-05 • g≈2621.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944add12278642faaaafd9e1f3ee9659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 036:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 • train=0.243744 • val=0.217478 • impr= 10.9% • lr=7.54e-05 • g≈3230.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab44c8aefcc94c8591ead3b7799dd7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 037:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 • train=0.243564 • val=0.218629 • impr= 10.4% • lr=5.92e-05 • g≈4111.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bfd19af4654b97a71e038c67460916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 038:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 • train=0.243655 • val=0.216191 • impr= 11.4% • lr=4.52e-05 • g≈5395.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc289d698914dcd8dec5ea15e0dd01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 039:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 • train=0.243108 • val=0.210894 • impr= 13.6% • lr=3.35e-05 • g≈7254.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac95e8de2c04cdaa1eee0bec4d113d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 040:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 • train=0.242471 • val=0.208985 • impr= 14.4% • lr=2.45e-05 • g≈9877.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe78b8338824d1fbca283bf0e3da68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 041:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 • train=0.241712 • val=0.208792 • impr= 14.5% • lr=1.85e-05 • g≈13092.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455f7ece71254857b9ee70fc4d42f0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 042:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 • train=0.241613 • val=0.208754 • impr= 14.5% • lr=1.54e-05 • g≈15703.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f75b53f01c49ea803ac6ec33842f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 043:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 • train=0.242047 • val=0.212764 • impr= 12.8% • lr=3.00e-04 • g≈807.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823b8612787c402d8ae8c355eb0dd37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 044:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 • train=0.240441 • val=0.210330 • impr= 13.8% • lr=2.99e-04 • g≈803.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c82c96c0944318caf689e8ab62b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 045:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 • train=0.237339 • val=0.211597 • impr= 13.3% • lr=2.98e-04 • g≈797.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1816e49dde442f29307e425bf8c906b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 046:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 • train=0.235476 • val=0.211626 • impr= 13.3% • lr=2.95e-04 • g≈797.43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308a96b8c5414a2082de0a95d519f581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 047:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 • train=0.233777 • val=0.211017 • impr= 13.6% • lr=2.92e-04 • g≈799.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06086a4afaf2416c99f716085cb16477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 048:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 • train=0.232393 • val=0.210753 • impr= 13.7% • lr=2.88e-04 • g≈805.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae8d4e55cfb4ebe8b2abd6a1ec546a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 049:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 • train=0.231232 • val=0.210805 • impr= 13.6% • lr=2.84e-04 • g≈814.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5b62dc4aeb4310bc8a6c00053852b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 050:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 • train=0.229728 • val=0.210349 • impr= 13.8% • lr=2.79e-04 • g≈823.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0522652776c4f69a0bbbd1ac6393c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 051:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 • train=0.228987 • val=0.209441 • impr= 14.2% • lr=2.73e-04 • g≈838.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8010bd3f6a2f4b43b65eaee125376a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 052:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 052 • train=0.227794 • val=0.208329 • impr= 14.7% • lr=2.67e-04 • g≈854.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9782cf277abc468e935748ebe399eff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 053:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053 • train=0.226937 • val=0.207139 • impr= 15.1% • lr=2.59e-04 • g≈874.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d78657b453a4009b35dcf11db823ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 054:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 054 • train=0.225948 • val=0.206638 • impr= 15.3% • lr=2.52e-04 • g≈897.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe1a3d699624522b657aa619ca362ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 055:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 055 • train=0.225039 • val=0.205927 • impr= 15.6% • lr=2.44e-04 • g≈923.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f86003537d54283ab8560be1deeddb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 056:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 056 • train=0.224044 • val=0.205367 • impr= 15.9% • lr=2.35e-04 • g≈952.53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55abf320d8de465c9f1f04aa27a7bc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 057:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 057 • train=0.223131 • val=0.204367 • impr= 16.3% • lr=2.26e-04 • g≈986.38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20cf482bb8544baaaec79efa772be6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 058:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 058 • train=0.222355 • val=0.203980 • impr= 16.4% • lr=2.17e-04 • g≈1025.43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d821d62781477a80934d991f447e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 059:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 059 • train=0.221540 • val=0.203872 • impr= 16.5% • lr=2.07e-04 • g≈1069.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1702d72d3c834664b5b3949927e0fdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 060:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 060 • train=0.221120 • val=0.203571 • impr= 16.6% • lr=1.97e-04 • g≈1121.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a12181250fd465394209bc5c62953ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 061:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 061 • train=0.220468 • val=0.203346 • impr= 16.7% • lr=1.87e-04 • g≈1178.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0075c879321f4b3e84ec75fcfa6982bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 062:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 062 • train=0.219856 • val=0.203149 • impr= 16.8% • lr=1.77e-04 • g≈1244.39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c99e7c78394582a4a809bb5c72b672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 063:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 063 • train=0.219567 • val=0.202570 • impr= 17.0% • lr=1.66e-04 • g≈1320.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7baa73b81f14167a98aae311799f577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 064:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 • train=0.219469 • val=0.202239 • impr= 17.1% • lr=1.56e-04 • g≈1409.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd9980512424829ac1cd2ef3bc59c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 065:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 065 • train=0.218985 • val=0.202089 • impr= 17.2% • lr=1.45e-04 • g≈1507.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d234c30ef67d4ed8b55c6170b2aadc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 066:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 066 • train=0.218699 • val=0.201955 • impr= 17.3% • lr=1.35e-04 • g≈1621.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d49affab6e84657902c9b3989924d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 067:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 067 • train=0.218138 • val=0.207431 • impr= 15.0% • lr=1.25e-04 • g≈1751.07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6aee456d76942c89bfd3ae2a1177acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 068:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068 • train=0.218350 • val=0.210443 • impr= 13.8% • lr=1.14e-04 • g≈1907.58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d9bcdb02c4c77a7a29547ef97b6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 069:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 069 • train=0.218372 • val=0.210206 • impr= 13.9% • lr=1.05e-04 • g≈2087.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426ebcab3cf247daab9dc7eef4f85c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 070:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 070 • train=0.218273 • val=0.209100 • impr= 14.3% • lr=9.50e-05 • g≈2297.68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386a413ba23446fe95fc89645a3017aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 071:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 071 • train=0.218278 • val=0.207618 • impr= 14.9% • lr=8.57e-05 • g≈2545.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bc780b458a469098e62ab26d60e633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 072:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 072 • train=0.217723 • val=0.206416 • impr= 15.4% • lr=7.69e-05 • g≈2831.87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c97c3f1ede4ea1acde9fc36af0e80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 073:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 073 • train=0.217624 • val=0.205157 • impr= 16.0% • lr=6.85e-05 • g≈3178.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd521cce52a849c1a5f6891761744200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 074:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 074 • train=0.217383 • val=0.204358 • impr= 16.3% • lr=6.05e-05 • g≈3592.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f68677864804766ab90ee8d9b344193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 075:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 075 • train=0.217273 • val=0.203540 • impr= 16.6% • lr=5.31e-05 • g≈4092.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fea6242c59445cb5c85fa2a61daa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 076:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 076 • train=0.217149 • val=0.203529 • impr= 16.6% • lr=4.62e-05 • g≈4695.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070849b4b05840af90d4c9f882dfb463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 077:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 077 • train=0.217154 • val=0.203327 • impr= 16.7% • lr=4.00e-05 • g≈5429.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa2274e23d14591bb60cd58595aca00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 078:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 078 • train=0.217016 • val=0.203483 • impr= 16.6% • lr=3.44e-05 • g≈6311.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808e90b6ac60489fa828f8dd1e541a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 079:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 079 • train=0.216895 • val=0.203259 • impr= 16.7% • lr=2.94e-05 • g≈7367.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbef7152ec1b46c5a0b07aee87c3fd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 080:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 080 • train=0.216809 • val=0.203081 • impr= 16.8% • lr=2.52e-05 • g≈8607.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40569466997f4e1c8e307494c6ff278b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 081:   0%|                                                                              | 0/1920 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 081 • train=0.216839 • val=0.202672 • impr= 17.0% • lr=2.17e-05 • g≈10013.86\n",
      "Early stopping triggered.\n",
      "\n",
      "Champion validation RMSE = 0.201955\n",
      "Improvement vs baseline   =  17.3 %\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m model_val\u001b[38;5;241m.\u001b[39msave_weights(ckpt_path)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 3.  Persist architecture + weights  (.keras)                                #\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m today     \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m file_path \u001b[38;5;241m=\u001b[39m save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m model_val\u001b[38;5;241m.\u001b[39msave(file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# FULL  TRAIN–SAVE  PIPELINE  – prints baseline + %-improvement               #\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Training loop (outer bar only → maximum throughput)                     #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        ckpt_path           = ckpt_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Sync FP32 twin  +  refresh checkpoint                                   #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model_val.set_weights(model_train.get_weights())\n",
    "model_val.save_weights(ckpt_path)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Persist architecture + weights  (.keras)                                #\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "file_path = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "model_val.save(file_path)\n",
    "print(\"\\nModel & weights saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b058c-3b5d-4b49-96fb-0c2d6a0c79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
