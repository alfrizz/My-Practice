{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da10cdb-d49c-4a4e-bd4a-d92b7be0fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae5f174-4751-495a-a9a9-0ccc3e5d35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:58:04.776730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750755484.854778    2851 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750755484.880255    2851 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750755484.945978    2851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750755484.946011    2851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750755484.946014    2851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750755484.946016    2851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'stockanalibs' from '/mnt/g/My Drive/Ingegneria/Data Science GD/My-Practice/my models/Trading/0.Stock Analysis/stockanalibs.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"   # 0 = all, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import pickle\n",
    "import contextlib\n",
    "\n",
    "import platform \n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, metrics, mixed_precision, regularizers\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, LearningRateSchedule\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt         # ‚Üê for progress curves\n",
    "from IPython.display import display, update_display, clear_output\n",
    "\n",
    "from tqdm.auto import tqdm             # progress bars that work in Jupyter\n",
    "from __future__ import annotations     # allows union types on Py <3.10\n",
    "\n",
    "import importlib\n",
    "import stockanalibs\n",
    "importlib.reload(stockanalibs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a7bd8-d68f-4f78-a4c2-f24be134d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.10.18 \n",
      "\n",
      "TF GPUs : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Torch   : 2.2.2+cu121 CUDA-OK: True\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # to deactivate gpu\n",
    "# tf.config.set_visible_devices([], 'GPU') # to deactivate gpu\n",
    "\n",
    "print(\"Python :\", platform.python_version(), \"\\n\")\n",
    "print(\"TF GPUs :\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Torch   :\", torch.__version__, \"CUDA-OK:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58faa285-737c-4b04-9b21-93672ec40d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>StrategyEarning</th>\n",
       "      <th>EarningDiff</th>\n",
       "      <th>signal_smooth_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:07:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:08:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>321.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:09:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:10:00</th>\n",
       "      <td>570.250</td>\n",
       "      <td>570.25</td>\n",
       "      <td>570.2500</td>\n",
       "      <td>570.25</td>\n",
       "      <td>298.0</td>\n",
       "      <td>570.0789</td>\n",
       "      <td>570.4211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 13:11:00</th>\n",
       "      <td>570.500</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.5000</td>\n",
       "      <td>570.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>570.3288</td>\n",
       "      <td>570.6712</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:49:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>174.02</td>\n",
       "      <td>173.8900</td>\n",
       "      <td>173.99</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>173.9378</td>\n",
       "      <td>174.0422</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:50:00</th>\n",
       "      <td>173.980</td>\n",
       "      <td>173.98</td>\n",
       "      <td>163.4137</td>\n",
       "      <td>173.92</td>\n",
       "      <td>524.0</td>\n",
       "      <td>173.8678</td>\n",
       "      <td>173.9722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:51:00</th>\n",
       "      <td>173.970</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8933</td>\n",
       "      <td>173.90</td>\n",
       "      <td>3898.0</td>\n",
       "      <td>173.8478</td>\n",
       "      <td>173.9522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:52:00</th>\n",
       "      <td>173.908</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8600</td>\n",
       "      <td>173.97</td>\n",
       "      <td>779.0</td>\n",
       "      <td>173.9178</td>\n",
       "      <td>174.0222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:53:00</th>\n",
       "      <td>173.960</td>\n",
       "      <td>174.00</td>\n",
       "      <td>173.8911</td>\n",
       "      <td>174.00</td>\n",
       "      <td>939.0</td>\n",
       "      <td>173.9478</td>\n",
       "      <td>174.0522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296484 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open    high       low   close  volume       bid  \\\n",
       "2014-04-03 13:07:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:08:00  570.500  570.50  570.5000  570.50   321.0  570.3288   \n",
       "2014-04-03 13:09:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:10:00  570.250  570.25  570.2500  570.25   298.0  570.0789   \n",
       "2014-04-03 13:11:00  570.500  570.50  570.5000  570.50   100.0  570.3288   \n",
       "...                      ...     ...       ...     ...     ...       ...   \n",
       "2025-06-18 20:49:00  173.980  174.02  173.8900  173.99  1604.0  173.9378   \n",
       "2025-06-18 20:50:00  173.980  173.98  163.4137  173.92   524.0  173.8678   \n",
       "2025-06-18 20:51:00  173.970  174.00  173.8933  173.90  3898.0  173.8478   \n",
       "2025-06-18 20:52:00  173.908  174.00  173.8600  173.97   779.0  173.9178   \n",
       "2025-06-18 20:53:00  173.960  174.00  173.8911  174.00   939.0  173.9478   \n",
       "\n",
       "                          ask  trade_action  StrategyEarning  EarningDiff  \\\n",
       "2014-04-03 13:07:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:08:00  570.6712             0              0.0        0.000   \n",
       "2014-04-03 13:09:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:10:00  570.4211             0              0.0        0.000   \n",
       "2014-04-03 13:11:00  570.6712             0              0.0        0.000   \n",
       "...                       ...           ...              ...          ...   \n",
       "2025-06-18 20:49:00  174.0422             0              0.0        1.395   \n",
       "2025-06-18 20:50:00  173.9722             0              0.0        1.465   \n",
       "2025-06-18 20:51:00  173.9522             0              0.0        1.485   \n",
       "2025-06-18 20:52:00  174.0222             0              0.0        1.415   \n",
       "2025-06-18 20:53:00  174.0522             0              0.0        1.385   \n",
       "\n",
       "                     signal_smooth_adjusted  \n",
       "2014-04-03 13:07:00                0.071363  \n",
       "2014-04-03 13:08:00                0.078887  \n",
       "2014-04-03 13:09:00                0.087231  \n",
       "2014-04-03 13:10:00                0.096444  \n",
       "2014-04-03 13:11:00                0.106627  \n",
       "...                                     ...  \n",
       "2025-06-18 20:49:00                0.000000  \n",
       "2025-06-18 20:50:00                0.000000  \n",
       "2025-06-18 20:51:00                0.000000  \n",
       "2025-06-18 20:52:00                0.000000  \n",
       "2025-06-18 20:53:00                0.000000  \n",
       "\n",
       "[1296484 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = stockanalibs.ticker\n",
    "\n",
    "df = pd.read_csv(f\"dfs training/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36dea1e-63f9-4443-998e-08d8f50a5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 0 ¬∑  DATA & PATHS                                                           #\n",
    "###############################################################################\n",
    "label_col      = stockanalibs.label_col\n",
    "feature_cols   = stockanalibs.feature_cols\n",
    "\n",
    "LOOK_BACK      = stockanalibs.look_back                                \n",
    "N_FEATS        = len(feature_cols) * LOOK_BACK     # final feature length\n",
    "\n",
    "today          = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "rth_start      = stockanalibs.regular_start\n",
    "\n",
    "save_dir       = Path(\"dfs training\")\n",
    "weights_path   = save_dir / f\"{ticker}_{today}.weights.h5\"   # auto-per‚Äêticker\n",
    "model_path     = save_dir / f\"{ticker}_{today}_model.keras\"\n",
    "\n",
    "# dataset split proportions\n",
    "TRAIN_PROP, VAL_PROP = 0.70, 0.15                 # ‚Üí 0.15 test remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f86df2-9bdc-4c22-8a50-9b42c842c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 ¬∑ MODEL HYPER-PARAMETERS (tuned defaults)\n",
    "###############################################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Architecture Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SHORT_UNITS         = 48       # LSTM short-term units (32‚Äì128 recommended)\n",
    "LONG_UNITS          = 128      # LSTM long-term units (64‚Äì256 recommended)\n",
    "DROPOUT_SHORT       = 0.30     # Dropout for short LSTM outputs (0.1‚Äì0.3)\n",
    "DROPOUT_LONG        = 0.25     # Dropout for long LSTM outputs (0.1‚Äì0.3)\n",
    "REC_DROP_SHORT      = 0.0      # Must be 0 for fused LSTM kernels\n",
    "REC_DROP_LONG       = 0.0      # Must be 0 for fused LSTM kernels\n",
    "KERNEL_REG          = 1e-3     # L2 regularization factor for LSTM layers\n",
    "\n",
    "# ‚îÄ‚îÄ Optimizer Settings: Cosine Decay with Restarts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "INITIAL_LR          = 5e-4     # Initial learning rate (1e-4 to 1e-3)\n",
    "FIRST_DECAY_EPOCHS  = 5        # Epochs before first decay\n",
    "T_MUL               = 2.0      # Cycle length multiplier\n",
    "M_MUL               = 1.0      # Cycle LR scaling factor after restarts\n",
    "ALPHA               = 0.03     # Minimum LR factor relative to INITIAL_LR\n",
    "LOSS_FN             = \"mse\"    # Loss (MSE for regression)\n",
    "CLIPNORM            = 1.0      # Gradient clipping norm (0.5‚Äì5.0)\n",
    "\n",
    "# ‚îÄ‚îÄ Training Control Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "TRAIN_BATCH         = 32       # Training batch size (32‚Äì128)\n",
    "VAL_BATCH           = 1        # Validation batch size (usually 1)\n",
    "MAX_EPOCHS          = 120       # Max training epochs (50‚Äì150)\n",
    "EARLY_STOP_PATIENCE = 12       # Early stopping patience (10‚Äì20 epochs)\n",
    "USE_FP16            = True     # Enable FP16 for faster training and lower memory use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb51c52b-e3a2-4e8b-b575-58ce0294835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_tensors(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    look_back: int,\n",
    "    feature_cols: Sequence[str],\n",
    "    label_col: str,\n",
    "    rth_start: dt.time\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts one big minute-bar DataFrame (many days) into NumPy arrays ready for the stateful LSTM.\n",
    "    \n",
    "    RULES ENFORCED:\n",
    "      ‚Ä¢ Windows never cross midnight.\n",
    "      ‚Ä¢ Features and labels are standardized per day (to avoid leakage).\n",
    "    \n",
    "    Returns:\n",
    "      X         : Design matrix; every row is a sliding window (flattened).\n",
    "      y         : One-step-ahead targets corresponding to each window.\n",
    "      raw_close : Raw (unstandardized) close prices for each target.\n",
    "      raw_bid   : Raw bid prices.\n",
    "      raw_ask   : Raw ask prices.\n",
    "    \"\"\"\n",
    "    X_rows, y_rows, close_rows, bid_rows, ask_rows = [], [], [], [], []\n",
    "    \n",
    "    # Process one calendar day at a time.\n",
    "    for date, day_df in df.groupby(df.index.normalize()):\n",
    "        day_df = day_df.sort_index()\n",
    "        \n",
    "        # Extract raw price columns before scaling.\n",
    "        raw_close = day_df[\"close\"].to_numpy(dtype=np.float32)\n",
    "        raw_bid   = day_df[\"bid\"].to_numpy(dtype=np.float32)\n",
    "        raw_ask   = day_df[\"ask\"].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Standardize features and target per day.\n",
    "        day_df[feature_cols] = StandardScaler().fit_transform(day_df[feature_cols])\n",
    "        day_df[label_col] = StandardScaler().fit_transform(day_df[[label_col]])\n",
    "        \n",
    "        feats_np = day_df[feature_cols].to_numpy(dtype=np.float32)  # shape: (T, n_feats)\n",
    "        label_np = day_df[label_col].to_numpy(dtype=np.float32)       # shape: (T,)\n",
    "        \n",
    "        # Create mask for Regular Trading Hours (RTH).\n",
    "        rth_mask = day_df.index.time >= rth_start\n",
    "        if not rth_mask.any():\n",
    "            continue\n",
    "        \n",
    "        T, _ = feats_np.shape\n",
    "        \n",
    "        # Build sliding windows using vectorized approach.\n",
    "        win_3d = sliding_window_view(feats_np, (look_back, feats_np.shape[1]), axis=(0, 1))\n",
    "        # After the sliding window operation, we get an array whose shape is roughly (T - look_back + 1, 1, look_back, n_feats).\n",
    "        win_3d = win_3d[:, 0, :, :]  # removes the extra dimension: (T - look_back + 1, look_back, n_feats)\n",
    "        \n",
    "        # Alignment fix: drop the last window so that targets align.\n",
    "        win_3d = win_3d[:-1] # drops the very last window. This makes the number of windows exactly equal to the number of available targets.\n",
    "        y_aligned = label_np[look_back:]              # (T - look_back,)\n",
    "        close_aligned = raw_close[look_back:]    # (T - look_back,)\n",
    "        bid_aligned   = raw_bid[look_back:]      # (T - look_back,)\n",
    "        ask_aligned   = raw_ask[look_back:]      # (T - look_back,)\n",
    "        \n",
    "        # Trim by RTH (apply mask to the target indices).\n",
    "        rth_mask_shifted = rth_mask[look_back:]\n",
    "        win_3d       = win_3d[rth_mask_shifted]\n",
    "        y_aligned    = y_aligned[rth_mask_shifted]\n",
    "        close_aligned = close_aligned[rth_mask_shifted]\n",
    "        bid_aligned   = bid_aligned[rth_mask_shifted]\n",
    "        ask_aligned   = ask_aligned[rth_mask_shifted]\n",
    "        \n",
    "        # Flatten each window\n",
    "        X_rows.append(win_3d.reshape(win_3d.shape[0], -1))\n",
    "        y_rows.append(y_aligned)\n",
    "        close_rows.append(close_aligned)\n",
    "        bid_rows.append(bid_aligned)\n",
    "        ask_rows.append(ask_aligned)\n",
    "    \n",
    "    if not X_rows:\n",
    "        raise ValueError(\"No valid RTH windows found; check rth_start or data gaps.\")\n",
    "    \n",
    "    X = np.concatenate(X_rows).astype(np.float32)\n",
    "    y = np.concatenate(y_rows).astype(np.float32)\n",
    "    raw_close = np.concatenate(close_rows).astype(np.float32)\n",
    "    raw_bid   = np.concatenate(bid_rows).astype(np.float32)\n",
    "    raw_ask   = np.concatenate(ask_rows).astype(np.float32)\n",
    "    \n",
    "    return X, y, raw_close, raw_bid, raw_ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0136670c-663c-4bf0-85da-7a4187e44eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1077116, 300)\n",
      "(1077116,)\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=LOOK_BACK,\n",
    "    feature_cols=feature_cols,\n",
    "    label_col=label_col,\n",
    "    rth_start=rth_start\n",
    ")\n",
    "\n",
    "print(X.shape) # we use 'm' features and 'n' previous look back values to predict each 1 label\n",
    "print(y.shape) # 'n' lookback values * 'n_days_df' (all pretrade values) are deducted from the original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec3a511-a74d-4d60-8cf2-a0a5d37fd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    raw_close: np.ndarray,\n",
    "    raw_bid: np.ndarray,\n",
    "    raw_ask: np.ndarray,\n",
    "    df: pd.DataFrame,           # same DataFrame used to build X, y\n",
    "    *,\n",
    "    look_back: int,\n",
    "    rth_start: dt.time,\n",
    "    train_prop: float,\n",
    "    val_prop: float,\n",
    "    TRAIN_BATCH: int  # added parameter for training batch size\n",
    ") -> Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_train, y_train)\n",
    "        Tuple[np.ndarray, np.ndarray],    # (X_val, y_val)\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],    # (X_test, y_test, raw_close_test, raw_bid_test, raw_ask_test)\n",
    "        List[int],                        # samples_per_day\n",
    "        np.ndarray, np.ndarray, np.ndarray  # day_id_tr, day_id_val, day_id_te\n",
    "    ]:\n",
    "    \"\"\"\n",
    "    Splits X, y (and the raw signals) into chronological train/val/test partitions by whole days.\n",
    "    The code creates a day-based vector (day_id_vec) and then applies masks to split all windows into training, validation, and test groups. \n",
    "    Each group is consecutive in terms of days, so no day is dropped or skipped‚Äîthe splits follow in order. \n",
    "    This ensures that if we later recombine the splits, they cover all days from 0 to D‚Äì1.\n",
    "\n",
    "            [ Training ]         [ Validation ]            [ Test ]\n",
    "    Days: 0   ...   cut_train | cut_train+1 ... cut_val | cut_val+1 ... D-1\n",
    "    \n",
    "    It uses the following logic:\n",
    "      1. Count the number of usable windows per calendar day.\n",
    "      2. Create a day_id vector that tags each sample with its day.\n",
    "      3. Compute the total number of calendar days, D.\n",
    "      4. Compute the \"original\" intended training set size as D * train_prop.\n",
    "      5. Round this count up (in day units) to the next multiple of TRAIN_BATCH.\n",
    "         (This ensures that the training set always contains full training batches.)\n",
    "      6. The validation split starts immediately after training ends. \n",
    "      7. The test split then follows, with no data dropped.\n",
    "    \n",
    "    Returns:\n",
    "      ((X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te),\n",
    "       samples_per_day, day_id_tr, day_id_val, day_id_te)\n",
    "       \n",
    "    samples_per_day : list[int]\n",
    "        Number of usable windows contributed by each calendar day.\n",
    "        +-----------------------------------------------------------+\n",
    "        |  Total minute rows (T)                                    |\n",
    "        |                                                           |\n",
    "        |   0, 1, 2, ... , look_back-1  | look_back, ..., T-1         |\n",
    "        |      (Not ready)            |  (Potential windows)         |\n",
    "        |                              |   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               |\n",
    "        |                              |   ‚îÇ rth_start‚îÇ <-- Only count rows with timestamp >= rth_start\n",
    "        |                              |   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               |\n",
    "        |                              |    Usable windows = count of rows satisfying both conditions\n",
    "        +-----------------------------------------------------------+\n",
    "\n",
    "    day_id_tr / day_id_val / day_id_te : np.ndarray\n",
    "        For every sample in each split, which day (0-based) it came from.\n",
    "    \"\"\"\n",
    "    # 1. Count usable windows per day (vectorized)\n",
    "    samples_per_day: List[int] = []\n",
    "    for _, day_df in df.groupby(df.index.normalize(), sort=False):\n",
    "        T = len(day_df)                # minute rows today\n",
    "        idx = np.arange(T)\n",
    "        mask_window_ready = idx >= look_back\n",
    "        mask_rth_target   = day_df.index.time >= rth_start\n",
    "        usable_today = int(np.count_nonzero(mask_window_ready & mask_rth_target))\n",
    "        samples_per_day.append(usable_today)\n",
    "    \n",
    "    # Verify that summed count equals len(X)\n",
    "    if sum(samples_per_day) != len(X):\n",
    "        raise ValueError(\n",
    "            \"Length mismatch between counted windows and X. Ensure look_back & rth_start match build_lstm_tensors().\"\n",
    "        )\n",
    "    \n",
    "    # 2. Build the day_id vector: each window gets the day index (0-based)\n",
    "    day_id_vec = np.repeat(np.arange(len(samples_per_day)), samples_per_day)\n",
    "    \n",
    "    # 3. Total number of days\n",
    "    D = len(samples_per_day)\n",
    "    \n",
    "    # 4. Compute the original intended training count (in days)\n",
    "    original_train_count = int(D * train_prop)\n",
    "    \n",
    "    # 5. Round up the training days to the next multiple of TRAIN_BATCH\n",
    "    # (We want the training portion to include full batches.)\n",
    "    new_train_count = int(np.ceil(original_train_count / TRAIN_BATCH) * TRAIN_BATCH)\n",
    "    # Make sure we don't exceed the total number of days.\n",
    "    new_train_count = min(new_train_count, D)\n",
    "    # In day_id_vec, days are 0-indexed, so the training cut is:\n",
    "    cut_train = new_train_count - 1\n",
    "    \n",
    "    # 6. Determine the validation cut-point using the original proportion.\n",
    "    # Validation ends at the day_index given by:\n",
    "    cut_val = int(D * (train_prop + val_prop))\n",
    "\n",
    "    # 7. Create masks for the splits.\n",
    "    mask_tr = day_id_vec <= cut_train\n",
    "    mask_val = (day_id_vec > cut_train) & (day_id_vec <= cut_val)\n",
    "    mask_te = day_id_vec > cut_val\n",
    "    \n",
    "    # 8. Slice X, y, and the raw arrays.\n",
    "    X_tr, y_tr = X[mask_tr], y[mask_tr]\n",
    "    X_val, y_val = X[mask_val], y[mask_val]\n",
    "    X_te, y_te = X[mask_te], y[mask_te]\n",
    "    \n",
    "    raw_close_te = raw_close[mask_te]\n",
    "    raw_bid_te   = raw_bid[mask_te]\n",
    "    raw_ask_te   = raw_ask[mask_te]\n",
    "    \n",
    "    # Also slice the day_id vector.\n",
    "    day_id_tr = day_id_vec[mask_tr]\n",
    "    day_id_val = day_id_vec[mask_val]\n",
    "    day_id_te = day_id_vec[mask_te]\n",
    "    \n",
    "    return (X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "           samples_per_day, day_id_tr, day_id_val, day_id_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8342cc3b-5a9a-4a4e-a23c-24bdbc2543b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1984 distinct calendar days retained, organized into batches of 32 days each (no partial batches).\n",
      "Validation: 409 distinct calendar days available for model validation.\n",
      "Test: 422 distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\n"
     ]
    }
   ],
   "source": [
    "# The splitter now requires raw_close, raw_bid, and raw_ask arrays as additional arguments.\n",
    "(X_tr, y_tr), (X_val, y_val), (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back=LOOK_BACK,\n",
    "    rth_start=rth_start,\n",
    "    train_prop=TRAIN_PROP,\n",
    "    val_prop=VAL_PROP,\n",
    "    TRAIN_BATCH=TRAIN_BATCH\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(np.unique(day_id_tr))} distinct calendar days retained, organized into batches of {TRAIN_BATCH} days each (no partial batches).\")\n",
    "print(f\"Validation: {len(np.unique(day_id_val))} distinct calendar days available for model validation.\")\n",
    "print(f\"Test: {len(np.unique(day_id_te))} distinct calendar days available for testing, with raw price signals (close, bid, ask) included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c61e236-6dd0-4161-b5cf-e0670d90974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_day_dataset(\n",
    "    X           : np.ndarray,    # (N, ‚Ä¶)\n",
    "    y           : np.ndarray,    # (N,)\n",
    "    day_id      : np.ndarray,    # (N,)\n",
    "    weekday_vec : np.ndarray,    # (N,)\n",
    "    raw_close   : np.ndarray = None,  # (N,) optional\n",
    "    raw_bid     : np.ndarray = None,  # (N,) optional\n",
    "    raw_ask     : np.ndarray = None   # (N,) optional\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset where each element corresponds to one calendar day.\n",
    "    \n",
    "    If raw price arrays are provided, yields a 6-tuple:\n",
    "      (x_day, y_day, raw_close_day, raw_bid_day, raw_ask_day, weekday)\n",
    "    Otherwise, yields a 3-tuple:\n",
    "      (x_day, y_day, weekday)\n",
    "    \n",
    "    - x_day: (1, T, n_feats) float32\n",
    "    - y_day: (1, T)         float32\n",
    "    - weekday: ()           int32\n",
    "    \"\"\"\n",
    "    # Sort inputs in chronological order.\n",
    "    idx = np.argsort(day_id, kind=\"stable\")\n",
    "    X, y, day_id, weekday_vec = [a[idx] for a in (X, y, day_id, weekday_vec)]\n",
    "    if raw_close is not None and raw_bid is not None and raw_ask is not None:\n",
    "        raw_close, raw_bid, raw_ask = [a[idx] for a in (raw_close, raw_bid, raw_ask)]\n",
    "    \n",
    "    # Determine boundaries for each day.\n",
    "    # the code splits the dataset into daily blocks based on the day_id, \n",
    "    # then produces a TensorFlow dataset where each element represents one complete day‚Äôs data \n",
    "    # (with features, targets, and possibly raw prices) along with the corresponding weekday\n",
    "    change = np.where(np.diff(day_id) != 0)[0] + 1\n",
    "    day_slices = np.split(np.arange(len(day_id)), change)\n",
    "\n",
    "    # the generator function walks through the dataset day by day and ‚Äúpacks‚Äù each day‚Äôs data into one neat bundle. \n",
    "    # For each day it: Selects the day's data, Formats the data, Yields the bundle \n",
    "    # Using a generator is both memory efficient and flexible. It creates each day's data on the fly rather than building a huge list all at once.\n",
    "    def gen():\n",
    "        for sl in day_slices:\n",
    "            x_block = X[sl]      # (T, ‚Ä¶)\n",
    "            y_block = y[sl]      # (T,)\n",
    "            weekday = int(weekday_vec[sl[0]])\n",
    "            if raw_close is None:\n",
    "                # Yield the original 3-tuple.\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "            else:\n",
    "                # Extract raw price slices.\n",
    "                close_block = raw_close[sl]  # (T,)\n",
    "                bid_block   = raw_bid[sl]    # (T,)\n",
    "                ask_block   = raw_ask[sl]    # (T,)\n",
    "                yield (\n",
    "                    np.expand_dims(x_block, 0).astype(np.float32),    # (1, T, ‚Ä¶)\n",
    "                    np.expand_dims(y_block, 0).astype(np.float32),      # (1, T)\n",
    "                    np.expand_dims(close_block, 0).astype(np.float32),  # (1, T)\n",
    "                    np.expand_dims(bid_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.expand_dims(ask_block, 0).astype(np.float32),    # (1, T)\n",
    "                    np.int32(weekday)\n",
    "                )\n",
    "    \n",
    "    feat_shape = X.shape[1:]\n",
    "    if raw_close is None:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    else:\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((1, None, *feat_shape), tf.float32),  # x_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # y_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_close_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_bid_day\n",
    "            tf.TensorSpec((1, None), tf.float32),                # raw_ask_day\n",
    "            tf.TensorSpec((), tf.int32)                          # weekday\n",
    "        )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(gen, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7be751-08ca-4729-ae75-6b06745cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_day_datasets(\n",
    "        X_tr, y_tr, day_id_tr,                 # training arrays\n",
    "        X_val, y_val, day_id_val,              # validation arrays\n",
    "        X_te, y_te, day_id_te,                 # test arrays\n",
    "        raw_close_te, raw_bid_te, raw_ask_te,  # test raw price arrays\n",
    "        *,\n",
    "        df,                                    # original DataFrame (for weekday vector)\n",
    "        train_batch: int\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Splits training, validation, and test arrays into day-level tf.data.Datasets.\n",
    "    \n",
    "    For training and validation, raw signals are not saved (3-tuple).\n",
    "    For testing, the raw price arrays (raw_close, raw_bid, raw_ask) are provided, yielding a 6-tuple.\n",
    "    \n",
    "    Returns:\n",
    "      ds_train_batched, ds_val_unbatched, ds_test_unbatched\n",
    "    \"\"\"\n",
    "    # Build one weekday vector covering all rows from the original DataFrame.\n",
    "    weekday_all = df.index.dayofweek.to_numpy(dtype=np.int8)\n",
    "    \n",
    "    # Determine split lengths.\n",
    "    n_tr = len(X_tr)\n",
    "    n_val = len(X_val)\n",
    "    n_te = len(X_te)\n",
    "    \n",
    "    # Create weekday vectors for each split.\n",
    "    # they will be used to identify the end of the week and reset the long-term state of the LSTM layers\n",
    "    weekday_vec_tr = weekday_all[:n_tr]\n",
    "    weekday_vec_val = weekday_all[n_tr:n_tr+n_val]\n",
    "    weekday_vec_te = weekday_all[n_tr+n_val:n_tr+n_val+n_te]\n",
    "    \n",
    "    # Build training and validation datasets (3-tuple).\n",
    "    ds_tr = make_day_dataset(X_tr, y_tr, day_id_tr, weekday_vec_tr)\n",
    "    ds_val = make_day_dataset(X_val, y_val, day_id_val, weekday_vec_val)\n",
    "    \n",
    "    # Build test dataset with raw price arrays (6-tuple).\n",
    "    ds_test = make_day_dataset(X_te, y_te, day_id_te, weekday_vec_te,\n",
    "                               raw_close=raw_close_te, raw_bid=raw_bid_te, raw_ask=raw_ask_te)\n",
    "    \n",
    "    # For training, strip the extra outer batch dimension.\n",
    "    # the _strip function only on the training dataset to remove that extra outer dimension \n",
    "    # In the validation or test datasets we don't need to strip this dimension because those pipelines expect one sample at a time \n",
    "    def _strip(x_day, y_day, wd):\n",
    "        return tf.squeeze(x_day, 0), tf.squeeze(y_day, 0), wd\n",
    "\n",
    "    ds_train_batched = (ds_tr\n",
    "                         .map(_strip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                         .padded_batch(train_batch, drop_remainder=True)\n",
    "                         .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    # Save the test dataset.\n",
    "    ds_test.save(str(save_dir / f\"{ticker}_ds_test\"), compression=\"GZIP\")\n",
    "    \n",
    "    return ds_train_batched, ds_val, ds_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf385a70-6c36-423a-9398-2d4e0d40849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750755528.019685    2851 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ds_train_batched, ds_val_unbatched, ds_test_unbatched = split_to_day_datasets(\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    X_val, y_val, day_id_val,\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    df=df,\n",
    "    train_batch=TRAIN_BATCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c042e82e-3475-4107-bec3-7c11c873e5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                       (inference / trading time)\\n\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ù∂  NETWORK  WEIGHTS  Œ∏  ‚Äì learned across all history, fixed at runtime   ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ù∑  CELL STATE  c‚Çú  ‚Äì slow integrator covering the *whole* current day    ‚îÇ\\n‚îÇ    ‚Ä¢ retains early-morning context                                       ‚îÇ\\n‚îÇ    ‚Ä¢ reset_states()  at every midnight ‚Üí zero on next session            ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ù∏  HIDDEN STATE  h‚Çú  ‚Äì fast dynamics (a few bars)                        ‚îÇ\\n‚îÇ    ‚Ä¢ captures spikes / micro-structure                                   ‚îÇ\\n‚îÇ    ‚Ä¢ reset together with c‚Çú midnight                                     ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n‚îÇ ‚ùπ  INPUT WINDOW  x‚Çú  ‚Äì last 60 minutes of raw features                   ‚îÇ\\n‚îÇ    ‚Ä¢ first RTH prediction uses 60 *pre-trade* minutes only               ‚îÇ\\n‚îÇ    ‚Ä¢ later predictions mix pre-trade + today‚Äôs RTH, never yesterday RTH  ‚îÇ\\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n        ‚îÇ\\n        ‚ñº\\n                        Predicted signal ≈∑‚Çú\\n\\n\\nDay i                               Day i+1\\n|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚Ä¶‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n08:00        09:30        16:00   08:00    09:30\\npre-trade       RTH                pre-trade  RTH\\nc‚Çú,h‚Çú: 0 ‚Üí accumulate ‚Üí reset_states() ‚Üí 0 ‚Üí accumulate\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "                       (inference / trading time)\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ù∂  NETWORK  WEIGHTS  Œ∏  ‚Äì learned across all history, fixed at runtime   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ù∑  CELL STATE  c‚Çú  ‚Äì slow integrator covering the *whole* current day    ‚îÇ\n",
    "‚îÇ    ‚Ä¢ retains early-morning context                                       ‚îÇ\n",
    "‚îÇ    ‚Ä¢ reset_states()  at every midnight ‚Üí zero on next session            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ù∏  HIDDEN STATE  h‚Çú  ‚Äì fast dynamics (a few bars)                        ‚îÇ\n",
    "‚îÇ    ‚Ä¢ captures spikes / micro-structure                                   ‚îÇ\n",
    "‚îÇ    ‚Ä¢ reset together with c‚Çú midnight                                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚ùπ  INPUT WINDOW  x‚Çú  ‚Äì last 60 minutes of raw features                   ‚îÇ\n",
    "‚îÇ    ‚Ä¢ first RTH prediction uses 60 *pre-trade* minutes only               ‚îÇ\n",
    "‚îÇ    ‚Ä¢ later predictions mix pre-trade + today‚Äôs RTH, never yesterday RTH  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "                        Predicted signal ≈∑‚Çú\n",
    "\n",
    "\n",
    "Day i                               Day i+1\n",
    "|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚Ä¶‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "08:00        09:30        16:00   08:00    09:30\n",
    "pre-trade       RTH                pre-trade  RTH\n",
    "c‚Çú,h‚Çú: 0 ‚Üí accumulate ‚Üí reset_states() ‚Üí 0 ‚Üí accumulate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6c6f36-49ec-4ed7-9703-4b86914c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# build_stateful_lstm_dual                                                    #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creates a two-tier stateful network:                                        #\n",
    "#   ‚Ä¢ Layer 0  ‚Äúshort_lstm‚Äù  (SHORT_UNITS units)  ‚Üí quick dynamics, daily reset        #\n",
    "#   ‚Ä¢ Layer 1  ‚Äúlong_lstm‚Äù   (LONG_UNITS units)  ‚Üí slow drift, weekly reset           #\n",
    "#                                                                             #\n",
    "# You decide what ‚Äúweek‚Äù means by calling reset_states() on long_lstm         #\n",
    "# whenever your outer loop hits Saturday close or Sunday 00:00.               #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def build_stateful_lstm_dual(\n",
    "        *,\n",
    "        n_feats: int,\n",
    "        batch_size: int,\n",
    "\n",
    "        # architecture\n",
    "        short_units: int,\n",
    "        long_units:  int,\n",
    "        dropout_short:    float,\n",
    "        dropout_long:     float,\n",
    "        rec_drop_short:   float,\n",
    "        rec_drop_long:    float,\n",
    "\n",
    "        # optimiser schedule\n",
    "        initial_lr:       float,\n",
    "        first_decay_steps:int,     # *absolute* #gradient steps\n",
    "        t_mul:   float,\n",
    "        m_mul:   float,\n",
    "        alpha:   float,\n",
    "\n",
    "        # misc\n",
    "        loss:            str,\n",
    "        clipnorm:        float,\n",
    "        use_mixed_fp16:  bool\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Returns a compiled two-tier LSTM (daily + weekly memory).\n",
    "    Pass *absolute* `first_decay_steps` (in gradient updates, not epochs).\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚îÄ‚îÄ 1 ¬∑ optional mixed-precision context ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    prev_policy = mixed_precision.global_policy().name\n",
    "    if use_mixed_fp16 and prev_policy != \"mixed_float16\":\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ 2 ¬∑ network definition ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    inp = layers.Input(batch_shape=(batch_size, None, n_feats), name=\"inp\")\n",
    "    \n",
    "    # Short-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            short_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used below for GPU efficiency\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"short_lstm\"\n",
    "        )(inp)\n",
    "    x = layers.Dropout(dropout_short, name=\"dropout_short\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_short\")(x)\n",
    "    \n",
    "    # Long-term LSTM block:\n",
    "    x = layers.LSTM(\n",
    "            long_units,\n",
    "            stateful=True,\n",
    "            return_sequences=True,\n",
    "            dropout=0,  # external dropout used instead\n",
    "            recurrent_dropout=0,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "            kernel_regularizer=regularizers.l2(KERNEL_REG),\n",
    "            name=\"long_lstm\"\n",
    "        )(x)\n",
    "    x = layers.Dropout(dropout_long, name=\"dropout_long\")(x)\n",
    "    x = layers.LayerNormalization(name=\"ln_long\")(x)\n",
    "    \n",
    "    # Output layer with time-distributed dense unit:\n",
    "    out = layers.TimeDistributed(layers.Dense(1), name=\"pred\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"dual_mem_lstm\")\n",
    "\n",
    "    # ‚îÄ‚îÄ 3 ¬∑ optimiser & schedule ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate = initial_lr,\n",
    "        first_decay_steps     = first_decay_steps,\n",
    "        t_mul                 = t_mul,\n",
    "        m_mul                 = m_mul,\n",
    "        alpha                 = alpha\n",
    "    )\n",
    "    opt = optimizers.Adam(learning_rate=lr_sched, clipnorm=clipnorm)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss      = loss,\n",
    "                  metrics   =[metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "    # ‚îÄ‚îÄ 4 ¬∑ restore dtype policy outside mixed-FP16 scope ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if mixed_precision.global_policy().name != prev_policy:\n",
    "        mixed_precision.set_global_policy(prev_policy)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f775d99-7204-4ba7-bbaf-28bcabf11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# BUILD THE TWO STATEFUL COPIES                                          #\n",
    "###############################################################################\n",
    "# fast FP16 train graph\n",
    "model_train = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = TRAIN_BATCH,\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = USE_FP16)\n",
    "\n",
    "#  plain FP32 validation / inference graph\n",
    "model_val = build_stateful_lstm_dual(\n",
    "    n_feats           = N_FEATS,\n",
    "    batch_size        = VAL_BATCH,      # always 1 for day-by-day eval\n",
    "    short_units       = SHORT_UNITS,\n",
    "    long_units        = LONG_UNITS,\n",
    "    dropout_short     = DROPOUT_SHORT,\n",
    "    dropout_long      = DROPOUT_LONG,\n",
    "    rec_drop_short    = REC_DROP_SHORT,\n",
    "    rec_drop_long     = REC_DROP_LONG,\n",
    "    initial_lr        = INITIAL_LR,     # same schedule object ‚Üí harmless\n",
    "    first_decay_steps = FIRST_DECAY_EPOCHS,\n",
    "    t_mul             = T_MUL,\n",
    "    m_mul             = M_MUL,\n",
    "    alpha             = ALPHA,\n",
    "    loss              = LOSS_FN,\n",
    "    clipnorm          = CLIPNORM,\n",
    "    use_mixed_fp16    = False)          # pure FP32 for numerical fidelity\n",
    "\n",
    "# save validation model, to reuse for inference\n",
    "model_val.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "809c6a21-d4d1-4849-8a40-ed1f4a2fb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# LiveRMSEPlot  ‚ñ∏  INLINE- & WIDGET-SAFE, SINGLE FIGURE\n",
    "###############################################################################\n",
    "class LiveRMSEPlot:\n",
    "    \"\"\"\n",
    "    LiveRMSEPlot updates a single figure to show training progress without spawning\n",
    "    a new image for each epoch. It works with different matplotlib backends, e.g.,\n",
    "    %matplotlib inline, widget, or notebook.\n",
    "\n",
    "    The plot displays:\n",
    "      - Blue line and dot: training RMSE history.\n",
    "      - Orange line and dot: validation RMSE history.\n",
    "\n",
    "    If the latest validation RMSE is not a number (NaN), the corresponding dot is\n",
    "    hidden by setting its offsets to an empty 2D array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Retrieve the current matplotlib backend and convert it to lowercase.\n",
    "        self.backend = matplotlib.get_backend().lower()\n",
    "        # Build the figure and axes.\n",
    "        self._build_figure()\n",
    "        # Display the figure once and keep a reference to the display_id so that we can\n",
    "        # update the same output cell on subsequent calls instead of spawning a new figure.\n",
    "        self.disp_id = display(self.fig, display_id=True)\n",
    "        # Initialize empty lists to store epoch numbers and RMSE metrics.\n",
    "        self.e, self.tr, self.va = [], [], []      # e = epochs, tr = train RMSE, va = validation RMSE\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _build_figure(self):\n",
    "        \"\"\"\n",
    "        Constructs and configures the matplotlib figure and axes.\n",
    "        - Creates empty line plots for training (blue) and validation (orange).\n",
    "        - Creates scatter plot objects (dots) for the latest RMSE values.\n",
    "        - Sets up grid, labels, title, and legend.\n",
    "        \"\"\"\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4), dpi=110)\n",
    "        self.ax.set(xlabel=\"epoch\", ylabel=\"RMSE\", title=\"Training progress\")\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        # Create a blue line for training RMSE.\n",
    "        (self.tr_line,) = self.ax.plot([], [], c=\"#1f77b4\", lw=1.5)\n",
    "        # Create an orange line for validation RMSE.\n",
    "        (self.va_line,) = self.ax.plot([], [], c=\"#ff7f0e\", lw=1.5)\n",
    "        # Create scatter objects for the latest training and validation points.\n",
    "        self.tr_dot = self.ax.scatter([], [], c=\"#1f77b4\", s=30)\n",
    "        self.va_dot = self.ax.scatter([], [], c=\"#ff7f0e\", s=30)\n",
    "        \n",
    "        # Add a legend to differentiate between training and validation RMSE.\n",
    "        self.ax.legend([\"train\", \"val\"])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def update(self, train_rmse: float, val_rmse: float):\n",
    "        \"\"\"\n",
    "        Updates the live plot with new training and validation RMSE values.\n",
    "\n",
    "        Steps:\n",
    "         1. Append the new epoch and metric values.\n",
    "         2. Update the line plots with the full RMSE history.\n",
    "         3. Update the latest dot position for both training and validation.\n",
    "            - If the validation RMSE is NaN, hide its dot by setting an empty 2D array.\n",
    "         4. Recalculate and update axis limits.\n",
    "         5. Redraw the figure using the appropriate method for the backend.\n",
    "        \"\"\"\n",
    "        # 1. Append new data:\n",
    "        #    - Epochs are automatically numbered starting from 1.\n",
    "        self.e.append(len(self.e) + 1)\n",
    "        self.tr.append(train_rmse)\n",
    "        self.va.append(val_rmse)\n",
    "        \n",
    "        # 2. Update line plots:\n",
    "        #    - For the training line, simply use all available data.\n",
    "        self.tr_line.set_data(self.e, self.tr)\n",
    "        \n",
    "        #    - For the validation line, filter out non-finite values (e.g., NaN).\n",
    "        finite = np.isfinite(self.va)\n",
    "        self.va_line.set_data(np.asarray(self.e)[finite],\n",
    "                              np.asarray(self.va)[finite])\n",
    "        \n",
    "        # 3. Update the latest dots:\n",
    "        #    - Always update the training dot with the most recent training RMSE.\n",
    "        self.tr_dot.set_offsets([[self.e[-1], self.tr[-1]]])\n",
    "        \n",
    "        #    - For the validation dot, only update if the latest value is finite.\n",
    "        if np.isfinite(self.va[-1]):\n",
    "            self.va_dot.set_offsets([[self.e[-1], self.va[-1]]])\n",
    "        else:\n",
    "            # Instead of an empty list, we pass an empty 2D NumPy array with shape (0,2)\n",
    "            # to properly hide the dot when the validation RMSE is NaN.\n",
    "            self.va_dot.set_offsets(np.empty((0, 2)))\n",
    "        \n",
    "        # 4. Rescale the axes:\n",
    "        #    - This ensures all data is visible in the plot.\n",
    "        self.ax.relim()\n",
    "        self.ax.autoscale_view()\n",
    "        \n",
    "        # 5. Redraw the figure:\n",
    "        #    - For widget backends, use draw_idle to schedule a redraw.\n",
    "        #    - For inline / notebook backends, force a redraw and update the output cell.\n",
    "        if \"widget\" in self.backend or \"ipympl\" in self.backend:\n",
    "            self.fig.canvas.draw_idle()\n",
    "        else:\n",
    "            self.fig.canvas.draw()\n",
    "            self.disp_id.update(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a851b9b0-4986-4952-b62f-4b0760ae2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. Low-level Training Step Function\n",
    "# =============================================================================\n",
    "@tf.function\n",
    "def _train_step(xb, yb, model, loss_fn, opt):\n",
    "    \"\"\"\n",
    "    Performs one training step on a batch of data.\n",
    "    \n",
    "    Workflow:\n",
    "      1. Uses a TensorFlow GradientTape to record operations for automatic differentiation.\n",
    "      2. Performs a forward pass through the model (with training=True).\n",
    "      3. Casts the predictions to float32 (common for mixed-precision training).\n",
    "      4. Checks the output (y_pred) for numerical issues (NaNs/Infs).\n",
    "      5. Computes the loss via the provided loss function.\n",
    "      6. Checks the loss value for numerical problems.\n",
    "      7. Computes gradients of the loss with respect to the model's trainable weights.\n",
    "      8. Iterates over each computed gradient and checks them for NaNs/Infs.\n",
    "      9. Applies gradients via the optimizer.\n",
    "     10. Returns the RMSE (root mean-squared error) computed as sqrt(loss).\n",
    "    \n",
    "    The function is decorated with @tf.function for performance.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Compute predictions for the current batch.\n",
    "        y_pred = model(xb, training=True)\n",
    "        # Explicitly cast predictions to float32.\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        # Check that the model outputs are numerically valid.\n",
    "        tf.debugging.check_numerics(y_pred, message=\"y_pred contains NaNs or Infs!\")\n",
    "        \n",
    "        # Compute the loss comparing true values and predictions.\n",
    "        loss = loss_fn(yb, y_pred)\n",
    "        # Ensure that the loss is finite.\n",
    "        tf.debugging.check_numerics(loss, message=\"Loss contains NaNs or Infs!\")\n",
    "    \n",
    "    # Calculate gradients of the loss with respect to all trainable parameters.\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Iterate through each gradient and check for numerical issues.\n",
    "    for grad in grads:\n",
    "        if grad is not None:\n",
    "            tf.debugging.check_numerics(grad, message=\"Gradient has NaNs or Infs!\")\n",
    "    \n",
    "    # Apply the gradients to adjust the model weights.\n",
    "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Return the RMSE computed from the loss.\n",
    "    return tf.sqrt(loss)  # RMSE\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Helper to Extract the Current Learning Rate\n",
    "# =============================================================================\n",
    "def current_lr_from(opt: tf.keras.optimizers.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Extracts and returns the scalar learning rate from an optimizer.\n",
    "    \n",
    "    This function supports various cases:\n",
    "      ‚Ä¢ The optimizer might be wrapped in a LossScaleOptimizer (for mixed-precision training).\n",
    "      ‚Ä¢ The learning rate can be a static value (or tf.Variable) or be managed dynamically\n",
    "        by a LearningRateSchedule.\n",
    "    \n",
    "    Steps:\n",
    "      1. If the optimizer is a LossScaleOptimizer, unwrap it to get the inner optimizer.\n",
    "      2. Retrieve the learning rate from the optimizer.\n",
    "      3. If the learning rate is part of a LearningRateSchedule, compute its current value based\n",
    "         on the optimizer's iteration count.\n",
    "      4. Otherwise, extract and return the numerical value of the learning rate.\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the current learning rate.\n",
    "    \"\"\"\n",
    "    # Unwrap if the optimizer is wrapped by a mixed-precision LossScaleOptimizer.\n",
    "    if isinstance(opt, LossScaleOptimizer):\n",
    "        opt = opt.inner_optimizer  # For some TF versions, this might be opt._optimizer\n",
    "    \n",
    "    # Retrieve the learning rate property from the optimizer.\n",
    "    lr = opt.learning_rate\n",
    "    # If the learning rate is scheduled (i.e., a LearningRateSchedule), determine its value.\n",
    "    if isinstance(lr, LearningRateSchedule):\n",
    "        return float(lr(opt.iterations))\n",
    "    else:\n",
    "        # Otherwise, extract the value directly (it might be stored in a tf.Variable).\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Custom Stateful Training Loop Function\n",
    "# =============================================================================\n",
    "def custom_stateful_training_loop(\n",
    "    model_train,\n",
    "    model_val,\n",
    "    ds_train_batched,          # Batched training dataset; yields (xb, yb, weekday_batch)\n",
    "    ds_val,                    # Validation dataset; yields (x_day, y_day, weekday)\n",
    "    *,\n",
    "    n_train_days: int,         # Total calendar days in the training epoch\n",
    "    max_epochs: int,           # Maximum number of epochs to run\n",
    "    early_stop_patience: int,  # Patience for early stopping (in epochs)\n",
    "    baseline_val_rmse: float,  # Baseline RMSE metric for comparison\n",
    "    weights_path               # Path to save the best (champion) model weights\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Executes a custom training loop for stateful models‚Äîsuch as LSTMs‚Äîwith separate \n",
    "    training and validation phases. This loop uses a progress bar to measure per-day \n",
    "    training progress and includes manual resetting of stateful layers (for both short-term \n",
    "    and long-term states) for each new day, as well as on weekend transitions.\n",
    "    \n",
    "    Overall workflow per epoch:\n",
    "      1. Reset states as required and iterate through training batches.\n",
    "      2. Perform forward and backward passes (using _train_step) on batched training data.\n",
    "      3. Use a progress bar (tqdm) to display progress over the training epoch.\n",
    "      4. After training, synchronize the weights from the training model to the validation model.\n",
    "      5. Evaluate the model on the validation dataset and compute RMSE for each day.\n",
    "      6. Compute epoch-level metrics (mean training RMSE, mean validation RMSE,\n",
    "         improvement over a baseline, current learning rate, and a gradient norm proxy).\n",
    "      7. Log the epoch metrics and update a live RMSE plot.\n",
    "      8. Implement early stopping: if no improvement over `early_stop_patience` epochs, stop training.\n",
    "      9. Saves the best model weights (champion) to checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        The best validation RMSE obtained during training.\n",
    "    \"\"\"\n",
    "    # Define the loss function as Mean Squared Error.\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Extract the optimizer from the training model.\n",
    "    opt = model_train.optimizer\n",
    "    # Set the gradient clipping norm for numerical stability (especially useful with FP16).\n",
    "    opt.clipnorm = 1.0\n",
    "    \n",
    "    # Instantiate a live plot helper (assumes LiveRMSEPlot is defined elsewhere).\n",
    "    live_plot = LiveRMSEPlot()\n",
    "    \n",
    "    # Get handles to stateful layers by name.\n",
    "    # This code assumes your stateful layers are named \"short_lstm\" and \"long_lstm\".\n",
    "    short_tr = [l for l in model_train.layers if l.name == \"short_lstm\"]\n",
    "    long_tr  = [l for l in model_train.layers if l.name == \"long_lstm\"]\n",
    "    short_val = [l for l in model_val.layers if l.name == \"short_lstm\"]\n",
    "    long_val  = [l for l in model_val.layers if l.name == \"long_lstm\"]\n",
    "    \n",
    "    # Initialize variables to track the best validation RMSE and early stopping counter.\n",
    "    best_val_rmse = math.inf\n",
    "    patience_ctr = 0\n",
    "    \n",
    "    # Epoch Loop: Iterate over epochs.\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 1. Training Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Create a tqdm progress bar where each \"tick\" corresponds to one training day.\n",
    "        pbar = tqdm(total=n_train_days,\n",
    "                    desc=f\"Epoch {epoch:03d}\",\n",
    "                    unit=\"day\",\n",
    "                    dynamic_ncols=True,\n",
    "                    ncols=80,\n",
    "                    leave=False)\n",
    "        \n",
    "        batch_rmses = []  # To store RMSE values computed for each training batch.\n",
    "        prev_wd_train = None  # To track day-of-week for detecting weekend state resets.\n",
    "    \n",
    "        # Iterate through the batched training dataset.\n",
    "        for xb, yb, wd_batch in ds_train_batched:  # Assume xb.shape[0] equals number of days for the batch.\n",
    "            # Reset the short-term states at the start of each day.\n",
    "            for lyr in short_tr:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # Detect a weekend transition. For example, if the first weekday in the current batch\n",
    "            # is less than the last day's weekday from the previous batch, reset the long-term state.\n",
    "            first_wd = int(wd_batch[0])\n",
    "            if prev_wd_train is not None and first_wd < prev_wd_train:\n",
    "                for lyr in long_tr:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_train = int(wd_batch[-1])\n",
    "    \n",
    "            # Execute one training step (forward pass, backward pass, and weight update).\n",
    "            rmse = _train_step(xb, yb, model_train, loss_fn, opt)\n",
    "            batch_rmses.append(float(rmse))\n",
    "    \n",
    "            # Advance the progress bar by the number of days in this batch.\n",
    "            pbar.update(int(xb.shape[0]))\n",
    "    \n",
    "        # Compute the average training RMSE over the epoch.\n",
    "        epoch_train = float(np.mean(batch_rmses))\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 2. Validation Phase\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # Synchronize the weights from the training model to the validation model.\n",
    "        model_val.set_weights(model_train.get_weights())\n",
    "    \n",
    "        val_rmses = []  # Store RMSE for each validation day.\n",
    "        prev_wd_val = None  # To track weekday for state resets in validation.\n",
    "        for x_day, y_day, wd in ds_val:\n",
    "            wd = int(wd)\n",
    "    \n",
    "            # Reset short-term state for the validation model.\n",
    "            for lyr in short_val:\n",
    "                lyr.reset_states()\n",
    "    \n",
    "            # For weekend transitions in validation, reset long-term states.\n",
    "            if prev_wd_val is not None and wd < prev_wd_val:\n",
    "                for lyr in long_val:\n",
    "                    lyr.reset_states()\n",
    "            prev_wd_val = wd\n",
    "    \n",
    "            # Run a forward pass for the validation day (no gradient computation).\n",
    "            y_pred = model_val(x_day, training=False)\n",
    "            # Squeeze extra dimensions and cast predictions to float32.\n",
    "            y_pred = tf.cast(tf.squeeze(y_pred, (0, 2)), tf.float32)\n",
    "            # Reshape true values as necessary.\n",
    "            y_true = tf.cast(tf.reshape(y_day, [-1]), tf.float32)\n",
    "    \n",
    "            # Calculate RMSE for the validation day.\n",
    "            day_rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "            val_rmses.append(float(day_rmse))\n",
    "    \n",
    "        # Compute the average validation RMSE over all validation days.\n",
    "        epoch_val = float(np.mean(val_rmses))\n",
    "    \n",
    "        # Compute improvement as a percentage relative to the baseline RMSE.\n",
    "        impr_pct = 100.0 * (1.0 - epoch_val / baseline_val_rmse)\n",
    "    \n",
    "        # Retrieve the current learning rate from the optimizer.\n",
    "        current_lr = current_lr_from(opt)\n",
    "    \n",
    "        # Compute a crude proxy for gradient norm: this is optional and may vary per use-case.\n",
    "        # simplified indicator of the relative gradient magnitude by relating the average batch error (RMSE) to the learning rate, \n",
    "        # rather than the exact calculation of the gradient norm itself.\n",
    "        # extremely high or unexpectedly low, it might suggest that the gradients (and thus the updates) \n",
    "        # are too large (risking instability) or too small (leading to slow learning)\n",
    "        grad_norm = np.mean(batch_rmses) / current_lr\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 3. Logging and Live Plot Updating\n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(f\"Epoch {epoch:03d} ‚Ä¢ train={epoch_train:.6f} ‚Ä¢ val={epoch_val:.6f} \"\n",
    "              f\"‚Ä¢ impr={impr_pct:5.1f}% ‚Ä¢ lr={current_lr:.2e} ‚Ä¢ g‚âà{grad_norm:.2f}\")\n",
    "    \n",
    "        # Update the live RMSE plot with the current epoch's metrics.\n",
    "        live_plot.update(epoch_train, epoch_val)\n",
    "    \n",
    "        # Close the progress bar for the current epoch.\n",
    "        pbar.close()\n",
    "    \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # 4. Early Stopping and Checkpointing\n",
    "        # -----------------------------------------------------------------------------\n",
    "        # If the current epoch's validation RMSE is better than the best seen so far, save the model.\n",
    "        if epoch_val < best_val_rmse:\n",
    "            best_val_rmse = epoch_val\n",
    "            patience_ctr = 0  # Reset the patience counter.\n",
    "            model_train.save_weights(weights_path)  # Save checkpoint of the best weights.\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # End of epoch loop.\n",
    "    \n",
    "    # Restore the best model weights (from checkpoint) before returning.\n",
    "    model_train.load_weights(weights_path)\n",
    "    return best_val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16280865-a8d7-430c-adf0-004e12bc29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (predict-zero) RMSE on validation = 0.913413\n",
      "Training sees 1984 calendar days per epoch\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 660x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f7021c43914d3d8d9a470bac4c556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 001:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750755555.832740    2928 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 ‚Ä¢ train=1.116505 ‚Ä¢ val=0.772927 ‚Ä¢ impr= 15.4% ‚Ä¢ lr=1.31e-04 ‚Ä¢ g‚âà8536.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f724ba5ba3e4f128b42c23a5a6e9fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 002:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 ‚Ä¢ train=0.985140 ‚Ä¢ val=0.756923 ‚Ä¢ impr= 17.1% ‚Ä¢ lr=1.74e-04 ‚Ä¢ g‚âà5675.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa363cbeaee74d1cb5ef3914e78033c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 003:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 ‚Ä¢ train=0.936743 ‚Ä¢ val=0.745417 ‚Ä¢ impr= 18.4% ‚Ä¢ lr=4.56e-04 ‚Ä¢ g‚âà2052.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e8b4abc19b47399ef96762215e0a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 004:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 ‚Ä¢ train=0.893969 ‚Ä¢ val=0.740365 ‚Ä¢ impr= 18.9% ‚Ä¢ lr=1.96e-04 ‚Ä¢ g‚âà4554.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5aa7d632d8413ebacf44e37d5571fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 005:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 ‚Ä¢ train=0.874082 ‚Ä¢ val=0.739896 ‚Ä¢ impr= 19.0% ‚Ä¢ lr=1.62e-05 ‚Ä¢ g‚âà54063.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b51b32ca8ec478281f2481374b7f672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 006:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 ‚Ä¢ train=0.864648 ‚Ä¢ val=0.730916 ‚Ä¢ impr= 20.0% ‚Ä¢ lr=4.63e-04 ‚Ä¢ g‚âà1867.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9942abcc24c460980ab2b5277fb6166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 007:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 ‚Ä¢ train=0.842586 ‚Ä¢ val=0.728495 ‚Ä¢ impr= 20.2% ‚Ä¢ lr=3.52e-04 ‚Ä¢ g‚âà2390.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1630b5241a4ef3b7cdd2b9dd3d61a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 008:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 ‚Ä¢ train=0.825898 ‚Ä¢ val=0.728528 ‚Ä¢ impr= 20.2% ‚Ä¢ lr=2.08e-04 ‚Ä¢ g‚âà3973.38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a557db880454fb5b6e5cb87fd1c6459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 009:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 ‚Ä¢ train=0.816581 ‚Ä¢ val=0.725183 ‚Ä¢ impr= 20.6% ‚Ä¢ lr=8.11e-05 ‚Ä¢ g‚âà10074.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cc6260ebe4406daa4fb2ae1bfc5ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 010:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 ‚Ä¢ train=0.811952 ‚Ä¢ val=0.724647 ‚Ä¢ impr= 20.7% ‚Ä¢ lr=1.76e-05 ‚Ä¢ g‚âà46068.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbf60fb572f489ebb87661120ede69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 011:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 ‚Ä¢ train=0.812572 ‚Ä¢ val=0.721702 ‚Ä¢ impr= 21.0% ‚Ä¢ lr=4.94e-04 ‚Ä¢ g‚âà1646.30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31311440423e45ff94a9634b90b380c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 012:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 ‚Ä¢ train=0.804054 ‚Ä¢ val=0.716945 ‚Ä¢ impr= 21.5% ‚Ä¢ lr=4.66e-04 ‚Ä¢ g‚âà1725.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce36f2973684981bfbd2f325bfc5afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 013:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 ‚Ä¢ train=0.794937 ‚Ä¢ val=0.714624 ‚Ä¢ impr= 21.8% ‚Ä¢ lr=4.19e-04 ‚Ä¢ g‚âà1895.10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4104dbea7c624662aaa4d58fcfea11e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 014:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 ‚Ä¢ train=0.787541 ‚Ä¢ val=0.714527 ‚Ä¢ impr= 21.8% ‚Ä¢ lr=3.58e-04 ‚Ä¢ g‚âà2200.19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39632c99d1694073a6420b9953822e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 015:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 ‚Ä¢ train=0.780937 ‚Ä¢ val=0.712615 ‚Ä¢ impr= 22.0% ‚Ä¢ lr=2.87e-04 ‚Ä¢ g‚âà2719.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03273eab15614f23aa8a1de366cd802c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 016:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 ‚Ä¢ train=0.776833 ‚Ä¢ val=0.709896 ‚Ä¢ impr= 22.3% ‚Ä¢ lr=2.14e-04 ‚Ä¢ g‚âà3635.19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b1aa8469ca4c36964e477a451ee9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 017:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 ‚Ä¢ train=0.773443 ‚Ä¢ val=0.711392 ‚Ä¢ impr= 22.1% ‚Ä¢ lr=1.44e-04 ‚Ä¢ g‚âà5362.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c390b6aeb7044931ac0e7a360f27cd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 018:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 ‚Ä¢ train=0.770426 ‚Ä¢ val=0.708113 ‚Ä¢ impr= 22.5% ‚Ä¢ lr=8.52e-05 ‚Ä¢ g‚âà9043.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c60523e9194573ab6d7a88a91244fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 019:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 ‚Ä¢ train=0.769251 ‚Ä¢ val=0.705521 ‚Ä¢ impr= 22.8% ‚Ä¢ lr=4.20e-05 ‚Ä¢ g‚âà18326.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532181a70eb34c3ab5e4e254f81e49f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 020:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 ‚Ä¢ train=0.768245 ‚Ä¢ val=0.706804 ‚Ä¢ impr= 22.6% ‚Ä¢ lr=1.86e-05 ‚Ä¢ g‚âà41369.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf883145964b4abc8f984b37008f5240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 021:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 ‚Ä¢ train=0.769620 ‚Ä¢ val=0.721207 ‚Ä¢ impr= 21.0% ‚Ä¢ lr=4.99e-04 ‚Ä¢ g‚âà1540.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbea41fa228e410bb79c8c3842892055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 022:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 ‚Ä¢ train=0.777932 ‚Ä¢ val=0.705060 ‚Ä¢ impr= 22.8% ‚Ä¢ lr=4.94e-04 ‚Ä¢ g‚âà1574.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad9af8814ce419b914e127e938b30e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 023:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 ‚Ä¢ train=0.769767 ‚Ä¢ val=0.706929 ‚Ä¢ impr= 22.6% ‚Ä¢ lr=4.84e-04 ‚Ä¢ g‚âà1591.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c504e0784c3e49db886e0ae1ed75f972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 024:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 ‚Ä¢ train=0.765841 ‚Ä¢ val=0.704734 ‚Ä¢ impr= 22.8% ‚Ä¢ lr=4.68e-04 ‚Ä¢ g‚âà1637.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ae6bc348f844778999c3e6695e6955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 025:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 ‚Ä¢ train=0.761967 ‚Ä¢ val=0.705580 ‚Ä¢ impr= 22.8% ‚Ä¢ lr=4.47e-04 ‚Ä¢ g‚âà1705.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d89fb7b907c4b7882b7c127be6f5bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 026:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 ‚Ä¢ train=0.760192 ‚Ä¢ val=0.704478 ‚Ä¢ impr= 22.9% ‚Ä¢ lr=4.22e-04 ‚Ä¢ g‚âà1802.81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a5e920273c463ab6b69303e0e691fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 027:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 ‚Ä¢ train=0.758075 ‚Ä¢ val=0.707203 ‚Ä¢ impr= 22.6% ‚Ä¢ lr=3.93e-04 ‚Ä¢ g‚âà1930.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ff970ac9434ef6bfbc25e1af5c74ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 028:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 ‚Ä¢ train=0.756969 ‚Ä¢ val=0.704450 ‚Ä¢ impr= 22.9% ‚Ä¢ lr=3.61e-04 ‚Ä¢ g‚âà2098.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb42b560984e4553b3fc627c80e9a64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 029:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 ‚Ä¢ train=0.755672 ‚Ä¢ val=0.713058 ‚Ä¢ impr= 21.9% ‚Ä¢ lr=3.26e-04 ‚Ä¢ g‚âà2316.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f7e88078d2448aa53ad0deec178920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 030:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 ‚Ä¢ train=0.757858 ‚Ä¢ val=0.701481 ‚Ä¢ impr= 23.2% ‚Ä¢ lr=2.90e-04 ‚Ä¢ g‚âà2612.08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8502a956c9c84e80b9cbb94e8974834c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 031:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 ‚Ä¢ train=0.752131 ‚Ä¢ val=0.700152 ‚Ä¢ impr= 23.3% ‚Ä¢ lr=2.53e-04 ‚Ä¢ g‚âà2968.93\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1b8235e16945ceb9123c9c32a6281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 032:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 ‚Ä¢ train=0.749997 ‚Ä¢ val=0.703756 ‚Ä¢ impr= 23.0% ‚Ä¢ lr=2.17e-04 ‚Ä¢ g‚âà3462.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113b5d0bdc324e1c8af62d343e738e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 033:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 ‚Ä¢ train=0.748165 ‚Ä¢ val=0.697685 ‚Ä¢ impr= 23.6% ‚Ä¢ lr=1.81e-04 ‚Ä¢ g‚âà4136.54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72c94d0c38b4665a64182dcdc1488eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 034:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 ‚Ä¢ train=0.745577 ‚Ä¢ val=0.696761 ‚Ä¢ impr= 23.7% ‚Ä¢ lr=1.47e-04 ‚Ä¢ g‚âà5076.19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5251134a1d0a488ba6f04f561d3726e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 035:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 ‚Ä¢ train=0.744135 ‚Ä¢ val=0.697445 ‚Ä¢ impr= 23.6% ‚Ä¢ lr=1.15e-04 ‚Ä¢ g‚âà6445.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ef078b33234cf28a292ad242558a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 036:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 ‚Ä¢ train=0.744369 ‚Ä¢ val=0.697566 ‚Ä¢ impr= 23.6% ‚Ä¢ lr=8.73e-05 ‚Ä¢ g‚âà8527.17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adc8355737b480e999d8f6231c739eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 037:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 ‚Ä¢ train=0.743356 ‚Ä¢ val=0.698424 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=6.31e-05 ‚Ä¢ g‚âà11784.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6e2a79b2f64ef4a0ef60fff68a04ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 038:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 ‚Ä¢ train=0.743252 ‚Ä¢ val=0.698687 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=4.34e-05 ‚Ä¢ g‚âà17143.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e75ba1e83d4442aead7065e0c1de5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 039:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 ‚Ä¢ train=0.741826 ‚Ä¢ val=0.699383 ‚Ä¢ impr= 23.4% ‚Ä¢ lr=2.86e-05 ‚Ä¢ g‚âà25955.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a06072398d4300a76d4faa32df9238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 040:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 ‚Ä¢ train=0.741534 ‚Ä¢ val=0.699949 ‚Ä¢ impr= 23.4% ‚Ä¢ lr=1.91e-05 ‚Ä¢ g‚âà38830.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8e9a93a35e4090bdf4bd223b6846ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 041:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 ‚Ä¢ train=0.741409 ‚Ä¢ val=0.700910 ‚Ä¢ impr= 23.3% ‚Ä¢ lr=1.51e-05 ‚Ä¢ g‚âà49023.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd150bffca54f95928ee21b45416dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 042:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 ‚Ä¢ train=0.750005 ‚Ä¢ val=0.698604 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=5.00e-04 ‚Ä¢ g‚âà1501.33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a2ab633f364fa7aeda7f8ad2b8b459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 043:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 ‚Ä¢ train=0.757905 ‚Ä¢ val=0.707456 ‚Ä¢ impr= 22.5% ‚Ä¢ lr=4.98e-04 ‚Ä¢ g‚âà1522.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e80c68f0a344b692e39994c6f53d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 044:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 ‚Ä¢ train=0.750610 ‚Ä¢ val=0.698576 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=4.95e-04 ‚Ä¢ g‚âà1517.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3105f08c692b49a1b951521cea17f5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 045:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 ‚Ä¢ train=0.747068 ‚Ä¢ val=0.696351 ‚Ä¢ impr= 23.8% ‚Ä¢ lr=4.90e-04 ‚Ä¢ g‚âà1524.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b32b60feac44f0babf9b3bf35bc195f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 046:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 ‚Ä¢ train=0.746509 ‚Ä¢ val=0.698310 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=4.84e-04 ‚Ä¢ g‚âà1542.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b5e0410642489991b61fda930d4289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 047:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 ‚Ä¢ train=0.746797 ‚Ä¢ val=0.698415 ‚Ä¢ impr= 23.5% ‚Ä¢ lr=4.77e-04 ‚Ä¢ g‚âà1566.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e35bc204c214aadbf348d5002496411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 048:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 ‚Ä¢ train=0.744357 ‚Ä¢ val=0.695742 ‚Ä¢ impr= 23.8% ‚Ä¢ lr=4.68e-04 ‚Ä¢ g‚âà1589.32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04bc4bab3cc42ba9f61da6f8290fa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 049:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 ‚Ä¢ train=0.745184 ‚Ä¢ val=0.696533 ‚Ä¢ impr= 23.7% ‚Ä¢ lr=4.59e-04 ‚Ä¢ g‚âà1624.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2054db284b419ba6b3384ce5cdb09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 050:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 ‚Ä¢ train=0.744826 ‚Ä¢ val=0.696018 ‚Ä¢ impr= 23.8% ‚Ä¢ lr=4.48e-04 ‚Ä¢ g‚âà1663.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f3c680d8744a2685c48fdd4fc745f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 051:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 ‚Ä¢ train=0.761925 ‚Ä¢ val=0.731849 ‚Ä¢ impr= 19.9% ‚Ä¢ lr=4.36e-04 ‚Ä¢ g‚âà1748.44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce20b7fb6004eb580f43c44d75538ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 052:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 052 ‚Ä¢ train=0.772411 ‚Ä¢ val=0.713504 ‚Ä¢ impr= 21.9% ‚Ä¢ lr=4.23e-04 ‚Ä¢ g‚âà1827.05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157e00f2f5ca46faa137ae3a08144ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 053:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053 ‚Ä¢ train=0.759864 ‚Ä¢ val=0.708134 ‚Ä¢ impr= 22.5% ‚Ä¢ lr=4.09e-04 ‚Ä¢ g‚âà1858.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5ba396a9594dd1a79d26936f8ad9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 054:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 054 ‚Ä¢ train=0.754886 ‚Ä¢ val=0.706986 ‚Ä¢ impr= 22.6% ‚Ä¢ lr=3.94e-04 ‚Ä¢ g‚âà1916.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75b1848ab94f39b62947a03e612d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 055:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 055 ‚Ä¢ train=0.750541 ‚Ä¢ val=0.704456 ‚Ä¢ impr= 22.9% ‚Ä¢ lr=3.78e-04 ‚Ä¢ g‚âà1983.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfc9238d6ca4d94b3863b4d06e41cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 056:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 056 ‚Ä¢ train=0.748705 ‚Ä¢ val=0.702981 ‚Ä¢ impr= 23.0% ‚Ä¢ lr=3.62e-04 ‚Ä¢ g‚âà2068.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dc4a7548824456841dae9a48f1c94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 057:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 057 ‚Ä¢ train=0.747296 ‚Ä¢ val=0.703184 ‚Ä¢ impr= 23.0% ‚Ä¢ lr=3.45e-04 ‚Ä¢ g‚âà2165.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925d8e68717543d9b4ff7083d91e3445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 058:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 058 ‚Ä¢ train=0.745573 ‚Ä¢ val=0.701396 ‚Ä¢ impr= 23.2% ‚Ä¢ lr=3.28e-04 ‚Ä¢ g‚âà2275.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd0c399b7ff402fbd3e45d0b1cf8ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 059:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 059 ‚Ä¢ train=0.744430 ‚Ä¢ val=0.701527 ‚Ä¢ impr= 23.2% ‚Ä¢ lr=3.10e-04 ‚Ä¢ g‚âà2403.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ec865ed0a34132aed460fa4ac1ee7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 060:   0%|                                                                              | 0/1984 [00:00<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 060 ‚Ä¢ train=0.742872 ‚Ä¢ val=0.701209 ‚Ä¢ impr= 23.2% ‚Ä¢ lr=2.92e-04 ‚Ä¢ g‚âà2547.49\n",
      "Early stopping triggered.\n",
      "\n",
      "Champion validation RMSE = 0.695742\n",
      "Improvement vs baseline   =  23.8 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  ONE-OFF  :  baseline on the untouched validation split\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def naive_rmse(ds_val):\n",
    "    \"\"\"RMSE when the forecast is always zero.\"\"\"\n",
    "    mse, n = 0.0, 0\n",
    "    for _, y_day, _ in ds_val:\n",
    "        y = y_day.numpy().squeeze()\n",
    "        mse += np.sum(y ** 2)\n",
    "        n   += y.size\n",
    "    return math.sqrt(mse / n)\n",
    "\n",
    "baseline_val_rmse = naive_rmse(ds_val_unbatched)\n",
    "print(f\"Baseline (predict-zero) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#  Training loop (outer bar only ‚Üí maximum throughput)                     #\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "n_train_days = len(np.unique(day_id_tr))\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "best_val_rmse = custom_stateful_training_loop(\n",
    "        model_train         = model_train,\n",
    "        model_val           = model_val,\n",
    "        ds_train_batched    = ds_train_batched,\n",
    "        ds_val              = ds_val_unbatched,\n",
    "        n_train_days        = n_train_days,\n",
    "        max_epochs          = MAX_EPOCHS,\n",
    "        early_stop_patience = EARLY_STOP_PATIENCE,\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        weights_path        = weights_path)\n",
    "\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "print(f\"Improvement vs baseline   = {(1 - best_val_rmse/baseline_val_rmse)*100:5.1f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e672-70da-480a-8159-42b6a9fb356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b4b02-67f9-43a1-adc5-b67ed3471b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d4eb-4930-4892-bfe5-ebd2499b7e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
