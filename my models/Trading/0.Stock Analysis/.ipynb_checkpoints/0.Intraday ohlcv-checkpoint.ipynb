{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7173f04e-288c-4a46-8516-89d4f857e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from eodhd import APIClient\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import datetime\n",
    "import pytz\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fea4994-5b82-4370-9c34-45cb909fe4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('EODHD_API_KEY')\n",
    "\n",
    "stocks = ['SPY', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'QQQ']\n",
    "\n",
    "start_date = \"2019-01-01\"\n",
    "end_date = \"2022-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a453b21-bcd8-48d2-9fc9-132dce8d9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac2ccbe-841b-44e1-b53a-320be220141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_intraday_stock_data(stocks, start_date, end_date, api_key, output_dir=\"Intraday stocks\"):\n",
    "    \"\"\"\n",
    "    Fetches intraday (1-minute interval) intraday stock data for each of the specified stocks \n",
    "    over a multi-day period using default midnight boundaries.\n",
    "    \n",
    "    Instead of using specific trading hours, this function uses midnight ET as both the start \n",
    "    and the end reference. The period extends from midnight on start_date ET to midnight \n",
    "    on the day following end_date ET (thus covering the full end_date).\n",
    "    \n",
    "    Parameters:\n",
    "      stocks : list or str\n",
    "          One or more stock tickers (without the \".US\" suffix), e.g., ['GOOGL', 'AAPL'].\n",
    "      start_date : str\n",
    "          Start date in \"YYYY-MM-DD\" format.\n",
    "      end_date : str\n",
    "          End date in \"YYYY-MM-DD\" format.\n",
    "      api_key : str\n",
    "          Your API key for the EODHD intraday API.\n",
    "      output_dir : str, optional\n",
    "          Directory in which to save CSV files. If None, files are saved in the current directory.\n",
    "    \n",
    "    Returns:\n",
    "      results : dict\n",
    "          A dictionary mapping each stock symbol to its DataFrame.\n",
    "    \"\"\"\n",
    "    # Make sure stocks is a list.\n",
    "    if isinstance(stocks, str):\n",
    "        stocks = [stocks]\n",
    "    \n",
    "    # Define Eastern Time.\n",
    "    et = pytz.timezone(\"US/Eastern\")\n",
    "    \n",
    "    # Parse input dates.\n",
    "    start_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt   = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the period using midnight as the default:\n",
    "    # Start at midnight on the start_date.\n",
    "    start_et = et.localize(datetime.datetime(start_dt.year, start_dt.month, start_dt.day, 0, 0, 0))\n",
    "    # End at midnight on the day after end_date.\n",
    "    end_et = et.localize(datetime.datetime(end_dt.year, end_dt.month, end_dt.day, 0, 0, 0)) + datetime.timedelta(days=1)\n",
    "    \n",
    "    # Convert the Eastern Time values to UTC.\n",
    "    start_utc = start_et.astimezone(pytz.utc)\n",
    "    end_utc   = end_et.astimezone(pytz.utc)\n",
    "    \n",
    "    # Convert to Unix timestamps.\n",
    "    from_timestamp = int(start_utc.timestamp())\n",
    "    to_timestamp   = int(end_utc.timestamp())\n",
    "    \n",
    "    print(\"Requesting data from (UTC):\", start_utc.isoformat())\n",
    "    print(\"Until (UTC):\", end_utc.isoformat())\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for stock in stocks:\n",
    "        # For US tickers, the API expects a suffix \".US\".\n",
    "        ticker = f\"{stock}.US\"\n",
    "        url = (\n",
    "            f\"https://eodhd.com/api/intraday/{ticker}\"\n",
    "            f\"?interval=1m&api_token={api_key}&fmt=json\"\n",
    "            f\"&from={from_timestamp}&to={to_timestamp}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFetching data for {stock} from URL:\")\n",
    "        print(url)\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data for {stock}: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            continue\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Construct a file name that includes the stock symbol and the date range.\n",
    "        file_name = f\"{stock}_{start_dt.strftime('%Y%m%d')}_{end_dt.strftime('%Y%m%d')}.csv\"\n",
    "        if output_dir:\n",
    "            file_path = os.path.join(output_dir, file_name)\n",
    "        else:\n",
    "            file_path = file_name\n",
    "        \n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved data for {stock} to {file_path}\")\n",
    "        \n",
    "        results[stock] = df\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec131e6-14de-4f0b-a3ae-187b36367aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting data from (UTC): 2019-01-01T05:00:00+00:00\n",
      "Until (UTC): 2022-01-02T05:00:00+00:00\n",
      "\n",
      "Fetching data for SPY from URL:\n",
      "https://eodhd.com/api/intraday/SPY.US?interval=1m&api_token=None&fmt=json&from=1546318800&to=1641099600\n",
      "Error fetching data for SPY: 401\n",
      "Unauthenticated\n",
      "\n",
      "Fetching data for AAPL from URL:\n",
      "https://eodhd.com/api/intraday/AAPL.US?interval=1m&api_token=None&fmt=json&from=1546318800&to=1641099600\n",
      "Error fetching data for AAPL: 401\n",
      "Unauthenticated\n",
      "\n",
      "Fetching data for MSFT from URL:\n",
      "https://eodhd.com/api/intraday/MSFT.US?interval=1m&api_token=None&fmt=json&from=1546318800&to=1641099600\n",
      "Error fetching data for MSFT: 401\n",
      "Unauthenticated\n",
      "\n",
      "Fetching data for NVDA from URL:\n",
      "https://eodhd.com/api/intraday/NVDA.US?interval=1m&api_token=None&fmt=json&from=1546318800&to=1641099600\n",
      "Error fetching data for NVDA: 401\n",
      "Unauthenticated\n",
      "\n",
      "Fetching data for TSLA from URL:\n",
      "https://eodhd.com/api/intraday/TSLA.US?interval=1m&api_token=None&fmt=json&from=1546318800&to=1641099600\n",
      "Error fetching data for TSLA: 401\n",
      "Unauthenticated\n",
      "\n",
      "Fetching data for QQQ from URL:\n",
      "https://eodhd.com/api/intraday/QQQ.US?interval=1m&api_token=None&fmt=json&from=1546318800&to=1641099600\n",
      "Error fetching data for QQQ: 401\n",
      "Unauthenticated\n"
     ]
    }
   ],
   "source": [
    "data = fetch_intraday_stock_data(stocks, start_date, end_date, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64576587-c2b0-4a09-8829-f99ce3b648a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SPY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SPY'"
     ]
    }
   ],
   "source": [
    "data['SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b802948-966b-40e6-8819-422a088ceeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee27944-82cb-4b3b-b765-4008414fe8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stock_files(stock, file_dir, date_col=\"datetime\", output_dir=\"Intraday stocks\"):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files for a given stock by reading, concatenating, \n",
    "    and sorting them in chronological order. The final output file is named using \n",
    "    the earliest date (from the first file) and the latest date (from the last file).\n",
    "    \n",
    "    Parameters:\n",
    "      stock    : str\n",
    "          The stock ticker (e.g., \"META\").\n",
    "      file_dir : str\n",
    "          The directory where the CSV files are stored.\n",
    "      date_col : str, default \"Date\"\n",
    "          The name of the column containing the datetime.\n",
    "          (This column will be parsed as a datetime and set as the index.)\n",
    "      output_dir : str, optional\n",
    "          Directory where the merged CSV file should be saved. \n",
    "          If None, the file is saved in the current directory.\n",
    "    \n",
    "    Returns:\n",
    "      merged_df : pd.DataFrame\n",
    "          A DataFrame containing the merged data in chronological order.\n",
    "      final_file_path : str\n",
    "          The path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    # Create a pattern to match files starting with the stock ticker (e.g., \"META_*.csv\")\n",
    "    pattern = os.path.join(file_dir, f\"{stock}_*.csv\")\n",
    "    file_list = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(f\"No files found for ticker {stock} in directory: {file_dir}\")\n",
    "    \n",
    "    df_list = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, parse_dates=[date_col])\n",
    "        df.set_index(date_col, inplace=True)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate and sort the resultant DataFrame.\n",
    "    merged_df = pd.concat(df_list)\n",
    "    merged_df.sort_index(inplace=True)\n",
    "    \n",
    "    # Retrieve the earliest and latest dates encountered.\n",
    "    earliest_date = merged_df.index.min()\n",
    "    latest_date = merged_df.index.max()\n",
    "    \n",
    "    earliest_str = earliest_date.strftime(\"%Y%m%d\")\n",
    "    latest_str = latest_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Construct the final file name using the stock ticker and these dates.\n",
    "    final_filename = f\"{stock}_{earliest_str}_{latest_str}.csv\"\n",
    "    if output_dir:\n",
    "        file_path = os.path.join(output_dir, final_filename)\n",
    "    else:\n",
    "        file_path = final_filename\n",
    "    \n",
    "    # Save the merged DataFrame.\n",
    "    merged_df.to_csv(file_path)\n",
    "    print(f\"Merged file saved as: {file_path}\")\n",
    "    \n",
    "    return merged_df, file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e60ac1-76dd-48de-a637-f7f0c6324bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary to store merged DataFrames for each stock.\n",
    "merged_data = {}\n",
    "\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        merged_df, final_path = merge_stock_files(\n",
    "            stock,\n",
    "            file_dir=\"Intraday stocks\",\n",
    "            date_col=\"datetime\",  # Adjust this if your date column is called something else\n",
    "            output_dir=\"Intraday stocks\"\n",
    "        )\n",
    "        merged_data[stock] = merged_df\n",
    "        print(f\"Merged data for {stock} saved to {final_path}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "# Now merged_data contains a DataFrame for each stock, merged in the correct chronological order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34047329-daf6-4f56-8f87-81075751b19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692c911-726e-4046-a9c2-6caa4896a8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8876a5-421f-4d24-b735-1bdeee7160b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e493caa-5c51-4868-841d-1c2c67dac375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
