{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7173f04e-288c-4a46-8516-89d4f857e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from eodhd import APIClient\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import datetime\n",
    "import pytz\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fea4994-5b82-4370-9c34-45cb909fe4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('EODHD_API_KEY')\n",
    "\n",
    "stocks = [\n",
    "    \"AAPL\",   # Apple Inc.\n",
    "    \"MSFT\",   # Microsoft Corporation\n",
    "    \"AMZN\",   # Amazon.com, Inc.\n",
    "    \"GOOGL\",  # Alphabet Inc.\n",
    "    \"TSLA\",   # Tesla, Inc.\n",
    "    \"NVDA\",   # NVIDIA Corporation\n",
    "    \"META\",   # Meta Platforms, Inc.\n",
    "    \"NFLX\",   # Netflix, Inc.\n",
    "    \"AMD\",    # Advanced Micro Devices, Inc.\n",
    "    \"INTC\",   # Intel Corporation\n",
    "    \"BAC\",    # Bank of America\n",
    "    \"JPM\",    # JPMorgan Chase & Co.\n",
    "    \"WFC\",    # Wells Fargo & Company\n",
    "    \"XOM\",    # ExxonMobil\n",
    "    \"CVX\",    # Chevron Corporation\n",
    "    \"PFE\",    # Pfizer Inc.\n",
    "    \"MRK\",    # Merck & Co.\n",
    "    \"UNH\",    # UnitedHealth Group\n",
    "    \"V\",      # Visa Inc.\n",
    "    \"MA\"      # Mastercard Incorporated\n",
    "]\n",
    "\n",
    "start_date = \"2025-04-01\"\n",
    "end_date = \"2025-06-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac2ccbe-841b-44e1-b53a-320be220141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_intraday_stock_data(stocks, start_date, end_date, api_key, output_dir=\"Intraday stocks\"):\n",
    "    \"\"\"\n",
    "    Fetches intraday (1-minute interval) intraday stock data for each of the specified stocks \n",
    "    over a multi-day period using default midnight boundaries.\n",
    "    \n",
    "    Instead of using specific trading hours, this function uses midnight ET as both the start \n",
    "    and the end reference. The period extends from midnight on start_date ET to midnight \n",
    "    on the day following end_date ET (thus covering the full end_date).\n",
    "    \n",
    "    Parameters:\n",
    "      stocks : list or str\n",
    "          One or more stock tickers (without the \".US\" suffix), e.g., ['GOOGL', 'AAPL'].\n",
    "      start_date : str\n",
    "          Start date in \"YYYY-MM-DD\" format.\n",
    "      end_date : str\n",
    "          End date in \"YYYY-MM-DD\" format.\n",
    "      api_key : str\n",
    "          Your API key for the EODHD intraday API.\n",
    "      output_dir : str, optional\n",
    "          Directory in which to save CSV files. If None, files are saved in the current directory.\n",
    "    \n",
    "    Returns:\n",
    "      results : dict\n",
    "          A dictionary mapping each stock symbol to its DataFrame.\n",
    "    \"\"\"\n",
    "    # Make sure stocks is a list.\n",
    "    if isinstance(stocks, str):\n",
    "        stocks = [stocks]\n",
    "    \n",
    "    # Define Eastern Time.\n",
    "    et = pytz.timezone(\"US/Eastern\")\n",
    "    \n",
    "    # Parse input dates.\n",
    "    start_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt   = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Define the period using midnight as the default:\n",
    "    # Start at midnight on the start_date.\n",
    "    start_et = et.localize(datetime.datetime(start_dt.year, start_dt.month, start_dt.day, 0, 0, 0))\n",
    "    # End at midnight on the day after end_date.\n",
    "    end_et = et.localize(datetime.datetime(end_dt.year, end_dt.month, end_dt.day, 0, 0, 0)) + datetime.timedelta(days=1)\n",
    "    \n",
    "    # Convert the Eastern Time values to UTC.\n",
    "    start_utc = start_et.astimezone(pytz.utc)\n",
    "    end_utc   = end_et.astimezone(pytz.utc)\n",
    "    \n",
    "    # Convert to Unix timestamps.\n",
    "    from_timestamp = int(start_utc.timestamp())\n",
    "    to_timestamp   = int(end_utc.timestamp())\n",
    "    \n",
    "    print(\"Requesting data from (UTC):\", start_utc.isoformat())\n",
    "    print(\"Until (UTC):\", end_utc.isoformat())\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for stock in stocks:\n",
    "        # For US tickers, the API expects a suffix \".US\".\n",
    "        ticker = f\"{stock}.US\"\n",
    "        url = (\n",
    "            f\"https://eodhd.com/api/intraday/{ticker}\"\n",
    "            f\"?interval=1m&api_token={api_key}&fmt=json\"\n",
    "            f\"&from={from_timestamp}&to={to_timestamp}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFetching data for {stock} from URL:\")\n",
    "        print(url)\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data for {stock}: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            continue\n",
    "        \n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Construct a file name that includes the stock symbol and the date range.\n",
    "        file_name = f\"{stock}_{start_dt.strftime('%Y%m%d')}_{end_dt.strftime('%Y%m%d')}.csv\"\n",
    "        if output_dir:\n",
    "            file_path = os.path.join(output_dir, file_name)\n",
    "        else:\n",
    "            file_path = file_name\n",
    "        \n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved data for {stock} to {file_path}\")\n",
    "        \n",
    "        results[stock] = df\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec131e6-14de-4f0b-a3ae-187b36367aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting data from (UTC): 2025-04-01T04:00:00+00:00\n",
      "Until (UTC): 2025-07-01T04:00:00+00:00\n",
      "\n",
      "Fetching data for AAPL from URL:\n",
      "https://eodhd.com/api/intraday/AAPL.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for AAPL to Intraday stocks\\AAPL_20250401_20250630.csv\n",
      "\n",
      "Fetching data for MSFT from URL:\n",
      "https://eodhd.com/api/intraday/MSFT.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for MSFT to Intraday stocks\\MSFT_20250401_20250630.csv\n",
      "\n",
      "Fetching data for AMZN from URL:\n",
      "https://eodhd.com/api/intraday/AMZN.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for AMZN to Intraday stocks\\AMZN_20250401_20250630.csv\n",
      "\n",
      "Fetching data for GOOGL from URL:\n",
      "https://eodhd.com/api/intraday/GOOGL.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for GOOGL to Intraday stocks\\GOOGL_20250401_20250630.csv\n",
      "\n",
      "Fetching data for TSLA from URL:\n",
      "https://eodhd.com/api/intraday/TSLA.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for TSLA to Intraday stocks\\TSLA_20250401_20250630.csv\n",
      "\n",
      "Fetching data for NVDA from URL:\n",
      "https://eodhd.com/api/intraday/NVDA.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for NVDA to Intraday stocks\\NVDA_20250401_20250630.csv\n",
      "\n",
      "Fetching data for META from URL:\n",
      "https://eodhd.com/api/intraday/META.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for META to Intraday stocks\\META_20250401_20250630.csv\n",
      "\n",
      "Fetching data for NFLX from URL:\n",
      "https://eodhd.com/api/intraday/NFLX.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for NFLX to Intraday stocks\\NFLX_20250401_20250630.csv\n",
      "\n",
      "Fetching data for AMD from URL:\n",
      "https://eodhd.com/api/intraday/AMD.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for AMD to Intraday stocks\\AMD_20250401_20250630.csv\n",
      "\n",
      "Fetching data for INTC from URL:\n",
      "https://eodhd.com/api/intraday/INTC.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for INTC to Intraday stocks\\INTC_20250401_20250630.csv\n",
      "\n",
      "Fetching data for BAC from URL:\n",
      "https://eodhd.com/api/intraday/BAC.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for BAC to Intraday stocks\\BAC_20250401_20250630.csv\n",
      "\n",
      "Fetching data for JPM from URL:\n",
      "https://eodhd.com/api/intraday/JPM.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for JPM to Intraday stocks\\JPM_20250401_20250630.csv\n",
      "\n",
      "Fetching data for WFC from URL:\n",
      "https://eodhd.com/api/intraday/WFC.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for WFC to Intraday stocks\\WFC_20250401_20250630.csv\n",
      "\n",
      "Fetching data for XOM from URL:\n",
      "https://eodhd.com/api/intraday/XOM.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for XOM to Intraday stocks\\XOM_20250401_20250630.csv\n",
      "\n",
      "Fetching data for CVX from URL:\n",
      "https://eodhd.com/api/intraday/CVX.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for CVX to Intraday stocks\\CVX_20250401_20250630.csv\n",
      "\n",
      "Fetching data for PFE from URL:\n",
      "https://eodhd.com/api/intraday/PFE.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for PFE to Intraday stocks\\PFE_20250401_20250630.csv\n",
      "\n",
      "Fetching data for MRK from URL:\n",
      "https://eodhd.com/api/intraday/MRK.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for MRK to Intraday stocks\\MRK_20250401_20250630.csv\n",
      "\n",
      "Fetching data for UNH from URL:\n",
      "https://eodhd.com/api/intraday/UNH.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for UNH to Intraday stocks\\UNH_20250401_20250630.csv\n",
      "\n",
      "Fetching data for V from URL:\n",
      "https://eodhd.com/api/intraday/V.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for V to Intraday stocks\\V_20250401_20250630.csv\n",
      "\n",
      "Fetching data for MA from URL:\n",
      "https://eodhd.com/api/intraday/MA.US?interval=1m&api_token=67b5ff99b63306.32749083&fmt=json&from=1743480000&to=1751342400\n",
      "Saved data for MA to Intraday stocks\\MA_20250401_20250630.csv\n"
     ]
    }
   ],
   "source": [
    "data = fetch_intraday_stock_data(stocks, start_date, end_date, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64576587-c2b0-4a09-8829-f99ce3b648a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gmtoffset</th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1738573200</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-03 09:00:00</td>\n",
       "      <td>200.01000000</td>\n",
       "      <td>200.09000000</td>\n",
       "      <td>198.26000000</td>\n",
       "      <td>200.03000000</td>\n",
       "      <td>5135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1738573260</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-03 09:01:00</td>\n",
       "      <td>200.08000000</td>\n",
       "      <td>200.09000000</td>\n",
       "      <td>199.86000000</td>\n",
       "      <td>199.90000000</td>\n",
       "      <td>2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1738573320</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-03 09:02:00</td>\n",
       "      <td>199.91000000</td>\n",
       "      <td>200.00000000</td>\n",
       "      <td>199.46000000</td>\n",
       "      <td>199.88000000</td>\n",
       "      <td>4996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1738573380</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-03 09:03:00</td>\n",
       "      <td>199.77000000</td>\n",
       "      <td>200.09000000</td>\n",
       "      <td>199.75000000</td>\n",
       "      <td>199.84000000</td>\n",
       "      <td>5094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1738573440</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-02-03 09:04:00</td>\n",
       "      <td>199.90000000</td>\n",
       "      <td>200.01000000</td>\n",
       "      <td>199.80000000</td>\n",
       "      <td>200.00000000</td>\n",
       "      <td>2940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68842</th>\n",
       "      <td>1747353300</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 23:55:00</td>\n",
       "      <td>164.72430000</td>\n",
       "      <td>164.72430000</td>\n",
       "      <td>164.68000000</td>\n",
       "      <td>164.68000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68843</th>\n",
       "      <td>1747353360</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 23:56:00</td>\n",
       "      <td>164.68000000</td>\n",
       "      <td>164.68000000</td>\n",
       "      <td>164.52050000</td>\n",
       "      <td>164.67000000</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68844</th>\n",
       "      <td>1747353420</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 23:57:00</td>\n",
       "      <td>164.60000000</td>\n",
       "      <td>164.73000000</td>\n",
       "      <td>164.60000000</td>\n",
       "      <td>164.68000000</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68845</th>\n",
       "      <td>1747353480</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 23:58:00</td>\n",
       "      <td>164.67000000</td>\n",
       "      <td>164.72880000</td>\n",
       "      <td>164.55010000</td>\n",
       "      <td>164.60000000</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68846</th>\n",
       "      <td>1747353540</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 23:59:00</td>\n",
       "      <td>164.60000000</td>\n",
       "      <td>164.72730000</td>\n",
       "      <td>164.60000000</td>\n",
       "      <td>164.66000000</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68847 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp  gmtoffset             datetime         open         high  \\\n",
       "0      1738573200          0  2025-02-03 09:00:00 200.01000000 200.09000000   \n",
       "1      1738573260          0  2025-02-03 09:01:00 200.08000000 200.09000000   \n",
       "2      1738573320          0  2025-02-03 09:02:00 199.91000000 200.00000000   \n",
       "3      1738573380          0  2025-02-03 09:03:00 199.77000000 200.09000000   \n",
       "4      1738573440          0  2025-02-03 09:04:00 199.90000000 200.01000000   \n",
       "...           ...        ...                  ...          ...          ...   \n",
       "68842  1747353300          0  2025-05-15 23:55:00 164.72430000 164.72430000   \n",
       "68843  1747353360          0  2025-05-15 23:56:00 164.68000000 164.68000000   \n",
       "68844  1747353420          0  2025-05-15 23:57:00 164.60000000 164.73000000   \n",
       "68845  1747353480          0  2025-05-15 23:58:00 164.67000000 164.72880000   \n",
       "68846  1747353540          0  2025-05-15 23:59:00 164.60000000 164.72730000   \n",
       "\n",
       "               low        close  volume  \n",
       "0     198.26000000 200.03000000    5135  \n",
       "1     199.86000000 199.90000000    2165  \n",
       "2     199.46000000 199.88000000    4996  \n",
       "3     199.75000000 199.84000000    5094  \n",
       "4     199.80000000 200.00000000    2940  \n",
       "...            ...          ...     ...  \n",
       "68842 164.68000000 164.68000000      25  \n",
       "68843 164.52050000 164.67000000     218  \n",
       "68844 164.60000000 164.68000000      49  \n",
       "68845 164.55010000 164.60000000     136  \n",
       "68846 164.60000000 164.66000000    1046  \n",
       "\n",
       "[68847 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['GOOGL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee27944-82cb-4b3b-b765-4008414fe8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stock_files(stock, file_dir, date_col=\"datetime\", output_dir=\"Intraday stocks\"):\n",
    "    \"\"\"\n",
    "    Merge multiple CSV files for a given stock by reading, concatenating, \n",
    "    and sorting them in chronological order. The final output file is named using \n",
    "    the earliest date (from the first file) and the latest date (from the last file).\n",
    "    \n",
    "    Parameters:\n",
    "      stock    : str\n",
    "          The stock ticker (e.g., \"META\").\n",
    "      file_dir : str\n",
    "          The directory where the CSV files are stored.\n",
    "      date_col : str, default \"Date\"\n",
    "          The name of the column containing the datetime.\n",
    "          (This column will be parsed as a datetime and set as the index.)\n",
    "      output_dir : str, optional\n",
    "          Directory where the merged CSV file should be saved. \n",
    "          If None, the file is saved in the current directory.\n",
    "    \n",
    "    Returns:\n",
    "      merged_df : pd.DataFrame\n",
    "          A DataFrame containing the merged data in chronological order.\n",
    "      final_file_path : str\n",
    "          The path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    # Create a pattern to match files starting with the stock ticker (e.g., \"META_*.csv\")\n",
    "    pattern = os.path.join(file_dir, f\"{stock}_*.csv\")\n",
    "    file_list = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(f\"No files found for ticker {stock} in directory: {file_dir}\")\n",
    "    \n",
    "    df_list = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, parse_dates=[date_col])\n",
    "        df.set_index(date_col, inplace=True)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate and sort the resultant DataFrame.\n",
    "    merged_df = pd.concat(df_list)\n",
    "    merged_df.sort_index(inplace=True)\n",
    "    \n",
    "    # Retrieve the earliest and latest dates encountered.\n",
    "    earliest_date = merged_df.index.min()\n",
    "    latest_date = merged_df.index.max()\n",
    "    \n",
    "    earliest_str = earliest_date.strftime(\"%Y%m%d\")\n",
    "    latest_str = latest_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Construct the final file name using the stock ticker and these dates.\n",
    "    final_filename = f\"{stock}_{earliest_str}_{latest_str}.csv\"\n",
    "    if output_dir:\n",
    "        file_path = os.path.join(output_dir, final_filename)\n",
    "    else:\n",
    "        file_path = final_filename\n",
    "    \n",
    "    # Save the merged DataFrame.\n",
    "    merged_df.to_csv(file_path)\n",
    "    print(f\"Merged file saved as: {file_path}\")\n",
    "    \n",
    "    return merged_df, file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47e60ac1-76dd-48de-a637-f7f0c6324bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as: Intraday stocks\\AAPL_20250102_20250603.csv\n",
      "Merged data for AAPL saved to Intraday stocks\\AAPL_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\MSFT_20250102_20250603.csv\n",
      "Merged data for MSFT saved to Intraday stocks\\MSFT_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\AMZN_20250102_20250603.csv\n",
      "Merged data for AMZN saved to Intraday stocks\\AMZN_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\GOOGL_20250102_20250603.csv\n",
      "Merged data for GOOGL saved to Intraday stocks\\GOOGL_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\TSLA_20250102_20250603.csv\n",
      "Merged data for TSLA saved to Intraday stocks\\TSLA_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\NVDA_20250102_20250603.csv\n",
      "Merged data for NVDA saved to Intraday stocks\\NVDA_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\META_20250102_20250603.csv\n",
      "Merged data for META saved to Intraday stocks\\META_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\NFLX_20250102_20250603.csv\n",
      "Merged data for NFLX saved to Intraday stocks\\NFLX_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\AMD_20250102_20250603.csv\n",
      "Merged data for AMD saved to Intraday stocks\\AMD_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\INTC_20250102_20250603.csv\n",
      "Merged data for INTC saved to Intraday stocks\\INTC_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\BAC_20250102_20250603.csv\n",
      "Merged data for BAC saved to Intraday stocks\\BAC_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\JPM_20250102_20250603.csv\n",
      "Merged data for JPM saved to Intraday stocks\\JPM_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\WFC_20250102_20250603.csv\n",
      "Merged data for WFC saved to Intraday stocks\\WFC_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\XOM_20250102_20250603.csv\n",
      "Merged data for XOM saved to Intraday stocks\\XOM_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\CVX_20250102_20250603.csv\n",
      "Merged data for CVX saved to Intraday stocks\\CVX_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\PFE_20250102_20250603.csv\n",
      "Merged data for PFE saved to Intraday stocks\\PFE_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\MRK_20250102_20250603.csv\n",
      "Merged data for MRK saved to Intraday stocks\\MRK_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\UNH_20250102_20250603.csv\n",
      "Merged data for UNH saved to Intraday stocks\\UNH_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\V_20250102_20250603.csv\n",
      "Merged data for V saved to Intraday stocks\\V_20250102_20250603.csv\n",
      "Merged file saved as: Intraday stocks\\MA_20250102_20250603.csv\n",
      "Merged data for MA saved to Intraday stocks\\MA_20250102_20250603.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to store merged DataFrames for each stock.\n",
    "merged_data = {}\n",
    "\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        merged_df, final_path = merge_stock_files(\n",
    "            stock,\n",
    "            file_dir=\"Intraday stocks\",\n",
    "            date_col=\"datetime\",  # Adjust this if your date column is called something else\n",
    "            output_dir=\"Intraday stocks\"\n",
    "        )\n",
    "        merged_data[stock] = merged_df\n",
    "        print(f\"Merged data for {stock} saved to {final_path}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "# Now merged_data contains a DataFrame for each stock, merged in the correct chronological order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34047329-daf6-4f56-8f87-81075751b19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692c911-726e-4046-a9c2-6caa4896a8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8876a5-421f-4d24-b735-1bdeee7160b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e493caa-5c51-4868-841d-1c2c67dac375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
