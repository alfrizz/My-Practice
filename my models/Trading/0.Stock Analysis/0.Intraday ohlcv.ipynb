{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7173f04e-288c-4a46-8516-89d4f857e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil                     \n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from eodhd import APIClient\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import pytz\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34047329-daf6-4f56-8f87-81075751b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1 · helper – split the full span into equal-length chunks                   #\n",
    "###############################################################################\n",
    "def date_chunks(start: dt.date, end: dt.date, chunk_len: int):\n",
    "    \"\"\"\n",
    "    Yields (chunk_start, chunk_end) inclusive date tuples no longer than chunk_len.\n",
    "    \"\"\"\n",
    "    cur = start\n",
    "    while cur <= end:\n",
    "        nxt = cur + dt.timedelta(days=chunk_len - 1)\n",
    "        yield (cur, min(nxt, end))\n",
    "        cur = nxt + dt.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9f6675-4f67-4287-8933-09a1e3e5630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get(url, tries=5, pause=2):\n",
    "    \"\"\"Attempts to perform a GET request with retries.\"\"\"\n",
    "    for k in range(tries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"  ⚠️  Error: {e} (retry {k+1}/{tries})\")\n",
    "            if k == tries - 1:\n",
    "                raise\n",
    "            time.sleep(pause * (k + 1))  # Exponential-ish back-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8692c911-726e-4046-a9c2-6caa4896a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2 · one call = one CSV                                                      #\n",
    "###############################################################################\n",
    "def fetch_chunk(stock, d0, d1, api_key, out_dir):\n",
    "    \"\"\"Fetches one 1-minute chunk and saves it to disk; returns Path.\"\"\"\n",
    "    et = pytz.timezone(\"US/Eastern\")\n",
    "    start_et = et.localize(dt.datetime.combine(d0, dt.time()))\n",
    "    end_et   = et.localize(dt.datetime.combine(d1 + dt.timedelta(days=1), dt.time()))\n",
    "    url = (\n",
    "        f\"https://eodhd.com/api/intraday/{stock}.US\"\n",
    "        f\"?interval=1m&api_token={api_key}&fmt=json\"\n",
    "        f\"&from={int(start_et.astimezone(pytz.utc).timestamp())}\"\n",
    "        f\"&to={int(end_et.astimezone(pytz.utc).timestamp())}\"\n",
    "    )\n",
    "    fname = f\"{stock}_{d0:%Y%m%d}_{d1:%Y%m%d}.csv\"\n",
    "    path  = Path(out_dir) / fname\n",
    "    print(f\"  → downloading chunk {fname}\")\n",
    "    \n",
    "    # Use safe_get if desired. Here use requests.get() directly.\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    pd.DataFrame(response.json()).to_csv(path, index=False)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8876a5-421f-4d24-b735-1bdeee7160b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3 · wrapper for ONE ticker                                                  #\n",
    "###############################################################################\n",
    "def download_and_merge(stock, api_key, start_date, chunk_days, output_dir):\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    archive_dir = output_dir / \"old\"\n",
    "    archive_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    start = dt.datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    today = dt.date.today()\n",
    "    \n",
    "    # Construct final merged file name.\n",
    "    final_name = f\"{stock}_{start:%Y%m%d}_{today:%Y%m%d}.csv\"\n",
    "    final_path = output_dir / final_name\n",
    "    if final_path.exists():\n",
    "        print(f\"Merged file already exists for {stock} at {final_path}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # --- Step A · download every chunk --------------------------------------\n",
    "    csv_paths = []\n",
    "    for d0, d1 in date_chunks(start, today, chunk_days):\n",
    "        try:\n",
    "            p = fetch_chunk(stock, d0, d1, api_key, output_dir)\n",
    "            csv_paths.append(p)\n",
    "            time.sleep(1) # small pause between calls if needed\n",
    "        except Exception as e:\n",
    "            print(f\"Chunk {d0}–{d1} failed:\", e)\n",
    "    \n",
    "    if not csv_paths:\n",
    "        print(\"No data downloaded for\", stock)\n",
    "        return\n",
    "    \n",
    "    # --- Step B · merge chronologically -------------------------------------\n",
    "    dfs = []\n",
    "    for p in sorted(csv_paths):\n",
    "        try:\n",
    "            df = pd.read_csv(p, parse_dates=[\"datetime\"])\n",
    "            df.set_index(\"datetime\", inplace=True)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {p}: {e}\")\n",
    "    if not dfs:\n",
    "        print(\"No chunk files parsed for\", stock)\n",
    "        return\n",
    "\n",
    "    merged = pd.concat(dfs).sort_index()\n",
    "    merged.to_csv(final_path)\n",
    "    print(\"Merged file saved to\", final_path)\n",
    "    \n",
    "    # --- Step C · archive chunk files ---------------------------------------\n",
    "    for p in csv_paths:\n",
    "        shutil.move(str(p), str(archive_dir / p.name))\n",
    "    print(f\"Archived {len(csv_paths)} chunk files for {stock}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fea4994-5b82-4370-9c34-45cb909fe4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('EODHD_API_KEY')\n",
    "\n",
    "stocks = ['SPY', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'QQQ']\n",
    "\n",
    "start_date = \"2019-01-01\"\n",
    "\n",
    "max_allow_days = 120\n",
    "\n",
    "output_dir=\"Intraday stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e493caa-5c51-4868-841d-1c2c67dac375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SPY ===\n",
      "Merged file already exists for SPY at Intraday stocks/SPY_20190101_20250619.csv. Skipping.\n",
      "\n",
      "=== AAPL ===\n",
      "  → downloading chunk AAPL_20190101_20190430.csv\n",
      "  → downloading chunk AAPL_20190501_20190828.csv\n",
      "  → downloading chunk AAPL_20190829_20191226.csv\n",
      "  → downloading chunk AAPL_20191227_20200424.csv\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# =============================== run ========================================#\n",
    "###############################################################################\n",
    "\n",
    "for s in stocks:\n",
    "    print(f\"\\n=== {s} ===\")\n",
    "    download_and_merge(s, api_key=api_key, start_date=start_date, chunk_days=max_allow_days, output_dir=output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b67cf7-2cdf-4351-87c6-17dd3529cc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0dbc1-0801-4103-9794-5821963ade17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865a757-617c-4160-ae7b-c19b88827693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
