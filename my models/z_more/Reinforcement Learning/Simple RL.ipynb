{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ca10981-2ef4-43b9-8ce2-54a5cd5f76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "class SampleEnvironment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 20 # maximum number of steps agent can take to get the reward\n",
    "        \n",
    "    def get_observation(self) -> List[float]:\n",
    "        return[0.0, 0.0, 0.0] # hardcoded information (logic to be implemented) that the envirment returns\n",
    "    \n",
    "    def get_actions(self) -> List[int]:\n",
    "        return random.choice([0, 1])\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.steps_left == 0 # when steps have been completed, the agent should not be moving anymore\n",
    "    \n",
    "    def get_reward(self, action: int) -> float: # takes an action int[0,1] as input, returns a reward float[0,1] as output\n",
    "        if self.is_done():\n",
    "            raise Exception('Game is Over')\n",
    "        self.steps_left -= 1\n",
    "        if action == 0:\n",
    "            return random.random()\n",
    "        else:\n",
    "            return 2*random.random()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed125014-adc3-4910-82c4-628481e34ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0 # initial assigned reward\n",
    "        \n",
    "    def step(self, env: SampleEnvironment): # it takes the class SampleEnvironment as input parameter\n",
    "        current_obs = env.get_observation()\n",
    "        print('Observation:',current_obs)\n",
    "        action = env.get_actions()\n",
    "        print('Action:',action)\n",
    "        reward = env.get_reward(action) # randomly pick a positive or negative reward\n",
    "        print('Reward:',reward)\n",
    "        self.total_reward += reward\n",
    "        print('Accumulated Reward:',self.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59c94a63-424f-4f57-9cee-216d35ba6a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........\n",
      "Step: 1\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.05329683873870794\n",
      "Accumulated Reward: 0.05329683873870794\n",
      ".........\n",
      "Step: 2\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 0.03166725033558504\n",
      "Accumulated Reward: 0.08496408907429298\n",
      ".........\n",
      "Step: 3\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.556438996862566\n",
      "Accumulated Reward: 0.641403085936859\n",
      ".........\n",
      "Step: 4\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.3146600047697795\n",
      "Accumulated Reward: 0.9560630907066385\n",
      ".........\n",
      "Step: 5\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.6420326379648259\n",
      "Accumulated Reward: 1.5980957286714643\n",
      ".........\n",
      "Step: 6\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.4502170889496833\n",
      "Accumulated Reward: 3.048312817621148\n",
      ".........\n",
      "Step: 7\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.7297008695084356\n",
      "Accumulated Reward: 3.7780136871295835\n",
      ".........\n",
      "Step: 8\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.4019478114198054\n",
      "Accumulated Reward: 4.179961498549389\n",
      ".........\n",
      "Step: 9\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.084707583105302\n",
      "Accumulated Reward: 5.264669081654691\n",
      ".........\n",
      "Step: 10\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.19714424515925844\n",
      "Accumulated Reward: 5.4618133268139495\n",
      ".........\n",
      "Step: 11\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.6319817162300112\n",
      "Accumulated Reward: 6.0937950430439605\n",
      ".........\n",
      "Step: 12\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.532792733548082\n",
      "Accumulated Reward: 6.626587776592043\n",
      ".........\n",
      "Step: 13\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.0460728059094382\n",
      "Accumulated Reward: 7.672660582501481\n",
      ".........\n",
      "Step: 14\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.5711069386096894\n",
      "Accumulated Reward: 9.24376752111117\n",
      ".........\n",
      "Step: 15\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.6799394388793754\n",
      "Accumulated Reward: 10.923706959990545\n",
      ".........\n",
      "Step: 16\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.784165576452269\n",
      "Accumulated Reward: 11.707872536442814\n",
      ".........\n",
      "Step: 17\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 0.5629527016779718\n",
      "Accumulated Reward: 12.270825238120786\n",
      ".........\n",
      "Step: 18\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.0813203284961728\n",
      "Accumulated Reward: 13.352145566616958\n",
      ".........\n",
      "Step: 19\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 1\n",
      "Reward: 1.2364100877783413\n",
      "Accumulated Reward: 14.5885556543953\n",
      ".........\n",
      "Step: 20\n",
      "Observation: [0.0, 0.0, 0.0]\n",
      "Action: 0\n",
      "Reward: 0.9128901244648817\n",
      "Accumulated Reward: 15.501445778860182\n",
      "----------------------\n",
      "Total reward: 15.501445778860182\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = SampleEnvironment()\n",
    "    agent = Agent()\n",
    "    i = 0\n",
    "    \n",
    "    while not env.is_done():\n",
    "        i = i+1\n",
    "        print('.........')\n",
    "        print('Step:',i)\n",
    "        agent.step(env)\n",
    "    \n",
    "    print('----------------------')\n",
    "    print('Total reward:', agent.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c648c43-18d5-4086-9173-a92d8a46ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.random()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
