{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f58e2f8-afd2-4814-9a50-3b3812fb3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
    "\n",
    "# # to avoid OutOfMemoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2086a5-35b3-4c78-99c2-78c239da52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# We know how fine-tuning with NSP and MLM works, but how exactly do we apply that in code?\n",
    "\n",
    "from transformers import BertTokenizer, BertForPreTraining, BertModel, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "with open('clean.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66064306-8b11-4339-9634-b2f4f138629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['From my grandfather Verus I learned good morals and the government of my temper.',\n",
       " 'From the reputation and remembrance of my father, modesty and a manly character.',\n",
       " 'From my mother, piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text))\n",
    "text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7575571b-c206-42f1-8960-b9829ac07042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To prepare our data for NSP, we need to create a mix of non-random sentences (where the two sentences were originally together) — and random sentences.\n",
    "For this, we’ll create a bag of sentences extracted from text which we can then randomly select a sentence from when creating a random NotNextSentence pair.\n",
    "'''\n",
    "bag = [item for sentence in text for item in sentence.split('.') if item != '']\n",
    "bag_size = len(bag)\n",
    "bag_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15276224-1607-4ffc-a186-c3a5001d5283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From Maximus I learned self-government, and not to be led aside by anything; and cheerfulness in all circumstances, as well as in illness; and a just admixture in the moral character of sweetness and dignity, and to do what was set before me without complaining. I observed that everybody believed that he thought as he spoke, and that in all that he did he never had any bad intention; and he never showed amazement and surprise, and was never in a hurry, and never put off doing a thing, nor was perplexed nor dejected, nor did he ever laugh to disguise his vexation, nor, on the other hand, was he ever passionate or suspicious. He was accustomed to do acts of beneficence, and was ready to forgive, and was free from all falsehood; and he presented the appearance of a man who could not be diverted from right rather than of a man who had been improved. I observed, too, that no man could ever think that he was despised by Maximus, or ever venture to think himself a better man. He had also the art of being humorous in an agreeable way.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b54ed7fc-928f-4664-9256-ff770159c72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From Maximus I learned self-government, and not to be led aside by anything; and cheerfulness in all circumstances, as well as in illness; and a just admixture in the moral character of sweetness and dignity, and to do what was set before me without complaining',\n",
       " ' I observed that everybody believed that he thought as he spoke, and that in all that he did he never had any bad intention; and he never showed amazement and surprise, and was never in a hurry, and never put off doing a thing, nor was perplexed nor dejected, nor did he ever laugh to disguise his vexation, nor, on the other hand, was he ever passionate or suspicious',\n",
       " ' He was accustomed to do acts of beneficence, and was ready to forgive, and was free from all falsehood; and he presented the appearance of a man who could not be diverted from right rather than of a man who had been improved',\n",
       " ' I observed, too, that no man could ever think that he was despised by Maximus, or ever venture to think himself a better man',\n",
       " ' He had also the art of being humorous in an agreeable way']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag[14:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162f33f4-1a19-468f-8144-05eb1c898813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAfter creating our bag we can go ahead and create our 50/50 random/non-random NSP training data. For this, we will create a list of sentence As, sentence Bs, and their respective IsNextSentence or NotNextSentence labels.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "After creating our bag we can go ahead and create our 50/50 random/non-random NSP training data. For this, we will create a list of sentence As, sentence Bs, and their respective IsNextSentence or NotNextSentence labels.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d803176e-8b2e-417b-b63d-e0431ad3730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for paragraph in text:\n",
    "    sentences = [\n",
    "        sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "    ]\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 1:\n",
    "        start = random.randint(0, num_sentences-2)\n",
    "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(sentences[start+1])\n",
    "            label.append(0)\n",
    "        else:\n",
    "            index = random.randint(0, bag_size-1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(bag[index])\n",
    "            label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e20518f-1aff-4b1c-96eb-3981e0f3b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " I observed that everybody believed that he thought as he spoke, and that in all that he did he never had any bad intention; and he never showed amazement and surprise, and was never in a hurry, and never put off doing a thing, nor was perplexed nor dejected, nor did he ever laugh to disguise his vexation, nor, on the other hand, was he ever passionate or suspicious\n",
      "---\n",
      " He was accustomed to do acts of beneficence, and was ready to forgive, and was free from all falsehood; and he presented the appearance of a man who could not be diverted from right rather than of a man who had been improved\n",
      "\n",
      "0\n",
      " His secrets were not but very few and very rare, and these only about public matters; and he showed prudence and economy in the exhibition of the public spectacles and the construction of public buildings, his donations to the people, and in such things, for he was a man who looked to what ought to be done, not to the reputation which is got by a man's acts\n",
      "---\n",
      " He did not take the bath at unseasonable hours; he was not fond of building houses, nor curious about what he ate, nor about the texture and colour of his clothes, nor about the beauty of his slaves\n",
      "\n",
      "1\n",
      " Further, I owe it to the gods that I was not hurried into any offence against any of them, though I had a disposition which, if opportunity had offered, might have led me to do something of this kind; but, through their favour, there never was such a concurrence of circumstances as put me to the trial\n",
      "---\n",
      "Both man and God and the universe produce fruit; at the proper seasons each produces it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print(sentence_a[i] + '\\n---')\n",
    "    print(sentence_b[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a899ac3-0fcf-4f01-ba95-c1db425a17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now tokenize our data. As is typical with BERT models, we truncate/pad our sequences to a length of 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7e35de-9d61-406b-abe1-96767370fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', #PyTorch tensors\n",
    "                   max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2627b0f9-563f-4529-be20-94111fb05292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a6d71a-f63b-479a-bafa-521fadb1557b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  5159,  ...,     0,     0,     0],\n",
       "        [  101,  2010,  7800,  ...,     0,     0,     0],\n",
       "        [  101,  2582,  1010,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3459,  2185,  ...,     0,     0,     0],\n",
       "        [  101,  2043, 15223,  ...,     0,     0,     0],\n",
       "        [  101,  7887,  3288,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " Because we tokenized two sentences, our tokenizer automatically applied 0 values to sentence A and 1 values to sentence B in the token_type_ids tensor\n",
    " In the input_ids tensor, the tokenizer automatically placed a SEP token (102) between these two sentences — marking the boundary between them both.\n",
    "'''\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7f03626-bede-48b3-9cf2-57a8995c5eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our NSP labels must be placed within a tensor called next_sentence_label\n",
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "inputs.next_sentence_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1d9e29-445d-4fbe-a960-f2bbf33cb195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For MLM we need to clone our current input_ids tensor to create a MLM labels tensor — then we move onto masking ~15% of tokens in the input_ids tensor.\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b8b5ec1-6b7b-447a-830f-3f1b4fe95a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9443, 0.9597, 0.9078,  ..., 0.1551, 0.3861, 0.8601],\n",
       "        [0.0504, 0.3356, 0.1775,  ..., 0.0284, 0.2632, 0.6820],\n",
       "        [0.6033, 0.3898, 0.1229,  ..., 0.3231, 0.8703, 0.2570],\n",
       "        ...,\n",
       "        [0.8861, 0.5028, 0.2513,  ..., 0.8347, 0.7820, 0.6361],\n",
       "        [0.5911, 0.6831, 0.8805,  ..., 0.4685, 0.8818, 0.0522],\n",
       "        [0.0803, 0.3528, 0.6854,  ..., 0.0636, 0.2178, 0.6776]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf0b2fc-5c7b-48b4-ab6a-e997a36d3502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([317, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create mask array\n",
    "# we are ensuring that we don’t mask any special tokens — such as CLS (101), SEP (102), and PAD (0) tokens.\n",
    "\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
    "print(mask_arr.shape)\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eace7bca-ed24-451d-92c3-60ea16206872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([317, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The inputs.input_ids is a 2D tensor that contains the tokenized versions of your input sentences. Each row corresponds to a sentence, and each column corresponds to a token in that sentence.\n",
    "\n",
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e92d966-ea2e-4905-8ddc-612379f218a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  5],\n",
       "        [  6],\n",
       "        [ 11],\n",
       "        [ 12],\n",
       "        [ 16],\n",
       "        [ 19],\n",
       "        [ 24],\n",
       "        [ 37],\n",
       "        [ 45],\n",
       "        [ 53],\n",
       "        [ 78],\n",
       "        [ 92],\n",
       "        [ 97],\n",
       "        [102],\n",
       "        [104]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(mask_arr[0].nonzero()))\n",
    "mask_arr[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17413b8f-65a9-43a9-8ba2-1228d2416de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5,   6,  11,  12,  16,  19,  24,  37,  45,  53,  78,  92,  97, 102,\n",
       "        104])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(mask_arr[0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "600f9a3f-0769-41fa-a4e1-be5175018c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 11, 12, 16, 19, 24, 37, 45, 53, 78, 92, 97, 102, 104]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(mask_arr[0].nonzero()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a638ce2a-851a-4084-a06f-f2d5641de7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 5, 9, 25, 32, 33, 61, 69, 70, 73, 76, 77, 87, 104, 109, 110, 122]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now take the indices of each True value within each vector.\n",
    "# Flattening the tensor with torch.flatten() turns it into a 1D tensor, and then .tolist() converts this tensor to a Python list.\n",
    "\n",
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "    \n",
    "print(len(selection))\n",
    "selection[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77021694-8153-4ae0-a861-36290231f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then apply these indices to each row in input_ids, assigning each value at these indices a value of 103.\n",
    "# The number 103 corresponds to the special [MASK] token in BERT and some other transformer-based models. \n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d92dfe17-2ad8-43bb-a45b-c70b8931fba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d97ff7f-2b9b-4424-bff1-1f764fee34a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1045,  5159,  2008,  7955,   103,   103,  2002,  2245,  2004,\n",
       "         2002,   103,   103,  1998,  2008,  1999,   103,  2008,  2002,   103,\n",
       "         2002,  2196,  2018,  2151,   103,  6808,  1025,  1998,  2002,  2196,\n",
       "         3662, 21606,  1998,  4474,  1010,  1998,  2001,   103,  1999,  1037,\n",
       "         9241,  1010,  1998,  2196,  2404,   103,  2725,  1037,  2518,  1010,\n",
       "         4496,  2001,  2566,   103,  2098,  4496,  2139, 24455,  1010,  4496,\n",
       "         2106,  2002,  2412,  4756,  2000, 14249,  2010,  2310, 18684,  3508,\n",
       "         1010,  4496,  1010,  2006,  1996,  2060,  2192,  1010,   103,  2002,\n",
       "         2412, 13459,  2030, 10027,   102,  2002,  2001, 17730,  2000,  2079,\n",
       "         4490,  1997,   103, 12879,  6610,  5897,  1010,   103,  2001,  3201,\n",
       "         2000,  9641,   103,  1998,   103,  2489,  2013,  2035,  6270,  9021,\n",
       "         1025,  1998,  2002,  3591,  1996,  3311,  1997,  1037,  2158,  2040,\n",
       "         2071,  2025,  2022, 18356,  2013,  2157,  2738,  2084,  1997,  1037,\n",
       "         2158,  2040,  2018,  2042,  5301,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63da170c-a3b8-4ffa-b6c5-32cf8f17e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataloader\\nAll of our input and label tensors are ready — all we need to do now is format them into a PyTorch dataset object so that it can be loaded into a PyTorch Dataloader — which will feed batches of data into our model during training.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Dataloader\n",
    "All of our input and label tensors are ready — all we need to do now is format them into a PyTorch dataset object so that it can be loaded into a PyTorch Dataloader — which will feed batches of data into our model during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf020934-2187-4c86-8946-7a38fb15d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a PyTorch dataset from our data.\n",
    "\n",
    "'''\n",
    "This is a custom PyTorch Dataset class named OurDataset. It’s designed to handle the encodings (or preprocessed input data) for a language model like BERT. Here’s a brief explanation of its methods:\n",
    "\n",
    "__init__(self, encodings): This is the initializer method that’s called when you create a new instance of OurDataset. It takes one argument, encodings, which should be a dictionary containing the preprocessed input data. This dictionary is stored in the instance variable self.encodings.\n",
    "\n",
    "__getitem__(self, idx): This method is used to get the item at a specific index, idx. It returns a dictionary where each value is a tensor containing the data for one input feature (like input IDs, attention mask, etc.) at the given index. This method is called when you access an item in the dataset like this: dataset[i].\n",
    "\n",
    "__len__(self): This method returns the number of items in the dataset. It’s implemented by returning the length of the input_ids in self.encodings, assuming that all input features have the same length. This method is called when you use the len() function on the dataset: len(dataset).\n",
    "\n",
    "This class allows PyTorch to handle the dataset in a way that’s optimized for machine learning tasks. You can use it with a DataLoader to easily generate batches of data for training or evaluation.\n",
    "'''\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eba12134-4216-44a5-852e-5cb68e1d72f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a422e06-30d3-4001-b371-c03b4ffd01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our data using the OurDataset class.\n",
    "\n",
    "dataset = OurDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd858890-ffc9-4bb5-aae7-ead150f30440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif you have a simple use case and don’t want to create a custom subclass, you might consider using torch.utils.data.TensorDataset. This is a utility class that wraps tensors into a dataset. For example, if inputs is a tensor of input features and labels is a tensor of labels, you can create a dataset like this:\\n\\ndataset = torch.utils.data.TensorDataset(inputs, labels)\\n\\nIn the custom case above, the dataloader expects the __len__ method for checking the total number of samples within our dataset, and the __getitem__ method for extracting samples.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if you have a simple use case and don’t want to create a custom subclass, you might consider using torch.utils.data.TensorDataset. This is a utility class that wraps tensors into a dataset. For example, if inputs is a tensor of input features and labels is a tensor of labels, you can create a dataset like this:\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(inputs, labels)\n",
    "\n",
    "In the custom case above, the dataloader expects the __len__ method for checking the total number of samples within our dataset, and the __getitem__ method for extracting samples.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15b478a7-08da-4aff-a96f-91b23b62416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "380d6aa3-5e05-4841-b5de-6a493df994f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a041221-aa67-4328-a87f-767cd9101cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory: Make sure you’re freeing up GPU memory whenever possible. You can do this by calling torch.cuda.empty_cache() to release cache that PyTorch is holding onto.\n",
    "\n",
    "torch.cuda.empty_cache() # batch size reduced as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4b5a334-108e-411c-bd28-d45e471703ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                       | 0/40 [00:00<?, ?it/s]C:\\Users\\Alienware\\AppData\\Local\\Temp\\ipykernel_48124\\264689798.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:29<00:00,  1.35it/s, loss=2.29]\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:33<00:00,  1.20it/s, loss=1.72]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "import torch.optim as optim\n",
    "\n",
    "# And initialize the dataloader, which we'll be using to load our data into the model during training.\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop: # The inner loop iterates over the DataLoader object (loader), which yields batches of data.\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optimizer.zero_grad() # For each batch, it first zeros out any previously calculated gradients.\n",
    "        # pull all tensor batches required for training\n",
    "        # It then moves all the tensor batches to the device where the computations will be performed (usually a GPU if available).\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        next_sentence_label = batch['next_sentence_label'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        # Forward Pass: The model processes the inputs and returns the outputs.\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids, # not used in DistilBertForSequenceClassification\n",
    "                        next_sentence_label=next_sentence_label, # not used in DistilBertForSequenceClassification\n",
    "                        labels=labels)\n",
    "        # extract loss from the outputs\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update (backward propagation)\n",
    "        loss.backward()\n",
    "        # update parameters, based on the calculated gradients\n",
    "        optimizer.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0b01a-5970-40ea-a291-b9f67822366a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8005b7b-b324-4416-9810-ebb184f80144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c3878-6dbe-4b25-8d3b-c7279bcb8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac719c-8546-4709-b225-00cbc243ef84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ae44e-7d74-4a5c-8f1e-97a6e173901c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
