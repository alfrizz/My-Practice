{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba628bc-b5b7-496d-9472-a2069d8e4df4",
   "metadata": {},
   "source": [
    "SpaCy keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f725e6-02cc-4710-91fa-c57bab22c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d907d5b4-d905-4245-86d7-60f6181b114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1e079457bb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "# Load the Spacy model and create a new document \n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae35032d-d566-48f8-bd4f-b7574758f090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is a sample text for keyword extraction."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"This is a sample text for keyword extraction.\") \n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58908d7-b5cf-409a-a4e2-bd6934a0f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x1e07935dbc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9cec29-3e7f-4d3c-a8fd-ba3f1491d44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'a sample text', 'keyword extraction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the noun_chunks property of the document to identify the noun phrases in the text \n",
    "noun_phrases = [chunk.text for chunk in doc.noun_chunks] \n",
    "\n",
    "noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76930a1a-a045-4b3e-a6d5-5dc5cae3350f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use term frequency-inverse document frequency (TF-IDF) analysis to rank the noun phrases \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vectorizer = TfidfVectorizer() \n",
    "tfidf = vectorizer.fit_transform([doc.text]) \n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c690b9ae-1680-4084-a407-dcd332039fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 6,\n",
       " 'is': 2,\n",
       " 'sample': 4,\n",
       " 'text': 5,\n",
       " 'for': 1,\n",
       " 'keyword': 3,\n",
       " 'extraction': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a354ace5-2d90-4823-8d54-616cb3e9440b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your corpus only contains one document, the IDF part of the TF-IDF will be the same for all words \n",
    "#    (since every word appears in 100% of the documents), so you’re essentially only looking at term frequency. \n",
    "#    This could lead to many words having the same TF-IDF score.\n",
    "\n",
    "[tfidf[0, vectorizer.vocabulary_[word]] for word in vectorizer.vocabulary_.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "921d9226-3c55-48ca-a5d0-5be91bc33840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'sample']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 3 most important noun phrases \n",
    "top_phrases = sorted(vectorizer.vocabulary_, key=lambda x: tfidf[0, vectorizer.vocabulary_[x]], reverse=True)[:3] \n",
    "\n",
    "top_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0740b-e7d0-4081-b79c-2251925b6312",
   "metadata": {},
   "source": [
    "NLTK keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4926c1-dead-4320-9b94-f52dbe2ee5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a sample text for keyword extraction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Preprocess the text by removing punctuation and converting to lowercase \n",
    "text = \"This is a sample text for keyword extraction.\" \n",
    "text = text.lower().replace(\".\", \"\") \n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882bb423-121f-498c-93be-c681d01af838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'sample', 'text', 'for', 'keyword', 'extraction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text into words \n",
    "tokens = nltk.word_tokenize(text) \n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7f98f1-ae08-4057-befd-154cbd095bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('sample', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('keyword', 'NN'),\n",
       " ('extraction', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use part-of-speech tagging to identify the nouns in the text \n",
    "tags = nltk.pos_tag(tokens) \n",
    "\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de6ff159-f696-4c45-84bc-079bddf163b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'keyword', 'extraction']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = [word for (word, tag) in tags if tag == \"NN\"] \n",
    "\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378fd0d2-e6a7-4f3f-adcd-7643184a8ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 6,\n",
       " 'is': 2,\n",
       " 'sample': 4,\n",
       " 'text': 5,\n",
       " 'for': 1,\n",
       " 'keyword': 3,\n",
       " 'extraction': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use term frequency-inverse document frequency (TF-IDF) analysis to rank the nouns \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vectorizer = TfidfVectorizer() \n",
    "tfidf = vectorizer.fit_transform([text]) \n",
    "\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87dcc4aa-9afb-4bed-b741-b458f4066f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272,\n",
       " 0.3779644730092272]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your corpus only contains one document, the IDF part of the TF-IDF will be the same for all words \n",
    "#    (since every word appears in 100% of the documents), so you’re essentially only looking at term frequency. \n",
    "#    This could lead to many words having the same TF-IDF score.\n",
    "\n",
    "words_import = [tfidf[0, vectorizer.vocabulary_[word]] for word in vectorizer.vocabulary_.keys()]\n",
    "\n",
    "words_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6586a6-f18b-4a1d-9262-81cffdbe6e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'sample']\n"
     ]
    }
   ],
   "source": [
    "# Get the top 3 most important nouns \n",
    "top_nouns = sorted(vectorizer.vocabulary_, key=lambda x: tfidf[0, vectorizer.vocabulary_[x]], reverse=True)[:3] \n",
    "\n",
    "# Print the top 3 keywords \n",
    "print(top_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ba184-975a-4814-b930-465f97999e9c",
   "metadata": {},
   "source": [
    "BERT keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbc0f196-32ea-4889-80b6-936667003772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers \n",
    "\n",
    "# Load the BERT model and create a new tokenizer \n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")  \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b4f5b0-f6eb-4dca-b48d-6309a1781736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f7a37a-cf21-47f5-acc6-a7c3b868527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 1037, 7099, 3793, 2005, 3145, 18351, 14676, 1012, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and encode the text \n",
    "input_ids = tokenizer.encode(\"This is a sample text for keyword extraction.\", add_special_tokens=True) \n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b096c1-2e05-41d6-8496-64eb1f13fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Use BERT to encode the meaning and context of the words and phrases in the text \n",
    "outputs = model(torch.tensor([input_ids]), output_hidden_states=True, output_attentions=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f1813-3a1c-41a6-8c16-007a56c3b5a0",
   "metadata": {},
   "source": [
    "The outputs variable contains the output from the BERT model. Specifically, it’s a tuple containing the following elements:\n",
    "\n",
    "Remember that hidden_size is 768 for BERT Base and 1024 for BERT Large. The batch size and sequence length depend on your input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2675c6be-2afa-4962-ac35-86f985f93507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3476, -0.4094, -0.2909,  ..., -0.4571, -0.0713,  0.8328],\n",
       "         [-0.3861, -0.6360, -0.2580,  ..., -0.2864,  0.7689,  0.4315],\n",
       "         [-0.3096, -0.5142,  0.2317,  ..., -0.1392, -0.0091,  0.9189],\n",
       "         ...,\n",
       "         [-0.1831, -0.0702,  0.1542,  ..., -0.6890, -0.2608,  0.3044],\n",
       "         [ 0.6661,  0.0375, -0.6640,  ...,  0.2270, -0.4533, -0.3437],\n",
       "         [ 0.1989, -0.1620, -0.3995,  ...,  0.5692, -0.7531, -0.0771]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence Output (each token encoding): \n",
    "\n",
    "# This is a tensor of shape (batch_size, sequence_length, hidden_size). \n",
    "# It represents the hidden states in the last layer for each token in the input sequence. \n",
    "# In other words, it’s a contextualized embedding for each token in the input.\n",
    "\n",
    "print(outputs[0].shape)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c0ca47e-560c-4ca7-a218-3bcc5954d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8658, -0.4209, -0.6395,  0.6057,  0.6033, -0.3086,  0.7101,  0.2981,\n",
       "         -0.5178, -1.0000, -0.3377,  0.6333,  0.9709,  0.0152,  0.8384, -0.5312,\n",
       "          0.0909, -0.5789,  0.4099, -0.1241,  0.4825,  0.9999,  0.3598,  0.4204,\n",
       "          0.5166,  0.8455, -0.5856,  0.8948,  0.9299,  0.7788, -0.5277,  0.2765,\n",
       "         -0.9860, -0.2951, -0.8095, -0.9860,  0.4320, -0.7537, -0.0907, -0.1106,\n",
       "         -0.8812,  0.4814,  1.0000, -0.4155,  0.4344, -0.3932, -1.0000,  0.3595,\n",
       "         -0.8635,  0.6884,  0.5974,  0.6493,  0.2502,  0.5112,  0.5414, -0.1730,\n",
       "         -0.0789,  0.2362, -0.2428, -0.6733, -0.5817,  0.3656, -0.4781, -0.9050,\n",
       "          0.6503,  0.4120, -0.2417, -0.3463, -0.1857,  0.0710,  0.7616,  0.2537,\n",
       "         -0.0180, -0.7102,  0.3489,  0.3787, -0.6414,  1.0000, -0.2656, -0.9651,\n",
       "          0.5244,  0.4501,  0.6277,  0.1057,  0.0520, -1.0000,  0.5785, -0.3051,\n",
       "         -0.9830,  0.2097,  0.5389, -0.2841,  0.3116,  0.6470, -0.3391, -0.3244,\n",
       "         -0.3486, -0.7008, -0.3771, -0.4158,  0.2680, -0.3523, -0.3249, -0.4290,\n",
       "          0.4189, -0.4630, -0.1671,  0.4829, -0.1785,  0.6926,  0.5418, -0.4759,\n",
       "          0.4100, -0.9154,  0.6270, -0.4483, -0.9818, -0.6345, -0.9798,  0.6562,\n",
       "         -0.0309, -0.3476,  0.9104,  0.2031,  0.3990, -0.1520, -0.7761, -1.0000,\n",
       "         -0.4006, -0.5609,  0.1026, -0.3013, -0.9719, -0.9494,  0.6919,  0.9499,\n",
       "          0.2832,  0.9998, -0.2946,  0.9095,  0.0751, -0.5055,  0.0927, -0.4486,\n",
       "          0.6821,  0.0798, -0.6465,  0.2288, -0.1557,  0.2005, -0.4877, -0.3795,\n",
       "         -0.5939, -0.8751, -0.3957,  0.8941, -0.2670, -0.6930,  0.3901, -0.1761,\n",
       "         -0.3255,  0.8357,  0.6009,  0.3185, -0.0989,  0.5296, -0.0662,  0.5650,\n",
       "         -0.8349,  0.1364,  0.4553, -0.3162, -0.6623, -0.9728, -0.3787,  0.5403,\n",
       "          0.9815,  0.7021,  0.4573,  0.5027, -0.4683,  0.4956, -0.9311,  0.9749,\n",
       "         -0.2052,  0.3110, -0.4598,  0.3442, -0.8472, -0.1850,  0.7086, -0.4246,\n",
       "         -0.8441, -0.0466, -0.5823, -0.5029, -0.5857,  0.5541, -0.4292, -0.4755,\n",
       "         -0.1263,  0.8965,  0.9282,  0.5779, -0.0467,  0.5544, -0.8091, -0.2335,\n",
       "          0.1622,  0.3000,  0.2573,  0.9865, -0.5427, -0.3033, -0.8960, -0.9668,\n",
       "          0.0246, -0.8279, -0.2070, -0.6999,  0.6172, -0.3456,  0.1387,  0.3880,\n",
       "         -0.9057, -0.7355,  0.3461, -0.5531,  0.5483, -0.3224,  0.9171,  0.7719,\n",
       "         -0.6517,  0.1887,  0.9512, -0.5940, -0.7708,  0.6479, -0.3076,  0.8564,\n",
       "         -0.6639,  0.9856,  0.7157,  0.3473, -0.8579, -0.4267, -0.8740, -0.1702,\n",
       "         -0.2888, -0.3418,  0.6160,  0.7171,  0.3761,  0.5300, -0.4845,  0.9706,\n",
       "         -0.9370, -0.9459, -0.7171,  0.0091, -0.9843,  0.6666,  0.2905,  0.3616,\n",
       "         -0.5849, -0.5058, -0.9540,  0.7641,  0.1434,  0.9591, -0.3651, -0.7970,\n",
       "         -0.5551, -0.8955, -0.0860, -0.2807,  0.1272, -0.0580, -0.9000,  0.5389,\n",
       "          0.5942,  0.4200, -0.5209,  0.9905,  1.0000,  0.9600,  0.7754,  0.7353,\n",
       "         -0.9991, -0.8177,  1.0000, -0.9495, -1.0000, -0.9121, -0.5250,  0.3076,\n",
       "         -1.0000, -0.2901, -0.0976, -0.8603,  0.2912,  0.9727,  0.9473, -1.0000,\n",
       "          0.8946,  0.9171, -0.6808,  0.6767, -0.5168,  0.9632,  0.4327,  0.5560,\n",
       "         -0.1505,  0.4919, -0.7710, -0.7537, -0.3985, -0.4102,  0.9929,  0.2037,\n",
       "         -0.7532, -0.8110,  0.4963,  0.0266, -0.1403, -0.9399, -0.3043,  0.0933,\n",
       "          0.6249,  0.3051,  0.3329, -0.6767,  0.4712, -0.2069,  0.1607,  0.6819,\n",
       "         -0.9072, -0.3042, -0.1632, -0.2810, -0.1367, -0.9719,  0.9366, -0.3942,\n",
       "          0.7726,  1.0000,  0.5673, -0.7605,  0.4984,  0.3199, -0.5202,  1.0000,\n",
       "          0.6101, -0.9743, -0.5869,  0.4966, -0.5817, -0.6355,  0.9993, -0.3283,\n",
       "         -0.4314, -0.1309,  0.9744, -0.9880,  0.9799, -0.7358, -0.9500,  0.9411,\n",
       "          0.8870, -0.3429, -0.8158,  0.1207, -0.4509,  0.3104, -0.8900,  0.5272,\n",
       "          0.3420, -0.1159,  0.8008, -0.5150, -0.6818,  0.4406, -0.3583,  0.2272,\n",
       "          0.8340,  0.5083, -0.3489,  0.0277, -0.3358, -0.7374, -0.9472,  0.3763,\n",
       "          1.0000, -0.0145,  0.5206, -0.0303, -0.1864, -0.0139,  0.5922,  0.6179,\n",
       "         -0.3374, -0.8345,  0.6218, -0.7959, -0.9844,  0.5191,  0.2261, -0.3263,\n",
       "          0.9999,  0.3537,  0.2734,  0.3561,  0.8932,  0.1183,  0.3539,  0.4584,\n",
       "          0.9750, -0.3698,  0.6271,  0.6675, -0.6115, -0.3218, -0.6988,  0.0593,\n",
       "         -0.9346, -0.0618, -0.9224,  0.9621,  0.7507,  0.4194,  0.2480,  0.6261,\n",
       "          1.0000, -0.8318,  0.5656,  0.2038,  0.5471, -0.9988, -0.5998, -0.4202,\n",
       "         -0.1876, -0.3432, -0.3443,  0.3051, -0.9582,  0.4857,  0.3179, -0.9273,\n",
       "         -0.9774, -0.0377,  0.5208,  0.1668, -0.9445, -0.4773, -0.5618,  0.3975,\n",
       "         -0.3871, -0.8740,  0.1740, -0.4217,  0.4810, -0.3683,  0.6798,  0.6602,\n",
       "          0.8118, -0.8144, -0.1301, -0.0901, -0.7456,  0.6820, -0.7621, -0.7177,\n",
       "         -0.2965,  1.0000, -0.5096,  0.6685,  0.6014,  0.5304, -0.3036,  0.2894,\n",
       "          0.7757,  0.2432, -0.1501, -0.6153, -0.0856, -0.3820,  0.5575,  0.3758,\n",
       "          0.3496,  0.7022,  0.8181,  0.2019, -0.1039,  0.0703,  0.9933, -0.1341,\n",
       "         -0.2781, -0.5349, -0.1655, -0.3171,  0.1460,  1.0000,  0.4116,  0.2880,\n",
       "         -0.9859, -0.6826, -0.8503,  1.0000,  0.7747, -0.7648,  0.6513,  0.4137,\n",
       "         -0.2418,  0.4890, -0.3490, -0.3487,  0.2557,  0.1499,  0.9145, -0.4544,\n",
       "         -0.9605, -0.6797,  0.4569, -0.9438,  0.9995, -0.6865, -0.2366, -0.4228,\n",
       "         -0.1770, -0.2336,  0.0215, -0.9640, -0.2479,  0.2281,  0.9356,  0.4106,\n",
       "         -0.6078, -0.8683,  0.6256,  0.5459, -0.6001, -0.8994,  0.9446, -0.9294,\n",
       "          0.5240,  1.0000,  0.5389, -0.1960,  0.1910, -0.3865,  0.3705, -0.2964,\n",
       "          0.4284, -0.9295, -0.3709, -0.3085,  0.3434, -0.1921, -0.4023,  0.4919,\n",
       "          0.3042, -0.5984, -0.5615, -0.3135,  0.3568,  0.6714, -0.3308, -0.3607,\n",
       "          0.1602, -0.1950, -0.7197, -0.3961, -0.4828, -0.9999,  0.6637, -1.0000,\n",
       "          0.2358, -0.2585, -0.2736,  0.8203,  0.6683,  0.5476, -0.6839, -0.4577,\n",
       "          0.6397,  0.6936, -0.3584, -0.0770, -0.5512,  0.3695, -0.1650,  0.2767,\n",
       "         -0.3147,  0.8183, -0.2771,  1.0000,  0.1726, -0.5641, -0.8554,  0.3663,\n",
       "         -0.3682,  1.0000, -0.7738, -0.9507,  0.2697, -0.6472, -0.7892,  0.4284,\n",
       "          0.1519, -0.6924, -0.8093,  0.8593,  0.7509, -0.5816,  0.4290, -0.3608,\n",
       "         -0.3617,  0.1365,  0.6545,  0.9807,  0.5303,  0.8374, -0.1036, -0.2084,\n",
       "          0.9569,  0.4162,  0.1119,  0.1760,  1.0000,  0.4673, -0.9176,  0.0852,\n",
       "         -0.9645, -0.2476, -0.9392,  0.2938,  0.2720,  0.8883, -0.4300,  0.9236,\n",
       "         -0.6038,  0.1521, -0.2990, -0.1215,  0.3862, -0.8637, -0.9767, -0.9785,\n",
       "          0.5805, -0.5128, -0.1926,  0.3878,  0.1466,  0.4869,  0.4108, -1.0000,\n",
       "          0.9219,  0.4880,  0.5755,  0.9403,  0.3689,  0.5331,  0.3475, -0.9778,\n",
       "         -0.8425, -0.3987, -0.4711,  0.6989,  0.6569,  0.7921,  0.3510, -0.5634,\n",
       "         -0.5363, -0.1084, -0.8534, -0.9889,  0.4645, -0.3339, -0.8746,  0.9449,\n",
       "         -0.2278, -0.1530,  0.3826, -0.6001,  0.7594,  0.7118,  0.3334,  0.1339,\n",
       "          0.5606,  0.8158,  0.9225,  0.9775, -0.5417,  0.6480, -0.3490,  0.4921,\n",
       "          0.8507, -0.9501,  0.2876,  0.5077, -0.1421,  0.3085, -0.2893, -0.8807,\n",
       "          0.7925, -0.3157,  0.4313, -0.4669,  0.0441, -0.4651, -0.3252, -0.8190,\n",
       "         -0.4141,  0.6327,  0.2219,  0.8515,  0.7308, -0.2052, -0.6055, -0.1978,\n",
       "         -0.3091, -0.9234,  0.8316,  0.0170,  0.1746,  0.4994,  0.0346,  0.9226,\n",
       "         -0.1012, -0.4215, -0.3376, -0.7660,  0.7774, -0.6156, -0.6295, -0.4070,\n",
       "          0.6094,  0.3982,  0.9999, -0.4521, -0.5204, -0.4889, -0.3015,  0.5171,\n",
       "         -0.3690, -1.0000,  0.4952, -0.2468,  0.4089, -0.4459,  0.4618, -0.2658,\n",
       "         -0.9430, -0.3365,  0.6899,  0.4475, -0.5124, -0.4171,  0.5835,  0.1938,\n",
       "          0.8659,  0.8007, -0.3070,  0.2820,  0.6322, -0.7661, -0.6556,  0.8262]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pooled Output (all text encoding) :  \n",
    "\n",
    "# This is a tensor of shape (batch_size, hidden_size). \n",
    "# It’s derived from the hidden state of the special [CLS] token in the last layer, \n",
    "# after being passed through a linear layer and a tanh activation function. \n",
    "# This output is often used for classification tasks.\n",
    "\n",
    "print(outputs[1].shape)\n",
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3e18be-3cc4-440f-b2e8-d2aa584da793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768]), torch.Size([1, 12, 768])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [-0.6485,  0.6739, -0.0932,  ...,  0.4475,  0.6696,  0.1820],\n",
       "          [-0.6270, -0.0633, -0.3143,  ...,  0.3427,  0.4636,  0.4594],\n",
       "          ...,\n",
       "          [-1.2840, -0.5970,  0.3023,  ...,  0.3650,  0.8621, -0.6559],\n",
       "          [-0.3585,  0.2777, -0.1210,  ...,  0.5949,  0.6856,  0.7453],\n",
       "          [-0.4771,  0.0871, -0.0770,  ..., -0.2191,  0.3020,  0.0196]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1370,  0.0181, -0.1411,  ...,  0.2278, -0.0924, -0.0353],\n",
       "          [-0.6273,  0.3076,  0.2524,  ...,  0.1998,  0.2308,  0.0458],\n",
       "          [-1.1002, -0.5674, -0.3320,  ...,  0.4697,  0.1516,  0.4446],\n",
       "          ...,\n",
       "          [-1.0307, -0.5239,  0.4133,  ...,  0.1205,  0.6193, -0.5466],\n",
       "          [-0.4261, -0.0376,  0.0413,  ...,  0.3959,  0.3216,  0.5345],\n",
       "          [-0.3650, -0.0494,  0.0122,  ..., -0.0882,  0.3893,  0.1930]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.0031, -0.2585, -0.3145,  ...,  0.3892,  0.1097, -0.0490],\n",
       "          [-0.9432,  0.4489,  0.5976,  ...,  0.0733,  0.2109,  0.0049],\n",
       "          [-1.0744, -0.3124,  0.1743,  ...,  0.4898, -0.0032,  0.4841],\n",
       "          ...,\n",
       "          [-0.5528,  0.0831,  0.6291,  ..., -0.3388,  0.5784, -0.6379],\n",
       "          [-0.4391, -0.1148,  0.3956,  ...,  0.1305,  0.1189,  0.4263],\n",
       "          [-0.3754, -0.1702,  0.1008,  ..., -0.0096,  0.3117, -0.1294]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1268, -0.3955, -0.0852,  ...,  0.3226,  0.2689,  0.0310],\n",
       "          [-0.4887,  0.0024,  0.4756,  ..., -0.0839,  0.0186,  0.1114],\n",
       "          [-1.0426, -0.4200,  0.6076,  ...,  0.2943, -0.1181,  0.3783],\n",
       "          ...,\n",
       "          [-0.0711,  0.2531,  0.0939,  ..., -0.2213,  0.4951, -0.5922],\n",
       "          [-0.6115, -0.2667,  0.6217,  ...,  0.0975, -0.1183,  0.0463],\n",
       "          [-0.0780, -0.1175,  0.1409,  ...,  0.0200,  0.0744, -0.0607]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3867, -0.6073, -0.4275,  ...,  0.4954,  0.3623,  0.4170],\n",
       "          [-0.4333, -0.0236, -0.0753,  ...,  0.0240,  0.0152, -0.2336],\n",
       "          [-1.3344, -0.5040,  0.6332,  ...,  0.0305, -0.0192,  0.6208],\n",
       "          ...,\n",
       "          [ 0.2086,  0.0194, -0.1014,  ...,  0.2540,  0.7641, -0.6471],\n",
       "          [-0.6118, -0.2763,  0.1629,  ...,  0.0772, -0.0323,  0.2564],\n",
       "          [-0.0159, -0.0559,  0.0040,  ...,  0.0136,  0.0589, -0.0452]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.2098, -0.6094, -0.4113,  ...,  0.0130,  0.1763,  0.4244],\n",
       "          [-0.2482,  0.0229, -0.1448,  ..., -0.1610,  0.2149, -0.2834],\n",
       "          [-1.0038, -0.5654,  0.3861,  ...,  0.5834, -0.3910,  0.8585],\n",
       "          ...,\n",
       "          [ 0.6472, -0.1916, -0.5088,  ...,  0.4398,  0.9820, -0.2716],\n",
       "          [-0.4816, -0.2405, -0.0131,  ...,  0.0303, -0.3160,  0.6689],\n",
       "          [-0.0134, -0.0324,  0.0129,  ...,  0.0122,  0.0120, -0.0465]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.0407, -0.7214, -0.7057,  ..., -0.1649,  0.1980,  0.4081],\n",
       "          [-0.4394,  0.0373, -0.7738,  ..., -0.8312, -0.0413,  0.0251],\n",
       "          [-0.7848, -0.4645,  0.1149,  ..., -0.1582, -0.6360,  1.1658],\n",
       "          ...,\n",
       "          [ 0.5782,  0.0967, -0.3974,  ...,  0.3603,  0.9822, -0.3485],\n",
       "          [-0.9586, -0.1055, -0.2727,  ..., -0.4445, -0.3287,  0.6277],\n",
       "          [ 0.0198, -0.0429, -0.0173,  ..., -0.0017, -0.0149, -0.0451]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.0199, -0.9914, -0.8369,  ..., -0.0647,  0.2464,  0.5701],\n",
       "          [-0.7266,  0.0543, -1.2377,  ..., -0.3138, -0.0619,  0.3342],\n",
       "          [-0.9782, -0.3039, -0.1703,  ..., -0.1899, -0.4181,  1.5608],\n",
       "          ...,\n",
       "          [ 0.6740,  0.1497, -0.2206,  ...,  0.3423,  0.9350, -0.0784],\n",
       "          [-0.9193, -0.8758, -0.8476,  ..., -0.5741,  0.1969,  1.0423],\n",
       "          [-0.0052, -0.0446, -0.0210,  ..., -0.0228,  0.0102, -0.0417]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1375, -0.6908, -0.6612,  ..., -0.1550,  0.2113,  0.5091],\n",
       "          [-0.3052,  0.0838, -1.0165,  ..., -0.4790,  0.0970,  0.3889],\n",
       "          [-0.4802, -0.3506, -0.3767,  ...,  0.1149, -0.4402,  1.3618],\n",
       "          ...,\n",
       "          [ 0.3539, -0.1508,  0.1160,  ...,  0.1243,  0.4234, -0.1133],\n",
       "          [-0.7170, -0.4577, -0.8950,  ..., -0.6074,  0.2123,  0.6119],\n",
       "          [ 0.0059, -0.0270,  0.0221,  ..., -0.0211, -0.0317, -0.0623]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.0631, -0.5889, -0.3870,  ...,  0.1262,  0.1599,  0.6561],\n",
       "          [-0.4698, -0.0152, -0.4423,  ..., -0.1643,  0.0504,  0.6959],\n",
       "          [-0.4072, -0.2561, -0.1866,  ...,  0.0026, -0.4374,  1.3654],\n",
       "          ...,\n",
       "          [ 0.2256, -0.2076,  0.2263,  ...,  0.1144,  0.4875, -0.0404],\n",
       "          [-0.3674,  0.1533, -0.4887,  ..., -0.6752, -0.0325,  0.2940],\n",
       "          [-0.0297, -0.0956,  0.0498,  ..., -0.0356, -0.0400, -0.0035]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-7.5195e-01, -6.7796e-01, -1.8124e-01,  ..., -1.3074e-01,\n",
       "           -3.0321e-01,  8.3753e-01],\n",
       "          [-8.2332e-01, -2.7601e-01, -1.4891e-01,  ..., -2.4558e-01,\n",
       "            4.6549e-01,  7.0632e-01],\n",
       "          [-7.4103e-01, -2.9941e-01, -1.1330e-01,  ..., -2.5375e-01,\n",
       "           -2.6092e-01,  1.4668e+00],\n",
       "          ...,\n",
       "          [ 1.9418e-02, -4.7396e-01,  5.0478e-01,  ..., -1.3719e-01,\n",
       "           -2.7068e-01, -1.3636e-01],\n",
       "          [-3.5701e-04,  1.2380e-02, -6.4739e-02,  ..., -2.0457e-02,\n",
       "           -2.1760e-02,  6.2121e-03],\n",
       "          [-2.9378e-01, -3.0801e-01,  1.9523e-01,  ...,  9.7387e-02,\n",
       "            6.1494e-02,  1.6953e-01]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.6772, -0.5063, -0.3142,  ..., -0.4397, -0.1345,  0.5262],\n",
       "          [-0.7052, -0.5000, -0.5246,  ..., -0.8531,  0.5686,  0.6347],\n",
       "          [-0.8371, -0.6609, -0.1933,  ..., -0.5279, -0.0081,  1.6375],\n",
       "          ...,\n",
       "          [-0.2165, -0.3796,  0.1860,  ..., -0.6534, -0.2990,  0.3090],\n",
       "          [ 0.0478,  0.0359, -0.0473,  ...,  0.0082, -0.0287, -0.0037],\n",
       "          [-0.0307, -0.1207, -0.0093,  ...,  0.0702, -0.0348,  0.0136]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.3476, -0.4094, -0.2909,  ..., -0.4571, -0.0713,  0.8328],\n",
       "          [-0.3861, -0.6360, -0.2580,  ..., -0.2864,  0.7689,  0.4315],\n",
       "          [-0.3096, -0.5142,  0.2317,  ..., -0.1392, -0.0091,  0.9189],\n",
       "          ...,\n",
       "          [-0.1831, -0.0702,  0.1542,  ..., -0.6890, -0.2608,  0.3044],\n",
       "          [ 0.6661,  0.0375, -0.6640,  ...,  0.2270, -0.4533, -0.3437],\n",
       "          [ 0.1989, -0.1620, -0.3995,  ...,  0.5692, -0.7531, -0.0771]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden States (each token encod. in each BERT block) : \n",
    "\n",
    "# If you set output_hidden_states=True when loading the model, \n",
    "# outputs will also include the hidden states from all layers of the model. \n",
    "# This is a tuple of length num_hidden_layers + 1 (for the initial embeddings and all transformer layers), \n",
    "# where each element is a tensor of shape (batch_size, sequence_length, hidden_size).\n",
    "\n",
    "print([outputs[2][i].shape for i in range(len(outputs[2]))])\n",
    "outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b7dc772-9f83-458c-89ca-2d28a87c0282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12]), torch.Size([1, 12, 12, 12])]\n"
     ]
    }
   ],
   "source": [
    "# Attention Weights  (each token dependencies with all other tokens: measure of how much each token in a sentence is influenced by others) : \n",
    "\n",
    "# If you set output_attentions=True when loading the model,\n",
    "# outputs will also include the attention weights from all layers of the model.\n",
    "# This is a tuple of length num_hidden_layers, where each element is a tensor of shape \n",
    "# (batch_size, num_attention_heads, sequence_length, sequence_length).\n",
    "\n",
    "print([outputs[3][i].shape for i in range(len(outputs[3]))])\n",
    "# outputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50b88ce7-3a60-4539-b048-d14297d0c9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[5.1807e-02, 9.1523e-02, 3.6972e-02,  ..., 4.2061e-02,\n",
       "           1.0813e-01, 2.7775e-01],\n",
       "          [1.8288e-01, 3.7304e-02, 1.3999e-01,  ..., 4.8936e-02,\n",
       "           8.8809e-02, 7.1775e-02],\n",
       "          [1.2814e-01, 5.2379e-02, 1.1465e-01,  ..., 6.7462e-02,\n",
       "           5.1583e-02, 4.8481e-02],\n",
       "          ...,\n",
       "          [2.9144e-02, 4.3075e-02, 4.1256e-02,  ..., 5.7146e-02,\n",
       "           3.3228e-02, 9.3269e-02],\n",
       "          [7.9261e-02, 8.3060e-02, 7.5335e-02,  ..., 6.6719e-02,\n",
       "           1.3202e-01, 9.0765e-02],\n",
       "          [8.8118e-02, 1.2103e-01, 6.8236e-02,  ..., 5.6765e-02,\n",
       "           1.5303e-01, 1.2386e-01]],\n",
       "\n",
       "         [[6.0402e-01, 6.7658e-03, 8.2498e-03,  ..., 1.3216e-02,\n",
       "           3.3096e-02, 5.9370e-03],\n",
       "          [9.8433e-03, 1.6769e-02, 1.3894e-01,  ..., 9.9515e-02,\n",
       "           2.4098e-02, 4.1944e-02],\n",
       "          [1.1230e-02, 3.0699e-02, 3.7648e-02,  ..., 1.4532e-01,\n",
       "           2.0109e-02, 2.8752e-02],\n",
       "          ...,\n",
       "          [7.4355e-02, 4.2029e-02, 7.6803e-02,  ..., 6.3780e-02,\n",
       "           1.5905e-02, 5.3684e-02],\n",
       "          [9.9650e-04, 3.0336e-01, 2.5221e-01,  ..., 1.3221e-02,\n",
       "           2.1796e-01, 9.9323e-03],\n",
       "          [3.4521e-02, 1.4651e-01, 8.8164e-02,  ..., 2.2774e-02,\n",
       "           3.3559e-01, 2.1685e-02]],\n",
       "\n",
       "         [[7.5463e-01, 3.1885e-02, 3.1356e-02,  ..., 7.1461e-03,\n",
       "           2.7984e-02, 4.7895e-02],\n",
       "          [6.3464e-01, 1.0114e-01, 5.8886e-02,  ..., 6.6379e-03,\n",
       "           1.4753e-02, 5.8100e-02],\n",
       "          [2.6484e-01, 6.1414e-01, 2.9887e-02,  ..., 7.7604e-04,\n",
       "           5.2916e-03, 1.5332e-02],\n",
       "          ...,\n",
       "          [5.1104e-01, 1.2983e-03, 5.2271e-03,  ..., 2.4008e-02,\n",
       "           4.1684e-02, 7.2929e-02],\n",
       "          [2.1782e-01, 1.5929e-02, 2.7370e-03,  ..., 1.2788e-01,\n",
       "           3.0896e-01, 1.5102e-01],\n",
       "          [2.6735e-01, 9.0931e-03, 3.2799e-03,  ..., 1.6561e-02,\n",
       "           5.2808e-01, 1.5200e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[9.2293e-02, 1.0755e-01, 4.5150e-02,  ..., 1.1128e-02,\n",
       "           3.5960e-02, 1.2790e-02],\n",
       "          [2.2719e-01, 2.4419e-02, 4.3165e-02,  ..., 2.9516e-02,\n",
       "           3.0857e-02, 7.5572e-02],\n",
       "          [3.8861e-01, 1.0446e-02, 2.1610e-02,  ..., 9.8563e-02,\n",
       "           2.1763e-02, 7.8513e-02],\n",
       "          ...,\n",
       "          [2.8014e-02, 6.4389e-02, 5.9029e-02,  ..., 2.1195e-01,\n",
       "           2.2206e-02, 3.0386e-02],\n",
       "          [5.5586e-01, 1.4639e-02, 4.5277e-02,  ..., 6.1511e-03,\n",
       "           2.0778e-02, 1.0060e-01],\n",
       "          [5.1244e-01, 4.7144e-02, 7.0793e-02,  ..., 1.2242e-02,\n",
       "           3.9383e-02, 2.1753e-02]],\n",
       "\n",
       "         [[7.9438e-01, 2.7228e-02, 1.5127e-02,  ..., 1.8155e-02,\n",
       "           1.9566e-02, 2.0315e-02],\n",
       "          [4.7433e-03, 8.3009e-02, 5.7406e-01,  ..., 1.5821e-03,\n",
       "           2.6550e-03, 8.2885e-03],\n",
       "          [1.1136e-03, 6.4295e-02, 6.4145e-02,  ..., 5.4798e-03,\n",
       "           1.2348e-03, 3.9389e-03],\n",
       "          ...,\n",
       "          [1.5639e-01, 3.4704e-03, 8.3103e-04,  ..., 2.2182e-02,\n",
       "           4.8166e-01, 2.6983e-01],\n",
       "          [5.7293e-03, 2.9801e-03, 1.0762e-03,  ..., 1.6958e-02,\n",
       "           4.2535e-02, 9.2004e-01],\n",
       "          [1.7790e-01, 6.0756e-03, 6.3202e-03,  ..., 8.4608e-02,\n",
       "           3.0379e-01, 3.8083e-01]],\n",
       "\n",
       "         [[8.4404e-01, 5.5135e-02, 2.6694e-03,  ..., 7.2614e-04,\n",
       "           3.6980e-07, 3.1269e-02],\n",
       "          [2.7148e-01, 1.1521e-01, 1.3579e-01,  ..., 1.4780e-02,\n",
       "           3.6512e-02, 3.8255e-02],\n",
       "          [1.0762e-01, 1.4324e-01, 2.5574e-02,  ..., 2.4997e-02,\n",
       "           7.0824e-03, 1.3396e-02],\n",
       "          ...,\n",
       "          [1.5663e-01, 5.6762e-02, 1.7263e-02,  ..., 4.5749e-02,\n",
       "           1.5053e-02, 1.7634e-01],\n",
       "          [4.0017e-01, 3.8504e-02, 4.7457e-03,  ..., 5.2102e-02,\n",
       "           1.8908e-03, 2.4617e-01],\n",
       "          [6.3082e-01, 4.1418e-02, 7.2186e-03,  ..., 1.5496e-02,\n",
       "           1.6359e-03, 1.8469e-01]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[3][0] # == attention_weights[0] below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9c62528-3972-4c6c-ad52-bcfafdf2f18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[5.1807e-02, 9.1523e-02, 3.6972e-02,  ..., 4.2061e-02,\n",
       "           1.0813e-01, 2.7775e-01],\n",
       "          [1.8288e-01, 3.7304e-02, 1.3999e-01,  ..., 4.8936e-02,\n",
       "           8.8809e-02, 7.1775e-02],\n",
       "          [1.2814e-01, 5.2379e-02, 1.1465e-01,  ..., 6.7462e-02,\n",
       "           5.1583e-02, 4.8481e-02],\n",
       "          ...,\n",
       "          [2.9144e-02, 4.3075e-02, 4.1256e-02,  ..., 5.7146e-02,\n",
       "           3.3228e-02, 9.3269e-02],\n",
       "          [7.9261e-02, 8.3060e-02, 7.5335e-02,  ..., 6.6719e-02,\n",
       "           1.3202e-01, 9.0765e-02],\n",
       "          [8.8118e-02, 1.2103e-01, 6.8236e-02,  ..., 5.6765e-02,\n",
       "           1.5303e-01, 1.2386e-01]],\n",
       " \n",
       "         [[6.0402e-01, 6.7658e-03, 8.2498e-03,  ..., 1.3216e-02,\n",
       "           3.3096e-02, 5.9370e-03],\n",
       "          [9.8433e-03, 1.6769e-02, 1.3894e-01,  ..., 9.9515e-02,\n",
       "           2.4098e-02, 4.1944e-02],\n",
       "          [1.1230e-02, 3.0699e-02, 3.7648e-02,  ..., 1.4532e-01,\n",
       "           2.0109e-02, 2.8752e-02],\n",
       "          ...,\n",
       "          [7.4355e-02, 4.2029e-02, 7.6803e-02,  ..., 6.3780e-02,\n",
       "           1.5905e-02, 5.3684e-02],\n",
       "          [9.9650e-04, 3.0336e-01, 2.5221e-01,  ..., 1.3221e-02,\n",
       "           2.1796e-01, 9.9323e-03],\n",
       "          [3.4521e-02, 1.4651e-01, 8.8164e-02,  ..., 2.2774e-02,\n",
       "           3.3559e-01, 2.1685e-02]],\n",
       " \n",
       "         [[7.5463e-01, 3.1885e-02, 3.1356e-02,  ..., 7.1461e-03,\n",
       "           2.7984e-02, 4.7895e-02],\n",
       "          [6.3464e-01, 1.0114e-01, 5.8886e-02,  ..., 6.6379e-03,\n",
       "           1.4753e-02, 5.8100e-02],\n",
       "          [2.6484e-01, 6.1414e-01, 2.9887e-02,  ..., 7.7604e-04,\n",
       "           5.2916e-03, 1.5332e-02],\n",
       "          ...,\n",
       "          [5.1104e-01, 1.2983e-03, 5.2271e-03,  ..., 2.4008e-02,\n",
       "           4.1684e-02, 7.2929e-02],\n",
       "          [2.1782e-01, 1.5929e-02, 2.7370e-03,  ..., 1.2788e-01,\n",
       "           3.0896e-01, 1.5102e-01],\n",
       "          [2.6735e-01, 9.0931e-03, 3.2799e-03,  ..., 1.6561e-02,\n",
       "           5.2808e-01, 1.5200e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[9.2293e-02, 1.0755e-01, 4.5150e-02,  ..., 1.1128e-02,\n",
       "           3.5960e-02, 1.2790e-02],\n",
       "          [2.2719e-01, 2.4419e-02, 4.3165e-02,  ..., 2.9516e-02,\n",
       "           3.0857e-02, 7.5572e-02],\n",
       "          [3.8861e-01, 1.0446e-02, 2.1610e-02,  ..., 9.8563e-02,\n",
       "           2.1763e-02, 7.8513e-02],\n",
       "          ...,\n",
       "          [2.8014e-02, 6.4389e-02, 5.9029e-02,  ..., 2.1195e-01,\n",
       "           2.2206e-02, 3.0386e-02],\n",
       "          [5.5586e-01, 1.4639e-02, 4.5277e-02,  ..., 6.1511e-03,\n",
       "           2.0778e-02, 1.0060e-01],\n",
       "          [5.1244e-01, 4.7144e-02, 7.0793e-02,  ..., 1.2242e-02,\n",
       "           3.9383e-02, 2.1753e-02]],\n",
       " \n",
       "         [[7.9438e-01, 2.7228e-02, 1.5127e-02,  ..., 1.8155e-02,\n",
       "           1.9566e-02, 2.0315e-02],\n",
       "          [4.7433e-03, 8.3009e-02, 5.7406e-01,  ..., 1.5821e-03,\n",
       "           2.6550e-03, 8.2885e-03],\n",
       "          [1.1136e-03, 6.4295e-02, 6.4145e-02,  ..., 5.4798e-03,\n",
       "           1.2348e-03, 3.9389e-03],\n",
       "          ...,\n",
       "          [1.5639e-01, 3.4704e-03, 8.3103e-04,  ..., 2.2182e-02,\n",
       "           4.8166e-01, 2.6983e-01],\n",
       "          [5.7293e-03, 2.9801e-03, 1.0762e-03,  ..., 1.6958e-02,\n",
       "           4.2535e-02, 9.2004e-01],\n",
       "          [1.7790e-01, 6.0756e-03, 6.3202e-03,  ..., 8.4608e-02,\n",
       "           3.0379e-01, 3.8083e-01]],\n",
       " \n",
       "         [[8.4404e-01, 5.5135e-02, 2.6694e-03,  ..., 7.2614e-04,\n",
       "           3.6980e-07, 3.1269e-02],\n",
       "          [2.7148e-01, 1.1521e-01, 1.3579e-01,  ..., 1.4780e-02,\n",
       "           3.6512e-02, 3.8255e-02],\n",
       "          [1.0762e-01, 1.4324e-01, 2.5574e-02,  ..., 2.4997e-02,\n",
       "           7.0824e-03, 1.3396e-02],\n",
       "          ...,\n",
       "          [1.5663e-01, 5.6762e-02, 1.7263e-02,  ..., 4.5749e-02,\n",
       "           1.5053e-02, 1.7634e-01],\n",
       "          [4.0017e-01, 3.8504e-02, 4.7457e-03,  ..., 5.2102e-02,\n",
       "           1.8908e-03, 2.4617e-01],\n",
       "          [6.3082e-01, 4.1418e-02, 7.2186e-03,  ..., 1.5496e-02,\n",
       "           1.6359e-03, 1.8469e-01]]], grad_fn=<UnbindBackward0>)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the attention weights of the tokens to identify the most important words and phrases \n",
    "'''\n",
    "The line top_tokens = sorted(attention_weights[0], key=lambda x: x[1], reverse=True)[:3] \n",
    "sorts these tuples based on the attention weight (the second element of each tuple) (?? to check ??)\n",
    "'''\n",
    "\n",
    "attention_weights = outputs[-1] \n",
    "top_tokens = sorted(attention_weights[0], key=lambda x: x[1], reverse=True)[:3] \n",
    "\n",
    "top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcfc5a79-3091-4404-9b47-13c07879982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# Decode the top tokens and print the top 3 keywords \n",
    "# top_keywords = [tokenizer.decode([token[0]]) for token in top_tokens] ----------------- to check ------------------\n",
    "top_keywords = [tokenizer.decode([token[0][0][5]]) for token in top_tokens] \n",
    "print(top_keywords)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
