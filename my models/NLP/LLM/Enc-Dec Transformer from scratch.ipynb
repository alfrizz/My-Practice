{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134d2c0a-c2f1-45f6-bc39-2cb800ad3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import math  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cce4e3-0cb2-42b4-b8fb-547a09f7242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************** PositionalEncoder VERSION WITH COMMENTS AND PRINTS FOR EXPLANATION *************************\n",
    "\n",
    "# Subclass the PyTorch nn.Module class to create a custom module for positional encoding\n",
    "# This class is used to add positional information to the input embeddings in a Transformer model\n",
    "class PositionalEncoder_explanations(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        # Call the parent class's constructor\n",
    "        super(PositionalEncoder_explanations, self).__init__()\n",
    "        \n",
    "        # Initialize the dimensions of the model and the maximum sequence length\n",
    "        self.d_model = d_model  # The dimension of the input embeddings\n",
    "        self.max_length = max_length  # The maximum length of words of tokens in the input sequences\n",
    "        \n",
    "        # Initialize the positional encoding matrix with zeros\n",
    "        # This matrix will store the positional encodings that will be added to the input embeddings\n",
    "        pe = torch.zeros(max_length, d_model)  \n",
    "\n",
    "        # Create a tensor of positions from 0 to max_length\n",
    "        # This tensor represents the positions of the words in a sequence\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)  #  we unsqueeze because the multiplication operation position * div_term requires position to be a 2D tensor to correctly broadcast with div_term (the extra dimension is added at position '1': second position)\n",
    "        print('position:', position)\n",
    "        \n",
    "        # Calculate the division term div_term for the positional encoding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))  #  Increasing the embedding size d_model creates more dimensions. The last dimensions will change slowly due to the exponentially decreasing div_term. This happens because sine and cosine of small values hover close to 0 or 1, meaning they change slowly (calculated below over: position * div_term). This slow change is great for capturing long-range dependencies and the global context in longer sequences. On the flip side, the first dimensions will change more rapidly and be more useful for shorter texts (so in a longer text, the difference between the encoding of adjacent words may not be as pronounced as the difference between adjacent words in a shorter text).\n",
    "        print('div_term:', div_term)\n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        # For even indices, use sine of the position times the division term\n",
    "        # For odd indices, use cosine of the position times the division term\n",
    "        # These encodings are based on sine and cosine functions of different frequencies\n",
    "        # The sine and cosine functions are used to ensure that the positional encodings are continuous and differentiable, which is important for the learning process. Also, these functions generate values between -1 and 1, which helps to keep the magnitude of the positional encodings manageable.\n",
    "        # Using sine for even indices and cosine for odd indices provides two different signals for each position, which helps the model distinguish between different positions more effectively.\n",
    "        print('position * div_term:', position * div_term)\n",
    "        print('torch.sin(position * div_term):', torch.sin(position * div_term))\n",
    "        print('torch.cos(position * div_term):', torch.cos(position * div_term))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "        print('pe:', pe)\n",
    "        \n",
    "        # Add an extra dimension to the positional encoding matrix, turning it from a 2D tensor into a 3D tensor(the extra dimension is added at position '0': first position)\n",
    "        # This is done to match the dimensions of the input embeddings (batch size, sequence length, and embedding size)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        \n",
    "        # Register the positional encoding matrix as a buffer that should not be considered a model parameter\n",
    "        # Buffers are tensors that are not updated during backpropagation but need to be part of the model's state\n",
    "        self.register_buffer('pe', pe)  # N.B. self.pe is defined when pe is registered as a buffer. \n",
    "    \n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Update the input tensor by adding the positional encodings\n",
    "        # The positional encodings are added to the input embeddings so that the model can take into account the position of words in a sequence\n",
    "        x = x + self.pe[:, :x.size(1)]  # returning the input sequence plus the positional encoding (ensuring that you're slicing the positional encodings to match the length of your input sequence)\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee96e1ca-332d-4061-a2ec-51b4b5d9b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.]])\n",
      "div_term: tensor([1.0000, 0.0100])\n",
      "position * div_term: tensor([[0.0000, 0.0000],\n",
      "        [1.0000, 0.0100],\n",
      "        [2.0000, 0.0200],\n",
      "        [3.0000, 0.0300],\n",
      "        [4.0000, 0.0400],\n",
      "        [5.0000, 0.0500],\n",
      "        [6.0000, 0.0600],\n",
      "        [7.0000, 0.0700]])\n",
      "torch.sin(position * div_term): tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.8415,  0.0100],\n",
      "        [ 0.9093,  0.0200],\n",
      "        [ 0.1411,  0.0300],\n",
      "        [-0.7568,  0.0400],\n",
      "        [-0.9589,  0.0500],\n",
      "        [-0.2794,  0.0600],\n",
      "        [ 0.6570,  0.0699]])\n",
      "torch.cos(position * div_term): tensor([[ 1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9999],\n",
      "        [-0.4161,  0.9998],\n",
      "        [-0.9900,  0.9996],\n",
      "        [-0.6536,  0.9992],\n",
      "        [ 0.2837,  0.9988],\n",
      "        [ 0.9602,  0.9982],\n",
      "        [ 0.7539,  0.9976]])\n",
      "pe: tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "        [ 0.6570,  0.7539,  0.0699,  0.9976]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PositionalEncoder_explanations()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "max_length = 8\n",
    "\n",
    "PositionalEncoder_explanations(d_model, max_length) # in this example of 8 words, the first two columns change rapidly, while the last two columns change slowly. For shorter texts, the rapid changes in the first columns help the model distinguish between closely spaced elements. For longer texts, the slow changes in the last columns help the model capture long-range dependencies and maintain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20584520-7d15-4a8a-b567-045a1ed57f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Create a positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add extra dimension to match input embeddings\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f81cc6b9-c4fd-49ba-a153-aca7734107be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Number of attention heads.\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Dimension of the input embeddings.\n",
    "        self.d_model = d_model \n",
    "\n",
    "        # Dimension of each head.\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear transformations for queries, keys, and values.\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final linear transformation for concatenated output.\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size): # splits the input vectors into multiple \"heads\" to allow parallel attention mechanisms. Each head processes the data differently, helping the model learn diverse representations and capture various aspects of the input data.\n",
    "        # Split input vectors into different attention heads.\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        # Rearrange dimensions to bring heads to the second dimension.\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim) \n",
    "    \n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # Compute attention weights.\n",
    "        scores = torch.matmul(query, key.permute(0, 2, 1))  # Fixed from original\n",
    "        if mask is not None:\n",
    "            # Apply mask to prevent focusing on certain positions.\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\")) # In the transformer’s encoder, you don’t want the attention mechanism to consider [PAD] tokens. So, the mask tells the model to ignore these positions\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Compute output of multi-head attention layer.\n",
    "        batch_size = query.size(0) \n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask) # These heads independently compute attention scores to focus on different parts of the input\n",
    "\n",
    "        output = torch.matmul(attention_weights, value) # The outputs from all heads are concatenated and linearly transformed in a final output: a context vector that combines information from all heads, representing a rich and comprehensive understanding of the input.\n",
    "        # Reshape output to match dimensions\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6e3116-b20c-41c4-843e-579eef900fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module): # helps in adding depth and complexity to the model's capability to learn intricate patterns and representations\n",
    "    \n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3376a16-5901-49f1-823d-57fa8d88fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "# In self-attention, especially within the encoder of a transformer, the same x is used for queries, keys, and values. Here's why:\n",
    "# Query (Q): Represents the current token that’s “asking” for information.\n",
    "# Key (K): Represents the tokens that can provide information.\n",
    "# Value (V): Represents the actual information content of the tokens.        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask) # The input x is a sequence of embeddings representing the input tokens, and its shape is generally (batch_size, sequence_length, d_model). In the context of self-attention mechanisms, such as the one used in Transformer models, x is used as the Query (Q), Key (K), and Value (V). This allows the model to compute attention scores based on the input itself.\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # The operation x + self.dropout(attn_output) is an example of a technique called residual connection: The idea is that it’s easier to model a residual (or difference) than to learn to model the full information. In this specific case,we are “adding the residual”, that is the output of the self-attention mechanism (which has learned how to modify the input) back to the original input. \n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca2f4c09-8492-4b39-aca1-d14795b8e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module): # whole transformer encoder structure that includes a num_layers number of encoder layers\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "# Initial x Input:\n",
    "# Shape: (batch_size, sequence_length)\n",
    "# This represents indices of tokens in the sequence.\n",
    "\n",
    "# After Embedding Layer:\n",
    "# Converts token indices into dense vectors of fixed size d_model.\n",
    "# Output x Shape: (batch_size, sequence_length, d_model)\n",
    "# This transforms each token index into a d_model-dimensional vector.   \n",
    "\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d2ca32-6967-4db0-9f5c-6c2f69f590d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "# having x shape (batch_size, sequence_length, d_model),\n",
    "# the below expression x[:, 0, :] retrieves the first token's embedding from each sequence in the batch. Here's the breakdown:\n",
    "# x[:, 0, :]:\n",
    "# : (first position) selects all batches.\n",
    "# 0 (second position) selects the first token in the sequence.\n",
    "# : (third position) selects all dimensions of the embedding vector.\n",
    "# So, for each sequence in the batch, this slice pulls out the embedding corresponding to the very first token. \n",
    "# Essentially, it narrows down x from (batch_size, sequence_length, d_model) to (batch_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a89eaf52-2dbf-44f5-b7fd-c56f21062c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 256\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a35e7da-1001-40cc-be8d-2a340c9ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9797, 3924, 4525,  ..., 6777, 9344, 4056],\n",
       "        [9984,  309, 2283,  ..., 8692,  793, 5878],\n",
       "        [3583, 6290, 8428,  ..., 9452, 7530, 1299],\n",
       "        ...,\n",
       "        [1988, 5750, 4841,  ..., 5073, 1621, 4851],\n",
       "        [4950, 6813, 7399,  ..., 4631, 2544, 4798],\n",
       "        [7983, 6744, 1579,  ..., 6352, 5008, 7510]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "\n",
    "print(input_sequence.shape) # (batch_size, sequence_length)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1df462c-a0e9-47df-890b-feab943ddc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 0, 1],\n",
       "        [1, 1, 1,  ..., 1, 0, 1],\n",
       "        [0, 1, 0,  ..., 1, 0, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 1, 0, 0],\n",
       "        [0, 1, 1,  ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "print(mask.shape) \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e0b16ba-3c0e-4366-8687-6a890b514fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a03e94-7280-464a-b49b-3ff6540a97d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierHead(\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e7d819-2ddd-4a9c-b864-c8df512e6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7435,  1.1385, -0.7223,  ...,  0.1889, -0.6962,  0.4831],\n",
       "         [ 0.8105, -1.6185,  1.3498,  ..., -1.6169, -1.1257, -2.0812],\n",
       "         [ 0.5484, -0.1069,  0.5720,  ..., -0.4243,  0.6431,  0.9754],\n",
       "         ...,\n",
       "         [ 2.3135,  0.1473,  0.4844,  ..., -1.1608, -0.8113, -0.1542],\n",
       "         [ 1.6193, -0.0295, -1.8626,  ...,  0.4774, -1.1475,  0.2491],\n",
       "         [-1.5673, -1.6159,  0.1447,  ...,  0.2701, -0.8520, -0.9383]],\n",
       "\n",
       "        [[-0.8274, -0.0733,  0.5979,  ..., -1.0813,  1.0623, -1.3290],\n",
       "         [ 0.4181, -1.2374, -0.2271,  ...,  0.0966,  0.0662, -2.1758],\n",
       "         [ 1.6146, -0.8896, -0.6006,  ..., -0.7868,  0.0171, -0.8474],\n",
       "         ...,\n",
       "         [ 2.4098,  0.4992,  0.2534,  ..., -0.9127, -0.1523,  0.7399],\n",
       "         [ 0.0809, -0.3021, -1.4943,  ..., -0.6124, -0.3575, -0.6111],\n",
       "         [-0.4675, -1.7376,  0.4175,  ...,  0.9002, -0.2969, -0.1444]],\n",
       "\n",
       "        [[ 0.5061,  0.2235, -1.3462,  ..., -0.5088, -0.1415,  0.5459],\n",
       "         [ 0.0041,  0.5678,  0.2001,  ..., -0.2438, -0.4469,  0.2113],\n",
       "         [ 1.1650, -1.6727, -0.4857,  ...,  0.4429, -0.5217, -1.1562],\n",
       "         ...,\n",
       "         [ 1.3179, -0.9332,  0.9028,  ...,  0.5017, -0.4692, -0.9876],\n",
       "         [-1.0251, -1.3803, -0.1074,  ...,  0.1826,  0.8781, -0.6732],\n",
       "         [ 0.1502, -1.1451, -0.3084,  ..., -0.8689, -0.5267, -0.7440]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3703,  1.0443, -0.7590,  ..., -0.1946,  1.4228, -0.4377],\n",
       "         [-1.4866, -1.6354,  1.5065,  ...,  1.1767, -1.5121,  0.3271],\n",
       "         [ 1.3219, -1.5514, -0.6728,  ...,  1.3674,  0.3762, -0.4529],\n",
       "         ...,\n",
       "         [-0.1478,  1.3032, -2.8529,  ...,  1.9985, -0.7437, -0.7224],\n",
       "         [-0.3461,  1.5209,  1.0973,  ...,  0.1179,  1.9634,  1.0685],\n",
       "         [-0.2155, -2.3630,  0.3739,  ...,  1.8857, -0.6379, -0.3012]],\n",
       "\n",
       "        [[ 0.3018,  0.6213, -0.4484,  ...,  0.5407,  0.4146, -0.4572],\n",
       "         [ 3.0412, -1.2127,  1.4135,  ...,  0.2771,  0.7343, -1.0361],\n",
       "         [-0.6659, -1.0073, -0.0409,  ...,  1.2651, -0.2771,  0.3667],\n",
       "         ...,\n",
       "         [ 1.9561,  0.0745, -1.1640,  ...,  0.7425, -0.8856, -0.1677],\n",
       "         [ 0.5145, -1.2212, -0.2386,  ..., -0.2575, -0.3593,  0.3179],\n",
       "         [ 0.7441,  0.6986,  0.3188,  ..., -0.1905,  0.1270, -0.7647]],\n",
       "\n",
       "        [[ 0.2198, -0.4239, -1.2836,  ...,  1.2498,  0.0602,  0.7195],\n",
       "         [ 2.0126,  0.6438, -0.7437,  ...,  0.2197, -1.1296,  0.6559],\n",
       "         [-0.7341, -1.2801, -0.4052,  ..., -0.5901, -0.3716, -1.2915],\n",
       "         ...,\n",
       "         [-0.7245,  0.0174, -0.3000,  ...,  1.1676,  1.0168, -0.2105],\n",
       "         [-0.5444, -0.6473, -0.3200,  ..., -0.1225, -0.5905,  0.1868],\n",
       "         [-0.2018,  0.3388,  0.8924,  ..., -0.4545,  0.3757, -0.5074]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the forward pass \n",
    "enc_output = encoder(input_sequence, mask)\n",
    "\n",
    "print(enc_output.shape) #(batch_size, sequence_length, d_model)\n",
    "enc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2056424-9171-414b-9011-5b26f1acbe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification outputs for a batch of  8 sequences:\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9858, -1.1400, -1.1808],\n",
       "        [-1.6497, -1.5691, -0.5114],\n",
       "        [-1.3324, -1.3797, -0.7246],\n",
       "        [-1.3645, -0.7959, -1.2265],\n",
       "        [-0.8527, -1.1873, -1.3142],\n",
       "        [-0.7921, -1.3617, -1.2348],\n",
       "        [-1.4061, -0.6102, -1.5528],\n",
       "        [-1.7412, -1.2330, -0.6287]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification = classifier(enc_output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print (enc_output[:,0,:].shape) # (batch_size, d_model)\n",
    "print(classification.shape) # (batch_size, n_classes)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44e16b75-3f55-4d58-a1b9-a10be7fc1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "# 1) Self-Attention in Encoder: Captures intra-sequence dependencies within the input (original language).\n",
    "# 2) Self-Attention in Decoder: Captures intra-sequence dependencies within the generated sequence (target language).\n",
    "# 3) Cross-Attention in Decoder: Integrates information from the input sequence to guide the generation of the output sequence.                \n",
    "        \n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask) # this causal mask hides future tokens to prevent the model from \"cheating\" by looking ahead (in the masked multihead attention) the tokens of the target language\n",
    "        x = self.norm1(x + self.dropout(self_attn_output)) # residual connection\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask) # it's a Padding Mask: Like the encoder, it ignores padding tokens\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output)) # residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f8fdcfa-d55c-413c-baa5-1ef645e1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module): # whole transformer decoder structure that includes a num_layers number of decoder layers\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) \n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size) # next word predicted among all words in the vocabulary of size vocab_size\n",
    "            \n",
    "    def forward(self, x, self_mask, encoder_output, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask, encoder_output, cross_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        # When you apply F.log_softmax(x, dim=-1), the softmax function is applied to the d_model dimension. This means that the softmax function is applied independently to each sequence in each batch, and the output tensor will have the same shape as the input tensor.\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d28bcc5c-d3b7-480c-a651-b94bdb9b4066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5471, 3530,  568,  ..., 4662, 1914, 4709],\n",
       "        [3909, 9180, 6682,  ..., 7176, 7367, 8959],\n",
       "        [2504,  730, 1121,  ..., 7738, 3483, 3910],\n",
       "        ...,\n",
       "        [7761, 1065, 6337,  ...,  730, 1216, 4146],\n",
       "        [ 975, 7459, 8713,  ..., 4435, 4343, 6762],\n",
       "        [1399, 4249, 8867,  ..., 4669,  972,  599]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "print(input_sequence.shape)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ed0b6e6-0bc1-4a04-a52e-2acc89e660dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "275c46fb-b6d5-4541-bbd8-3ec1025ebd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, 8, 8), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5f0f76-af6f-4023-8ffb-4e46d86b2414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75930524-056e-4a59-96c4-8d868e92f66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  ..., False, False, False],\n",
       "         [ True,  True, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()  # Upper triangular mask\n",
    "\n",
    "print(self_attention_mask.shape)\n",
    "self_attention_mask # This mask allows each position to attend only to itself and previous positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a217c58-6b86-492d-85b9-4e919a54c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17ffcd35-d9f0-4055-ad88-7482e1c559a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 0, 1],\n",
       "        [0, 0, 1,  ..., 1, 1, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 1],\n",
       "        ...,\n",
       "        [0, 1, 0,  ..., 1, 1, 0],\n",
       "        [0, 0, 0,  ..., 1, 0, 1],\n",
       "        [1, 1, 1,  ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder cross_mask\n",
    "\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0018a3cb-a64a-48e0-9f14-1e956a794d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 10000])\n",
      "tensor([[[ -8.8514, -10.4841,  -9.6291,  ...,  -9.3091,  -9.4755, -10.2760],\n",
      "         [-10.0028,  -9.9526,  -9.6459,  ...,  -9.7123,  -9.2278, -10.1058],\n",
      "         [ -9.5896,  -9.9621,  -9.3631,  ..., -10.1368,  -9.4306,  -9.3334],\n",
      "         ...,\n",
      "         [ -9.9828,  -9.6016,  -9.4854,  ...,  -8.7922, -10.0499, -10.7145],\n",
      "         [-10.3352,  -9.1532,  -8.5892,  ...,  -8.9328, -10.1256,  -9.3146],\n",
      "         [-10.3416,  -9.1305,  -9.9421,  ...,  -9.1008, -10.3433,  -9.2755]],\n",
      "\n",
      "        [[-10.0274, -10.2554, -10.3774,  ...,  -9.1231,  -9.1468,  -9.9129],\n",
      "         [-10.1445, -10.1929, -10.3947,  ...,  -9.2896,  -9.2572,  -9.5090],\n",
      "         [-10.1883,  -9.8544,  -9.8537,  ...,  -8.6108,  -9.1650,  -9.6822],\n",
      "         ...,\n",
      "         [ -8.9103,  -9.6868,  -9.4214,  ...,  -8.6500,  -9.5056,  -9.9902],\n",
      "         [ -9.8133,  -9.6981,  -9.5066,  ...,  -8.6682,  -9.3694, -10.1616],\n",
      "         [ -9.5896,  -9.2353,  -8.9476,  ...,  -8.3409,  -9.9228,  -9.3737]],\n",
      "\n",
      "        [[ -9.7239,  -9.5813,  -9.3821,  ...,  -9.9822,  -9.5840, -10.4879],\n",
      "         [ -9.6212,  -9.3136,  -8.7197,  ...,  -8.9502,  -9.8370, -10.1451],\n",
      "         [ -9.9904,  -9.1932,  -9.5127,  ...,  -9.1467,  -9.0272,  -9.8305],\n",
      "         ...,\n",
      "         [ -9.5756,  -9.2066,  -9.3247,  ...,  -8.9013,  -9.3855, -10.1623],\n",
      "         [ -9.1652,  -9.6653,  -9.3061,  ...,  -8.8757,  -9.6144, -10.1595],\n",
      "         [ -9.0795, -10.1725, -10.1460,  ...,  -8.3955,  -9.1561, -10.2709]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-10.7734, -10.1175, -10.0159,  ...,  -9.3294,  -9.9753, -10.7193],\n",
      "         [ -9.1841,  -9.8811,  -8.8052,  ...,  -9.0631,  -9.2089, -10.2081],\n",
      "         [ -8.8893, -10.2095,  -9.2055,  ...,  -9.4981,  -9.6695, -10.2782],\n",
      "         ...,\n",
      "         [ -9.7253,  -9.0853,  -8.6378,  ...,  -8.3366,  -9.7916,  -9.8202],\n",
      "         [-10.0509,  -8.6676,  -9.3417,  ...,  -8.7660,  -9.9710,  -9.3256],\n",
      "         [ -9.7821, -10.3139,  -9.4373,  ...,  -9.1696,  -9.4713,  -9.1032]],\n",
      "\n",
      "        [[ -9.5750,  -9.1117,  -8.9213,  ..., -10.3366,  -9.7933,  -9.6135],\n",
      "         [-10.1562,  -9.2119,  -9.3963,  ...,  -9.6500,  -8.5006,  -9.8187],\n",
      "         [ -9.0475,  -9.1960,  -8.8760,  ...,  -8.5810,  -9.9009,  -9.4986],\n",
      "         ...,\n",
      "         [ -9.7409,  -9.1285,  -8.8727,  ...,  -8.5075,  -9.4416,  -9.5391],\n",
      "         [ -9.6883,  -9.5388,  -8.6208,  ...,  -9.1957,  -9.6083,  -9.8516],\n",
      "         [-10.1618,  -9.0625,  -8.1463,  ...,  -9.0478,  -9.7771,  -9.7562]],\n",
      "\n",
      "        [[ -9.8125,  -9.6456,  -9.1153,  ...,  -9.0218,  -9.8593, -10.1368],\n",
      "         [ -9.1951,  -9.4174,  -9.3285,  ...,  -9.3234,  -8.9995,  -9.5442],\n",
      "         [ -9.3279,  -9.5685,  -9.7124,  ...,  -8.1815,  -9.5708,  -8.9808],\n",
      "         ...,\n",
      "         [ -9.8489,  -8.8494,  -9.0943,  ...,  -8.7621, -10.2757, -10.0229],\n",
      "         [ -9.8120, -10.0008,  -8.3755,  ...,  -8.5757, -10.4608,  -9.4001],\n",
      "         [ -9.6478,  -9.7166,  -8.5426,  ...,  -9.3731,  -9.8807,  -8.9433]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dec_output = decoder(input_sequence, self_attention_mask, enc_output, padding_mask)\n",
    "\n",
    "print(dec_output.shape) # batch_size, sequence_length, vocab_size\n",
    "\n",
    "# For each token position (of sequence_lenght shape 8) in each sequence (of batch_size shape 256), the model predicts the next token. This means for each of the 256 token positions in every of the 8 sequence, it outputs a distribution over the 10000 vocabulary tokens.\n",
    "\n",
    "print(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7a44492-2148-438f-b054-49590d0832ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuming above:\n",
    "\n",
    "# The classification head we applied earlier was a custom component tailored for a different task—specifically, classification. It was indeed separate from the standard encoder-decoder transformer architecture.\n",
    "# So, the classification head predicted the class for each entire sequence (sentence) rather than each word, while the encoder-decoder transformer focuses on generating a comprehensive output for each token in the sequence. This duality allows transformers to be versatile across different NLP tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f653f-8795-44f3-813b-2c4c7f30b8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
