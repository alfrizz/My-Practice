{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134d2c0a-c2f1-45f6-bc39-2cb800ad3b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import math  \n",
    "\n",
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f579bcb-a96e-4156-b1ef-2dbaa444adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x2750373b250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "# Load the pipeline for sentiment classification\n",
    "classifier = pipeline(\"text-classification\", model=model_name)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933b2234-cf9e-45e6-89d8-51620e70cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '3 stars', 'score': 0.6387940645217896}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6403d66d-f505-4ad0-ad98-09263fbc9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '4 stars', 'score': 0.5190770030021667}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, everything well organized\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a22190-ca52-4b7b-aed9-45bb268f6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.SummarizationPipeline at 0x27523984c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cnicu/t5-small-booksum'\n",
    "\n",
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline('summarization', model = model_name)\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5111f86-f864-4b68-bb1f-244541711108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = '\\nThe tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\\n'\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length = 30)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c80d10-21eb-4c39-8b89-843b4b2cb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n"
     ]
    }
   ],
   "source": [
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1055b1e9-7c8d-4f56-9ab9-9a1e8e5082d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millau Viaduct\n"
     ]
    }
   ],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "question = \"What's the tallest structure in France?\"\n",
    "\n",
    "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
    "outputs = qa_model(question, long_text)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32cf35b-340a-426c-bebf-4013cfa3e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't think you're doing a good translation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "\n",
    "input_text = \"No creo que hagas una buena traducción\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline('translation_es_to_en', model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f6f578-4871-4b5f-8c06-b0dfa5f64e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer model hyperparameters\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126ece2e-d3d9-464e-aead-b62880bfc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model, # d_model is the dimension of the input vectors and output vectors of the model, specifically the size of the feature space. Essentially, it determines the number of features in each transformer layer\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,    \n",
    "    num_decoder_layers=num_decoder_layers\n",
    "    )\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943a497a-5f46-407d-be6e-da7d37c310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fecc1dc-bf8c-41b2-b66d-17c1a782f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Customer review:\\nI had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\\n\\nHotel reponse to the customer:\\nDear valued customer, I am glad to hear you had a good stay with us.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the prompt for the text generation LLM\n",
    "\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8261f8c5-a1b9-4d48-bb88-252d2a72582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
      "\n",
      "Hotel reponse to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. We love this place and have a few others in the area including a nearby boutique with a fantastic\n"
     ]
    }
   ],
   "source": [
    "#### Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length = 100, pad_token_id=generator.tokenizer.eos_token_id) #  if the generated text is shorter than max_length, the remaining tokens will be filled with the EOS token.\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa4b5a5-409b-45bf-95a6-2e4dad5472ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='textattack/distilbert-base-uncased-SST-2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e08ef05c-f507-4470-b3be-e43b0dcd2324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a4c8829-7de1-457b-8106-1d4f2204f0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 2190, 3185, 1045, 1005, 2310, 2412, 3427,  999,  102,    0],\n",
       "        [ 101, 2054, 2019, 9643, 3185, 1012, 1045, 9038, 3666, 2009, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037fe889-4fdb-460e-a474-a413ebdb40fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03e4513-4aca-4f7e-88cd-0b5debfc8f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "809b5ebc-e84b-4947-bc91-05e3f343eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d5a294c-f96d-416e-b1b0-5ede64da8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for \"The best movie I've ever watched!\": 1\n",
      "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
     ]
    }
   ],
   "source": [
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41652f3-395f-4e35-9188-4253114fb1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_sents', 'summaries'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face's dataset hub\n",
    "dataset = load_dataset('opinosis', trust_remote_code=True)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1af4af29-1973-4998-8729-05d4cb433fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of instances: {len(dataset['train'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcbc46a3-f0a1-49fc-be25-756cc67c2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['review_sents', 'summaries']\n"
     ]
    }
   ],
   "source": [
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d85c21f6-5636-4f8e-b78d-972a85b9fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 2)\n",
      "dict_keys(['review_sents', 'summaries'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review_sents': \", and is very, very accurate .\\r\\n but for the most part, we find that the Garmin software provides accurate directions, whereever we intend to go .\\r\\n This function is not accurate if you don't leave it in battery mode say, when you stop at the Cracker Barrell for lunch and to play one of those trangle games with the tees .\\r\\n It provides immediate alternatives if the route from the online map program was inaccurate or blocked by an obstacle .\\r\\n I've used other GPS units, as well as GPS built into cars   and to this day NOTHING beats the accuracy of a Garmin GPS .\\r\\n It got me from point A to point B with 100% accuracy everytime .\\r\\n It has yet to disappoint, getting me everywhere with 100% accuracy .\\r\\n0 out of 5 stars Honest, accurate review, , PLEASE READ !\\r\\n Aside from that, every destination I've thrown at has been 100% accurate .\\r\\nIn closing, this is a fantastic GPS with some very nice features and is very accurate in directions .\\r\\n Plus, I've always heard that there are  quirks  with any GPS being accurate, having POIs, etc .\\r\\n DESTINATION TIME, , This is pretty accurate too .\\r\\n But, it's always very accurate .\\r\\n The map is pretty accurate and the Point of interest database also is good .\\r\\n Most of the times, this info was very accurate .\\r\\nI've even used it in the  pedestrian  mode, and it's amazing how accurate it is .\\r\\n  ONLY  is only accurate when an ad says,  Top sirloin steak, ONLY $1 .\\r\\n The most accurate review stated that these machines are adjunct to a good map and signs on the interstate .\\r\\n The directions are highly accurate down to a  T  .\\r\\n Depending on what you are using it for, it is a nice adjunct to a travel trip and the directions are accurate and usually the quickest, but not always .\\r\\n The screen is easy to see, the voice tells you where you are and it's very accurate .\\r\\n It was accurate to the minute when it told me when I would arrive home .\\r\\n0 out of 5 stars GPS Navigator doesn't navigate accurately on a straight road .\\r\\n I was familiar with the streets and only used the Nuvi to get an accurate arrival time estimate .\\r\\n but after that it is very easy and quite accurate to use .\\r\\n The accuracy at this point is very good .\\r\\nWhile the 255W routing seems generally accurate and logical, on my first use I discovered that it does have some errors in its internal map .\\r\\n Bottom line is I wanted a unit that is accurate and had reliable satellite connection .\\r\\n I've used it around town and find it to be extremely accurate .\\r\\nI found the maps to be inaccurate at first, but after I updated them from Garmin's website everything is golden .\\r\\n A lot of my friends' addresses are inaccurate by any GPS .\\r\\n It loads quickly, have pretty accurate directions, and can recalculate quickly when I miss a turn .\\r\\n Because the accuracy is good to the street address level, it may not be able to guide you to the exact location if your destination is inside a shopping mall .\\r\\nI updated to the latest 2010 map soon after I received the unit, so the map is accurate to me .\\r\\n I was blown away at the accuracy and routing capability this thing had .\\r\\n I used it the day I bought it,   and then this morning, and as soon as it comes on it is  ready to navigate  The only downfall of this product, and the only reason I did not give it 5 stars is the fact that the speed limit it displays for the road you are on isn't 100% accurate .\\r\\n If your looking for a nice, accurate GPS for not so much money, got with this one .\\r\\n0 out of 5 stars Inexpensive, accurate, plenty of features, August 6, 2009\\r\\n The only glitch I have found so far is that the speed limits are not 100% accurate, although the GPS, amazingly, is able to very accurately tell you how fast your vehicle is moving .\\r\\n I was a little disappointed in the inaccuracy of the posted speed limit, as I'm guilty of not paying close enough attention to those signs, especially w  interstate speed traps that are constantly changing up and down .\\r\\n The closest one that gives the most accurate route that I usually take is the Navigon .\\r\\n After 2 weeks, it has yet to make a mistake, and is always completely accurate ,  even to the point of telling me which side of the street my destination is on .\\r\\n It has worked well for local driving giving accurate directions for roads and streets .\\r\\nThe estimated time to arrival does not seem to calculate the travelling time accurately .\\r\\nAccuracy is as good as any other unit, they all sometimes tell you you have arrived when you haven't, or continue to tell you to turn when you're already there .\\r\\n Accuracy is determined by the maps .\\r\\n Less traveled rural roads will not be accurate on any unit .\\r\\n Accuracy is within a few yards .\\r\\nWhat the 255w does best is find a street address, business, point of interest, hospital or airport and give you turn, by, turn directions with amazing accuracy .\\r\\n The Garmin is loaded with very accurate maps that generally know the roads in even the remotest areas .\\r\\nI'm really glad I bought it though, and like the easy to read graphics, the voice used to tell you the name of the street you are to turn on, the uncannily accurate estimates of mileage and time of arrival at your destination .\\r\\nMy new Garmin 255w had very Easy Set Up, Accurate Directions to locations, User Friendly Unit to anyone in my vehicle who tried it .\\r\\n I had a GPS 10, years ago when I owned a boat that was difficult to use and with very poor accuracy so I had assumed that the road GPS wasn't any better .\\r\\n Practiced visiting places I already knew to see how accurate the directions and maps would be .\\r\\n Easy to use, excellent accuracy, nice and intuitive interface .\\r\\n The directions provided have all been quite accurate thus far .\\r\\n,  Very Accurate but with one small glitch I found ,  I'll explain in the CONS\\r\\nThis is a great GPS, it is so easy to use and it is always accurate .\\r\\nVery easy to operate and pretty accurate as well, only led me astray once and that was in northern Maine where roads are few and paved ones fewer .\\r\\n Easy to use and amazed at how accurate this item is .\\r\\nTo date it's been a very easy to use and accurate .\\r\\n Mounted really easily and has been very accurate .\\r\\n seems to be rather accurate .\\r\\n It was accurate on determing original directions and recalculating when necessary .\\r\\nHighly accurate, POIs are great .\\r\\n I can't believe how accurate and detailed the information estimated time of arrival,speed limits along the way,and detailed map of my route, to name a few .\\r\\n Speed of calculation, accuracy, and simplicity of operation are top notch .\\r\\n\",\n",
       " 'summaries': ['This unit is generally quite accurate.  \\r\\nSet-up and usage are considered to be very easy. \\r\\nThe maps can be updated, and tend to be reliable.',\n",
       "  \"The Garmin seems to be generally very accurate.\\r\\nIt's easy to use with an intuitive interface.\",\n",
       "  'It is very accurate, even in destination time.',\n",
       "  'Very accurate with travel and destination time.\\r\\nNegatives are not accurate with speed limits and rural roads.',\n",
       "  'Its accurate, fast and its simple operations make this a for sure buy.']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'].shape)\n",
    "print(dataset['train'][0].keys())\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d5b6c-d2dc-400a-92f9-298af290b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][-2]['review_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ac89b-0714-4100-8875-fec54785af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14010bce-ddff-4d7b-986c-b095399d5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c225539-d3a1-423f-af36-98eac35410f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa7054-04bc-4d68-a154-8146a9cd133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b5825-25c6-43a0-bc14-e99996465bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd820626-ebb8-45c8-a330-c77d65785125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why the input and translated IDs vectors have more elements than the corresponding word inputs is due to the way the tokenizer works.\n",
    "# In your code, the tokenizer.encode function is used to convert the input text into a sequence of IDs, which represent the tokens in the text. These tokens can be individual words, but they can also be smaller units depending on the tokenizer. For example, a word might be split into multiple subwords, each with its own ID.\n",
    "# Additionally, special tokens are often added to the sequence. For instance, a common practice is to add a special token at the beginning and end of the sequence. In your case, the 0 at the end of each input_ids and translated_ids tensor is likely a special token, such as an end-of-sequence token\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    print('english_input', english_input)\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
    "    print('input_ids', input_ids)\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    print('translated_ids', translated_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a180b31-33ec-4984-9b7c-e9cdd955d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "mlqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671edd08-a89d-4458-87a9-d983f359f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea420d-7e1f-4798-831e-3f013e593b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a96dda-066d-4279-adf9-857af82a91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dcdce-caea-4b31-9139-4dc4dc0485fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd781f-c2c5-4388-ac12-1e3511634879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cf16e-e65e-4dbd-a9a7-626e7b1516b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2161c-bb10-4245-8a2c-62be2ed387b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "answer_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1c323-1b76-473a-81c0-69bd1a7ab90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8b999-8f13-47a1-8250-cab93c023205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb9446-576e-4166-bc80-9853aefda005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163c51c-1724-4570-b838-6b4093a61e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "tokenized_datasets = []\n",
    "\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928e249-2aaa-40c9-b3c0-7aac99a0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b817ae-9314-484f-94c0-9f3c07106629",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774792d6-ea1d-488d-9030-75fade5c2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "dataset = load_dataset('emotion', trust_remote_code=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280e72a-80ef-4fc7-b518-8d72e086dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode your dataset\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "emotions_encoded = dataset.map(encode, batched=True)\n",
    "\n",
    "emotions_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80aee8-81ce-4070-8fd1-d9a544a9b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n",
    "    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee495e68-4020-4df7-b410-a52280e040b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys of the first example in the training dataset\n",
    "print(emotions_encoded[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661d92a-9e65-4877-8cdf-8a156b75494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for example in emotions_encoded[\"train\"]:\n",
    "    unique_labels.add(example[\"label\"])\n",
    "print(f\"Unique labels: {sorted(list(unique_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffd65a-a5ff-45ca-9b28-cb7b89a65b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop to fine-tune the model\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221a882-ffd6-40ad-84c0-df2bf9c33c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597455a9-ff17-434d-9e61-b902021d03d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e38d8-6a31-4441-97ba-33476121482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain class labels from raw predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd845ce-4035-4633-bbd9-ce458944a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b5687-013e-4ada-8145-323df266236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "test_examples = [{'text': 'I love this product!', 'label': 1},\n",
    "                 {'text': 'The service was terrible.', 'label': 0},\n",
    "                 {'text': 'This movie is amazing.', 'label': 1},\n",
    "                 {'text': \"I'm disappointed with the quality.\", 'label': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be27ae-994b-496d-999f-55115b8ad3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e602e-ef39-4c73-a089-2bba48d0b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1baf2e-7d14-416a-9838-454af1507e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8c898-4d6e-4403-9aa9-538ad6823bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assuming true_labels and predicted_labels are defined\n",
    "result = accuracy_score(true_labels, predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95310d6-52fe-4d68-be6e-cb76a4a3be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08298bec-b7c1-4d01-92cd-75964b962d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy, precision, recall and F1 score metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385dad1c-cea7-4381-b19b-0d0f198e0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Pass the examples to the pipeline, and obtain a list predicted labels\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf56cc-d82f-43c4-abe8-7151fcd42113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"Fantastic hotel, exceeded expectations!\",\n",
    "    \"Quiet despite central location, great stay.\",\n",
    "    \"Friendly staff, welcoming atmosphere.\",\n",
    "    \"Spacious, comfy room—a perfect retreat.\",\n",
    "    \"Cleanliness could improve, overall decent stay.\",\n",
    "      \"Disappointing stay, noisy and unclean room.\",\n",
    "    \"Terrible service, unfriendly staff, won't return.\"\n",
    "]\n",
    "\n",
    "test_labels = [1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49785c-9eff-4c15-a056-5768d5dca1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sentiment_analysis([example for example in test_examples])\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac1d45-969b-4a01-b29d-70c105e8bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(f1.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=test_labels, predictions=predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abff60-d03c-403a-823e-b7a63f887ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dadc3-aaaa-45e6-b876-3baeff5a0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Current trends show that by 2030 \"\n",
    "\n",
    "# Encode the prompt, generate text and decode it\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e52113-d928-4378-b974-91b02f1ea68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(prompt_ids, max_length=50)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48beed3e-4f7b-458c-a4be-bdf80b607cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625510fd-e877-4f02-91cb-c86754fc900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id=model_name,\n",
    "                             predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731511c-2b79-4b53-b346-4f0311d5cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rouge_score\n",
    "\n",
    "# Load the rouge metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions,references=references)\n",
    "print(\"ROUGE results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c4aa1-4079-4cd1-b577-16624aed5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
    "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=llm_outputs, references=references)\n",
    "print(\"Meteor: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e40f7-ead6-48ba-89cc-92381bee75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(\"EM results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc18bd-fae3-4578-bbdc-002f53b694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
    "\n",
    "reference_1 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
    "     ]\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd083c-774c-4f2b-919e-c4a7cf655fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why there are multiple reference sentences for each input sentence is because of the inherent ambiguity and variability in translation. There can be several equally correct translations for a given sentence, depending on factors like context, tone, and style. By providing multiple reference translations, we can capture some of this variability and get a more robust estimate of the model’s performance.\n",
    "\n",
    "# In the code you posted, the BLEU score is being calculated for the translations. The BLEU score is a metric that measures the quality of a translation by comparing it to one or more reference translations. It does this by counting the number of n-gram matches between the translation and the reference(s), and then normalizing by the total number of n-grams in the translation. The more the translation resembles the reference(s), the higher the BLEU score will be.\n",
    "\n",
    "# In your example, the first input sentence “Hola, ¿cómo estás?” is translated and then the translation is compared to two reference translations: “Hello, how are you?” and “Hi, how are you?”. The BLEU score is then computed for this translation.\n",
    "\n",
    "# The same process is repeated for the second set of input sentences and references. The final BLEU score is a measure of how well the translations match the reference translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49442a73-3f15-403b-9025-7b088cbfb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the first input sentence\n",
    "translated_output = translator(input_sentence_1)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "# Calculate BLEU metric for translation quality\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824753e7-3f19-493b-b701-15d12927718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translated_outputs = translator(input_sentences_2)\n",
    "\n",
    "translated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e780d-2c9f-4a28-bec9-3bf185cb4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7ab11-2129-46c5-823e-ca6874125b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb9c7a-cd48-4bcd-86e6-0073e6c38e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trl\n",
    "from trl import PPOTrainer, PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787e0ec-e281-424f-93d2-f4f279cac82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a reference model\n",
    "\n",
    "# When you call create_reference_model(model), it creates a copy of the model and freezes its parameters. This means that the weights of the reference model will not be updated during training.\n",
    "# This reference model is then used to compare with the updated model at each step of the training process. The idea is to ensure that the policy (i.e., the behavior of the model) does not change too drastically from one update to the next\n",
    "\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47816608-d588-4f58-83ef-b6ab1aa9bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if the parameters of a model are frozen, you can iterate over the parameters and check their requires_grad attribute. Here’s a small function that can do this:\n",
    "\n",
    "def check_if_frozen(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is not frozen\")\n",
    "        else:\n",
    "            print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d672e16-34e3-4e81-90d2-140eb381a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_frozen(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e860b1-5b6e-4241-8732-149251c6f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_frozen(model_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d53836-3a01-4e6d-b898-a5e2be57acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa787b53-013d-4599-95c6-16da82810d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer configuration\n",
    "# this code is setting up a configuration for a PPO trainer with specific batch and mini-batch sizes. This configuration would be used when training a model using the PPO algorithm. \n",
    "\n",
    "ppo_config = PPOConfig(batch_size=1, mini_batch_size=1)\n",
    "\n",
    "ppo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863c35b-4fb9-443f-b302-c44d4be31764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PPOTrainer instance\n",
    "# this line of code is setting up a PPO trainer with a specific configuration, model, reference model, and tokenizer. The trainer can then be used to train the model using the PPO algorithm.Typically, the trainer would have a method like train() that you can call to start the training process. The training process involves repeatedly sampling data, using the data to update the model, and then evaluating the performance of the model. The goal is to improve the model’s performance on some task, such as generating text. The PPO algorithm is particularly well-suited to tasks where the data is sequential or temporal in nature. It’s also known for its stability and efficiency, which makes it a popular choice for many reinforcement learning tasks.\n",
    "\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "\n",
    "ppo_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fbe5d-f5d9-4976-8b43-bddbe8a6dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Next year, I \"\n",
    "\n",
    "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792a051-b10a-4f39-a1ed-1c49a1d8e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is using a pretrained language model to generate a response to a given prompt. The response is calculated by feeding the encoded input into the model and then decoding the model’s output back into text. The response represents what the model thinks is the most likely continuation of the input prompt. The exact details of how the response is calculated depend on the specifics of the model and the respond_to_batch function. \n",
    "\n",
    "from trl.core import respond_to_batch\n",
    "\n",
    "response  = respond_to_batch(model, input) # function to generate a response from the model. The function takes the model and the encoded input as arguments.\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091fd2f1-bb64-4b24-a6d6-288f9d8e0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a more complex scenario, you might want to design a reward function that gives higher rewards for better responses and lower rewards for worse ones. This would require a way to evaluate the quality of the responses, which could be based on various factors such as the relevance of the response to the input, the grammatical correctness of the response, etc. This is typically the challenging part in reinforcement learning - designing a good reward function.\n",
    "\n",
    "import torch\n",
    "reward = [torch.tensor(1.0)]\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf05e00-7038-4045-a53b-1e5ca26448af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LLM for one step with PPO\n",
    "# while step() is used for a single step of training, train() is used for full-scale training over multiple epochs. The code is likely a simplified example or a debugging scenario where only a single step of training is being performed. For training a model to completion, you would generally use a train() function or similar.\n",
    "\n",
    "train_stats = ppo_trainer.step([input[0]], [response[0]], reward) # The step function is used to perform one step of training, where the model’s parameters are updated to maximize the expected reward.\n",
    "\n",
    "train_stats\n",
    "\n",
    "# The train_stats dictionary contains various statistics and metrics that are calculated during the training step. Here’s a brief explanation of some of the key metrics:\n",
    "# ‘objective/kl’: This is the Kullback-Leibler (KL) divergence, which measures how one probability distribution diverges from a second, expected probability distribution. In this case, it’s 0.0, indicating no divergence.\n",
    "# ‘objective/logprobs’: These are the log probabilities of the actions taken by the model. They are used in the calculation of the policy gradient.\n",
    "# ‘ppo/mean_scores’: This is the mean of the scores (rewards) obtained during the training step.\n",
    "# ‘tokens/queries_len_mean’: This is the average length of the queries processed in the training step.\n",
    "# ‘ppo/loss/policy’, ‘ppo/loss/value’, ‘ppo/loss/total’: These are the losses for the policy, value function, and the total loss respectively. The policy loss is related to how well the model is doing in terms of taking the right actions. The value loss is related to how well the model is predicting the expected future rewards.\n",
    "# ‘ppo/policy/entropy’: This is the entropy of the policy. It’s a measure of the randomness of the policy. A higher entropy means the policy is more random, while a lower entropy means the policy is more deterministic.\n",
    "# ‘ppo/returns/mean’: This is the mean of the returns (sum of rewards) obtained during the training step.\n",
    "# ‘ppo/val/vpred’: This is the predicted value of the state by the model.\n",
    "# ‘time/ppo/total’: This is the total time taken for the training step.\n",
    "# The warnings about degrees of freedom being less than or equal to 0 are due to the standard deviation (std()) function being called on a dataset with insufficient size. This can happen when the batch size or mini-batch size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4396e-4852-44e9-bb28-81079b28b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training steps\n",
    "num_steps = 100\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, num_steps+1):\n",
    "    # Generate input and response here...\n",
    "    # ...\n",
    "    reward = [torch.tensor(1.0)]\n",
    "    train_stats = ppo_trainer.step([input[0]], [response[0]], reward)\n",
    "    if i % 10 == 0:\n",
    "        print(i, 'training steps')\n",
    "    \n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163c3a8-bf7e-4b63-9b65-f63fd3a6bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
    "\n",
    "emp_1 = [\"Everyone in the team adores him\",\n",
    "           \"He is a true genius, pure talent\"]\n",
    "emp_2 = [\"Nobody in the team likes him\",\n",
    "           \"He is a useless 'good-for-nothing'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f9f82-b535-41a2-aeab-2547d3b8b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "toxicity_metric = load(\"toxicity\")\n",
    "\n",
    "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
    "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549b980-b7a6-4f9c-b3d2-358f07499d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation=\"maximum\")\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation=\"maximum\")\n",
    "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c292b-8fc6-466d-a3e3-9b9bdc8964b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation=\"ratio\")\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_2, aggregation=\"ratio\")\n",
    "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f008d-59cc-4bef-824b-e53b13301737",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = ['abc are described as loyal employees', \n",
    "          'abc are ambitious in their career expectations']\n",
    "group2 = ['abc are known for causing lots of team conflicts',\n",
    "          'abc are verbally violent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981893d6-721b-4f83-9c6d-046b534b82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the regard and regard-comparison metrics\n",
    "\n",
    "regard = evaluate.load(\"regard\")\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "# regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f5daa-a99e-478d-a380-14b17fa238ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the regard (polarities) of each group separately\n",
    "\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f12334-abb2-4cdc-b03c-3eba57609dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the relative regard between the two groups for comparison\n",
    "\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f653f-8795-44f3-813b-2c4c7f30b8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
