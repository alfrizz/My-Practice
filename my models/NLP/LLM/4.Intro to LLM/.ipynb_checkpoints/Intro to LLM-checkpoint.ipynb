{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f579bcb-a96e-4156-b1ef-2dbaa444adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x29dff8438b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "# Load the pipeline for sentiment classification\n",
    "classifier = pipeline(\"text-classification\", model=model_name)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933b2234-cf9e-45e6-89d8-51620e70cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '3 stars', 'score': 0.6387940645217896}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a22190-ca52-4b7b-aed9-45bb268f6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|█████████████████████████████████████████████████████| 1.38k/1.38k [00:00<00:00, 687kB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████| 242M/242M [00:05<00:00, 46.2MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████████████████████████████████████████████| 1.92k/1.92k [00:00<?, ?B/s]\n",
      "Downloading spiece.model: 100%|█████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 2.14MB/s]\n",
      "Downloading tokenizer.json: 100%|█████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 5.99MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████| 1.79k/1.79k [00:00<00:00, 1.78MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.SummarizationPipeline at 0x29dff8ad390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cnicu/t5-small-booksum'\n",
    "\n",
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline('summarization', model = model_name)\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5111f86-f864-4b68-bb1f-244541711108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = '\\nThe tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\\n'\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length = 30)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c80d10-21eb-4c39-8b89-843b4b2cb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n"
     ]
    }
   ],
   "source": [
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1055b1e9-7c8d-4f56-9ab9-9a1e8e5082d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 years\n"
     ]
    }
   ],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "\n",
    "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
    "outputs = qa_model(question, long_text)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f32cf35b-340a-426c-bebf-4013cfa3e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|████████████████████████████████████████████████████████████| 1.44k/1.44k [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████| 312M/312M [00:07<00:00, 41.9MB/s]\n",
      "Downloading generation_config.json: 100%|██████████████████████████████████████████████| 293/293 [00:00<00:00, 302kB/s]\n",
      "Downloading tokenizer_config.json: 100%|████████████████████████████████████████████████████| 44.0/44.0 [00:00<?, ?B/s]\n",
      "Downloading source.spm: 100%|███████████████████████████████████████████████████████| 826k/826k [00:00<00:00, 10.9MB/s]\n",
      "Downloading target.spm: 100%|███████████████████████████████████████████████████████| 802k/802k [00:00<00:00, 2.01MB/s]\n",
      "Downloading vocab.json: 100%|█████████████████████████████████████████████████████| 1.59M/1.59M [00:00<00:00, 3.18MB/s]\n",
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course on LLMs is getting very interesting.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "\n",
    "input_text = \"Este curso sobre LLMs se está poniendo muy interesante\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline('translation_es_to_en', model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8f6f578-4871-4b5f-8c06-b0dfa5f64e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer model hyperparameters\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "126ece2e-d3d9-464e-aead-b62880bfc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,    \n",
    "    num_decoder_layers=num_decoder_layers\n",
    "    )\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "943a497a-5f46-407d-be6e-da7d37c310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fecc1dc-bf8c-41b2-b66d-17c1a782f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Customer review:\\nI had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\\n\\nHotel reponse to the customer:\\nDear valued customer, I am glad to hear you had a good stay with us.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the prompt for the text generation LLM\n",
    "\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815306fd-4acc-4cd7-997a-2b1c0e16b78e",
   "metadata": {},
   "source": [
    "#### Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length = 150, pad_token_id=generator.tokenizer.eos_token_id) #  if the generated text is shorter than max_length, the remaining tokens will be filled with the EOS token.\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75cce4e3-0cb2-42b4-b8fb-547a09f7242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import math  \n",
    "\n",
    "# Subclass the PyTorch nn.Module class to create a custom module for positional encoding\n",
    "# This class is used to add positional information to the input embeddings in a Transformer model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        # Call the parent class's constructor\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        \n",
    "        # Initialize the dimensions of the model and the maximum sequence length\n",
    "        self.d_model = d_model  # The dimension of the input embeddings\n",
    "        self.max_length = max_length  # The maximum length of the input sequences\n",
    "        \n",
    "        # Initialize the positional encoding matrix with zeros\n",
    "        # This matrix will store the positional encodings that will be added to the input embeddings\n",
    "        pe = torch.zeros(max_length, d_model)  \n",
    "\n",
    "        # Create a tensor of positions from 0 to max_length\n",
    "        # This tensor represents the positions of the words in a sequence\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)  #  we unsqueeze because the multiplication operation position * div_term requires position to be a 2D tensor to correctly broadcast with div_term (the extra dimension is added at position '1': second position)\n",
    "        \n",
    "        # Calculate the division term for the positional encoding\n",
    "        # This term is used in the calculation of the positional encodings\n",
    "        # The div_term values decrease exponentially, which means the positional encoding changes more rapidly for lower-dimensional embeddings and more slowly for higher-dimensional embeddings. This allows the model to learn to attend to both nearby words (local attention) and far-away words (global attention), which is crucial for understanding the context of a sentence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))  \n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        # For even indices, use sine of the position times the division term\n",
    "        # For odd indices, use cosine of the position times the division term\n",
    "        # These encodings are based on sine and cosine functions of different frequencies\n",
    "        # The sine and cosine functions are used to ensure that the positional encodings are continuous and differentiable, which is important for the learning process. Also, these functions generate values between -1 and 1, which helps to keep the magnitude of the positional encodings manageable.\n",
    "        # Using sine for even indices and cosine for odd indices provides two different signals for each position, which helps the model distinguish between different positions more effectively.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "        \n",
    "        # Add an extra dimension to the positional encoding matrix, turning it from a 2D tensor into a 3D tensor(the extra dimension is added at position '0': first position)\n",
    "        # This is done to match the dimensions of the input embeddings (batch size, sequence length, and embedding size)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        \n",
    "        # Register the positional encoding matrix as a buffer that should not be considered a model parameter\n",
    "        # Buffers are tensors that are not updated during backpropagation but need to be part of the model's state\n",
    "        self.register_buffer('pe', pe)  # N.B. self.pe is defined when pe is registered as a buffer. \n",
    "    \n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Update the input tensor by adding the positional encodings\n",
    "        # The positional encodings are added to the input embeddings so that the model can take into account the position of words in a sequence\n",
    "        # The size(1) method returns the size of the second dimension of x, which represents the sequence length.\n",
    "        # By slicing pe to :x.size(1), we ensure that the positional encodings are correctly aligned with the words in each input sequence.\n",
    "        x = x + self.pe[:, :x.size(1)]  \n",
    "        \n",
    "        # Return the updated tensor\n",
    "        # This updated tensor is then passed on to the next layer of the Transformer model\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f81cc6b9-c4fd-49ba-a153-aca7734107be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # The number of attention heads. This is the number of different \n",
    "        # weighted sums of the input vectors that we will compute.\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # The dimension of the input embeddings. This is the size of the \n",
    "        # vectors that will be processed by the attention mechanism.\n",
    "        self.d_model = d_model \n",
    "\n",
    "        # The dimension of each head. This is the size of the vectors that \n",
    "        # each attention head will process independently.\n",
    "        # N.B. each attention head processes the entire data, but they do so in their own learned representation space. The head_dim is the dimensionality of this representation space. The division by num_heads is done to ensure that the dimensionality of the input (d_model) is preserved through this process. This is important for the subsequent layers in the model, which expect inputs of a d_model dimensionality.\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear transformations for the queries, keys and values. These \n",
    "        # are standard fully connected layers that will transform the input \n",
    "        # vectors into the queries, keys and values.\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # The final linear transformation. This layer will transform the \n",
    "        # concatenated output of the attention heads into the final output \n",
    "        # vector.\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # This function splits the input vectors into the different attention \n",
    "        # heads. It first reshapes the input vectors into a tensor of shape \n",
    "        # (batch_size, sequence_length, num_heads, head_dim), and then \n",
    "        # rearranges the dimensions to bring the number of heads to the \n",
    "        # second dimension.\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        # The contiguous method is used to ensure that the tensor in memory is \n",
    "        # properly ordered, allowing us to view its data with a different shape.\n",
    "        # Contiguous creates a new tensor with the same data but with all the data contiguously in memory. \n",
    "        # The view method is used twice to first change the shape of the tensor \n",
    "        # and then flatten the tensor (from batch_size, self.num_heads, -1, self.head_dim --> to batch_size * self.num_heads, -1, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim) \n",
    "    \n",
    "    \n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # This function computes the attention weights. It first computes \n",
    "        # the dot product of the query and key tensors, applies a mask if \n",
    "        # provided, and then applies a softmax function to obtain the \n",
    "        # attention weights.\n",
    "        scores = torch.matmul(query, key.permute(0, 2, 1))  # fixed from the original: torch.matmul(query, key.permute(1, 2, 0))\n",
    "        if mask is not None:\n",
    "            # The mask is used to prevent the attention mechanism from focusing \n",
    "            # on certain positions. This is done by setting the scores of these \n",
    "            # positions to a very large negative value, effectively zeroing out \n",
    "            # their impact on the softmax result.\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # The forward function computes the output of the multi-head \n",
    "        # attention layer. It first applies the linear transformations and \n",
    "        # splits the input into multiple heads, then computes the attention \n",
    "        # weights, applies these weights to the values, and finally \n",
    "        # concatenates and linearly transforms the result into the output \n",
    "        # vector.\n",
    "        # query.size(0) returns the size of the batch dimension. It is used \n",
    "        # to reshape the input tensors before splitting them into heads.\n",
    "        batch_size = query.size(0) \n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model) # (from batch_size, -1, self.num_heads, self.head_dim --> to batch_size, -1, self.d_model), being self.d_model = self.num_heads * self.head_dim\n",
    "        return self.output_linear(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d6e3116-b20c-41c4-843e-579eef900fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    \n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3376a16-5901-49f1-823d-57fa8d88fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # The input x is a sequence of embeddings representing the input tokens, and its shape is generally (batch_size, sequence_length, d_model). In the context of self-attention mechanisms, such as the one used in Transformer models, x is used as the Query (Q), Key (K), and Value (V).  In self-attention, the same input (in this case, x) is used as the Query, Key, and Value. This allows the model to compute attention scores based on the input itself.\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        # The operation x + self.dropout(attn_output) is an example of a technique called residual connection: The idea is that it’s easier to model a residual (or difference) than to learn to model the full information. In this specific case,we are “adding the residual”, that is the output of the self-attention mechanism (which has learned how to modify the input) back to the original input. \n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca2f4c09-8492-4b39-aca1-d14795b8e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54d2ca32-6967-4db0-9f5c-6c2f69f590d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    # the dimensionality of the embeddings is:\n",
    "# batch_size is the number of sequences in the batch.\n",
    "# sequence_length is the length of each sequence.\n",
    "# d_model is the dimensionality of the embeddings (i.e., the size of the feature vector for each token).\n",
    "# The slice x[:, 0, :] selects the entire feature vector for the first token in each sequence, that is the [CLS] token, which includes an aggregate representation of the entire sequence, useful in classification tasks.\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a89eaf52-2dbf-44f5-b7fd-c56f21062c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 256\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a35e7da-1001-40cc-be8d-2a340c9ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3276, 5084, 3060,  ..., 1444, 6205, 8953],\n",
       "        [7178, 9596, 3913,  ..., 2767, 9638, 1859],\n",
       "        [3357, 3281, 4249,  ..., 8888, 5844,  721],\n",
       "        ...,\n",
       "        [4958, 3438, 2315,  ..., 6940, 9966,  563],\n",
       "        [6210, 2765, 2150,  ...,  895, 9827, 9617],\n",
       "        [5198, 1807, 9069,  ...,  472, 4867, 2383]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "\n",
    "print(input_sequence.shape)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d1df462c-a0e9-47df-890b-feab943ddc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0,  ..., 1, 0, 1],\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 1,  ..., 0, 0, 1],\n",
       "        [0, 0, 1,  ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "print(mask.shape)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e0b16ba-3c0e-4366-8687-6a890b514fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18a03e94-7280-464a-b49b-3ff6540a97d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierHead(\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e7e7d819-2ddd-4a9c-b864-c8df512e6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5396e+00, -7.0473e-01, -1.6752e-01,  ..., -4.3265e-01,\n",
       "           1.6643e-01, -6.7266e-01],\n",
       "         [ 3.7615e-01, -2.1864e-01,  1.9548e+00,  ...,  7.2642e-01,\n",
       "          -5.6769e-01,  4.9699e-01],\n",
       "         [-5.7968e-01, -4.6237e-01,  8.9962e-01,  ...,  5.6324e-01,\n",
       "          -1.7061e+00,  3.1116e-01],\n",
       "         ...,\n",
       "         [ 8.2552e-02, -1.3685e+00, -1.0649e-01,  ...,  8.2046e-01,\n",
       "          -3.9226e-01,  6.9439e-01],\n",
       "         [-1.7032e-01, -5.6079e-01, -1.4322e+00,  ...,  1.2878e+00,\n",
       "           7.4063e-01,  2.2125e+00],\n",
       "         [-6.5154e-01, -6.4432e-01,  3.9084e-01,  ...,  2.0036e+00,\n",
       "           9.3471e-01,  6.8842e-01]],\n",
       "\n",
       "        [[-1.1413e+00, -1.9727e-01,  1.2967e+00,  ...,  9.2610e-01,\n",
       "          -3.9590e-02,  1.1990e+00],\n",
       "         [ 4.1502e-01,  8.3782e-01, -1.3256e-01,  ...,  9.2458e-01,\n",
       "          -1.0622e+00,  8.6573e-05],\n",
       "         [-6.5585e-01, -2.5852e-01, -5.0404e-01,  ...,  6.7345e-01,\n",
       "           1.0727e+00,  3.8947e-02],\n",
       "         ...,\n",
       "         [-3.7751e-02, -7.9839e-01, -4.0597e-01,  ...,  1.6500e+00,\n",
       "           4.2274e-01,  8.6835e-01],\n",
       "         [ 6.0016e-01, -1.3786e+00, -5.2069e-01,  ...,  1.4229e+00,\n",
       "           6.8101e-01,  7.1355e-01],\n",
       "         [-9.0637e-01, -8.0684e-01,  8.3691e-01,  ...,  1.0128e+00,\n",
       "           1.5494e-01, -1.4933e-01]],\n",
       "\n",
       "        [[-7.3755e-01,  1.1610e+00, -9.2341e-01,  ...,  1.2282e+00,\n",
       "          -3.1872e-01, -5.9400e-01],\n",
       "         [-4.4247e-01,  6.2561e-01,  1.5159e-01,  ..., -7.2018e-01,\n",
       "          -8.5258e-01, -4.6829e-01],\n",
       "         [ 2.1779e-01, -1.4539e+00,  8.9856e-01,  ...,  1.4881e+00,\n",
       "          -5.7973e-01, -9.5025e-01],\n",
       "         ...,\n",
       "         [-1.5953e+00, -6.4625e-01,  2.0645e-01,  ...,  1.1559e+00,\n",
       "           7.7670e-02,  1.3538e+00],\n",
       "         [-1.4310e+00, -1.8994e+00, -4.2984e-01,  ...,  1.1912e+00,\n",
       "           7.4361e-02,  5.2992e-01],\n",
       "         [ 8.3669e-01, -2.9272e+00,  1.8808e+00,  ...,  4.3356e-01,\n",
       "          -3.0647e-01,  2.4073e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.4521e-02,  5.9025e-01,  6.9238e-01,  ..., -5.2135e-02,\n",
       "          -1.9570e+00, -4.0545e-01],\n",
       "         [-5.7318e-01, -6.5088e-02,  2.0008e-01,  ...,  2.6903e-01,\n",
       "          -9.8765e-01, -4.3892e-01],\n",
       "         [-2.5754e-01, -1.7477e-01, -6.2533e-03,  ...,  2.3729e-01,\n",
       "           8.8117e-01,  4.1768e-01],\n",
       "         ...,\n",
       "         [ 8.9350e-01, -2.4603e-01, -6.6543e-01,  ..., -7.2205e-01,\n",
       "          -4.8032e-01,  2.9998e-01],\n",
       "         [ 6.0054e-01, -2.9988e+00,  7.1289e-01,  ...,  3.2696e-01,\n",
       "           1.4112e+00,  2.0665e-01],\n",
       "         [-4.6093e-01, -6.3567e-01,  7.8318e-01,  ...,  1.1394e+00,\n",
       "           1.1283e-01,  8.0347e-01]],\n",
       "\n",
       "        [[-6.5473e-01,  1.0349e+00,  9.6712e-01,  ...,  1.5556e+00,\n",
       "          -1.1754e+00, -8.7536e-01],\n",
       "         [-4.6748e-01,  4.6627e-01,  7.1472e-01,  ...,  1.9679e+00,\n",
       "          -3.4973e-02, -7.8805e-01],\n",
       "         [ 9.3725e-02, -1.4718e+00,  1.3035e+00,  ...,  2.8570e+00,\n",
       "          -5.4548e-01,  1.4812e-01],\n",
       "         ...,\n",
       "         [-3.8172e-01, -5.0280e-01, -6.6942e-01,  ..., -3.4280e-01,\n",
       "           1.2689e+00,  4.9661e-01],\n",
       "         [-7.0539e-02,  2.6321e-01,  2.0907e-01,  ...,  2.0025e+00,\n",
       "           6.8504e-01, -2.4255e-02],\n",
       "         [-1.0755e+00, -7.7415e-02, -1.4848e-01,  ...,  9.8376e-01,\n",
       "          -1.7595e+00,  1.0367e+00]],\n",
       "\n",
       "        [[ 1.0519e+00,  1.6198e-01,  4.0816e-01,  ..., -1.2757e-01,\n",
       "          -7.3589e-01, -4.5305e-01],\n",
       "         [-5.0425e-01, -6.1705e-01,  1.7800e+00,  ...,  1.5860e+00,\n",
       "          -4.1189e-01,  4.9942e-01],\n",
       "         [ 9.3010e-01, -1.3383e+00,  2.2208e+00,  ...,  9.5001e-02,\n",
       "          -1.0590e+00, -6.9205e-01],\n",
       "         ...,\n",
       "         [-1.0706e+00, -1.2146e+00, -1.1144e+00,  ...,  1.1755e+00,\n",
       "           6.6280e-01,  1.0968e+00],\n",
       "         [-3.7145e-01, -6.2847e-01,  4.9736e-02,  ...,  1.2294e-01,\n",
       "          -1.0109e+00, -9.6902e-01],\n",
       "         [-7.8528e-01, -8.6582e-01,  1.3985e+00,  ..., -1.5466e-01,\n",
       "          -9.7539e-01,  1.3974e+00]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the forward pass \n",
    "enc_output = encoder(input_sequence, mask)\n",
    "\n",
    "print(enc_output.shape)\n",
    "enc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d2056424-9171-414b-9011-5b26f1acbe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification outputs for a batch of  8 sequences:\n",
      "torch.Size([8, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6070, -2.4803, -0.9907],\n",
       "        [-1.0721, -1.7006, -0.7441],\n",
       "        [-0.9410, -0.9812, -1.4486],\n",
       "        [-0.5018, -1.3718, -1.9598],\n",
       "        [-0.6680, -2.1002, -1.0083],\n",
       "        [-0.6394, -2.7262, -0.8991],\n",
       "        [-0.8657, -1.0693, -1.4439],\n",
       "        [-0.5899, -1.5080, -1.4949]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification = classifier(output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification.shape)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "44e16b75-3f55-4d58-a1b9-a10be7fc1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f8fdcfa-d55c-413c-baa5-1ef645e1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) # using EncoderLayer, but it should be DecoderLayer\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # def forward(self, x, self_mask):\n",
    "    #     x = self.embedding(x)\n",
    "    #     x = self.positional_encoding(x)\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x, self_mask)\n",
    "            \n",
    "    def forward(self, x, self_mask, encoder_output, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask, encoder_output, cross_mask)\n",
    "        # return x\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        # When you apply F.log_softmax(x, dim=-1), the softmax function is applied to the d_model dimension. This means that the softmax function is applied independently to each sequence in each batch, and the output tensor will have the same shape as the input tensor.\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d28bcc5c-d3b7-480c-a651-b94bdb9b4066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3781, 4176, 9546,  ...,  608,  945, 8430],\n",
       "        [6604, 7862, 5303,  ..., 7011, 8362,  568],\n",
       "        [6679, 6208, 8972,  ..., 4628, 3468, 2262],\n",
       "        ...,\n",
       "        [7556, 6363, 4086,  ..., 7937, 2155, 6590],\n",
       "        [ 692,  910, 1684,  ..., 2832, 3182, 2349],\n",
       "        [2623, 2797, 6853,  ..., 8498, 2130,  256]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "print(input_sequence.shape)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2ed0b6e6-0bc1-4a04-a52e-2acc89e660dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "275c46fb-b6d5-4541-bbd8-3ec1025ebd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, 8, 8), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b5f0f76-af6f-4023-8ffb-4e46d86b2414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "75930524-056e-4a59-96c4-8d868e92f66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  ..., False, False, False],\n",
       "         [ True,  True, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()  # Upper triangular mask\n",
    "\n",
    "print(self_attention_mask.shape)\n",
    "self_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a217c58-6b86-492d-85b9-4e919a54c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "17ffcd35-d9f0-4055-ad88-7482e1c559a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 1, 1],\n",
       "        [1, 0, 1,  ..., 0, 0, 1],\n",
       "        ...,\n",
       "        [0, 1, 0,  ..., 0, 1, 0],\n",
       "        [1, 0, 1,  ..., 1, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder cross_mask\n",
    "\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0018a3cb-a64a-48e0-9f14-1e956a794d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 10000])\n",
      "tensor([[[ -8.7817, -10.2451,  -9.6885,  ..., -10.1293,  -9.3597,  -9.2839],\n",
      "         [ -8.4072, -10.3035,  -9.8577,  ...,  -9.9263,  -9.5064, -10.0248],\n",
      "         [ -8.6076,  -9.8351, -10.0339,  ...,  -9.2415,  -8.4927,  -8.5276],\n",
      "         ...,\n",
      "         [ -8.9352, -10.3473,  -9.9254,  ...,  -9.2700,  -9.4030,  -9.0087],\n",
      "         [ -8.3065,  -9.9286, -10.0848,  ...,  -9.1453,  -9.5359,  -9.3849],\n",
      "         [ -8.9925,  -9.0589,  -9.7228,  ...,  -9.2637,  -8.6402,  -9.2644]],\n",
      "\n",
      "        [[ -9.4488,  -9.6935,  -9.9559,  ..., -10.2595,  -8.2290,  -9.0882],\n",
      "         [ -8.0041,  -9.3526, -10.6797,  ...,  -9.4334,  -8.1505,  -8.3640],\n",
      "         [ -8.5836,  -8.8375, -10.2864,  ..., -10.2429,  -9.6378,  -8.9388],\n",
      "         ...,\n",
      "         [ -8.3471,  -9.4997, -10.1958,  ...,  -9.1147,  -9.8314,  -9.1181],\n",
      "         [ -7.8797, -10.4160, -10.3558,  ...,  -9.9459, -10.0784,  -9.8117],\n",
      "         [ -7.8935,  -9.3017, -10.1916,  ...,  -9.3722,  -8.7363,  -8.8932]],\n",
      "\n",
      "        [[ -8.4499,  -9.1731,  -9.4475,  ..., -10.3154,  -9.8344,  -9.7101],\n",
      "         [ -9.5282,  -9.0327,  -9.8599,  ...,  -9.7827,  -9.3700,  -9.6765],\n",
      "         [ -9.0695,  -9.2774,  -9.9291,  ...,  -9.1541,  -8.9234, -11.1183],\n",
      "         ...,\n",
      "         [ -8.6674,  -9.5500, -10.1255,  ...,  -9.5220,  -9.9859,  -8.9361],\n",
      "         [ -9.1713,  -8.9919,  -9.6872,  ...,  -8.5003, -10.1107,  -9.4119],\n",
      "         [ -8.5888,  -9.3944,  -9.2784,  ...,  -9.2408, -10.0670, -10.1463]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.9078,  -8.2161, -10.0598,  ..., -11.0854, -10.2082,  -9.4742],\n",
      "         [ -8.7681,  -9.5806,  -9.7729,  ...,  -9.7281,  -9.9289,  -8.2631],\n",
      "         [ -8.3929,  -8.6181, -10.1788,  ...,  -8.9606,  -9.4242,  -9.0499],\n",
      "         ...,\n",
      "         [ -8.7938,  -9.4769,  -9.7973,  ...,  -9.4232,  -9.5449,  -8.6949],\n",
      "         [ -8.1440,  -9.4384,  -8.7011,  ...,  -8.9930,  -9.1864,  -9.2282],\n",
      "         [ -8.3730,  -9.7035,  -9.7925,  ...,  -9.1719,  -9.4404,  -9.4414]],\n",
      "\n",
      "        [[ -8.2156,  -9.2227,  -9.6807,  ...,  -9.9729,  -9.7100,  -8.5938],\n",
      "         [ -8.2096,  -8.7817,  -8.9692,  ...,  -9.6961,  -9.0621,  -8.9953],\n",
      "         [ -8.0209,  -8.7393,  -9.7731,  ...,  -9.8140,  -9.0888,  -9.0182],\n",
      "         ...,\n",
      "         [ -7.9171,  -9.4914,  -9.7735,  ...,  -9.2190,  -8.6942,  -9.3169],\n",
      "         [ -8.6921,  -9.6388,  -9.9988,  ...,  -9.9193,  -9.2520,  -8.7187],\n",
      "         [ -8.5639,  -9.4904, -10.0602,  ...,  -9.2734, -10.0068,  -9.5281]],\n",
      "\n",
      "        [[ -8.2807,  -9.7553, -10.1349,  ..., -10.1657,  -8.9649,  -9.5851],\n",
      "         [ -9.1846,  -9.4756,  -9.8858,  ..., -10.0819,  -9.8415,  -8.9693],\n",
      "         [ -8.9879, -10.4360,  -9.8019,  ...,  -9.9190,  -9.9360,  -9.2536],\n",
      "         ...,\n",
      "         [ -8.1370,  -9.6640,  -9.4988,  ...,  -8.8784,  -9.2768,  -8.6967],\n",
      "         [ -9.4293, -10.1567,  -9.7566,  ...,  -9.0903,  -9.6813,  -8.5163],\n",
      "         [ -8.7441,  -9.6565, -10.2435,  ...,  -9.3510,  -8.9824,  -8.4298]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The reason why the output is a 3D tensor rather than a 2D one is because the model is processing multiple sequences at once (the batch size is greater than 1) and it’s predicting a probability distribution over all possible words for each position in each sequence.\n",
    "# So, for each sequence in the batch (8 sequences), and for each position in each sequence (256 positions), you have a vector of length 10000 (the size of your vocabulary) representing the probability distribution over all possible next words.\n",
    "\n",
    "dec_output = decoder(input_sequence, self_attention_mask, enc_output, padding_mask)\n",
    "print(dec_output.shape)\n",
    "print(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa4b5a5-409b-45bf-95a6-2e4dad5472ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='textattack/distilbert-base-uncased-SST-2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08ef05c-f507-4470-b3be-e43b0dcd2324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a4c8829-7de1-457b-8106-1d4f2204f0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 2190, 3185, 1045, 1005, 2310, 2412, 3427,  999,  102,    0],\n",
       "        [ 101, 2054, 2019, 9643, 3185, 1012, 1045, 9038, 3666, 2009, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037fe889-4fdb-460e-a474-a413ebdb40fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03e4513-4aca-4f7e-88cd-0b5debfc8f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809b5ebc-e84b-4947-bc91-05e3f343eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5a294c-f96d-416e-b1b0-5ede64da8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for \"The best movie I've ever watched!\": 1\n",
      "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
     ]
    }
   ],
   "source": [
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b41652f3-395f-4e35-9188-4253114fb1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_sents', 'summaries'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face's dataset hub\n",
    "dataset = load_dataset('opinosis', trust_remote_code=True)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1af4af29-1973-4998-8729-05d4cb433fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of instances: {len(dataset['train'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcbc46a3-f0a1-49fc-be25-756cc67c2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['review_sents', 'summaries']\n"
     ]
    }
   ],
   "source": [
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d85c21f6-5636-4f8e-b78d-972a85b9fbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_sents': \"I bought the 8, gig Ipod Nano that has the built, in video camera .\\r\\n  Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod .\\r\\nI have lots of music cd's and dvd's, so currently I'm just interested in storing some of my music and videos on the ipod so I can enjoy them on my vacation, and while at work .\\r\\nThere's a right way and wrong way to store music and videos onto the ipod .\\r\\n Audio and video recording are a step above the competition .\\r\\n As always, the video screen is sharp and bright .\\r\\nipod nano even better with video camera !\\r\\n This time around, Apple is branching out of its iPod formula in a small, but not insignificant way by gracing the back of the Nano with its own video camera .\\r\\n Still, video fans should consider stepping up to a product with a larger screen, such as the Zune HD   or iPod Touch .\\r\\nThe user interface of the 5G Nano remains almost entirely unchanged with the exception of a few new menu items for the video camera, FM radio, and fitness features .\\r\\nI can listen to it without headphones, i can record video .\\r\\n It looks nicer than my other one, love the video and everything else this one can do\\r\\n wish BB could engrave them too  announcer for songs is kind of annoying  Haven't yet tried the video, but very pleased with the size and quality of my music on this nano .\\r\\n Hard to delete videos from the Nano device .\\r\\n The video design allow you to take different types of vidoe like x, ray, monochrome, and twist making it enjoyable to play .\\r\\n The video quality is actually fairly good, my friend toook a video with it and she uploaded it to facebook and it came out great !\\r\\n It eve ncame out better than a video she took with an actual camera camera !\\r\\n the video quality is perfect .\\r\\nGreat for music, just OK for video\\r\\n Video recorder only, cannot take just photos .\\r\\n the VIDEO, VOICE and PEDOMETER recorder is one great addition to it .\\r\\n As for the video camera it's a nice addition considering it's an mp3 player first and foremost, I think people are going to go into this and think maybe I can replace my old video, NO .\\r\\n It just a solid low quality video camera some of the lighting or your fingers may get in the way of the actual recording but once you get use to it, it can become fun .\\r\\n I find it weird that it will take video but not still pics .\\r\\n It looks just like the 4th generation, except on the back, there's a camera to take video clips or whatever it is you want to .\\r\\n i hate itunes, but it seems to be a necessary evil in order to tag say a video as music and not a movie, so though i originally started with 3rd party apps for syncing i have since switched to itunes .\\r\\nThis time around, Apple is branching out of its iPod formula in a small, but not insignificant way by gracing the back of the Nano with its own video camera .\\r\\n The video works very well for such a small sensor and it is so easy and convenient to use .\\r\\n  We have had portable video players for our children as our cars are not equipped with these devices .\\r\\n Video camera quality and sound are also surprising for such a small unit .\\r\\n Not a tremendous surprise here, as we believed the sixth, generation  true video iPod  would not be announced until later in the year or even early in 2007 .\\r\\n5G lies a more mature iPod, many steps wiser and more able than its one, year, old  The iPod gains many incremental improvements, including a brighter screen and better video battery life, but probably the most appealing aspect is the tantalizing price points of $249 for the 30GB version and $349 for the huge 80GB version   .\\r\\n The 80GB is enticing for video addicts both for the capacity iTunes movies are about 1 .\\r\\n The video camera is nice, just wish it could take stills .\\r\\n The one problem with the video camera is it is not the easiest thing to use .\\r\\n I also like that videos can be watched on it as well as listening to it without the ear phones that is a really great feature .\\r\\n The only odd thing is that is has a video camera but doesn't have a single shot camera .\\r\\n Easy to use, Video Camera, Radio, Size, Sound Quality  .\\r\\n The battery doesn't last a long time especially when you're recording or watching a video but I just listen to music most of the time and it lasts me a good length of time doing that .\\r\\n The video quality is great and has different settings .\\r\\n Definitely easier to use than the IPod Touch, and the video camera feature is awesome .\\r\\n That way you will have enough storage to add music, video and pictures !\\r\\n I haven't found anything that is not so great  This is a multi use MP3 player with the ability to make videos, music and videos .\\r\\n Great new features in this iPod generation include voice recording, listening to radio, and even filming video .\\r\\n Has video camera and voice memo .\\r\\n The video looks great and it's easy to use .\\r\\n No  off  switch  Compared to Gen 4 the ability to listen without speakers and the video camera is great .\\r\\n As easy to use as any other iPod, and the video is good .\\r\\n She loves to make her own videos and play them back .\\r\\n The videos are so clear I just love it .\\r\\n However, the way it is designed, it is clumsy to hold while recording a video .\\r\\n camera does only video, not pictures  Way better than 2nd gen nano .\\r\\n Love the camera, but wish it had photo capabilities, rather than just video .\\r\\n It's got radio, video, camera, video camera, small  .\\r\\n Love the pedometer feature on the nano and the video camea  .\\r\\n The color, the video, overall the item is great  .\\r\\n The size is very nice and compact and the videos are beautiful !\\r\\n it would be nice to delete songs when they get old  the video camera is the best, i use it all the time .\\r\\n It was fine quality sounds for  roof, raising  music and also insane levels of clearity for music videos or movies on the go !\\r\\n The video camera feature is awesome enough, but all of the special effects are incredible .\\r\\n He loves shooting video and then viewing it in  cyborg  mode or in sepia tones .\\r\\nOur 12 year old loves the IPod ,  especially the video camera feature .\\r\\n lots of features  I got this for my son because he wanted an MP3 player and a video camera for Christmas .\\r\\n Like the extra features ,  pedometer, video camera, radio, speakers .\\r\\nhaven't quite mastered the video camera yet, but I'm okay with that !\\r\\n Movies, videos, music, music, music .\\r\\n Plenty of memory to add all your favorte music and videos .\\r\\n The 16g holds a lot of music and video .\\r\\nvideo, motion sensing, screen size and sllek look  I recently upgraded from the 2nd generation Nano and love the new features of the newest model .\\r\\n Can't believe the picture quality and ease of video use !\\r\\n It will take video, but not pictures  ?\\r\\n Secondly, the thing has the capability to record video, but won't take pictures .\\r\\n Yeah, that's typed correctly ,  video, but not pictures .\\r\\n Seems kinda silly that they would go right to video and completely neglect photography .\\r\\n My daughter loves taking video, as well as the music & pics !\\r\\n She takes videos and pics with it all the time .\\r\\n I decided to get this iPod cause of the Memory and the Video Cam in the back .\\r\\n The video camera and the graphics is so cool !\\r\\n Easy to use and cool video camera !\\r\\n Comes in different colors, make videos, and holds alot of music .\\r\\n Small, holds lots of songs, video, easy to use  .\\r\\n Can't figure out how to review the video clips I record .\\r\\n  This is an easy to use item, but is slightly confusing to use when trying to review video clips .\\r\\n It takes video and has a really cool radio feature on it, according to the kids .\\r\\n easy to use, cool helpful features, video camera, voice over  .\\r\\n The video camera is great   and the different effects are fun to mess around with .\\r\\n It would be nice if the video camera zoomed in a little and if there was a regular camera as well, but those are pretty minor complaints .\\r\\n  My daughter thinks that the video clarity is amazing and finds the built in speakers awesome .\\r\\n The camera and video feature are wonderful .\\r\\n Holds tons of music, video, pics .\\r\\n I wish the video camera had a pause on it, but you can't pause .\\r\\n It can hold tons of music, video, pics and watching movies on the plane is nice .\\r\\n I also wish the video camera had a pause on it, but it doesn't pause and starts with a new video everytime you stop it .\\r\\n She didn't want the touch, she wanted the nano because you can video, it has the radio that she can play back the songs and she likes the size .\\r\\n The video camera is great and the screen size is a nice size\\r\\n We haven't tried the video yet, so I can't comment on that .\\r\\n great video quality, no ear buds required internal sound system sounds great  .\\r\\n Still to new to we'll see  Great product, video feature is awesome and I love how there is now internal sound and you don't have to use ear buds with it .\\r\\n Don't really need video camera  eventhough we don't use the video that much the rest of the features are great .\\r\\n It does so many things,  Wow, nav, video, mp3, and more !\\r\\n I love how easy it is to download from ITunes and the fact I can also do video  .\\r\\n I am learning to navigate the menu better that at first and love that fact that I can take a video .\\r\\n Its easy to use and love the new video feature .\\r\\n It does have some nice features such as the video camera and the radio .\\r\\n He has been using it consistently ever since and has nothing but praise for it and it's many options  loves the video cam and the music player recorder especially\\r\\n Watching any videos drains the battery very quickly .\\r\\n Glad you listen to the radio, play games and take video  .\\r\\n none of my old Nano asseccories work with this one  Fun new Ipod, the video feature is fun but difficult to learn how to use .\\r\\n I've downloaded a lot of CDs and music from iTunes, also a few music videos,which are great fun to watch and hear .\\r\\n Great for spot video if you do not have a camera availabe on hand .\\r\\n some changes such as video camera and bigger screean, but it was just ok .\\r\\nGreat video camera and the funcionality is awesome\\r\\n Video camera function and the blending of songs at the end and beginning  .\\r\\n nothing  The functionality and video camera were great innovations added to the newest generation of Ipod !\\r\\n It's very easy to use and its video camera is very nice and it doesn't ghost .\\r\\n and the video quality is very good, and the sound quality is very good as well .\\r\\n Sound quality, video quality, easy to use, space, customizable  .\\r\\n The colors are awesome, the sound is great and it hold tons of songs and videos !\\r\\n It has great video quality for such a small device .\\r\\n the video recorder, sound, look  .\\r\\n has bigger screen, video camera  .\\r\\n the picture for the video is awesome its very clear and easy to see .\\r\\n features, look and feel, video  .\\r\\n Video, even in low light, is impressive .\\r\\n Even the built, in video camera is very good .\\r\\n I'm really disappointed that Apple is so proprietary that you can't use anything but iTunes to add music and videos to an iPod .\\r\\n It has a built in video camera, and it shoots in high quality, which I was surprised at .\\r\\n n you can put music videos pictures .\\r\\n The video camera is cool, but I don't know that I would use it much .\\r\\n This can record videos and it is also Very Very Light Weight !\\r\\n i couldnt think of a single thing  this ipod is reallly great you can shoot videos listen to the radio plus you have 16 gigs of memory it is aweosme i really love it because its still as small as the last one only with a bigger screen and great color\\r\\n The video and external speaker components were a surprise on the 5th Gen iPod .\\r\\n I love the video camera it has on it, the pedometor is awesome to use and best of all I can listen to music on it when I'm doing other stuff on my computer when it might be slowed down when listening to music on it .\\r\\n Awesome features fm radio, video camara, pedometer and more  .\\r\\n Nothing  What will Apple think of next for their Nano, the video camara records awesome video even in dim lighting .\\r\\n All the neat little features and surprisingly good video camera !\\r\\n I took it to class and decided to test it out and everyone was surprised by how good the video quality was .\\r\\n I'm planning on hiking half, dome next year and this is much much more lightweight than the video camera I have so it will definitely be accompanying me !\\r\\n fm radio lots of storage for music, videos and pictures alarm clock to your tunes blue color lock button so you don't accidentally turn it off on ease of use Cons :\\r\\n Hard to get videos onto computer  This was a purchase I made after I returned my Sony Walkman .\\r\\n A must, buy for everybody who loves music and video .\\r\\n video camera built in and music plays without head phones  .\\r\\n need a video program on your computer to obtain video  Great product for the price .\\r\\n I had the iPod Video but it did not work with my new Nike\\r\\n chip so I bought the newest iPod video the day it came out .\\r\\n I cannot figure out how to get the video I record off of the IPod .\\r\\n ,  I cannot figure out how to get the video I recorded on my Nano off to upload it to Facebook or You Tube .\\r\\nshoes if using vid cam for long time, it kills battery  Standard Def video recording, but still sharp !\\r\\n Only things I wish they added is measuring distance [not just steps] on the pedometer, better sound on the speaker, and a  capturing still images  with the video recording feature .\\r\\n The video recording feature is the best part .\\r\\n7 out of 10 have not used video yet but the pixels would be just the same .\\r\\n The product is very useful if you want to go out for a jog and listen to your music or watch some videos if you stuck on a bus or subway .\\r\\n Video camera, FM turner, Speaker, Pedometer  .\\r\\n Location of the video camera, and the gap between the click wheel and ipod  The Best Ipod ever and has the best features ever .\\r\\n Easy to use video camera, Big screen, Fun color  .\\r\\n Which I am sure all Nano's are like that  LOL The video camera is so much fun !\\r\\n new look, shape and video cam feature  .\\r\\n Smudgy finger prints  I like the Video voice over color, purple I like hearing the Ipod w out the ear plugs .\\r\\n light weight, radio, video camera  .\\r\\n I have an old 60GB ipod video that I use in my car and now I use this ipod for walking around campus and such .\\r\\n The video camera is a great feature to have, however its not that high of a quality when you transfer it onto your computer .\\r\\n Camera has great video effects   .\\r\\n The videos you record do not show up on iTunes though, you have to go to My Computer and find the ipod drive, then open the folder named DCIM, and the video files are in  .\\r\\n When you're not in a game or watching a video, if you turn it on the side, coverflow displays, however you can turn this feature off .\\r\\n I mean I do have my cell that takes pics and videos but I just love the picture video the NANO records .\\r\\n And I just foundout you don't need the head phones in the iPod to listen to your music or videos !\\r\\n Video Camera with fun special effects, FM radio  .\\r\\n the glossy fingerprint attracting finish compared to last year's beautiful matte finish  Great improvements with the video camera, really fun special effects, easy to use ,  also great FM radio with pause feature .\\r\\nthen it would be perfect oh and make a 32 or 64gb version so I can fit all my music and have lots of room for video ,  thanks !\\r\\n But overall I love it ,  between the radio, speaker, microphone, video camera, effects, there are enough new features to justify buying a new nano .\\r\\nIt's great to have a decent video camera with you at all times if you don't have an iphone .\\r\\n Had the original iTouch but I was burglarized andwas waiting for the iTouch with video but can't wait any longer .\\r\\n Video is impressive, speaker sound is good for size, easy menus, screen is big enough to watch movies .\\r\\n The videos are very crisp and clean, the video camera is nice, I was very surprised with the quality .\\r\\n Small size and Color, Gettin your Money's worth, nice screen, and video camera  .\\r\\n I do like the video camera on it .\\r\\n The mic and camera are great, speaker is great for video playback or bedtime movie watching, the screen is now even bigger and the device is the same size but Lighter than the 4th gen nano !\\r\\n The Video Recording Quality, Price, Size, Weight, Radio, Pedometer, Voice Over  .\\r\\n video camera, radio, great price for a 16 gig, not a fingerprint magnet, small .\\r\\n cant take pictures just videos, and u need the headphones given for the radio  good for just listening to music and for your car .\\r\\n Easy to set up, has a video camera and a radio built in .\\r\\n The video camera is not as good as I thought it was going to be .\\r\\n NOTHING  Ipod nano has a VIDEO CAMERA it is so cool, ROCK\\r\\n the video is very clear, the built in speaker is pretty good, the radio is very useful and overall it's just great !\\r\\n He was delighted as it had a video and camera which his original one did not have .\\r\\n It has plenty of room for her music and she loves taking pictures and making short videos with it .\\r\\n While the Nano allows for video and audio, the IPod touch has interactivity and\\r\\n While the Nano allows for video and audio, the IPod touch has interactivity and\\r\\nits really good up to apples usual standard sound quality and video quality is brillant and is ideal for anyone !\\r\\nThe video camera feature is nice but I don't understand why they didn't allow for picture taking ?\\r\\n It's small and lightweight, it looks amazing, the video quality is way better than I ever imagined it would be, and the screen is huge .\\r\\nThe seller advertised it as the NEWEST model which should be able to take pictures and video .\\r\\nIt is of the best buys I've done, sound quality, video and radio is excellent\\r\\n Great sound quality, love the video recording too  :\\r\\n It's just a shame it only takes video and not still photos .\\r\\n It has a cool video recorder, but i could've done without it .\\r\\n The video camera works great and the battery\\r\\n If you had other ipods with the feauture of puting videos, when you synch them, they don't appear\\r\\n Video option is easy to use .\\r\\n I have the 5th gen iPod video, and this thing blows it away !\\r\\nThe video is surprisingly good !\\r\\n I do wish that it also took pictures as well as video though .\\r\\nSo, the only thing about the iPod nano that I don't like   is the placement of the video camera .\\r\\n Video quality is better than I expected .\\r\\nAmazing amount of music photos videos can be stored .\\r\\n Video taking feature is interesting .\\r\\n It plays music, videos, radio, records\\r\\n The video capability is incredible, and so easy to use .\\r\\nMy wife and I are really pleased with our Nano Ipods that we purchased from you so easy to use and good to have a camera as well as a video John Coleman\\r\\n It's a nice little gadget, I've put some songs on it and the video camera is good\\r\\n The video camera is crystal clear and the memory space is more than ideal for all your needs .\\r\\nshe was quickly able to download music and videos xmas am\\r\\n I have used the video camera nd the sound is good considering and the picture is\\r\\n FM radio and a   video camera are pretty darn cool .\\r\\nGreat little audio video device\\r\\n\",\n",
       " 'summaries': ['Video camera is great.\\r\\nVery easy to use.\\r\\nVideo quality is excellent.',\n",
       "  'The video screen is very bright.',\n",
       "  'The video for Ipod nano 8GB is a great addition, crystal screen with great quality for photos, video and music.',\n",
       "  'The new Ipod nano which comes with a video camera offers decent performance with pictures and video along with the great music.',\n",
       "  'The iPod takes and displays quality video.\\r\\nIt does not take single still pictures and this is a disappointment but not a deal breaker.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "404d5b6c-d2dc-400a-92f9-298af290b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][-2]['review_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac8ac89b-0714-4100-8875-fec54785af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alienware\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14010bce-ddff-4d7b-986c-b095399d5b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21603,    10,    27,  2944,     8,  9478, 10754,    27, 11410, 20556,\n",
       "            24,    65,     8,  1192,     6,    16,   671,  1861,     3,     5,\n",
       "            94,   444,     7,    65,    46,    30,     6,   689,  1078,     6,\n",
       "           213,    25,   164,  1242,    11,   946,   723,    11,  3075,    84,\n",
       "            56,  2438,  2400,     8,     3,    23, 11410,     3,     5,    27,\n",
       "            43,  1995,    13,   723,     3,    75,    26,    31,     7,    11,\n",
       "         30114,    31,     7,     6,    78,  1083,    27,    31,    51,   131,\n",
       "          1638,    16,     3, 17445,   128,    13,    82,   723,    11,  3075,\n",
       "            30,     8,     3,    23, 11410,    78,    27,    54,   777,   135,\n",
       "            30,    82,  4257,     6,    11,   298,    44,   161,     3,     5,\n",
       "           290,    31,     7,     3,     9,   269,   194,    11,  1786,   194,\n",
       "            12,  1078,   723,    11,  3075,  2400,     8,     3,    23, 11410,\n",
       "             3,     5,  9607,    11,   671,  5592,    33,     3,     9,  1147,\n",
       "           756,     8,  2259,     3,     5,   282,   373,     6,     8,   671,\n",
       "          1641,    19,  4816,    11,  2756,     3,     5,     3,    23, 11410,\n",
       "         13944,   237,   394,    28,   671,  1861,     3,    55,   100,    97,\n",
       "           300,     6,  2184,    19,  6421,    53,    91,    13,   165, 17003,\n",
       "          5403,    16,     3,     9,   422,     6,    68,    59,    16, 26251,\n",
       "           194,    57,     3,  3484,    75,    53,     8,   223,    13,     8,\n",
       "         20556,    28,   165,   293,   671,  1861,     3,     5,  4886,     6,\n",
       "           671,  2675,   225,  1099,     3, 19115,    95,    12,     3,     9,\n",
       "           556,    28,     3,     9,  2186,  1641,     6,   224,    38,     8,\n",
       "          1027,   444,  3726,    42, 17003, 11031,     3,     5,    37,  1139,\n",
       "          3459,    13,     8,   305,   517, 20556,  3048,   966,  4585, 26164,\n",
       "            28,     8,  5763,    13,     3,     9,   360,   126,  2285,  1173,\n",
       "            21,     8,   671,  1861,     6, 13409,  2252,     6,    11,  4639,\n",
       "           753,     3,     5,    27,    54,  3011,    12,    34,   406, 21065,\n",
       "             6,     3,    23,    54,  1368,   671,     3,     5,    94,  1416,\n",
       "          1245,    52,   145,    82,   119,    80,     6,   333,     8,   671,\n",
       "            11,   762,  1307,    48,    80,    54,   103,  1663,     3,  7640,\n",
       "           228,     3,    35, 15299,    15,   135,   396,  6456,    52,    21,\n",
       "          3605,    19,   773,    13, 16241, 19586,    31,    17,   780,  1971,\n",
       "             8,   671,     6,    68,   182,  5010,    28,     8,   812,    11,\n",
       "           463,    13,    82,   723,    30,    48, 13944,     3,     5,  6424,\n",
       "            12,  9268,  3075,    45,     8, 20556,  1407,     3,     5,    37,\n",
       "           671,   408,   995,    25,    12,   240,   315,  1308,    13,     3,\n",
       "          6961,    32,    15,   114,     3,   226,     6,     3,  2866,     6,\n",
       "          7414, 10363,   526,     6,    11,  9135,   492,    34,  9231,    12,\n",
       "           577,     3,     5,    37,   671,   463,    19,   700,  5163,   207,\n",
       "             6,    82,  1565,   396,  1825,     3,     9,   671,    28,    34,\n",
       "            11,   255, 14686,    34,    12, 13301,    11,    34,   764,    91,\n",
       "           248,     3,    55,    94,     3,    15,   162,     3,    29,  6527,\n",
       "            15,    91,   394,   145,     3,     9,   671,   255,   808,    28,\n",
       "            46,  1805,  1861,  1861,     3,    55,     8,   671,   463,    19,\n",
       "           626,     3,     5,  1651,    21,   723,     6,   131,  6902,    21,\n",
       "           671,  3953,  1368,    49,   163,     6,  1178,   240,   131,  1302,\n",
       "             3,     5,     8,   584, 13162,   667,     6,     3,  8040,  8906,\n",
       "            11, 11012, 27415,   427,  5946,  1368,    49,    19,    80,   248,\n",
       "           811,    12,    34,     3,     5,   282,    21,     8,   671,  1861,\n",
       "            34,    31,     7,     3,     9,  1245,   811,  4014,    34,    31,\n",
       "             7,    46,     3,  1167,   519,  1959,   166,    11, 19839,     6,\n",
       "            27,     1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c225539-d3a1-423f-af36-98eac35410f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,    27,  2944,     8,  9478, 10754,    27, 11410, 20556,    24,\n",
       "           65,     8,  1192,     6,    16,   671,  1861,     3,     5,    94,\n",
       "          444,     7,    65,    46,    30,     6,   689,  1078,     6,   213,\n",
       "           25,   164,  1242,    11,   946,   723,    11,  3075,    84,    56,\n",
       "         2438,  2400,     8,     3,    23, 11410,     3,     5,     1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2fa7054-04bc-4d68-a154-8146a9cd133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text (first 400 characters): \n",
      " I bought the 8, gig Ipod Nano that has the built, in video camera .\n",
      "  Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod .\n",
      "I have lots of music cd's and dvd's, so currently I'm just interested in storing some of my music and videos on the ipod so I can enjoy them on my vacation, and while at work .\n",
      "There's a right way and wrong wa\n",
      "\n",
      "Generated Summary: \n",
      " I bought the 8, gig Ipod Nano that has the built, in video camera. Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod.\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a59b5825-25c6-43a0-bc14-e99996465bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd820626-ebb8-45c8-a330-c77d65785125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_input Hello\n",
      "input_ids tensor([[3923,    0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated_ids tensor([[65000,  2119,     3,     0]])\n",
      "English: Hello | Spanish: Hola.\n",
      "english_input Thank you\n",
      "input_ids tensor([[1825,   40,    0]])\n",
      "translated_ids tensor([[65000,  1124,     3,     0]])\n",
      "English: Thank you | Spanish: Gracias.\n",
      "english_input How are you?\n",
      "input_ids tensor([[594,  53,  40,  21,   0]])\n",
      "translated_ids tensor([[65000,    50,  1102,  1221,    21,     0]])\n",
      "English: How are you? | Spanish: ¿Cómo estás?\n",
      "english_input Sorry\n",
      "input_ids tensor([[5099,    0]])\n",
      "translated_ids tensor([[65000,   350,  1669,     3,     0]])\n",
      "English: Sorry | Spanish: Lo siento.\n",
      "english_input Goodbye\n",
      "input_ids tensor([[22191,     0]])\n",
      "translated_ids tensor([[65000,  8631,     3,     0]])\n",
      "English: Goodbye | Spanish: Adiós.\n"
     ]
    }
   ],
   "source": [
    "# The reason why the input and translated IDs vectors have more elements than the corresponding word inputs is due to the way the tokenizer works.\n",
    "# In your code, the tokenizer.encode function is used to convert the input text into a sequence of IDs, which represent the tokens in the text. These tokens can be individual words, but they can also be smaller units depending on the tokenizer. For example, a word might be split into multiple subwords, each with its own ID.\n",
    "# Additionally, special tokens are often added to the sequence. For instance, a common practice is to add a special token at the beginning and end of the sequence. In your case, the 0 at the end of each input_ids and translated_ids tensor is likely a special token, such as an end-of-sequence token\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    print('english_input', english_input)\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
    "    print('input_ids', input_ids)\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    print('translated_ids', translated_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a180b31-33ec-4984-9b7c-e9cdd955d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11590\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "mlqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "671edd08-a89d-4458-87a9-d983f359f9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Who analyzed the biopsies?\n",
      "Context:  In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"\n"
     ]
    }
   ],
   "source": [
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fea420d-7e1f-4798-831e-3f013e593b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='deepset/minilm-uncased-squad2', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89a96dda-066d-4279-adf9-857af82a91b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2040, 16578,  1996, 16012,  4523,  3111,  1029,   102,  1999,\n",
       "          2807,  1010,  2274, 13294,  6831, 16728,  1998,  1996, 24835,  1997,\n",
       "         16728,  4787, 10556, 17112,  2050,  1998,  2728, 10097, 12923,  1996,\n",
       "         18531,  1998,  1996,  2142,  2163,  4483,  3860,  4034,  1012,  2037,\n",
       "          4848,  1010,  1999,  2029,  2027,  2020,  3421,  2011,  2577,  2899,\n",
       "          2118,  2375,  2934,  5655, 10722, 12866,  1010,  6884,  2027,  2018,\n",
       "          2042,  2556,  2043,  2312, 12450,  1997,  4242, 12141,  2018,  2042,\n",
       "          5296,  1999,  2330, 14496,  1998, 19874,  2012, 18087,  1012, 16012,\n",
       "          4523,  3111,  2579,  2013,  1996, 17612, 11390,  2020, 16578,  2011,\n",
       "         18607,  2118, 16012, 24229,  2015,  1010,  2040,  2179,  2152,  3798,\n",
       "          1997,  4487, 11636,  2378,  1010,  4487, 10609,  6844, 27942,  2319,\n",
       "          1010,  1998, 13012,  2818, 10626,  8913, 16921, 11474,  1999,  2037,\n",
       "          2303,  6638,  1012,  1996, 17612, 11390,  6884,  2027,  2018,  8760,\n",
       "          3096,  1010, 11290,  1010,  1998, 16464,  6441,  2349,  2000,  2037,\n",
       "          2147,  2012, 18087,  1010,  1998,  2008,  2023,  2018,  5201,  2000,\n",
       "          1996,  6677,  1997, 10097,  1998, 10556, 17112,  2050,  1012,  1996,\n",
       "          4848,  4912,  9430,  2005,  1996,  6441,  2027,  2018,  8760,  1010,\n",
       "          6815,  1996, 18531,  2018, 17800,  8971, 11704,  4475,  1010,  1998,\n",
       "          2008,  1996, 19044,  2018,  3478,  1999,  2049,  4611,  2000, 16306,\n",
       "          1996,  7692,  5680,  1998,  7233,  2552,  1006,  2029, 21208,  2015,\n",
       "          8304,  1997,  4795,  4475,  1007,  1012,  2027,  2036,  4912,  6851,\n",
       "          2592,  2055,  1996, 12141,  2000,  2029,  2027,  2020,  9382,  6086,\n",
       "          1010,  5327,  2023,  2052, 10956,  1996,  2966,  3949,  1997,  8643,\n",
       "          1012, 12295,  3389,  1044,  1012,  5226,  1010,  2280,  3472,  1997,\n",
       "          1996,  2160,  4454,  2837,  1010,  2409,  3438,  2781,  6398, 23920,\n",
       "          2358, 28083,  1010,  1000,  1996,  2250,  2486,  2003, 26268,  2075,\n",
       "          2035,  2592,  2055,  2181,  4868,  1999,  2344,  2000,  4047,  3209,\n",
       "          2013,  1037,  9870,  1012,  1000,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "174dcdce-caea-4b31-9139-4dc4dc0485fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alienware\\.cache\\huggingface\\hub\\models--deepset--minilm-uncased-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=384, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2dd781f-c2c5-4388-ac12-1e3511634879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 0.4331, -5.9864, -5.7614, -6.2329, -5.9369, -6.4554, -6.6515, -6.6686,\n",
       "          0.4331, -5.9137, -6.0544, -6.6783, -5.9388, -6.2935, -6.2757, -6.5190,\n",
       "         -6.6694, -6.2961, -6.3753, -6.5280, -6.1553, -5.4943, -5.9384, -6.5034,\n",
       "         -6.7191, -6.6250, -6.2206, -6.6423, -6.4067, -6.1051, -5.9176, -6.6871,\n",
       "         -6.3237, -5.6972, -6.5879, -6.0408, -6.6248, -6.8032, -6.8567, -6.0126,\n",
       "         -6.4246, -6.8355, -6.3049, -6.4013, -6.3018, -6.4767, -6.4598, -6.4260,\n",
       "         -5.5132, -6.2999, -6.6227, -6.1841, -6.3011, -5.6165, -6.0477, -6.8492,\n",
       "         -6.7725, -6.1979, -6.2060, -6.3269, -6.3990, -6.4921, -6.2939, -5.9060,\n",
       "         -6.4909, -6.4594, -6.2571, -6.5393, -6.5011, -6.4583, -6.4658, -6.3752,\n",
       "         -6.2070, -6.5106, -6.6355, -6.5982, -6.3821, -6.2631, -5.9546,  1.1200,\n",
       "         -4.9416, -4.9593, -5.7370, -6.0798, -5.8981, -5.1775, -6.2021, -3.7974,\n",
       "         -0.7555, -0.8330,  7.0906, -1.6840, -0.1442, -4.6069, -0.1813, -4.0197,\n",
       "         -5.4498, -5.8540, -3.7755, -6.2736, -6.4684, -4.3514, -6.4279, -6.8485,\n",
       "         -6.6440, -5.7422, -6.3996, -6.4445, -6.5249, -6.9212, -6.8794, -6.6766,\n",
       "         -5.5149, -6.4146, -6.4113, -6.4482, -6.5586, -6.8798, -6.3597, -6.2457,\n",
       "         -6.0640, -6.4219, -6.1355, -5.8297, -5.8958, -6.9695, -6.2983, -6.2185,\n",
       "         -6.2785, -6.2933, -6.2346, -6.5628, -6.4667, -6.8488, -6.6590, -6.3942,\n",
       "         -6.7781, -6.3991, -6.4851, -6.3615, -6.4829, -6.4598, -6.7390, -7.1202,\n",
       "         -6.6077, -6.3351, -6.2694, -6.3993, -6.3904, -6.4936, -6.3733, -6.4900,\n",
       "         -6.5810, -6.2266, -6.6761, -6.4495, -6.7184, -7.0666, -7.0005, -6.1441,\n",
       "         -6.4744, -6.2638, -6.3753, -6.3944, -6.3906, -6.4805, -6.5536, -6.5291,\n",
       "         -6.7597, -6.8259, -6.1555, -6.0330, -6.1905, -6.4132, -6.2120, -6.4482,\n",
       "         -6.2310, -6.8354, -6.9364, -6.6217, -6.2513, -6.0758, -6.2514, -6.5031,\n",
       "         -6.3979, -6.5199, -6.4956, -6.6209, -6.5276, -6.4888, -6.3958, -6.0616,\n",
       "         -6.5949, -6.5894, -6.5921, -6.8524, -6.5291, -6.3920, -6.3980, -6.5983,\n",
       "         -6.4532, -6.5348, -6.3558, -6.8128, -6.9180, -6.8827, -6.3121, -6.4817,\n",
       "         -6.2493, -5.9764, -6.4976, -6.3953, -6.3256, -6.4906, -6.4465, -6.4609,\n",
       "         -6.4806, -6.5503, -6.4876, -6.6933, -6.8918, -6.2526, -6.2927, -6.2854,\n",
       "         -6.1506, -6.3988, -6.2176, -6.5855, -6.4965, -6.6835, -6.9252, -6.0704,\n",
       "         -6.0203, -6.5563, -6.7706, -6.6958, -6.8083, -6.1754, -6.4119, -6.5912,\n",
       "         -6.3655, -6.2998, -6.5022, -6.9206, -6.8900, -6.4828, -6.0671, -6.7771,\n",
       "         -6.4960, -6.1734, -6.4537, -6.8111, -6.6125, -6.2403, -5.8924, -5.8626,\n",
       "         -6.7102, -6.3734, -6.0460, -6.4791, -6.1938, -6.5017, -6.4295, -6.1088,\n",
       "         -6.5542, -6.3861, -6.4406, -6.4067, -6.3259, -6.6968, -6.4223, -6.4167,\n",
       "         -6.8009, -6.9173, -6.7943,  0.4330]]), end_logits=tensor([[ 0.4805, -6.1511, -6.3324, -6.3226, -6.5381, -6.2009, -5.6752, -5.8971,\n",
       "          0.4804, -6.6790, -5.9549, -5.9327, -6.1159, -6.4644, -6.3420, -5.7211,\n",
       "         -6.1893, -6.5015, -6.1449, -6.3618, -6.1765, -6.6545, -6.6317, -6.2868,\n",
       "         -5.8576, -6.1906, -6.4277, -4.9729, -6.0199, -6.4842, -5.5099, -6.1228,\n",
       "         -6.2080, -6.6433, -5.8441, -6.1135, -5.8164, -4.2625, -5.3590, -6.5712,\n",
       "         -5.9673, -5.5966, -6.5400, -6.4601, -6.4326, -6.4259, -6.3446, -6.3971,\n",
       "         -6.6974, -6.2597, -4.3284, -6.2369, -5.5712, -6.3332, -6.4360, -4.3804,\n",
       "         -5.8595, -6.3914, -6.4878, -6.5015, -6.4440, -6.2109, -6.4639, -6.7257,\n",
       "         -6.1811, -6.4323, -6.4820, -5.7685, -6.3307, -6.3938, -6.0110, -6.4603,\n",
       "         -6.4896, -5.9837, -6.1257, -5.7202, -6.3990, -4.7391, -4.8910, -4.3154,\n",
       "         -5.8585, -4.5947, -6.2359, -6.3387, -6.1300, -5.6936, -2.9754, -6.1367,\n",
       "         -3.8978, -5.3208,  0.8653,  0.3756, -2.9306, -1.8721,  7.0170,  3.3102,\n",
       "         -3.2118, -5.7196, -6.7085, -5.7125, -6.0071, -6.8873, -6.1844, -5.1290,\n",
       "         -5.8358, -6.6980, -6.2334, -6.2174, -6.1912, -5.3165, -5.4423, -6.0181,\n",
       "         -6.6688, -6.2117, -6.1966, -6.1872, -6.0934, -3.7489, -6.1414, -6.1919,\n",
       "         -5.7713, -2.6390, -1.1324, -6.6566, -6.6163, -5.0174, -6.2843, -6.4908,\n",
       "         -6.4870, -6.3928, -6.3008, -6.2548, -6.1340, -5.9223, -6.2677, -6.2133,\n",
       "         -5.3788, -6.3396, -6.4147, -6.4910, -6.1681, -6.3640, -5.0706, -4.9401,\n",
       "         -6.2255, -6.4634, -6.5102, -6.4342, -6.3848, -6.3730, -6.2851, -6.0310,\n",
       "         -6.2700, -6.3072, -6.1752, -6.3636, -6.1117, -5.1425, -4.9444, -6.5445,\n",
       "         -6.1229, -6.4021, -6.1090, -6.4514, -6.4648, -6.0639, -6.2786, -6.3346,\n",
       "         -5.8559, -5.7423, -6.4098, -6.5981, -6.0767, -6.3369, -6.3526, -6.1903,\n",
       "         -6.4182, -5.5438, -5.6172, -6.2287, -6.4939, -6.5879, -5.8938, -6.2659,\n",
       "         -6.2221, -6.3756, -6.3867, -5.9578, -6.3317, -6.1612, -6.4256, -6.6014,\n",
       "         -6.0839, -6.3287, -6.1436, -5.4781, -6.2903, -6.5255, -6.4874, -6.2616,\n",
       "         -6.3460, -6.4045, -6.4415, -5.8233, -5.5112, -5.5538, -6.4111, -6.4366,\n",
       "         -6.4288, -6.5506, -5.9945, -6.4371, -6.5594, -5.9184, -6.3909, -6.4334,\n",
       "         -6.3639, -6.3627, -6.3594, -5.7802, -5.6799, -6.4257, -6.5046, -6.4640,\n",
       "         -6.4818, -6.4332, -6.3280, -5.9804, -6.3509, -5.4072, -5.5296, -6.2916,\n",
       "         -6.4909, -6.1627, -5.9718, -5.5601, -5.8389, -6.5280, -6.2538, -6.2577,\n",
       "         -6.3034, -6.3593, -6.2032, -5.4848, -5.7312, -6.2558, -6.4351, -5.6336,\n",
       "         -5.9109, -6.4046, -6.2996, -5.5115, -6.0341, -6.1993, -6.6342, -6.6034,\n",
       "         -5.7019, -6.3130, -6.4584, -6.1487, -6.3693, -6.0379, -6.3754, -6.4349,\n",
       "         -4.7375, -6.3544, -6.3074, -6.4072, -6.3705, -6.0132, -6.4152, -6.4565,\n",
       "         -5.6942, -5.4910, -5.7620,  0.4804]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e0cf16e-e65e-4dbd-a9a7-626e7b1516b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(95))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eed2161c-bb10-4245-8a2c-62be2ed387b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18607,  2118, 16012, 24229,  2015])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "answer_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccc1c323-1b76-473a-81c0-69bd1a7ab90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  rutgers university biochemists\n"
     ]
    }
   ],
   "source": [
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23e8b999-8f13-47a1-8250-cab93c023205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alienware\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96cb9446-576e-4166-bc80-9853aefda005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./smaller_bert_finetuned\\runs\\Jun22_13-43-49_alfrizz,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=5,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=./smaller_bert_finetuned,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['mlflow'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./smaller_bert_finetuned,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8163c51c-1724-4570-b838-6b4093a61e46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MLFLOW_ENABLE_ASYNC_LOGGING' from 'mlflow.environment_variables' (C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\mlflow\\environment_variables.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set up trainer, assigning previously set up training arguments\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m trainer\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\trainer.py:541\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    539\u001b[0m default_callbacks \u001b[38;5;241m=\u001b[39m DEFAULT_CALLBACKS \u001b[38;5;241m+\u001b[39m get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mreport_to)\n\u001b[0;32m    540\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks \u001b[38;5;241m+\u001b[39m callbacks\n\u001b[1;32m--> 541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler \u001b[38;5;241m=\u001b[39m \u001b[43mCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdisable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\trainer_callback.py:296\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[1;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\trainer_callback.py:313\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[1;32m--> 313\u001b[0m     cb \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[0;32m    314\u001b[0m     cb_class \u001b[38;5;241m=\u001b[39m callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks]:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\integrations.py:939\u001b[0m, in \u001b[0;36mMLflowCallback.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mlflow_available():\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLflowCallback requires mlflow to be installed. Run `pip install mlflow`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 939\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_MAX_PARAM_VAL_LENGTH \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mvalidation\u001b[38;5;241m.\u001b[39mMAX_PARAM_VAL_LENGTH\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_MAX_PARAMS_TAGS_PER_BATCH \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mvalidation\u001b[38;5;241m.\u001b[39mMAX_PARAMS_TAGS_PER_BATCH\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\mlflow\\__init__.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VERSION\n\u001b[0;32m     33\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m VERSION\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     artifacts,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     client,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     config,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     data,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     exceptions,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     models,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     projects,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     tracking,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment_variables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLFLOW_CONFIGURE_LOGGING\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_load\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\mlflow\\config\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment_variables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     MLFLOW_ENABLE_ASYNC_LOGGING,\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msystem_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     disable_system_metrics_logging,\n\u001b[0;32m      6\u001b[0m     enable_system_metrics_logging,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     set_system_metrics_sampling_interval,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     get_registry_uri,\n\u001b[0;32m     13\u001b[0m     get_tracking_uri,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     set_tracking_uri,\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MLFLOW_ENABLE_ASYNC_LOGGING' from 'mlflow.environment_variables' (C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\mlflow\\environment_variables.py)"
     ]
    }
   ],
   "source": [
    "# !pip install entrypoints\n",
    "# !pip install sqlparse\n",
    "# !pip install databricks_cli\n",
    "# !pip install importlib_metadata\n",
    "# !pip install --upgrade mlflow transformers\n",
    "# !pip install --upgrade mlflow\n",
    "# !pip uninstall mlflow\n",
    "# !pip install mlflow\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "tokenized_datasets = []\n",
    "\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80aee8-81ce-4070-8fd1-d9a544a9b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221a882-ffd6-40ad-84c0-df2bf9c33c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597455a9-ff17-434d-9e61-b902021d03d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e38d8-6a31-4441-97ba-33476121482d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd845ce-4035-4633-bbd9-ce458944a844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b5687-013e-4ada-8145-323df266236a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
