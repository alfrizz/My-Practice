{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f579bcb-a96e-4156-b1ef-2dbaa444adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x1e008052ec0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "# Load the pipeline for sentiment classification\n",
    "classifier = pipeline(\"text-classification\", model=model_name)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933b2234-cf9e-45e6-89d8-51620e70cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '3 stars', 'score': 0.6387940645217896}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6403d66d-f505-4ad0-ad98-09263fbc9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '4 stars', 'score': 0.5190770030021667}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, everything well organized\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a22190-ca52-4b7b-aed9-45bb268f6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.SummarizationPipeline at 0x1e04a5d1600>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cnicu/t5-small-booksum'\n",
    "\n",
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline('summarization', model = model_name)\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5111f86-f864-4b68-bb1f-244541711108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = '\\nThe tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\\n'\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length = 30)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c80d10-21eb-4c39-8b89-843b4b2cb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n"
     ]
    }
   ],
   "source": [
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1055b1e9-7c8d-4f56-9ab9-9a1e8e5082d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millau Viaduct\n"
     ]
    }
   ],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "question = \"What's the tallest structure in France?\"\n",
    "\n",
    "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
    "outputs = qa_model(question, long_text)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f32cf35b-340a-426c-bebf-4013cfa3e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't think you're doing a good translation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "\n",
    "input_text = \"No creo que hagas una buena traducci√≥n\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline('translation_es_to_en', model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f6f578-4871-4b5f-8c06-b0dfa5f64e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer model hyperparameters\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126ece2e-d3d9-464e-aead-b62880bfc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model, # d_model is the dimension of the input vectors and output vectors of the model, specifically the size of the feature space. Essentially, it determines the number of features in each transformer layer\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,    \n",
    "    num_decoder_layers=num_decoder_layers\n",
    "    )\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "943a497a-5f46-407d-be6e-da7d37c310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fecc1dc-bf8c-41b2-b66d-17c1a782f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Customer review:\\nI had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\\n\\nHotel reponse to the customer:\\nDear valued customer, I am glad to hear you had a good stay with us.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the prompt for the text generation LLM\n",
    "\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8261f8c5-a1b9-4d48-bb88-252d2a72582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
      "\n",
      "Hotel reponse to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. We appreciate your service and would like to know from every customer what exactly we purchased which accommodations we\n"
     ]
    }
   ],
   "source": [
    "#### Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length = 100, pad_token_id=generator.tokenizer.eos_token_id) #  if the generated text is shorter than max_length, the remaining tokens will be filled with the EOS token.\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75cce4e3-0cb2-42b4-b8fb-547a09f7242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import math  \n",
    "\n",
    "# Subclass the PyTorch nn.Module class to create a custom module for positional encoding\n",
    "# This class is used to add positional information to the input embeddings in a Transformer model\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        # Call the parent class's constructor\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        \n",
    "        # Initialize the dimensions of the model and the maximum sequence length\n",
    "        self.d_model = d_model  # The dimension of the input embeddings\n",
    "        self.max_length = max_length  # The maximum length of the input sequences\n",
    "        \n",
    "        # Initialize the positional encoding matrix with zeros\n",
    "        # This matrix will store the positional encodings that will be added to the input embeddings\n",
    "        pe = torch.zeros(max_length, d_model)  \n",
    "\n",
    "        # Create a tensor of positions from 0 to max_length\n",
    "        # This tensor represents the positions of the words in a sequence\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)  #  we unsqueeze because the multiplication operation position * div_term requires position to be a 2D tensor to correctly broadcast with div_term (the extra dimension is added at position '1': second position)\n",
    "        \n",
    "        # Calculate the division term for the positional encoding\n",
    "        # This term is used in the calculation of the positional encodings\n",
    "        # The div_term values decrease exponentially, which means the positional encoding changes more rapidly for lower-dimensional embeddings and more slowly for higher-dimensional embeddings. This allows the model to learn to attend to both nearby words (local attention) and far-away words (global attention), which is crucial for understanding the context of a sentence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))  \n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        # For even indices, use sine of the position times the division term\n",
    "        # For odd indices, use cosine of the position times the division term\n",
    "        # These encodings are based on sine and cosine functions of different frequencies\n",
    "        # The sine and cosine functions are used to ensure that the positional encodings are continuous and differentiable, which is important for the learning process. Also, these functions generate values between -1 and 1, which helps to keep the magnitude of the positional encodings manageable.\n",
    "        # Using sine for even indices and cosine for odd indices provides two different signals for each position, which helps the model distinguish between different positions more effectively.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "        \n",
    "        # Add an extra dimension to the positional encoding matrix, turning it from a 2D tensor into a 3D tensor(the extra dimension is added at position '0': first position)\n",
    "        # This is done to match the dimensions of the input embeddings (batch size, sequence length, and embedding size)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        \n",
    "        # Register the positional encoding matrix as a buffer that should not be considered a model parameter\n",
    "        # Buffers are tensors that are not updated during backpropagation but need to be part of the model's state\n",
    "        self.register_buffer('pe', pe)  # N.B. self.pe is defined when pe is registered as a buffer. \n",
    "    \n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Update the input tensor by adding the positional encodings\n",
    "        # The positional encodings are added to the input embeddings so that the model can take into account the position of words in a sequence\n",
    "        # The size(1) method returns the size of the second dimension of x, which represents the sequence length.\n",
    "        # By slicing pe to :x.size(1), we ensure that the positional encodings are correctly aligned with the words in each input sequence.\n",
    "        x = x + self.pe[:, :x.size(1)]  \n",
    "        \n",
    "        # Return the updated tensor\n",
    "        # This updated tensor is then passed on to the next layer of the Transformer model\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81cc6b9-c4fd-49ba-a153-aca7734107be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # The number of attention heads. This is the number of different \n",
    "        # weighted sums of the input vectors that we will compute.\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # The dimension of the input embeddings. This is the size of the \n",
    "        # vectors that will be processed by the attention mechanism.\n",
    "        self.d_model = d_model \n",
    "\n",
    "        # The dimension of each head. This is the size of the vectors that \n",
    "        # each attention head will process independently.\n",
    "        # N.B. each attention head processes the entire data, but they do so in their own learned representation space. The head_dim is the dimensionality of this representation space. The division by num_heads is done to ensure that the dimensionality of the input (d_model) is preserved through this process. This is important for the subsequent layers in the model, which expect inputs of a d_model dimensionality.\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear transformations for the queries, keys and values. These \n",
    "        # are standard fully connected layers that will transform the input \n",
    "        # vectors into the queries, keys and values.\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # The final linear transformation. This layer will transform the \n",
    "        # concatenated output of the attention heads into the final output \n",
    "        # vector.\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # This function splits the input vectors into the different attention \n",
    "        # heads. It first reshapes the input vectors into a tensor of shape \n",
    "        # (batch_size, sequence_length, num_heads, head_dim), and then \n",
    "        # rearranges the dimensions to bring the number of heads to the \n",
    "        # second dimension.\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        # The contiguous method is used to ensure that the tensor in memory is \n",
    "        # properly ordered, allowing us to view its data with a different shape.\n",
    "        # Contiguous creates a new tensor with the same data but with all the data contiguously in memory. \n",
    "        # The view method is used twice to first change the shape of the tensor \n",
    "        # and then flatten the tensor (from batch_size, self.num_heads, -1, self.head_dim --> to batch_size * self.num_heads, -1, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim) \n",
    "    \n",
    "    \n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # This function computes the attention weights. It first computes \n",
    "        # the dot product of the query and key tensors, applies a mask if \n",
    "        # provided, and then applies a softmax function to obtain the \n",
    "        # attention weights.\n",
    "        scores = torch.matmul(query, key.permute(0, 2, 1))  # fixed from the original: torch.matmul(query, key.permute(1, 2, 0))\n",
    "        if mask is not None:\n",
    "            # The mask is used to prevent the attention mechanism from focusing \n",
    "            # on certain positions. This is done by setting the scores of these \n",
    "            # positions to a very large negative value, effectively zeroing out \n",
    "            # their impact on the softmax result.\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # The forward function computes the output of the multi-head \n",
    "        # attention layer. It first applies the linear transformations and \n",
    "        # splits the input into multiple heads, then computes the attention \n",
    "        # weights, applies these weights to the values, and finally \n",
    "        # concatenates and linearly transforms the result into the output \n",
    "        # vector.\n",
    "        # query.size(0) returns the size of the batch dimension. It is used \n",
    "        # to reshape the input tensors before splitting them into heads.\n",
    "        batch_size = query.size(0) \n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model) # (from batch_size, -1, self.num_heads, self.head_dim --> to batch_size, -1, self.d_model), being self.d_model = self.num_heads * self.head_dim\n",
    "        return self.output_linear(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d6e3116-b20c-41c4-843e-579eef900fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    \n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3376a16-5901-49f1-823d-57fa8d88fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # The input x is a sequence of embeddings representing the input tokens, and its shape is generally (batch_size, sequence_length, d_model). In the context of self-attention mechanisms, such as the one used in Transformer models, x is used as the Query (Q), Key (K), and Value (V).  In self-attention, the same input (in this case, x) is used as the Query, Key, and Value. This allows the model to compute attention scores based on the input itself.\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        # The operation x + self.dropout(attn_output) is an example of a technique called residual connection: The idea is that it‚Äôs easier to model a residual (or difference) than to learn to model the full information. In this specific case,we are ‚Äúadding the residual‚Äù, that is the output of the self-attention mechanism (which has learned how to modify the input) back to the original input. \n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2f4c09-8492-4b39-aca1-d14795b8e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54d2ca32-6967-4db0-9f5c-6c2f69f590d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    # the dimensionality of the embeddings is:\n",
    "# batch_size is the number of sequences in the batch.\n",
    "# sequence_length is the length of each sequence.\n",
    "# d_model is the dimensionality of the embeddings (i.e., the size of the feature vector for each token).\n",
    "# The slice x[:, 0, :] selects the entire feature vector for the first token in each sequence, that is the [CLS] token, which includes an aggregate representation of the entire sequence, useful in classification tasks.\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a89eaf52-2dbf-44f5-b7fd-c56f21062c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 256\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a35e7da-1001-40cc-be8d-2a340c9ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5969, 8949, 5746,  ..., 2773, 8030, 5480],\n",
       "        [1419, 9100, 5027,  ..., 6214, 8246, 9817],\n",
       "        [9706, 3859, 4062,  ..., 4531, 7151, 8748],\n",
       "        ...,\n",
       "        [ 181, 6658, 3194,  ..., 5632, 9985, 1703],\n",
       "        [9076, 3413, 5868,  ..., 4123,   60, 7898],\n",
       "        [6636, 4520, 6837,  ..., 7825, 7024, 2239]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "\n",
    "print(input_sequence.shape)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1df462c-a0e9-47df-890b-feab943ddc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 1, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 1],\n",
       "        [1, 0, 1,  ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "print(mask.shape)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e0b16ba-3c0e-4366-8687-6a890b514fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a03e94-7280-464a-b49b-3ff6540a97d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierHead(\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e7d819-2ddd-4a9c-b864-c8df512e6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5497e+00,  8.3425e-01, -5.7210e-01,  ...,  8.1865e-01,\n",
       "           1.7531e-01,  1.6560e+00],\n",
       "         [ 7.8067e-01, -1.1346e+00,  7.4688e-01,  ...,  1.8037e+00,\n",
       "          -2.7236e-01,  1.3512e+00],\n",
       "         [ 3.7972e-02, -5.0163e-02,  3.8260e-01,  ...,  1.7472e+00,\n",
       "           5.4422e-02,  5.9329e-01],\n",
       "         ...,\n",
       "         [ 1.9421e+00,  8.1733e-01,  4.1246e-01,  ...,  1.6968e+00,\n",
       "          -3.3675e-02,  1.8066e+00],\n",
       "         [ 1.9257e+00, -8.1500e-01, -3.5197e-01,  ...,  1.0309e+00,\n",
       "          -9.8270e-01,  8.5594e-01],\n",
       "         [ 1.4772e+00, -1.4910e-01,  8.8285e-01,  ...,  1.2714e+00,\n",
       "           4.4124e-01,  1.4253e+00]],\n",
       "\n",
       "        [[-1.1298e+00,  1.4997e+00, -2.2737e-01,  ...,  2.3606e+00,\n",
       "          -4.2508e-01, -2.6108e-01],\n",
       "         [ 4.1484e-01,  8.7193e-02,  1.0362e+00,  ...,  1.4725e+00,\n",
       "          -2.4606e+00,  2.9562e+00],\n",
       "         [-1.2097e+00, -2.5619e-01,  8.6063e-01,  ..., -6.8835e-01,\n",
       "          -8.3816e-01,  1.2612e+00],\n",
       "         ...,\n",
       "         [ 1.5858e+00,  1.0711e+00, -2.7713e-01,  ...,  5.7558e-01,\n",
       "          -1.7131e-01,  5.6316e-01],\n",
       "         [-4.1182e-01,  1.5199e-01, -9.2994e-01,  ...,  2.1475e+00,\n",
       "          -1.6272e+00,  5.8275e-02],\n",
       "         [-2.3511e-01, -1.7710e+00,  4.4452e-01,  ...,  7.9843e-01,\n",
       "          -9.6930e-01,  3.0884e+00]],\n",
       "\n",
       "        [[ 5.7151e-01,  3.9623e-01,  4.4009e-01,  ...,  2.4600e-01,\n",
       "          -4.7648e-02,  2.6513e-01],\n",
       "         [ 3.6208e-01, -2.3937e-01, -1.0014e+00,  ...,  1.2900e+00,\n",
       "          -1.2901e-01,  3.0512e-01],\n",
       "         [ 4.3751e-01,  1.4937e+00,  8.1072e-01,  ...,  7.7729e-01,\n",
       "          -1.4947e+00,  1.4394e+00],\n",
       "         ...,\n",
       "         [ 8.9975e-01,  8.3564e-01, -5.9100e-01,  ...,  1.0753e+00,\n",
       "          -7.5840e-01,  1.0135e+00],\n",
       "         [ 7.7869e-01,  1.2348e+00, -2.5423e-01,  ..., -5.1301e-01,\n",
       "           5.5561e-01, -8.5875e-01],\n",
       "         [-1.0049e+00,  9.2682e-01,  1.1109e+00,  ...,  5.9792e-01,\n",
       "           2.3143e-01,  1.3972e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0378e-02,  2.5043e+00, -1.1651e-01,  ..., -9.2029e-02,\n",
       "          -2.5496e-01, -9.4453e-01],\n",
       "         [ 5.2355e-01,  1.5196e+00,  7.8194e-01,  ..., -6.6038e-01,\n",
       "          -1.9088e+00,  7.0033e-02],\n",
       "         [ 2.3296e-01,  8.9891e-02,  5.3194e-01,  ..., -6.6719e-01,\n",
       "          -1.7369e-01,  5.4798e-02],\n",
       "         ...,\n",
       "         [-3.2510e-01,  6.1958e-01, -9.3633e-01,  ...,  1.3171e+00,\n",
       "          -2.4101e-01,  1.8346e+00],\n",
       "         [ 8.4941e-01, -5.1748e-01, -2.5986e-02,  ...,  1.6430e+00,\n",
       "           2.2477e-01,  8.5655e-01],\n",
       "         [ 8.1476e-01, -9.1990e-01,  2.8102e-01,  ...,  9.7167e-01,\n",
       "           1.3259e+00,  1.1131e+00]],\n",
       "\n",
       "        [[-1.0652e+00, -8.1582e-01, -9.3600e-02,  ..., -6.0160e-02,\n",
       "           1.8876e+00,  6.5929e-02],\n",
       "         [ 3.6901e-01,  2.3327e-01,  1.1996e-02,  ...,  2.7969e+00,\n",
       "          -5.2093e-01, -1.4206e+00],\n",
       "         [-3.6668e-01, -4.9531e-02,  3.1096e-02,  ...,  1.1743e+00,\n",
       "          -1.2075e+00, -2.0228e-01],\n",
       "         ...,\n",
       "         [ 1.7556e+00,  5.5408e-01, -1.2138e+00,  ...,  1.3212e+00,\n",
       "           3.5634e-01,  7.6788e-01],\n",
       "         [-4.5574e-01, -5.1748e-01, -1.1955e+00,  ...,  8.7726e-01,\n",
       "           8.2182e-01,  6.2365e-01],\n",
       "         [ 1.9361e-01, -3.2764e-01,  3.1309e-02,  ..., -3.2526e-01,\n",
       "           3.2231e-01,  1.1831e+00]],\n",
       "\n",
       "        [[ 7.8367e-01,  9.9850e-01,  9.7742e-01,  ...,  9.8482e-01,\n",
       "           1.3700e+00,  5.0427e-01],\n",
       "         [ 3.4732e-01, -1.6497e-01,  1.8705e+00,  ...,  2.9307e+00,\n",
       "          -2.9919e-01,  1.2443e+00],\n",
       "         [-7.0966e-01,  5.4979e-01,  6.9265e-02,  ...,  2.5981e-01,\n",
       "          -4.6969e-01,  1.1242e+00],\n",
       "         ...,\n",
       "         [ 2.6788e-03, -2.0296e+00, -9.2113e-01,  ...,  1.5175e+00,\n",
       "          -2.8841e-01,  7.4264e-01],\n",
       "         [ 1.6253e+00,  8.9008e-01, -2.5145e-01,  ...,  3.5258e-01,\n",
       "           1.7230e+00,  1.4975e+00],\n",
       "         [-3.5639e-01, -1.5221e-01,  1.0540e+00,  ...,  1.5205e+00,\n",
       "           2.1153e-01,  5.1346e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the forward pass \n",
    "enc_output = encoder(input_sequence, mask)\n",
    "\n",
    "print(enc_output.shape)\n",
    "enc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2056424-9171-414b-9011-5b26f1acbe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification outputs for a batch of  8 sequences:\n",
      "torch.Size([8, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6493, -1.3413, -1.5321],\n",
       "        [-1.2763, -0.6592, -1.5912],\n",
       "        [-0.3984, -2.2159, -1.5161],\n",
       "        [-0.5531, -1.4643, -1.6419],\n",
       "        [-0.7159, -1.0302, -1.8688],\n",
       "        [-0.8728, -0.7931, -2.0417],\n",
       "        [-0.5889, -1.5289, -1.4771],\n",
       "        [-0.5819, -1.1680, -2.0391]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification = classifier(enc_output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification.shape)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44e16b75-3f55-4d58-a1b9-a10be7fc1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f8fdcfa-d55c-413c-baa5-1ef645e1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) # using EncoderLayer, but it should be DecoderLayer\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # def forward(self, x, self_mask):\n",
    "    #     x = self.embedding(x)\n",
    "    #     x = self.positional_encoding(x)\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x, self_mask)\n",
    "            \n",
    "    def forward(self, x, self_mask, encoder_output, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask, encoder_output, cross_mask)\n",
    "        # return x\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        # When you apply F.log_softmax(x, dim=-1), the softmax function is applied to the d_model dimension. This means that the softmax function is applied independently to each sequence in each batch, and the output tensor will have the same shape as the input tensor.\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d28bcc5c-d3b7-480c-a651-b94bdb9b4066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1657, 1108, 7823,  ..., 2909, 9790, 1824],\n",
       "        [5676, 7182, 3361,  ..., 2109, 7619, 8453],\n",
       "        [7652, 1069, 4869,  ..., 9620, 7889, 5683],\n",
       "        ...,\n",
       "        [2359, 9565,  298,  ..., 4286, 6436, 3059],\n",
       "        [4781, 2745, 8367,  ..., 6143, 9304, 9352],\n",
       "        [7625, 4028, 1008,  ..., 1698, 8988, 8261]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "print(input_sequence.shape)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ed0b6e6-0bc1-4a04-a52e-2acc89e660dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "275c46fb-b6d5-4541-bbd8-3ec1025ebd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, 8, 8), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b5f0f76-af6f-4023-8ffb-4e46d86b2414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75930524-056e-4a59-96c4-8d868e92f66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  ..., False, False, False],\n",
       "         [ True,  True, False,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()  # Upper triangular mask\n",
    "\n",
    "print(self_attention_mask.shape)\n",
    "self_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a217c58-6b86-492d-85b9-4e919a54c7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17ffcd35-d9f0-4055-ad88-7482e1c559a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0,  ..., 1, 0, 0],\n",
       "        [1, 1, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 0,  ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder cross_mask\n",
    "\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0018a3cb-a64a-48e0-9f14-1e956a794d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 10000])\n",
      "tensor([[[ -9.4482,  -9.3230, -10.5091,  ...,  -8.9697,  -9.6845,  -8.9071],\n",
      "         [ -9.8141,  -8.8747,  -9.7218,  ...,  -8.5592,  -9.5105,  -9.9770],\n",
      "         [ -9.0077,  -9.8012,  -9.4000,  ...,  -8.8119,  -7.9053,  -9.9523],\n",
      "         ...,\n",
      "         [ -9.5091,  -7.7884,  -8.6185,  ...,  -9.5684,  -9.5830,  -9.7031],\n",
      "         [ -9.4629,  -8.8974,  -9.1689,  ...,  -8.7508, -10.1699,  -9.8727],\n",
      "         [ -8.7902,  -8.6300,  -9.0303,  ...,  -9.2289,  -9.8179,  -9.1708]],\n",
      "\n",
      "        [[ -8.6415,  -9.0139,  -9.3738,  ...,  -8.9831,  -9.3313,  -9.6528],\n",
      "         [ -8.9123,  -8.7154,  -8.8092,  ...,  -9.7207,  -9.7151,  -9.9233],\n",
      "         [ -8.9029,  -8.9250,  -9.2294,  ...,  -9.2723,  -9.6684, -10.0517],\n",
      "         ...,\n",
      "         [ -9.6115,  -9.1534,  -8.9948,  ...,  -9.9716,  -9.3570,  -9.4313],\n",
      "         [ -9.1855,  -9.2785,  -8.1049,  ...,  -9.5754,  -9.0475,  -8.7596],\n",
      "         [ -8.9178,  -9.0751,  -9.8033,  ...,  -9.3072,  -9.4373,  -9.8124]],\n",
      "\n",
      "        [[ -9.3459,  -9.0008, -10.0062,  ...,  -9.6446,  -9.0150,  -9.1605],\n",
      "         [ -9.2675,  -9.0424,  -9.7130,  ...,  -8.8077,  -8.9378,  -9.7716],\n",
      "         [ -9.2681,  -8.5543,  -9.3900,  ...,  -9.0328,  -8.9692, -10.4336],\n",
      "         ...,\n",
      "         [ -8.9922,  -8.7916,  -9.4511,  ...,  -9.6895,  -9.5762,  -9.4014],\n",
      "         [ -9.7674,  -8.5321,  -9.4604,  ...,  -9.0950,  -9.7549,  -9.4305],\n",
      "         [ -9.1329,  -8.6599, -10.0394,  ...,  -8.6888,  -9.7282,  -8.8413]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.4076,  -8.5147,  -9.2068,  ...,  -8.7278,  -9.4548,  -9.3838],\n",
      "         [ -9.8072,  -8.8711,  -9.2590,  ..., -10.1599,  -9.6322,  -9.8721],\n",
      "         [ -8.9372,  -8.6178,  -9.5943,  ...,  -9.3103,  -8.8878,  -9.8568],\n",
      "         ...,\n",
      "         [-10.3398,  -9.3230,  -8.8819,  ...,  -9.3392,  -9.0331,  -9.5959],\n",
      "         [ -9.0920,  -8.5007,  -8.7149,  ...,  -9.5117,  -9.6542,  -9.9726],\n",
      "         [ -9.2304,  -8.5964,  -9.4750,  ...,  -9.6753,  -9.6478,  -9.3145]],\n",
      "\n",
      "        [[ -9.3548,  -8.5424, -10.4143,  ...,  -9.8082,  -9.4031,  -9.3100],\n",
      "         [ -9.2084,  -8.0968, -10.4395,  ...,  -9.3604,  -9.7863,  -9.8297],\n",
      "         [ -8.9838,  -7.9833,  -8.8929,  ...,  -9.0078,  -9.9315,  -8.2247],\n",
      "         ...,\n",
      "         [ -9.1412,  -8.4972,  -9.4880,  ...,  -9.6523,  -9.5353,  -9.8261],\n",
      "         [ -8.5783,  -8.6451,  -9.3576,  ...,  -9.9513,  -9.3397,  -8.9803],\n",
      "         [ -9.3297,  -8.6438,  -9.7149,  ..., -10.0013,  -9.7595,  -9.0366]],\n",
      "\n",
      "        [[ -9.4310,  -8.5689,  -9.9630,  ...,  -9.0000, -10.5244,  -9.8829],\n",
      "         [ -9.1692,  -9.0123,  -9.7857,  ..., -10.4438, -10.5555, -10.0039],\n",
      "         [ -8.8073,  -9.6535,  -8.8751,  ...,  -9.2874,  -9.2568, -10.0171],\n",
      "         ...,\n",
      "         [ -9.4452,  -8.3036,  -9.8553,  ...,  -9.9472, -10.2337,  -9.7233],\n",
      "         [ -9.2577,  -8.1594,  -8.7802,  ...,  -9.6066,  -9.9022,  -9.2488],\n",
      "         [ -9.4746,  -8.8233,  -9.1166,  ..., -10.2189,  -9.9833,  -9.8994]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The reason why the output is a 3D tensor rather than a 2D one is because the model is processing multiple sequences at once (the batch size is greater than 1) and it‚Äôs predicting a probability distribution over all possible words for each position in each sequence.\n",
    "# So, for each sequence in the batch (8 sequences), and for each position in each sequence (256 positions), you have a vector of length 10000 (the size of your vocabulary) representing the probability distribution over all possible next words.\n",
    "\n",
    "dec_output = decoder(input_sequence, self_attention_mask, enc_output, padding_mask)\n",
    "print(dec_output.shape)\n",
    "print(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fa4b5a5-409b-45bf-95a6-2e4dad5472ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='textattack/distilbert-base-uncased-SST-2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e08ef05c-f507-4470-b3be-e43b0dcd2324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a4c8829-7de1-457b-8106-1d4f2204f0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 2190, 3185, 1045, 1005, 2310, 2412, 3427,  999,  102,    0],\n",
       "        [ 101, 2054, 2019, 9643, 3185, 1012, 1045, 9038, 3666, 2009, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "037fe889-4fdb-460e-a474-a413ebdb40fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f03e4513-4aca-4f7e-88cd-0b5debfc8f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "809b5ebc-e84b-4947-bc91-05e3f343eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d5a294c-f96d-416e-b1b0-5ede64da8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for \"The best movie I've ever watched!\": 1\n",
      "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
     ]
    }
   ],
   "source": [
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b41652f3-395f-4e35-9188-4253114fb1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_sents', 'summaries'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face's dataset hub\n",
    "dataset = load_dataset('opinosis', trust_remote_code=True)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1af4af29-1973-4998-8729-05d4cb433fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of instances: {len(dataset['train'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcbc46a3-f0a1-49fc-be25-756cc67c2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['review_sents', 'summaries']\n"
     ]
    }
   ],
   "source": [
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d85c21f6-5636-4f8e-b78d-972a85b9fbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_sents': \"I bought the 8, gig Ipod Nano that has the built, in video camera .\\r\\n  Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod .\\r\\nI have lots of music cd's and dvd's, so currently I'm just interested in storing some of my music and videos on the ipod so I can enjoy them on my vacation, and while at work .\\r\\nThere's a right way and wrong way to store music and videos onto the ipod .\\r\\n Audio and video recording are a step above the competition .\\r\\n As always, the video screen is sharp and bright .\\r\\nipod nano even better with video camera !\\r\\n This time around, Apple is branching out of its iPod formula in a small, but not insignificant way by gracing the back of the Nano with its own video camera .\\r\\n Still, video fans should consider stepping up to a product with a larger screen, such as the Zune HD   or iPod Touch .\\r\\nThe user interface of the 5G Nano remains almost entirely unchanged with the exception of a few new menu items for the video camera, FM radio, and fitness features .\\r\\nI can listen to it without headphones, i can record video .\\r\\n It looks nicer than my other one, love the video and everything else this one can do\\r\\n wish BB could engrave them too  announcer for songs is kind of annoying  Haven't yet tried the video, but very pleased with the size and quality of my music on this nano .\\r\\n Hard to delete videos from the Nano device .\\r\\n The video design allow you to take different types of vidoe like x, ray, monochrome, and twist making it enjoyable to play .\\r\\n The video quality is actually fairly good, my friend toook a video with it and she uploaded it to facebook and it came out great !\\r\\n It eve ncame out better than a video she took with an actual camera camera !\\r\\n the video quality is perfect .\\r\\nGreat for music, just OK for video\\r\\n Video recorder only, cannot take just photos .\\r\\n the VIDEO, VOICE and PEDOMETER recorder is one great addition to it .\\r\\n As for the video camera it's a nice addition considering it's an mp3 player first and foremost, I think people are going to go into this and think maybe I can replace my old video, NO .\\r\\n It just a solid low quality video camera some of the lighting or your fingers may get in the way of the actual recording but once you get use to it, it can become fun .\\r\\n I find it weird that it will take video but not still pics .\\r\\n It looks just like the 4th generation, except on the back, there's a camera to take video clips or whatever it is you want to .\\r\\n i hate itunes, but it seems to be a necessary evil in order to tag say a video as music and not a movie, so though i originally started with 3rd party apps for syncing i have since switched to itunes .\\r\\nThis time around, Apple is branching out of its iPod formula in a small, but not insignificant way by gracing the back of the Nano with its own video camera .\\r\\n The video works very well for such a small sensor and it is so easy and convenient to use .\\r\\n  We have had portable video players for our children as our cars are not equipped with these devices .\\r\\n Video camera quality and sound are also surprising for such a small unit .\\r\\n Not a tremendous surprise here, as we believed the sixth, generation  true video iPod  would not be announced until later in the year or even early in 2007 .\\r\\n5G lies a more mature iPod, many steps wiser and more able than its one, year, old  The iPod gains many incremental improvements, including a brighter screen and better video battery life, but probably the most appealing aspect is the tantalizing price points of $249 for the 30GB version and $349 for the huge 80GB version   .\\r\\n The 80GB is enticing for video addicts both for the capacity iTunes movies are about 1 .\\r\\n The video camera is nice, just wish it could take stills .\\r\\n The one problem with the video camera is it is not the easiest thing to use .\\r\\n I also like that videos can be watched on it as well as listening to it without the ear phones that is a really great feature .\\r\\n The only odd thing is that is has a video camera but doesn't have a single shot camera .\\r\\n Easy to use, Video Camera, Radio, Size, Sound Quality  .\\r\\n The battery doesn't last a long time especially when you're recording or watching a video but I just listen to music most of the time and it lasts me a good length of time doing that .\\r\\n The video quality is great and has different settings .\\r\\n Definitely easier to use than the IPod Touch, and the video camera feature is awesome .\\r\\n That way you will have enough storage to add music, video and pictures !\\r\\n I haven't found anything that is not so great  This is a multi use MP3 player with the ability to make videos, music and videos .\\r\\n Great new features in this iPod generation include voice recording, listening to radio, and even filming video .\\r\\n Has video camera and voice memo .\\r\\n The video looks great and it's easy to use .\\r\\n No  off  switch  Compared to Gen 4 the ability to listen without speakers and the video camera is great .\\r\\n As easy to use as any other iPod, and the video is good .\\r\\n She loves to make her own videos and play them back .\\r\\n The videos are so clear I just love it .\\r\\n However, the way it is designed, it is clumsy to hold while recording a video .\\r\\n camera does only video, not pictures  Way better than 2nd gen nano .\\r\\n Love the camera, but wish it had photo capabilities, rather than just video .\\r\\n It's got radio, video, camera, video camera, small  .\\r\\n Love the pedometer feature on the nano and the video camea  .\\r\\n The color, the video, overall the item is great  .\\r\\n The size is very nice and compact and the videos are beautiful !\\r\\n it would be nice to delete songs when they get old  the video camera is the best, i use it all the time .\\r\\n It was fine quality sounds for  roof, raising  music and also insane levels of clearity for music videos or movies on the go !\\r\\n The video camera feature is awesome enough, but all of the special effects are incredible .\\r\\n He loves shooting video and then viewing it in  cyborg  mode or in sepia tones .\\r\\nOur 12 year old loves the IPod ,  especially the video camera feature .\\r\\n lots of features  I got this for my son because he wanted an MP3 player and a video camera for Christmas .\\r\\n Like the extra features ,  pedometer, video camera, radio, speakers .\\r\\nhaven't quite mastered the video camera yet, but I'm okay with that !\\r\\n Movies, videos, music, music, music .\\r\\n Plenty of memory to add all your favorte music and videos .\\r\\n The 16g holds a lot of music and video .\\r\\nvideo, motion sensing, screen size and sllek look  I recently upgraded from the 2nd generation Nano and love the new features of the newest model .\\r\\n Can't believe the picture quality and ease of video use !\\r\\n It will take video, but not pictures  ?\\r\\n Secondly, the thing has the capability to record video, but won't take pictures .\\r\\n Yeah, that's typed correctly ,  video, but not pictures .\\r\\n Seems kinda silly that they would go right to video and completely neglect photography .\\r\\n My daughter loves taking video, as well as the music & pics !\\r\\n She takes videos and pics with it all the time .\\r\\n I decided to get this iPod cause of the Memory and the Video Cam in the back .\\r\\n The video camera and the graphics is so cool !\\r\\n Easy to use and cool video camera !\\r\\n Comes in different colors, make videos, and holds alot of music .\\r\\n Small, holds lots of songs, video, easy to use  .\\r\\n Can't figure out how to review the video clips I record .\\r\\n  This is an easy to use item, but is slightly confusing to use when trying to review video clips .\\r\\n It takes video and has a really cool radio feature on it, according to the kids .\\r\\n easy to use, cool helpful features, video camera, voice over  .\\r\\n The video camera is great   and the different effects are fun to mess around with .\\r\\n It would be nice if the video camera zoomed in a little and if there was a regular camera as well, but those are pretty minor complaints .\\r\\n  My daughter thinks that the video clarity is amazing and finds the built in speakers awesome .\\r\\n The camera and video feature are wonderful .\\r\\n Holds tons of music, video, pics .\\r\\n I wish the video camera had a pause on it, but you can't pause .\\r\\n It can hold tons of music, video, pics and watching movies on the plane is nice .\\r\\n I also wish the video camera had a pause on it, but it doesn't pause and starts with a new video everytime you stop it .\\r\\n She didn't want the touch, she wanted the nano because you can video, it has the radio that she can play back the songs and she likes the size .\\r\\n The video camera is great and the screen size is a nice size\\r\\n We haven't tried the video yet, so I can't comment on that .\\r\\n great video quality, no ear buds required internal sound system sounds great  .\\r\\n Still to new to we'll see  Great product, video feature is awesome and I love how there is now internal sound and you don't have to use ear buds with it .\\r\\n Don't really need video camera  eventhough we don't use the video that much the rest of the features are great .\\r\\n It does so many things,  Wow, nav, video, mp3, and more !\\r\\n I love how easy it is to download from ITunes and the fact I can also do video  .\\r\\n I am learning to navigate the menu better that at first and love that fact that I can take a video .\\r\\n Its easy to use and love the new video feature .\\r\\n It does have some nice features such as the video camera and the radio .\\r\\n He has been using it consistently ever since and has nothing but praise for it and it's many options  loves the video cam and the music player recorder especially\\r\\n Watching any videos drains the battery very quickly .\\r\\n Glad you listen to the radio, play games and take video  .\\r\\n none of my old Nano asseccories work with this one  Fun new Ipod, the video feature is fun but difficult to learn how to use .\\r\\n I've downloaded a lot of CDs and music from iTunes, also a few music videos,which are great fun to watch and hear .\\r\\n Great for spot video if you do not have a camera availabe on hand .\\r\\n some changes such as video camera and bigger screean, but it was just ok .\\r\\nGreat video camera and the funcionality is awesome\\r\\n Video camera function and the blending of songs at the end and beginning  .\\r\\n nothing  The functionality and video camera were great innovations added to the newest generation of Ipod !\\r\\n It's very easy to use and its video camera is very nice and it doesn't ghost .\\r\\n and the video quality is very good, and the sound quality is very good as well .\\r\\n Sound quality, video quality, easy to use, space, customizable  .\\r\\n The colors are awesome, the sound is great and it hold tons of songs and videos !\\r\\n It has great video quality for such a small device .\\r\\n the video recorder, sound, look  .\\r\\n has bigger screen, video camera  .\\r\\n the picture for the video is awesome its very clear and easy to see .\\r\\n features, look and feel, video  .\\r\\n Video, even in low light, is impressive .\\r\\n Even the built, in video camera is very good .\\r\\n I'm really disappointed that Apple is so proprietary that you can't use anything but iTunes to add music and videos to an iPod .\\r\\n It has a built in video camera, and it shoots in high quality, which I was surprised at .\\r\\n n you can put music videos pictures .\\r\\n The video camera is cool, but I don't know that I would use it much .\\r\\n This can record videos and it is also Very Very Light Weight !\\r\\n i couldnt think of a single thing  this ipod is reallly great you can shoot videos listen to the radio plus you have 16 gigs of memory it is aweosme i really love it because its still as small as the last one only with a bigger screen and great color\\r\\n The video and external speaker components were a surprise on the 5th Gen iPod .\\r\\n I love the video camera it has on it, the pedometor is awesome to use and best of all I can listen to music on it when I'm doing other stuff on my computer when it might be slowed down when listening to music on it .\\r\\n Awesome features fm radio, video camara, pedometer and more  .\\r\\n Nothing  What will Apple think of next for their Nano, the video camara records awesome video even in dim lighting .\\r\\n All the neat little features and surprisingly good video camera !\\r\\n I took it to class and decided to test it out and everyone was surprised by how good the video quality was .\\r\\n I'm planning on hiking half, dome next year and this is much much more lightweight than the video camera I have so it will definitely be accompanying me !\\r\\n fm radio lots of storage for music, videos and pictures alarm clock to your tunes blue color lock button so you don't accidentally turn it off on ease of use Cons :\\r\\n Hard to get videos onto computer  This was a purchase I made after I returned my Sony Walkman .\\r\\n A must, buy for everybody who loves music and video .\\r\\n video camera built in and music plays without head phones  .\\r\\n need a video program on your computer to obtain video  Great product for the price .\\r\\n I had the iPod Video but it did not work with my new Nike\\r\\n chip so I bought the newest iPod video the day it came out .\\r\\n I cannot figure out how to get the video I record off of the IPod .\\r\\n ,  I cannot figure out how to get the video I recorded on my Nano off to upload it to Facebook or You Tube .\\r\\nshoes if using vid cam for long time, it kills battery  Standard Def video recording, but still sharp !\\r\\n Only things I wish they added is measuring distance [not just steps] on the pedometer, better sound on the speaker, and a  capturing still images  with the video recording feature .\\r\\n The video recording feature is the best part .\\r\\n7 out of 10 have not used video yet but the pixels would be just the same .\\r\\n The product is very useful if you want to go out for a jog and listen to your music or watch some videos if you stuck on a bus or subway .\\r\\n Video camera, FM turner, Speaker, Pedometer  .\\r\\n Location of the video camera, and the gap between the click wheel and ipod  The Best Ipod ever and has the best features ever .\\r\\n Easy to use video camera, Big screen, Fun color  .\\r\\n Which I am sure all Nano's are like that  LOL The video camera is so much fun !\\r\\n new look, shape and video cam feature  .\\r\\n Smudgy finger prints  I like the Video voice over color, purple I like hearing the Ipod w out the ear plugs .\\r\\n light weight, radio, video camera  .\\r\\n I have an old 60GB ipod video that I use in my car and now I use this ipod for walking around campus and such .\\r\\n The video camera is a great feature to have, however its not that high of a quality when you transfer it onto your computer .\\r\\n Camera has great video effects   .\\r\\n The videos you record do not show up on iTunes though, you have to go to My Computer and find the ipod drive, then open the folder named DCIM, and the video files are in  .\\r\\n When you're not in a game or watching a video, if you turn it on the side, coverflow displays, however you can turn this feature off .\\r\\n I mean I do have my cell that takes pics and videos but I just love the picture video the NANO records .\\r\\n And I just foundout you don't need the head phones in the iPod to listen to your music or videos !\\r\\n Video Camera with fun special effects, FM radio  .\\r\\n the glossy fingerprint attracting finish compared to last year's beautiful matte finish  Great improvements with the video camera, really fun special effects, easy to use ,  also great FM radio with pause feature .\\r\\nthen it would be perfect oh and make a 32 or 64gb version so I can fit all my music and have lots of room for video ,  thanks !\\r\\n But overall I love it ,  between the radio, speaker, microphone, video camera, effects, there are enough new features to justify buying a new nano .\\r\\nIt's great to have a decent video camera with you at all times if you don't have an iphone .\\r\\n Had the original iTouch but I was burglarized andwas waiting for the iTouch with video but can't wait any longer .\\r\\n Video is impressive, speaker sound is good for size, easy menus, screen is big enough to watch movies .\\r\\n The videos are very crisp and clean, the video camera is nice, I was very surprised with the quality .\\r\\n Small size and Color, Gettin your Money's worth, nice screen, and video camera  .\\r\\n I do like the video camera on it .\\r\\n The mic and camera are great, speaker is great for video playback or bedtime movie watching, the screen is now even bigger and the device is the same size but Lighter than the 4th gen nano !\\r\\n The Video Recording Quality, Price, Size, Weight, Radio, Pedometer, Voice Over  .\\r\\n video camera, radio, great price for a 16 gig, not a fingerprint magnet, small .\\r\\n cant take pictures just videos, and u need the headphones given for the radio  good for just listening to music and for your car .\\r\\n Easy to set up, has a video camera and a radio built in .\\r\\n The video camera is not as good as I thought it was going to be .\\r\\n NOTHING  Ipod nano has a VIDEO CAMERA it is so cool, ROCK\\r\\n the video is very clear, the built in speaker is pretty good, the radio is very useful and overall it's just great !\\r\\n He was delighted as it had a video and camera which his original one did not have .\\r\\n It has plenty of room for her music and she loves taking pictures and making short videos with it .\\r\\n While the Nano allows for video and audio, the IPod touch has interactivity and\\r\\n While the Nano allows for video and audio, the IPod touch has interactivity and\\r\\nits really good up to apples usual standard sound quality and video quality is brillant and is ideal for anyone !\\r\\nThe video camera feature is nice but I don't understand why they didn't allow for picture taking ?\\r\\n It's small and lightweight, it looks amazing, the video quality is way better than I ever imagined it would be, and the screen is huge .\\r\\nThe seller advertised it as the NEWEST model which should be able to take pictures and video .\\r\\nIt is of the best buys I've done, sound quality, video and radio is excellent\\r\\n Great sound quality, love the video recording too  :\\r\\n It's just a shame it only takes video and not still photos .\\r\\n It has a cool video recorder, but i could've done without it .\\r\\n The video camera works great and the battery\\r\\n If you had other ipods with the feauture of puting videos, when you synch them, they don't appear\\r\\n Video option is easy to use .\\r\\n I have the 5th gen iPod video, and this thing blows it away !\\r\\nThe video is surprisingly good !\\r\\n I do wish that it also took pictures as well as video though .\\r\\nSo, the only thing about the iPod nano that I don't like   is the placement of the video camera .\\r\\n Video quality is better than I expected .\\r\\nAmazing amount of music photos videos can be stored .\\r\\n Video taking feature is interesting .\\r\\n It plays music, videos, radio, records\\r\\n The video capability is incredible, and so easy to use .\\r\\nMy wife and I are really pleased with our Nano Ipods that we purchased from you so easy to use and good to have a camera as well as a video John Coleman\\r\\n It's a nice little gadget, I've put some songs on it and the video camera is good\\r\\n The video camera is crystal clear and the memory space is more than ideal for all your needs .\\r\\nshe was quickly able to download music and videos xmas am\\r\\n I have used the video camera nd the sound is good considering and the picture is\\r\\n FM radio and a   video camera are pretty darn cool .\\r\\nGreat little audio video device\\r\\n\",\n",
       " 'summaries': ['Video camera is great.\\r\\nVery easy to use.\\r\\nVideo quality is excellent.',\n",
       "  'The video screen is very bright.',\n",
       "  'The video for Ipod nano 8GB is a great addition, crystal screen with great quality for photos, video and music.',\n",
       "  'The new Ipod nano which comes with a video camera offers decent performance with pictures and video along with the great music.',\n",
       "  'The iPod takes and displays quality video.\\r\\nIt does not take single still pictures and this is a disappointment but not a deal breaker.']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "404d5b6c-d2dc-400a-92f9-298af290b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][-2]['review_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac8ac89b-0714-4100-8875-fec54785af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14010bce-ddff-4d7b-986c-b095399d5b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21603,    10,    27,  2944,     8,  9478, 10754,    27, 11410, 20556,\n",
       "            24,    65,     8,  1192,     6,    16,   671,  1861,     3,     5,\n",
       "            94,   444,     7,    65,    46,    30,     6,   689,  1078,     6,\n",
       "           213,    25,   164,  1242,    11,   946,   723,    11,  3075,    84,\n",
       "            56,  2438,  2400,     8,     3,    23, 11410,     3,     5,    27,\n",
       "            43,  1995,    13,   723,     3,    75,    26,    31,     7,    11,\n",
       "         30114,    31,     7,     6,    78,  1083,    27,    31,    51,   131,\n",
       "          1638,    16,     3, 17445,   128,    13,    82,   723,    11,  3075,\n",
       "            30,     8,     3,    23, 11410,    78,    27,    54,   777,   135,\n",
       "            30,    82,  4257,     6,    11,   298,    44,   161,     3,     5,\n",
       "           290,    31,     7,     3,     9,   269,   194,    11,  1786,   194,\n",
       "            12,  1078,   723,    11,  3075,  2400,     8,     3,    23, 11410,\n",
       "             3,     5,  9607,    11,   671,  5592,    33,     3,     9,  1147,\n",
       "           756,     8,  2259,     3,     5,   282,   373,     6,     8,   671,\n",
       "          1641,    19,  4816,    11,  2756,     3,     5,     3,    23, 11410,\n",
       "         13944,   237,   394,    28,   671,  1861,     3,    55,   100,    97,\n",
       "           300,     6,  2184,    19,  6421,    53,    91,    13,   165, 17003,\n",
       "          5403,    16,     3,     9,   422,     6,    68,    59,    16, 26251,\n",
       "           194,    57,     3,  3484,    75,    53,     8,   223,    13,     8,\n",
       "         20556,    28,   165,   293,   671,  1861,     3,     5,  4886,     6,\n",
       "           671,  2675,   225,  1099,     3, 19115,    95,    12,     3,     9,\n",
       "           556,    28,     3,     9,  2186,  1641,     6,   224,    38,     8,\n",
       "          1027,   444,  3726,    42, 17003, 11031,     3,     5,    37,  1139,\n",
       "          3459,    13,     8,   305,   517, 20556,  3048,   966,  4585, 26164,\n",
       "            28,     8,  5763,    13,     3,     9,   360,   126,  2285,  1173,\n",
       "            21,     8,   671,  1861,     6, 13409,  2252,     6,    11,  4639,\n",
       "           753,     3,     5,    27,    54,  3011,    12,    34,   406, 21065,\n",
       "             6,     3,    23,    54,  1368,   671,     3,     5,    94,  1416,\n",
       "          1245,    52,   145,    82,   119,    80,     6,   333,     8,   671,\n",
       "            11,   762,  1307,    48,    80,    54,   103,  1663,     3,  7640,\n",
       "           228,     3,    35, 15299,    15,   135,   396,  6456,    52,    21,\n",
       "          3605,    19,   773,    13, 16241, 19586,    31,    17,   780,  1971,\n",
       "             8,   671,     6,    68,   182,  5010,    28,     8,   812,    11,\n",
       "           463,    13,    82,   723,    30,    48, 13944,     3,     5,  6424,\n",
       "            12,  9268,  3075,    45,     8, 20556,  1407,     3,     5,    37,\n",
       "           671,   408,   995,    25,    12,   240,   315,  1308,    13,     3,\n",
       "          6961,    32,    15,   114,     3,   226,     6,     3,  2866,     6,\n",
       "          7414, 10363,   526,     6,    11,  9135,   492,    34,  9231,    12,\n",
       "           577,     3,     5,    37,   671,   463,    19,   700,  5163,   207,\n",
       "             6,    82,  1565,   396,  1825,     3,     9,   671,    28,    34,\n",
       "            11,   255, 14686,    34,    12, 13301,    11,    34,   764,    91,\n",
       "           248,     3,    55,    94,     3,    15,   162,     3,    29,  6527,\n",
       "            15,    91,   394,   145,     3,     9,   671,   255,   808,    28,\n",
       "            46,  1805,  1861,  1861,     3,    55,     8,   671,   463,    19,\n",
       "           626,     3,     5,  1651,    21,   723,     6,   131,  6902,    21,\n",
       "           671,  3953,  1368,    49,   163,     6,  1178,   240,   131,  1302,\n",
       "             3,     5,     8,   584, 13162,   667,     6,     3,  8040,  8906,\n",
       "            11, 11012, 27415,   427,  5946,  1368,    49,    19,    80,   248,\n",
       "           811,    12,    34,     3,     5,   282,    21,     8,   671,  1861,\n",
       "            34,    31,     7,     3,     9,  1245,   811,  4014,    34,    31,\n",
       "             7,    46,     3,  1167,   519,  1959,   166,    11, 19839,     6,\n",
       "            27,     1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c225539-d3a1-423f-af36-98eac35410f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,    27,  2944,     8,  9478, 10754,    27, 11410, 20556,    24,\n",
       "           65,     8,  1192,     6,    16,   671,  1861,     3,     5,    94,\n",
       "          444,     7,    65,    46,    30,     6,   689,  1078,     6,   213,\n",
       "           25,   164,  1242,    11,   946,   723,    11,  3075,    84,    56,\n",
       "         2438,  2400,     8,     3,    23, 11410,     3,     5,     1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2fa7054-04bc-4d68-a154-8146a9cd133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text (first 400 characters): \n",
      " I bought the 8, gig Ipod Nano that has the built, in video camera .\n",
      "  Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod .\n",
      "I have lots of music cd's and dvd's, so currently I'm just interested in storing some of my music and videos on the ipod so I can enjoy them on my vacation, and while at work .\n",
      "There's a right way and wrong wa\n",
      "\n",
      "Generated Summary: \n",
      " I bought the 8, gig Ipod Nano that has the built, in video camera. Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod.\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a59b5825-25c6-43a0-bc14-e99996465bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd820626-ebb8-45c8-a330-c77d65785125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_input Hello\n",
      "input_ids tensor([[3923,    0]])\n",
      "translated_ids tensor([[65000,  2119,     3,     0]])\n",
      "English: Hello | Spanish: Hola.\n",
      "english_input Thank you\n",
      "input_ids tensor([[1825,   40,    0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated_ids tensor([[65000,  1124,     3,     0]])\n",
      "English: Thank you | Spanish: Gracias.\n",
      "english_input How are you?\n",
      "input_ids tensor([[594,  53,  40,  21,   0]])\n",
      "translated_ids tensor([[65000,    50,  1102,  1221,    21,     0]])\n",
      "English: How are you? | Spanish: ¬øC√≥mo est√°s?\n",
      "english_input Sorry\n",
      "input_ids tensor([[5099,    0]])\n",
      "translated_ids tensor([[65000,   350,  1669,     3,     0]])\n",
      "English: Sorry | Spanish: Lo siento.\n",
      "english_input Goodbye\n",
      "input_ids tensor([[22191,     0]])\n",
      "translated_ids tensor([[65000,  8631,     3,     0]])\n",
      "English: Goodbye | Spanish: Adi√≥s.\n"
     ]
    }
   ],
   "source": [
    "# The reason why the input and translated IDs vectors have more elements than the corresponding word inputs is due to the way the tokenizer works.\n",
    "# In your code, the tokenizer.encode function is used to convert the input text into a sequence of IDs, which represent the tokens in the text. These tokens can be individual words, but they can also be smaller units depending on the tokenizer. For example, a word might be split into multiple subwords, each with its own ID.\n",
    "# Additionally, special tokens are often added to the sequence. For instance, a common practice is to add a special token at the beginning and end of the sequence. In your case, the 0 at the end of each input_ids and translated_ids tensor is likely a special token, such as an end-of-sequence token\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    print('english_input', english_input)\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
    "    print('input_ids', input_ids)\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    print('translated_ids', translated_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a180b31-33ec-4984-9b7c-e9cdd955d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11590\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "mlqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "671edd08-a89d-4458-87a9-d983f359f9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Who analyzed the biopsies?\n",
      "Context:  In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"\n"
     ]
    }
   ],
   "source": [
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fea420d-7e1f-4798-831e-3f013e593b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='deepset/minilm-uncased-squad2', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89a96dda-066d-4279-adf9-857af82a91b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2040, 16578,  1996, 16012,  4523,  3111,  1029,   102,  1999,\n",
       "          2807,  1010,  2274, 13294,  6831, 16728,  1998,  1996, 24835,  1997,\n",
       "         16728,  4787, 10556, 17112,  2050,  1998,  2728, 10097, 12923,  1996,\n",
       "         18531,  1998,  1996,  2142,  2163,  4483,  3860,  4034,  1012,  2037,\n",
       "          4848,  1010,  1999,  2029,  2027,  2020,  3421,  2011,  2577,  2899,\n",
       "          2118,  2375,  2934,  5655, 10722, 12866,  1010,  6884,  2027,  2018,\n",
       "          2042,  2556,  2043,  2312, 12450,  1997,  4242, 12141,  2018,  2042,\n",
       "          5296,  1999,  2330, 14496,  1998, 19874,  2012, 18087,  1012, 16012,\n",
       "          4523,  3111,  2579,  2013,  1996, 17612, 11390,  2020, 16578,  2011,\n",
       "         18607,  2118, 16012, 24229,  2015,  1010,  2040,  2179,  2152,  3798,\n",
       "          1997,  4487, 11636,  2378,  1010,  4487, 10609,  6844, 27942,  2319,\n",
       "          1010,  1998, 13012,  2818, 10626,  8913, 16921, 11474,  1999,  2037,\n",
       "          2303,  6638,  1012,  1996, 17612, 11390,  6884,  2027,  2018,  8760,\n",
       "          3096,  1010, 11290,  1010,  1998, 16464,  6441,  2349,  2000,  2037,\n",
       "          2147,  2012, 18087,  1010,  1998,  2008,  2023,  2018,  5201,  2000,\n",
       "          1996,  6677,  1997, 10097,  1998, 10556, 17112,  2050,  1012,  1996,\n",
       "          4848,  4912,  9430,  2005,  1996,  6441,  2027,  2018,  8760,  1010,\n",
       "          6815,  1996, 18531,  2018, 17800,  8971, 11704,  4475,  1010,  1998,\n",
       "          2008,  1996, 19044,  2018,  3478,  1999,  2049,  4611,  2000, 16306,\n",
       "          1996,  7692,  5680,  1998,  7233,  2552,  1006,  2029, 21208,  2015,\n",
       "          8304,  1997,  4795,  4475,  1007,  1012,  2027,  2036,  4912,  6851,\n",
       "          2592,  2055,  1996, 12141,  2000,  2029,  2027,  2020,  9382,  6086,\n",
       "          1010,  5327,  2023,  2052, 10956,  1996,  2966,  3949,  1997,  8643,\n",
       "          1012, 12295,  3389,  1044,  1012,  5226,  1010,  2280,  3472,  1997,\n",
       "          1996,  2160,  4454,  2837,  1010,  2409,  3438,  2781,  6398, 23920,\n",
       "          2358, 28083,  1010,  1000,  1996,  2250,  2486,  2003, 26268,  2075,\n",
       "          2035,  2592,  2055,  2181,  4868,  1999,  2344,  2000,  4047,  3209,\n",
       "          2013,  1037,  9870,  1012,  1000,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "174dcdce-caea-4b31-9139-4dc4dc0485fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=384, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2dd781f-c2c5-4388-ac12-1e3511634879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 0.4331, -5.9864, -5.7614, -6.2329, -5.9369, -6.4554, -6.6515, -6.6686,\n",
       "          0.4331, -5.9137, -6.0544, -6.6783, -5.9388, -6.2935, -6.2757, -6.5190,\n",
       "         -6.6694, -6.2961, -6.3753, -6.5280, -6.1553, -5.4943, -5.9384, -6.5034,\n",
       "         -6.7191, -6.6250, -6.2206, -6.6423, -6.4067, -6.1051, -5.9176, -6.6871,\n",
       "         -6.3237, -5.6972, -6.5879, -6.0408, -6.6248, -6.8032, -6.8567, -6.0126,\n",
       "         -6.4246, -6.8355, -6.3049, -6.4013, -6.3018, -6.4767, -6.4598, -6.4260,\n",
       "         -5.5132, -6.2999, -6.6227, -6.1841, -6.3011, -5.6165, -6.0477, -6.8492,\n",
       "         -6.7725, -6.1979, -6.2060, -6.3269, -6.3990, -6.4921, -6.2939, -5.9060,\n",
       "         -6.4909, -6.4594, -6.2571, -6.5393, -6.5011, -6.4583, -6.4658, -6.3752,\n",
       "         -6.2070, -6.5106, -6.6355, -6.5982, -6.3821, -6.2631, -5.9546,  1.1200,\n",
       "         -4.9416, -4.9593, -5.7370, -6.0798, -5.8981, -5.1775, -6.2021, -3.7974,\n",
       "         -0.7555, -0.8330,  7.0906, -1.6840, -0.1442, -4.6069, -0.1813, -4.0197,\n",
       "         -5.4498, -5.8540, -3.7755, -6.2736, -6.4684, -4.3514, -6.4279, -6.8485,\n",
       "         -6.6440, -5.7422, -6.3996, -6.4445, -6.5249, -6.9212, -6.8794, -6.6766,\n",
       "         -5.5149, -6.4146, -6.4113, -6.4482, -6.5586, -6.8798, -6.3597, -6.2457,\n",
       "         -6.0640, -6.4219, -6.1355, -5.8297, -5.8958, -6.9695, -6.2983, -6.2185,\n",
       "         -6.2785, -6.2933, -6.2346, -6.5628, -6.4667, -6.8488, -6.6590, -6.3942,\n",
       "         -6.7781, -6.3991, -6.4851, -6.3615, -6.4829, -6.4598, -6.7390, -7.1202,\n",
       "         -6.6077, -6.3351, -6.2694, -6.3993, -6.3904, -6.4936, -6.3733, -6.4900,\n",
       "         -6.5810, -6.2266, -6.6761, -6.4495, -6.7184, -7.0666, -7.0005, -6.1441,\n",
       "         -6.4744, -6.2638, -6.3753, -6.3944, -6.3906, -6.4805, -6.5536, -6.5291,\n",
       "         -6.7597, -6.8259, -6.1555, -6.0330, -6.1905, -6.4132, -6.2120, -6.4482,\n",
       "         -6.2310, -6.8354, -6.9364, -6.6217, -6.2513, -6.0758, -6.2514, -6.5031,\n",
       "         -6.3979, -6.5199, -6.4956, -6.6209, -6.5276, -6.4888, -6.3958, -6.0616,\n",
       "         -6.5949, -6.5894, -6.5921, -6.8524, -6.5291, -6.3920, -6.3980, -6.5983,\n",
       "         -6.4532, -6.5348, -6.3558, -6.8128, -6.9180, -6.8827, -6.3121, -6.4817,\n",
       "         -6.2493, -5.9764, -6.4976, -6.3953, -6.3256, -6.4906, -6.4465, -6.4609,\n",
       "         -6.4806, -6.5503, -6.4876, -6.6933, -6.8918, -6.2526, -6.2927, -6.2854,\n",
       "         -6.1506, -6.3988, -6.2176, -6.5855, -6.4965, -6.6835, -6.9252, -6.0704,\n",
       "         -6.0203, -6.5563, -6.7706, -6.6958, -6.8083, -6.1754, -6.4119, -6.5912,\n",
       "         -6.3655, -6.2998, -6.5022, -6.9206, -6.8900, -6.4828, -6.0671, -6.7771,\n",
       "         -6.4960, -6.1734, -6.4537, -6.8111, -6.6125, -6.2403, -5.8924, -5.8626,\n",
       "         -6.7102, -6.3734, -6.0460, -6.4791, -6.1938, -6.5017, -6.4295, -6.1088,\n",
       "         -6.5542, -6.3861, -6.4406, -6.4067, -6.3259, -6.6968, -6.4223, -6.4167,\n",
       "         -6.8009, -6.9173, -6.7943,  0.4330]]), end_logits=tensor([[ 0.4805, -6.1511, -6.3324, -6.3226, -6.5381, -6.2009, -5.6752, -5.8971,\n",
       "          0.4804, -6.6790, -5.9549, -5.9327, -6.1159, -6.4644, -6.3420, -5.7211,\n",
       "         -6.1893, -6.5015, -6.1449, -6.3618, -6.1765, -6.6545, -6.6317, -6.2868,\n",
       "         -5.8576, -6.1906, -6.4277, -4.9729, -6.0199, -6.4842, -5.5099, -6.1228,\n",
       "         -6.2080, -6.6433, -5.8441, -6.1135, -5.8164, -4.2625, -5.3590, -6.5712,\n",
       "         -5.9673, -5.5966, -6.5400, -6.4601, -6.4326, -6.4259, -6.3446, -6.3971,\n",
       "         -6.6974, -6.2597, -4.3284, -6.2369, -5.5712, -6.3332, -6.4360, -4.3804,\n",
       "         -5.8595, -6.3914, -6.4878, -6.5015, -6.4440, -6.2109, -6.4639, -6.7257,\n",
       "         -6.1811, -6.4323, -6.4820, -5.7685, -6.3307, -6.3938, -6.0110, -6.4603,\n",
       "         -6.4896, -5.9837, -6.1257, -5.7202, -6.3990, -4.7391, -4.8910, -4.3154,\n",
       "         -5.8585, -4.5947, -6.2359, -6.3387, -6.1300, -5.6936, -2.9754, -6.1367,\n",
       "         -3.8978, -5.3208,  0.8653,  0.3756, -2.9306, -1.8721,  7.0170,  3.3102,\n",
       "         -3.2118, -5.7196, -6.7085, -5.7125, -6.0071, -6.8873, -6.1844, -5.1290,\n",
       "         -5.8358, -6.6980, -6.2334, -6.2174, -6.1912, -5.3165, -5.4423, -6.0181,\n",
       "         -6.6688, -6.2117, -6.1966, -6.1872, -6.0934, -3.7489, -6.1414, -6.1919,\n",
       "         -5.7713, -2.6390, -1.1324, -6.6566, -6.6163, -5.0174, -6.2843, -6.4908,\n",
       "         -6.4870, -6.3928, -6.3008, -6.2548, -6.1340, -5.9223, -6.2677, -6.2133,\n",
       "         -5.3788, -6.3396, -6.4147, -6.4910, -6.1681, -6.3640, -5.0706, -4.9401,\n",
       "         -6.2255, -6.4634, -6.5102, -6.4342, -6.3848, -6.3730, -6.2851, -6.0310,\n",
       "         -6.2700, -6.3072, -6.1752, -6.3636, -6.1117, -5.1425, -4.9444, -6.5445,\n",
       "         -6.1229, -6.4021, -6.1090, -6.4514, -6.4648, -6.0639, -6.2786, -6.3346,\n",
       "         -5.8559, -5.7423, -6.4098, -6.5981, -6.0767, -6.3369, -6.3526, -6.1903,\n",
       "         -6.4182, -5.5438, -5.6172, -6.2287, -6.4939, -6.5879, -5.8938, -6.2659,\n",
       "         -6.2221, -6.3756, -6.3867, -5.9578, -6.3317, -6.1612, -6.4256, -6.6014,\n",
       "         -6.0839, -6.3287, -6.1436, -5.4781, -6.2903, -6.5255, -6.4874, -6.2616,\n",
       "         -6.3460, -6.4045, -6.4415, -5.8233, -5.5112, -5.5538, -6.4111, -6.4366,\n",
       "         -6.4288, -6.5506, -5.9945, -6.4371, -6.5594, -5.9184, -6.3909, -6.4334,\n",
       "         -6.3639, -6.3627, -6.3594, -5.7802, -5.6799, -6.4257, -6.5046, -6.4640,\n",
       "         -6.4818, -6.4332, -6.3280, -5.9804, -6.3509, -5.4072, -5.5296, -6.2916,\n",
       "         -6.4909, -6.1627, -5.9718, -5.5601, -5.8389, -6.5280, -6.2538, -6.2577,\n",
       "         -6.3034, -6.3593, -6.2032, -5.4848, -5.7312, -6.2558, -6.4351, -5.6336,\n",
       "         -5.9109, -6.4046, -6.2996, -5.5115, -6.0341, -6.1993, -6.6342, -6.6034,\n",
       "         -5.7019, -6.3130, -6.4584, -6.1487, -6.3693, -6.0379, -6.3754, -6.4349,\n",
       "         -4.7375, -6.3544, -6.3074, -6.4072, -6.3705, -6.0132, -6.4152, -6.4565,\n",
       "         -5.6942, -5.4910, -5.7620,  0.4804]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e0cf16e-e65e-4dbd-a9a7-626e7b1516b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(95))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eed2161c-bb10-4245-8a2c-62be2ed387b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18607,  2118, 16012, 24229,  2015])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "answer_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccc1c323-1b76-473a-81c0-69bd1a7ab90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  rutgers university biochemists\n"
     ]
    }
   ],
   "source": [
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23e8b999-8f13-47a1-8250-cab93c023205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96cb9446-576e-4166-bc80-9853aefda005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./smaller_bert_finetuned\\runs\\Jun22_16-09-54_alfrizz,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=5,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=./smaller_bert_finetuned,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['mlflow'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./smaller_bert_finetuned,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8163c51c-1724-4570-b838-6b4093a61e46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x274b8f8bf70>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "tokenized_datasets = []\n",
    "\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d928e249-2aaa-40c9-b3c0-7aac99a0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98b817ae-9314-484f-94c0-9f3c07106629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "774792d6-ea1d-488d-9030-75fade5c2712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "dataset = load_dataset('emotion', trust_remote_code=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d280e72a-80ef-4fc7-b518-8d72e086dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 3154.58 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode your dataset\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "emotions_encoded = dataset.map(encode, batched=True)\n",
    "\n",
    "emotions_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de80aee8-81ce-4070-8fd1-d9a544a9b46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x274b55c7b20>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n",
    "    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee495e68-4020-4df7-b410-a52280e040b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys of the first example in the training dataset\n",
    "print(emotions_encoded[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a661d92a-9e65-4877-8cdf-8a156b75494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for example in emotions_encoded[\"train\"]:\n",
    "    unique_labels.add(example[\"label\"])\n",
    "print(f\"Unique labels: {sorted(list(unique_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acffd65a-a5ff-45ca-9b28-cb7b89a65b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop to fine-tune the model\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1221a882-ffd6-40ad-84c0-df2bf9c33c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2009,  1005,  1055,  2601,  1998, 16373,  2648,   102],\n",
       "        [  101,  1045,  2293, 18134,   999,   102,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "597455a9-ff17-434d-9e61-b902021d03d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3346,  0.1305,  0.2033, -0.0921, -0.2327, -0.4993],\n",
       "        [ 0.3221, -0.0413,  0.0839,  0.0180, -0.2351, -0.3689]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e72e38d8-6a31-4441-97ba-33476121482d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain class labels from raw predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fd845ce-4035-4633-bbd9-ce458944a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input Text 1: It's dark and rainy outside\n",
      "Predicted Label: 0\n",
      "\n",
      " Input Text 2: I love penguins!\n",
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "959b5687-013e-4ada-8145-323df266236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "test_examples = [{'text': 'I love this product!', 'label': 1},\n",
    "                 {'text': 'The service was terrible.', 'label': 0},\n",
    "                 {'text': 'This movie is amazing.', 'label': 1},\n",
    "                 {'text': \"I'm disappointed with the quality.\", 'label': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04be27ae-994b-496d-999f-55115b8ad3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998855590820312},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996507167816162},\n",
       " {'label': 'POSITIVE', 'score': 0.9998838901519775},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997726082801819}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a1e602e-ef39-4c73-a089-2bba48d0b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab1baf2e-7d14-416a-9838-454af1507e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3a8c898-4d6e-4403-9aa9-538ad6823bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assuming true_labels and predicted_labels are defined\n",
    "result = accuracy_score(true_labels, predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f95310d6-52fe-4d68-be6e-cb76a4a3be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.20k/4.20k [00:00<00:00, 1.07MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08298bec-b7c1-4d01-92cd-75964b962d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.55k/7.55k [00:00<00:00, 3.89MB/s]\n",
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.36k/7.36k [00:00<00:00, 7.69MB/s]\n",
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.77k/6.77k [00:00<00:00, 6.53MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "\n",
      "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n",
      "Precision = TP / (TP + FP)\n",
      "where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
      "\n",
      "\n",
      "Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\n",
      "Recall = TP / (TP + FN)\n",
      "Where TP is the true positives and FN is the false negatives.\n",
      "\n",
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the accuracy, precision, recall and F1 score metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "385dad1c-cea7-4381-b19b-0d0f198e0709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x274b90ea6b0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Pass the examples to the pipeline, and obtain a list predicted labels\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69cf56cc-d82f-43c4-abe8-7151fcd42113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"Fantastic hotel, exceeded expectations!\",\n",
    "    \"Quiet despite central location, great stay.\",\n",
    "    \"Friendly staff, welcoming atmosphere.\",\n",
    "    \"Spacious, comfy room‚Äîa perfect retreat.\",\n",
    "    \"Cleanliness could improve, overall decent stay.\",\n",
    "      \"Disappointing stay, noisy and unclean room.\",\n",
    "    \"Terrible service, unfriendly staff, won't return.\"\n",
    "]\n",
    "\n",
    "test_labels = [1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ba49785c-9eff-4c15-a056-5768d5dca1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sentiment_analysis([example for example in test_examples])\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "caac1d45-969b-4a01-b29d-70c105e8bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.888888888888889}\n",
      "{'precision': 0.8}\n",
      "{'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(f1.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=test_labels, predictions=predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25abff60-d03c-403a-823e-b7a63f887ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "242dadc3-aaaa-45e6-b876-3baeff5a0989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11297, 11257,   905,   326,   416, 25054,   220]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Current trends show that by 2030 \"\n",
    "\n",
    "# Encode the prompt, generate text and decode it\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7e52113-d928-4378-b974-91b02f1ea68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11297, 11257,   905,   326,   416, 25054,   220,  1849,  1169,  1271,\n",
       "           286,   661,  2877,   287,  8098,   481,   307,   379,   663,  9016,\n",
       "          1241,  1201,   262,  8069,    82,    13,   198,   464,  1271,   286,\n",
       "           661,  2877,   287,  8098,   287,   262,  3482,   468,  9292,   416,\n",
       "           517,   621,  2063,  1201,   262,  8069,    82,    13,   198,   464]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(prompt_ids, max_length=50)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48beed3e-4f7b-458c-a4be-bdf80b607cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Current trends show that by 2030 ¬†the number of people living in poverty will be at its lowest level since the 1970s.\n",
      "The number of people living in poverty in the UK has fallen by more than half since the 1970s.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "625510fd-e877-4f02-91cb-c86754fc900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.46k/8.46k [00:00<00:00, 2.77MB/s]\n",
      "Using pad_token, but it is not set yet.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:01<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  3386.8988407452903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id=model_name,\n",
    "                             predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8731511c-2b79-4b53-b346-4f0311d5cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\users\\alienware\\appdata\\roaming\\python\\python310\\site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from rouge_score) (1.23.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from nltk->rouge_score) (8.1.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from nltk->rouge_score) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\alienware\\miniconda3\\envs\\py310\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=61ed15457e821f5c45cc71d084f36b3a9f8cc7b523392a9bd81b42c5c92bd488\n",
      "  Stored in directory: C:\\Users\\Alienware\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-620hvy0o\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "ROUGE results:  {'rouge1': 0.7719298245614034, 'rouge2': 0.6181818181818182, 'rougeL': 0.736842105263158, 'rougeLsum': 0.736842105263158}\n"
     ]
    }
   ],
   "source": [
    "# ! pip install rouge_score\n",
    "\n",
    "# Load the rouge metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions,references=references)\n",
    "print(\"ROUGE results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "545c4aa1-4079-4cd1-b577-16624aed5ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.93k/6.93k [00:00<00:00, 2.31MB/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteor:  {'meteor': 0.5350702240481536}\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
    "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=llm_outputs, references=references)\n",
    "print(\"Meteor: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "118e40f7-ead6-48ba-89cc-92381bee75ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.67k/5.67k [00:00<00:00, 5.71MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM results:  {'exact_match': 0.3333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(\"EM results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "28dc18bd-fae3-4578-bbdc-002f53b694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_1 = \"Hola, ¬øc√≥mo est√°s?\"\n",
    "\n",
    "reference_1 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
    "     ]\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¬øc√≥mo est√°s?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd083c-774c-4f2b-919e-c4a7cf655fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why there are multiple reference sentences for each input sentence is because of the inherent ambiguity and variability in translation. There can be several equally correct translations for a given sentence, depending on factors like context, tone, and style. By providing multiple reference translations, we can capture some of this variability and get a more robust estimate of the model‚Äôs performance.\n",
    "\n",
    "# In the code you posted, the BLEU score is being calculated for the translations. The BLEU score is a metric that measures the quality of a translation by comparing it to one or more reference translations. It does this by counting the number of n-gram matches between the translation and the reference(s), and then normalizing by the total number of n-grams in the translation. The more the translation resembles the reference(s), the higher the BLEU score will be.\n",
    "\n",
    "# In your example, the first input sentence ‚ÄúHola, ¬øc√≥mo est√°s?‚Äù is translated and then the translation is compared to two reference translations: ‚ÄúHello, how are you?‚Äù and ‚ÄúHi, how are you?‚Äù. The BLEU score is then computed for this translation.\n",
    "\n",
    "# The same process is repeated for the second set of input sentences and references. The final BLEU score is a measure of how well the translations match the reference translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "49442a73-3f15-403b-9025-7b088cbfb875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.94k/5.94k [00:00<00:00, 5.68MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 1.02MB/s]                                                                    \n",
      "Downloading extra modules: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.34k/3.34k [00:00<00:00, 3.35MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: Hey, how are you?\n",
      "{'bleu': 0.7598356856515925, 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the first input sentence\n",
    "translated_output = translator(input_sentence_1)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "# Calculate BLEU metric for translation quality\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "824753e7-3f19-493b-b701-15d12927718b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hey, how are you?'},\n",
       " {'translation_text': \"I'm great, thanks.\"}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translated_outputs = translator(input_sentences_2)\n",
    "\n",
    "translated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dd4e780d-2c9f-4a28-bec9-3bf185cb4bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey, how are you?', \"I'm great, thanks.\"]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "beb7ab11-2129-46c5-823e-ca6874125b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.8627788640890415, 'precisions': [0.9090909090909091, 0.8888888888888888, 0.8571428571428571, 0.8], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 11, 'reference_length': 11}\n"
     ]
    }
   ],
   "source": [
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abb9c7a-cd48-4bcd-86e6-0073e6c38e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alienware\\.cache\\huggingface\\hub\\models--sshleifer--tiny-gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 2)\n",
       "      (wpe): Embedding(1024, 2)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-1): 2 x GPT2Block(\n",
       "          (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=2, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install trl\n",
    "from trl import PPOTrainer, PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0787e0ec-e281-424f-93d2-f4f279cac82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 2)\n",
       "      (wpe): Embedding(1024, 2)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-1): 2 x GPT2Block(\n",
       "          (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=2, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a reference model\n",
    "\n",
    "# When you call create_reference_model(model), it creates a copy of the model and freezes its parameters. This means that the weights of the reference model will not be updated during training.\n",
    "# This reference model is then used to compare with the updated model at each step of the training process. The idea is to ensure that the policy (i.e., the behavior of the model) does not change too drastically from one update to the next\n",
    "\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47816608-d588-4f58-83ef-b6ab1aa9bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if the parameters of a model are frozen, you can iterate over the parameters and check their requires_grad attribute. Here‚Äôs a small function that can do this:\n",
    "\n",
    "def check_if_frozen(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is not frozen\")\n",
    "        else:\n",
    "            print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d672e16-34e3-4e81-90d2-140eb381a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model.transformer.wte.weight is not frozen\n",
      "pretrained_model.transformer.wpe.weight is not frozen\n",
      "pretrained_model.transformer.h.0.ln_1.weight is not frozen\n",
      "pretrained_model.transformer.h.0.ln_1.bias is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.weight is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.bias is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.h.0.ln_2.weight is not frozen\n",
      "pretrained_model.transformer.h.0.ln_2.bias is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.weight is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.bias is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.h.1.ln_1.weight is not frozen\n",
      "pretrained_model.transformer.h.1.ln_1.bias is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.weight is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.bias is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.h.1.ln_2.weight is not frozen\n",
      "pretrained_model.transformer.h.1.ln_2.bias is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.weight is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.bias is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.ln_f.weight is not frozen\n",
      "pretrained_model.transformer.ln_f.bias is not frozen\n",
      "v_head.summary.weight is not frozen\n",
      "v_head.summary.bias is not frozen\n"
     ]
    }
   ],
   "source": [
    "check_if_frozen(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e860b1-5b6e-4241-8732-149251c6f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model.transformer.wte.weight is frozen\n",
      "pretrained_model.transformer.wpe.weight is frozen\n",
      "pretrained_model.transformer.h.0.ln_1.weight is frozen\n",
      "pretrained_model.transformer.h.0.ln_1.bias is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.weight is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.bias is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.bias is frozen\n",
      "pretrained_model.transformer.h.0.ln_2.weight is frozen\n",
      "pretrained_model.transformer.h.0.ln_2.bias is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.weight is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.bias is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.bias is frozen\n",
      "pretrained_model.transformer.h.1.ln_1.weight is frozen\n",
      "pretrained_model.transformer.h.1.ln_1.bias is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.weight is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.bias is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.bias is frozen\n",
      "pretrained_model.transformer.h.1.ln_2.weight is frozen\n",
      "pretrained_model.transformer.h.1.ln_2.bias is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.weight is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.bias is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.bias is frozen\n",
      "pretrained_model.transformer.ln_f.weight is frozen\n",
      "pretrained_model.transformer.ln_f.bias is frozen\n",
      "v_head.summary.weight is frozen\n",
      "v_head.summary.bias is frozen\n"
     ]
    }
   ],
   "source": [
    "check_if_frozen(model_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d53836-3a01-4e6d-b898-a5e2be57acc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='sshleifer/tiny-gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa787b53-013d-4599-95c6-16da82810d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with=None, task_name=None, model_name='gpt2', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=1, forward_batch_size=None, mini_batch_size=1, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, gradient_checkpointing=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=1, global_backward_batch_size=None, global_batch_size=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize trainer configuration\n",
    "# this code is setting up a configuration for a PPO trainer with specific batch and mini-batch sizes. This configuration would be used when training a model using the PPO algorithm. \n",
    "\n",
    "ppo_config = PPOConfig(batch_size=1, mini_batch_size=1)\n",
    "\n",
    "ppo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1863c35b-4fb9-443f-b302-c44d4be31764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:266: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<trl.trainer.ppo_trainer.PPOTrainer at 0x23d8cdd4ee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOTrainer instance\n",
    "# this line of code is setting up a PPO trainer with a specific configuration, model, reference model, and tokenizer. The trainer can then be used to train the model using the PPO algorithm.Typically, the trainer would have a method like train() that you can call to start the training process. The training process involves repeatedly sampling data, using the data to update the model, and then evaluating the performance of the model. The goal is to improve the model‚Äôs performance on some task, such as generating text. The PPO algorithm is particularly well-suited to tasks where the data is sequential or temporal in nature. It‚Äôs also known for its stability and efficiency, which makes it a popular choice for many reinforcement learning tasks.\n",
    "\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "\n",
    "ppo_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "771fbe5d-f5d9-4976-8b43-bddbe8a6dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10019,   614,    11,   314,   220]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Next year, I \"\n",
    "\n",
    "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2792a051-b10a-4f39-a1ed-1c49a1d8e5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28936, 37056, 32947, 46773, 16886, 34670, 46499, 16330, 19193, 27667,\n",
       "          7433, 39579, 29584, 47000, 37807, 45925, 34793, 12861, 38280, 30797]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code is using a pretrained language model to generate a response to a given prompt. The response is calculated by feeding the encoded input into the model and then decoding the model‚Äôs output back into text. The response represents what the model thinks is the most likely continuation of the input prompt. The exact details of how the response is calculated depend on the specifics of the model and the respond_to_batch function. \n",
    "\n",
    "from trl.core import respond_to_batch\n",
    "\n",
    "response  = respond_to_batch(model, input) # function to generate a response from the model. The function takes the model and the encoded input as arguments.\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091fd2f1-bb64-4b24-a6d6-288f9d8e0110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a more complex scenario, you might want to design a reward function that gives higher rewards for better responses and lower rewards for worse ones. This would require a way to evaluate the quality of the responses, which could be based on various factors such as the relevance of the response to the input, the grammatical correctness of the response, etc. This is typically the challenging part in reinforcement learning - designing a good reward function.\n",
    "\n",
    "import torch\n",
    "reward = [torch.tensor(1.0)]\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf05e00-7038-4045-a53b-1e5ca26448af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1304: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  std_scores = data[\"scores\"].std()\n",
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1331: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1334: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective/kl': 0.0,\n",
       " 'objective/kl_dist': 0.0,\n",
       " 'objective/logprobs': array([[-10.803115 , -10.834713 , -10.795107 , -10.808006 , -10.844018 ,\n",
       "         -10.866432 , -10.825367 , -10.817469 , -10.893092 , -10.837414 ,\n",
       "         -10.79999  , -10.785171 , -10.854528 , -10.8445015, -10.844265 ,\n",
       "         -10.780741 , -10.829426 , -10.827827 , -10.83161  , -10.8276   ,\n",
       "         -10.8046465, -10.827068 , -10.825418 , -10.84352  ]],\n",
       "       dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-10.803115 , -10.834713 , -10.795107 , -10.808006 , -10.844018 ,\n",
       "         -10.866432 , -10.825367 , -10.817469 , -10.893092 , -10.837414 ,\n",
       "         -10.79999  , -10.785171 , -10.854528 , -10.8445015, -10.844265 ,\n",
       "         -10.780741 , -10.829426 , -10.827827 , -10.83161  , -10.8276   ,\n",
       "         -10.8046465, -10.827068 , -10.825418 , -10.84352  ]],\n",
       "       dtype=float32),\n",
       " 'objective/kl_coef': 0.2,\n",
       " 'objective/entropy': 216.61012268066406,\n",
       " 'ppo/mean_non_score_reward': 0.0,\n",
       " 'ppo/mean_scores': 1.0,\n",
       " 'ppo/std_scores': nan,\n",
       " 'tokens/queries_len_mean': 5.0,\n",
       " 'tokens/queries_len_std': nan,\n",
       " 'tokens/queries_dist': 5.0,\n",
       " 'tokens/responses_len_mean': 20.0,\n",
       " 'tokens/responses_len_std': nan,\n",
       " 'tokens/responses_dist': 20.0,\n",
       " 'ppo/loss/policy': -0.00012423991574905813,\n",
       " 'ppo/loss/value': 0.1909959316253662,\n",
       " 'ppo/loss/total': 0.018975354731082916,\n",
       " 'ppo/policy/entropy': 10.824566841125488,\n",
       " 'ppo/policy/approxkl': 2.6794925389594937e-08,\n",
       " 'ppo/policy/policykl': 4.855394217884168e-05,\n",
       " 'ppo/policy/clipfrac': 0.0,\n",
       " 'ppo/policy/advantages': array([-0.11175462, -0.05274466,  0.00937105,  0.07475607, -1.3300463 ,\n",
       "         0.35257792, -1.6408085 ,  0.22990203,  0.48310402, -1.4952306 ,\n",
       "         0.5661799 ,  0.658926  ,  0.7591126 , -1.0450048 , -1.0255854 ,\n",
       "         0.8841015 ,  0.99537075, -0.36810705, -0.755042  ,  1.1804726 ,\n",
       "         1.2772218 , -0.6233227 , -0.5826141 ,  1.4787922 , -0.11175462,\n",
       "        -0.05274466,  0.00937105,  0.07475607, -1.3300463 ,  0.35257792,\n",
       "        -1.6408085 ,  0.22990203,  0.48310402, -1.4952306 ,  0.5661799 ,\n",
       "         0.658926  ,  0.7591126 , -1.0450048 , -1.0255854 ,  0.8841015 ,\n",
       "         0.99537075, -0.36810705, -0.755042  ,  1.1804726 ,  1.2772218 ,\n",
       "        -0.6233227 , -0.5826141 ,  1.4787922 , -0.11175462, -0.05274466,\n",
       "         0.00937105,  0.07475607, -1.3300463 ,  0.35257792, -1.6408085 ,\n",
       "         0.22990203,  0.48310402, -1.4952306 ,  0.5661799 ,  0.658926  ,\n",
       "         0.7591126 , -1.0450048 , -1.0255854 ,  0.8841015 ,  0.99537075,\n",
       "        -0.36810705, -0.755042  ,  1.1804726 ,  1.2772218 , -0.6233227 ,\n",
       "        -0.5826141 ,  1.4787922 , -0.11175462, -0.05274466,  0.00937105,\n",
       "         0.07475607, -1.3300463 ,  0.35257792, -1.6408085 ,  0.22990203,\n",
       "         0.48310402, -1.4952306 ,  0.5661799 ,  0.658926  ,  0.7591126 ,\n",
       "        -1.0450048 , -1.0255854 ,  0.8841015 ,  0.99537075, -0.36810705,\n",
       "        -0.755042  ,  1.1804726 ,  1.2772218 , -0.6233227 , -0.5826141 ,\n",
       "         1.4787922 ], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': 0.0,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99999905,\n",
       "        1.000001  , 1.000063  , 1.000001  , 0.9995357 , 1.0000572 ,\n",
       "        0.99994373, 1.0000172 , 1.000062  , 0.99994373, 1.0000544 ,\n",
       "        1.0000534 , 1.0000572 , 0.99987316, 0.9998684 , 1.0000534 ,\n",
       "        1.0000019 , 0.9998493 , 0.99991417, 1.0000563 , 1.0000467 ,\n",
       "        0.99994373, 0.9999447 , 1.0000601 , 0.99999714, 1.        ,\n",
       "        1.0001249 , 1.000001  , 0.9990887 , 1.0001154 , 0.9998884 ,\n",
       "        1.0000315 , 1.000124  , 0.99988747, 1.0001097 , 1.0001078 ,\n",
       "        1.0001135 , 0.9997483 , 0.99973875, 1.0001087 , 1.0000019 ,\n",
       "        0.99970347, 0.9998293 , 1.0001125 , 1.0000925 , 0.99988747,\n",
       "        0.9998894 , 1.0001211 , 0.9999962 , 1.000001  , 1.000185  ,\n",
       "        1.0000019 , 0.99865717, 1.0001736 , 0.99983215, 1.0000458 ,\n",
       "        1.000186  , 0.99983025, 1.000165  , 1.0001612 , 1.0001707 ,\n",
       "        0.9996243 , 0.99961096, 1.0001621 , 1.0000019 , 0.9995624 ,\n",
       "        0.9997454 , 1.0001678 , 1.0001383 , 0.9998312 , 0.99983406,\n",
       "        1.0001812 ], dtype=float32),\n",
       " 'ppo/returns/mean': 0.7319895029067993,\n",
       " 'ppo/returns/var': 0.023072266951203346,\n",
       " 'ppo/val/vpred': 0.24820883572101593,\n",
       " 'ppo/val/error': 0.3819918632507324,\n",
       " 'ppo/val/clipfrac': 0.0,\n",
       " 'ppo/val/mean': 0.2467016726732254,\n",
       " 'ppo/val/var': 0.1319371908903122,\n",
       " 'ppo/val/var_explained': -15.55632209777832,\n",
       " 'ppo/learning_rate': 1.41e-05,\n",
       " 'time/ppo/forward_pass': 0.024001359939575195,\n",
       " 'time/ppo/compute_rewards': 0.001998424530029297,\n",
       " 'time/ppo/compute_advantages': 0.003998994827270508,\n",
       " 'time/ppo/optimize_step': 0.2110004425048828,\n",
       " 'time/ppo/calc_stats': 0.01000070571899414,\n",
       " 'time/ppo/total': 0.2519989013671875}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LLM for one step with PPO\n",
    "# while step() is used for a single step of training, train() is used for full-scale training over multiple epochs. The code is likely a simplified example or a debugging scenario where only a single step of training is being performed. For training a model to completion, you would generally use a train() function or similar.\n",
    "\n",
    "train_stats = ppo_trainer.step([input[0]], [response[0]], reward) # The step function is used to perform one step of training, where the model‚Äôs parameters are updated to maximize the expected reward.\n",
    "\n",
    "train_stats\n",
    "\n",
    "# The train_stats dictionary contains various statistics and metrics that are calculated during the training step. Here‚Äôs a brief explanation of some of the key metrics:\n",
    "# ‚Äòobjective/kl‚Äô: This is the Kullback-Leibler (KL) divergence, which measures how one probability distribution diverges from a second, expected probability distribution. In this case, it‚Äôs 0.0, indicating no divergence.\n",
    "# ‚Äòobjective/logprobs‚Äô: These are the log probabilities of the actions taken by the model. They are used in the calculation of the policy gradient.\n",
    "# ‚Äòppo/mean_scores‚Äô: This is the mean of the scores (rewards) obtained during the training step.\n",
    "# ‚Äòtokens/queries_len_mean‚Äô: This is the average length of the queries processed in the training step.\n",
    "# ‚Äòppo/loss/policy‚Äô, ‚Äòppo/loss/value‚Äô, ‚Äòppo/loss/total‚Äô: These are the losses for the policy, value function, and the total loss respectively. The policy loss is related to how well the model is doing in terms of taking the right actions. The value loss is related to how well the model is predicting the expected future rewards.\n",
    "# ‚Äòppo/policy/entropy‚Äô: This is the entropy of the policy. It‚Äôs a measure of the randomness of the policy. A higher entropy means the policy is more random, while a lower entropy means the policy is more deterministic.\n",
    "# ‚Äòppo/returns/mean‚Äô: This is the mean of the returns (sum of rewards) obtained during the training step.\n",
    "# ‚Äòppo/val/vpred‚Äô: This is the predicted value of the state by the model.\n",
    "# ‚Äòtime/ppo/total‚Äô: This is the total time taken for the training step.\n",
    "# The warnings about degrees of freedom being less than or equal to 0 are due to the standard deviation (std()) function being called on a dataset with insufficient size. This can happen when the batch size or mini-batch size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16a4396e-4852-44e9-bb28-81079b28b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 training steps\n",
      "20 training steps\n",
      "30 training steps\n",
      "40 training steps\n",
      "50 training steps\n",
      "60 training steps\n",
      "70 training steps\n",
      "80 training steps\n",
      "90 training steps\n",
      "100 training steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective/kl': -0.7741384506225586,\n",
       " 'objective/kl_dist': -0.7741384506225586,\n",
       " 'objective/logprobs': array([[-10.87762 , -10.886727, -10.792533, -10.858321, -10.956419,\n",
       "         -10.894209, -10.930903, -10.934779, -10.861883, -10.944152,\n",
       "         -10.680028, -10.667747, -10.893733, -10.955993, -10.953577,\n",
       "         -10.661098, -10.710297, -10.921848, -10.909364, -10.898013,\n",
       "         -10.92437 , -10.918778, -10.887274, -10.879778]], dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-10.803115 , -10.834713 , -10.795107 , -10.808006 , -10.844018 ,\n",
       "         -10.866432 , -10.825367 , -10.817469 , -10.893092 , -10.837414 ,\n",
       "         -10.79999  , -10.785171 , -10.854528 , -10.8445015, -10.844265 ,\n",
       "         -10.780741 , -10.829426 , -10.827827 , -10.83161  , -10.8276   ,\n",
       "         -10.8046465, -10.827068 , -10.825418 , -10.84352  ]],\n",
       "       dtype=float32),\n",
       " 'objective/kl_coef': 0.19808520577927524,\n",
       " 'objective/entropy': 217.38421630859375,\n",
       " 'ppo/mean_non_score_reward': 0.0076672681607306,\n",
       " 'ppo/mean_scores': 1.0,\n",
       " 'ppo/std_scores': nan,\n",
       " 'tokens/queries_len_mean': 5.0,\n",
       " 'tokens/queries_len_std': nan,\n",
       " 'tokens/queries_dist': 5.0,\n",
       " 'tokens/responses_len_mean': 20.0,\n",
       " 'tokens/responses_len_std': nan,\n",
       " 'tokens/responses_dist': 20.0,\n",
       " 'ppo/loss/policy': -6.845220923423767e-05,\n",
       " 'ppo/loss/value': 0.10534877330064774,\n",
       " 'ppo/loss/total': 0.010466424748301506,\n",
       " 'ppo/policy/entropy': 10.824481010437012,\n",
       " 'ppo/policy/approxkl': 5.636843436462868e-09,\n",
       " 'ppo/policy/policykl': 4.53710526926443e-05,\n",
       " 'ppo/policy/clipfrac': 0.0,\n",
       " 'ppo/policy/advantages': array([ 1.1299566 ,  1.2458622 ,  1.3678682 ,  1.4962957 , -0.7448365 ,\n",
       "        -0.7862224 , -0.80603   , -0.85414827, -0.893935  , -0.89718604,\n",
       "         1.5613223 ,  1.77968   , -0.4801579 , -0.5039644 , -0.547142  ,\n",
       "         1.9234475 ,  2.153075  , -0.10088075, -0.11825637, -0.11694539,\n",
       "        -0.1149914 , -0.14715558, -0.15798377, -0.14768901,  1.1299566 ,\n",
       "         1.2458622 ,  1.3678682 ,  1.4962957 , -0.7448365 , -0.7862224 ,\n",
       "        -0.80603   , -0.85414827, -0.893935  , -0.89718604,  1.5613223 ,\n",
       "         1.77968   , -0.4801579 , -0.5039644 , -0.547142  ,  1.9234475 ,\n",
       "         2.153075  , -0.10088075, -0.11825637, -0.11694539, -0.1149914 ,\n",
       "        -0.14715558, -0.15798377, -0.14768901,  1.1299566 ,  1.2458622 ,\n",
       "         1.3678682 ,  1.4962957 , -0.7448365 , -0.7862224 , -0.80603   ,\n",
       "        -0.85414827, -0.893935  , -0.89718604,  1.5613223 ,  1.77968   ,\n",
       "        -0.4801579 , -0.5039644 , -0.547142  ,  1.9234475 ,  2.153075  ,\n",
       "        -0.10088075, -0.11825637, -0.11694539, -0.1149914 , -0.14715558,\n",
       "        -0.15798377, -0.14768901,  1.1299566 ,  1.2458622 ,  1.3678682 ,\n",
       "         1.4962957 , -0.7448365 , -0.7862224 , -0.80603   , -0.85414827,\n",
       "        -0.893935  , -0.89718604,  1.5613223 ,  1.77968   , -0.4801579 ,\n",
       "        -0.5039644 , -0.547142  ,  1.9234475 ,  2.153075  , -0.10088075,\n",
       "        -0.11825637, -0.11694539, -0.1149914 , -0.14715558, -0.15798377,\n",
       "        -0.14768901], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': -5.9604645663569045e-09,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.9999695 ,\n",
       "        0.99997044, 1.000001  , 0.99997044, 0.9999418 , 0.9999409 ,\n",
       "        0.9999428 , 0.9999466 , 0.99992657, 0.9999409 , 1.0000639 ,\n",
       "        1.0000668 , 0.9999409 , 0.9999409 , 0.9999418 , 1.0000648 ,\n",
       "        1.0000639 , 0.9999542 , 0.9999542 , 0.9999571 , 0.99995327,\n",
       "        0.99994564, 0.999959  , 0.9999504 , 0.99993896, 0.9999399 ,\n",
       "        1.        , 0.9999399 , 0.9998846 , 0.9998808 , 0.99988556,\n",
       "        0.9998932 , 0.99985313, 0.99988174, 1.0001268 , 1.0001345 ,\n",
       "        0.99988174, 0.99988174, 0.99988365, 1.0001287 , 1.0001278 ,\n",
       "        0.99990845, 0.99990845, 0.99991417, 0.9999075 , 0.9998913 ,\n",
       "        0.99991703, 0.9999018 , 0.9999075 , 0.9999094 , 1.000001  ,\n",
       "        0.9999113 , 0.99982643, 0.9998207 , 0.99982834, 0.9998388 ,\n",
       "        0.99977875, 0.99982166, 1.0001917 , 1.0002003 , 0.99982166,\n",
       "        0.9998236 , 0.9998245 , 1.0001917 , 1.0001917 , 0.9998636 ,\n",
       "        0.9998627 , 0.9998703 , 0.9998617 , 0.9998369 , 0.99987507,\n",
       "        0.9998512 ], dtype=float32),\n",
       " 'ppo/returns/mean': 0.9016730189323425,\n",
       " 'ppo/returns/var': 0.00978412851691246,\n",
       " 'ppo/val/vpred': 0.5620213747024536,\n",
       " 'ppo/val/error': 0.21069754660129547,\n",
       " 'ppo/val/clipfrac': 0.0,\n",
       " 'ppo/val/mean': 0.5620013475418091,\n",
       " 'ppo/val/var': 0.09609532356262207,\n",
       " 'ppo/val/var_explained': -20.534626007080078,\n",
       " 'ppo/learning_rate': 1.41e-05,\n",
       " 'time/ppo/forward_pass': 0.02499985694885254,\n",
       " 'time/ppo/compute_rewards': 0.0,\n",
       " 'time/ppo/compute_advantages': 0.005000591278076172,\n",
       " 'time/ppo/optimize_step': 0.21000146865844727,\n",
       " 'time/ppo/calc_stats': 0.004000663757324219,\n",
       " 'time/ppo/total': 0.2440025806427002}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training steps\n",
    "num_steps = 100\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, num_steps+1):\n",
    "    # Generate input and response here...\n",
    "    # ...\n",
    "    reward = [torch.tensor(1.0)]\n",
    "    train_stats = ppo_trainer.step([input[0]], [response[0]], reward)\n",
    "    if i % 10 == 0:\n",
    "        print(i, 'training steps')\n",
    "    \n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8163c3a8-bf7e-4b63-9b65-f63fd3a6bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
    "\n",
    "emp_1 = [\"Everyone in the team adores him\",\n",
    "           \"He is a true genius, pure talent\"]\n",
    "emp_2 = [\"Nobody in the team likes him\",\n",
    "           \"He is a useless 'good-for-nothing'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "238f9f82-b535-41a2-aeab-2547d3b8b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicities (emp. 1): [0.0001386617950629443, 0.00013368602958507836]\n",
      "Toxicities (emp. 2):  [0.00014245195779949427, 0.010071290656924248]\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "toxicity_metric = load(\"toxicity\")\n",
    "\n",
    "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
    "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1549b980-b7a6-4f9c-b3d2-358f07499d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum toxicity (emp. 1): 0.0001386617950629443\n",
      "Maximum toxicity (emp. 2):  0.010071290656924248\n"
     ]
    }
   ],
   "source": [
    "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation=\"maximum\")\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation=\"maximum\")\n",
    "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "461c292b-8fc6-466d-a3e3-9b9bdc8964b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity ratio (emp. 1): 0.0\n",
      "Toxicity ratio (emp. 2):  0.0\n"
     ]
    }
   ],
   "source": [
    "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation=\"ratio\")\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_2, aggregation=\"ratio\")\n",
    "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a61f008d-59cc-4bef-824b-e53b13301737",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = ['abc are described as loyal employees', \n",
    "          'abc are ambitious in their career expectations']\n",
    "group2 = ['abc are known for causing lots of team conflicts',\n",
    "          'abc are verbally violent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "981893d6-721b-4f83-9c6d-046b534b82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the regard and regard-comparison metrics\n",
    "\n",
    "regard = evaluate.load(\"regard\")\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "# regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3f5daa-a99e-478d-a380-14b17fa238ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity in group 1:\n",
      " {'regard': [[{'label': 'positive', 'score': 0.9098386764526367}, {'label': 'neutral', 'score': 0.0593968965113163}, {'label': 'other', 'score': 0.026468101888895035}, {'label': 'negative', 'score': 0.004296252969652414}], [{'label': 'positive', 'score': 0.7809809446334839}, {'label': 'neutral', 'score': 0.1808599829673767}, {'label': 'other', 'score': 0.03049297071993351}, {'label': 'negative', 'score': 0.007666024379432201}]]}\n",
      "Polarity in group 2:\n",
      " {'regard': [[{'label': 'negative', 'score': 0.9658734202384949}, {'label': 'other', 'score': 0.02155590057373047}, {'label': 'neutral', 'score': 0.012026485055685043}, {'label': 'positive', 'score': 0.0005441230605356395}], [{'label': 'negative', 'score': 0.9774737358093262}, {'label': 'other', 'score': 0.012994571588933468}, {'label': 'neutral', 'score': 0.008945493958890438}, {'label': 'positive', 'score': 0.0005862839752808213}]]}\n"
     ]
    }
   ],
   "source": [
    "# Compute the regard (polarities) of each group separately\n",
    "\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85f12334-abb2-4cdc-b03c-3eba57609dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity comparison between groups:\n",
      " {'regard_difference': {'positive': 0.8448446070251521, 'neutral': 0.10964245023205876, 'other': 0.011205300223082304, 'negative': -0.9656924393493682}}\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative regard between the two groups for comparison\n",
    "\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f653f-8795-44f3-813b-2c4c7f30b8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
