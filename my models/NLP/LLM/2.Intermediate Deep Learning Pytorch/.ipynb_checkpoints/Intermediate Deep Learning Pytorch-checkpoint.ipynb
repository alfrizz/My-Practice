{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e732e63-c8d2-4005-9be3-4d43bb3387f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # Load data to pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Convert data to a NumPy array and assign to self.data\n",
    "        self.data = df.to_numpy()\n",
    "        \n",
    "    # Implement __len__ to return the number of data samples\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx, :-1]\n",
    "        # Assign last data column to label\n",
    "        label = self.data[idx, -1]\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182c0a58-3c99-4e03-af21-c23da705c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.587349</td>\n",
       "      <td>0.577747</td>\n",
       "      <td>0.386298</td>\n",
       "      <td>0.568199</td>\n",
       "      <td>0.647347</td>\n",
       "      <td>0.292985</td>\n",
       "      <td>0.654522</td>\n",
       "      <td>0.795029</td>\n",
       "      <td>0.630115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643654</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>0.314381</td>\n",
       "      <td>0.439304</td>\n",
       "      <td>0.514545</td>\n",
       "      <td>0.356685</td>\n",
       "      <td>0.377248</td>\n",
       "      <td>0.202914</td>\n",
       "      <td>0.520358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.388934</td>\n",
       "      <td>0.470876</td>\n",
       "      <td>0.506122</td>\n",
       "      <td>0.524364</td>\n",
       "      <td>0.561537</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.249922</td>\n",
       "      <td>0.401487</td>\n",
       "      <td>0.219973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725820</td>\n",
       "      <td>0.715942</td>\n",
       "      <td>0.506141</td>\n",
       "      <td>0.521683</td>\n",
       "      <td>0.751819</td>\n",
       "      <td>0.148683</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.658678</td>\n",
       "      <td>0.242428</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.610517</td>\n",
       "      <td>0.532588</td>\n",
       "      <td>0.237701</td>\n",
       "      <td>0.270288</td>\n",
       "      <td>0.495155</td>\n",
       "      <td>0.494792</td>\n",
       "      <td>0.409721</td>\n",
       "      <td>0.469762</td>\n",
       "      <td>0.585049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>0.636224</td>\n",
       "      <td>0.580511</td>\n",
       "      <td>0.277748</td>\n",
       "      <td>0.418063</td>\n",
       "      <td>0.522486</td>\n",
       "      <td>0.342184</td>\n",
       "      <td>0.310364</td>\n",
       "      <td>0.402799</td>\n",
       "      <td>0.627156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0.470143</td>\n",
       "      <td>0.548826</td>\n",
       "      <td>0.301347</td>\n",
       "      <td>0.538273</td>\n",
       "      <td>0.498565</td>\n",
       "      <td>0.231359</td>\n",
       "      <td>0.565061</td>\n",
       "      <td>0.175889</td>\n",
       "      <td>0.395061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>0.817826</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>0.656389</td>\n",
       "      <td>0.670774</td>\n",
       "      <td>0.369089</td>\n",
       "      <td>0.431872</td>\n",
       "      <td>0.563265</td>\n",
       "      <td>0.285745</td>\n",
       "      <td>0.578674</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.424187</td>\n",
       "      <td>0.464092</td>\n",
       "      <td>0.459656</td>\n",
       "      <td>0.541633</td>\n",
       "      <td>0.615572</td>\n",
       "      <td>0.388360</td>\n",
       "      <td>0.397780</td>\n",
       "      <td>0.449156</td>\n",
       "      <td>0.440004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.322425</td>\n",
       "      <td>0.492891</td>\n",
       "      <td>0.841409</td>\n",
       "      <td>0.492136</td>\n",
       "      <td>0.656047</td>\n",
       "      <td>0.588709</td>\n",
       "      <td>0.471422</td>\n",
       "      <td>0.503458</td>\n",
       "      <td>0.591867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2011 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph  Hardness    Solids  Chloramines   Sulfate  Conductivity  \\\n",
       "0     0.587349  0.577747  0.386298     0.568199  0.647347      0.292985   \n",
       "1     0.643654  0.441300  0.314381     0.439304  0.514545      0.356685   \n",
       "2     0.388934  0.470876  0.506122     0.524364  0.561537      0.142913   \n",
       "3     0.725820  0.715942  0.506141     0.521683  0.751819      0.148683   \n",
       "4     0.610517  0.532588  0.237701     0.270288  0.495155      0.494792   \n",
       "...        ...       ...       ...          ...       ...           ...   \n",
       "2006  0.636224  0.580511  0.277748     0.418063  0.522486      0.342184   \n",
       "2007  0.470143  0.548826  0.301347     0.538273  0.498565      0.231359   \n",
       "2008  0.817826  0.087434  0.656389     0.670774  0.369089      0.431872   \n",
       "2009  0.424187  0.464092  0.459656     0.541633  0.615572      0.388360   \n",
       "2010  0.322425  0.492891  0.841409     0.492136  0.656047      0.588709   \n",
       "\n",
       "      Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0           0.654522         0.795029   0.630115           0  \n",
       "1           0.377248         0.202914   0.520358           0  \n",
       "2           0.249922         0.401487   0.219973           0  \n",
       "3           0.467200         0.658678   0.242428           0  \n",
       "4           0.409721         0.469762   0.585049           0  \n",
       "...              ...              ...        ...         ...  \n",
       "2006        0.310364         0.402799   0.627156           1  \n",
       "2007        0.565061         0.175889   0.395061           1  \n",
       "2008        0.563265         0.285745   0.578674           1  \n",
       "2009        0.397780         0.449156   0.440004           1  \n",
       "2010        0.471422         0.503458   0.591867           1  \n",
       "\n",
       "[2011 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'water_potability.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe7d459-f108-4fd8-ab14-7c008d5c8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the training set and test set into csv files\n",
    "train_df.to_csv('water_train.csv', index=False)\n",
    "test_df.to_csv('water_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47e0b20-309f-4933-855b-5b27b887a050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.WaterDataset at 0x2420b359060>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the WaterDataset\n",
    "dataset_train = WaterDataset('water_train.csv')\n",
    "\n",
    "dataset_test = WaterDataset('water_test.csv')\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6303fe33-2242-47df-927d-f067acf47c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2420b359e70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader based on dataset_train\n",
    "# Shuffling the training data is important because it helps to ensure that the model does not learn the order of the data, which can lead to overfitting. By shuffling, you provide the model with a more varied and representative sample of the data in each epoch.\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Create a DataLoader based on dataset_test\n",
    "# For the test data, shuffling is generally not necessary and can be avoided. The test set is used to evaluate the model’s performance, and shuffling does not provide any benefit in this context. It is often better to keep the test data in a fixed order to ensure consistent evaluation.\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "dataloader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b409be-ba98-4907-b00b-550cfc65e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5218, 0.6625, 0.5739, 0.4648, 0.7334, 0.3813, 0.3243, 0.3111, 0.3391],\n",
      "        [0.5984, 0.2418, 0.3689, 0.5633, 0.6136, 0.2881, 0.3444, 0.4846, 0.2829]],\n",
      "       dtype=torch.float64) tensor([0., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of features and labels\n",
    "features, labels = next(iter(dataloader_train))\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0eb7b8-0076-4aad-9995-746d8151a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The benefit of defining a class like Net instead of using nn.Sequential is flexibility. With nn.Sequential, you can only define simple, feed-forward networks where the output from one layer is used as input to the next layer. However, with a custom class, you can define more complex architectures. For example, you can have layers where the output from one layer is used as input to multiple layers, or you can use the output of a layer as input to a previous layer (creating a loop, or recurrent connection).\n",
    "\n",
    "# In your Net class, you could easily add more complex behavior in the forward method, such as conditionally skipping layers, having multiple input or output layers, changing the order of layers dynamically, etc.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # As for super(Net, self).__init__(), it’s calling the initialization method of the parent class, which is nn.Module in this case. This is necessary because nn.Module does some behind-the-scenes work that’s necessary for your Net class to function properly with PyTorch, such as keeping track of its trainable parameters and enabling handy methods like to(device) for moving the model to a GPU or eval() for setting the model to evaluation mode. By calling super(Net, self).__init__(), you ensure that your subclass correctly inherits this functionality from nn.Module.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the three linear layers\n",
    "        self.fc1 = nn.Linear(9,16)\n",
    "        self.fc2 = nn.Linear(16,8)\n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass x through linear layers adding activations\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b049f9ad-f9e7-4f78-bcf9-5688bc68bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98dbc4c-fae9-402b-9fc8-a909829303df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, labels.view(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5badbdcf-6012-453d-8c8e-89806f34e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Calculation: The BCELoss (Binary Cross Entropy Loss) function computes the loss between the model’s raw output probabilities (which can be any value between 0 and 1) and the true labels. It doesn’t require the outputs to be converted into discrete predictions (0 or 1) because it operates on the probability scale. It uses the actual probabilities output by the model to calculate how far off the model’s predictions are from the true labels.\n",
    "\n",
    "def train_model(optimizer, net, num_epochs):\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.\n",
    "        for features, labels in dataloader_train:\n",
    "            # Convert features to float32\n",
    "            features = features.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels.view(-1, 1)) # labels.view(-1, 1) reshapes the labels tensor to have as many rows as needed (to fit the total number of elements) and 1 column. This is often done to match the expected input/output shape of certain functions or methods. In this case, it’s used to ensure that the labels tensor has the same shape as the outputs tensor, as required by the BCELoss function.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    train_loss = running_loss / len(dataloader_train)\n",
    "    print(f\"Training loss after {num_epochs} epochs: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5013cf8a-9a34-4b30-bcdc-46f44109535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 10 epochs: 0.6713392564758139\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    optimizer=optimizer,\n",
    "    net=net,\n",
    "    num_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20d0f517-0be7-4898-a6e9-af2d0dca7f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5732010006904602\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Calculation: Accuracy, on the other hand, is a metric that measures the proportion of correct predictions (both true positives and true negatives) in total predictions made. To calculate accuracy, we need discrete predictions (0 or 1), not probabilities. That’s why the outputs are thresholded at 0.5 to convert them into binary predictions (preds = (outputs >= 0.5).float()). If the output probability is greater than or equal to 0.5, it’s considered a prediction of class 1, otherwise, it’s considered a prediction of class 0.\n",
    "\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Set up binary accuracy metric\n",
    "acc = Accuracy(task='binary')\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        # Convert features to float32\n",
    "        features = features.float()\n",
    "        labels = labels.float()\n",
    "        # Get predicted probabilities for test data batch\n",
    "        outputs = net(features)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        acc(preds, labels.view(-1, 1))\n",
    "\n",
    "# Compute total test accuracy\n",
    "test_accuracy = acc.compute()\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb766c99-7ef0-44e4-8fea-c0e657855566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "        # Apply He initialization\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update ReLU activation to ELU\n",
    "        x = nn.functional.elu(self.fc1(x))\n",
    "        x = nn.functional.elu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b610e62f-327c-4a9e-b229-db4d163e35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        # Add two batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        \n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        # Pass x through the second set of layers\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3219ec6b-b1f0-4de2-864c-5c7ec3d28603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 10 epochs: 0.6646026831435327\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    optimizer=optimizer,\n",
    "    net=net,\n",
    "    num_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592a3b85-7ca7-46dc-b5fb-193152cc844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5756824016571045\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        # Convert features to float32\n",
    "        features = features.float()\n",
    "        labels = labels.float()\n",
    "        # Get predicted probabilities for test data batch\n",
    "        outputs = net(features)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        acc(preds, labels.view(-1, 1))\n",
    "\n",
    "# Compute total test accuracy\n",
    "test_accuracy = acc.compute()\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e66f8259-b4a6-4442-ba1b-b4dfdcba8010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Compose transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "])\n",
    "\n",
    "train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b6b112d-b9bc-4800-a1be-a0b62fb7b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 474\n",
       "    Root location: clouds_train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "           )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset using ImageFolder\n",
    "dataset_train = ImageFolder(\n",
    "    \"clouds_train\",\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da20da83-c5d1-494f-af57-051db7bef265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "    ToTensor()\n",
       "    Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    # Add horizontal flip and rotation\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "])\n",
    "train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c759d0-1d97-470b-b996-8a154d723fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 474\n",
       "    Root location: clouds_train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "               ToTensor()\n",
       "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "           )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = ImageFolder(\n",
    "  \"clouds_train\",\n",
    "  transform=train_transforms,\n",
    ")\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b838f97-110c-4b14-9bd2-eabcce523a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x24210ee3910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train = DataLoader(\n",
    "  dataset_train, shuffle=True, batch_size=1\n",
    ")\n",
    "\n",
    "dataloader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e3970c1-9b40-4153-a84b-0fee1931e874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([128, 128, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image, label = next(iter(dataloader_train))\n",
    "# Reshape the image tensor\n",
    "print(image.shape)\n",
    "image = image.squeeze().permute(1, 2, 0)\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df380e2b-2ef2-438e-9a97-bcb0e4784dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a461881-f185-49c9-9bdc-28f1fb003833",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Define feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Define classifier\n",
    "        self.classifier = nn.Linear(64*16*16, num_classes)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        # Pass input through feature extractor and classifier\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28873320-cc15-4ac5-b1b9-9c6841f1a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "    RandomAutocontrast(p=0.5)\n",
       "    ToTensor()\n",
       "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64))\n",
    "])\n",
    "\n",
    "train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8fae61b-69e0-4915-83a7-82d3f6fd99f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 474\n",
       "    Root location: clouds_train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "               RandomAutocontrast(p=0.5)\n",
       "               ToTensor()\n",
       "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "           )"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = ImageFolder(\n",
    "  \"clouds_train\",\n",
    "  transform=train_transforms,\n",
    ")\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d96d823-885c-4a47-b6f4-d27b0a1482f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x242110bff10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train = DataLoader(\n",
    "  dataset_train, shuffle=True, batch_size=16\n",
    ")\n",
    "\n",
    "dataloader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e283cb7b-01fd-461d-949d-17025e6b2fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "665a577b-c34d-4aa9-bade-7f0fe7cbb4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=16384, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "net = Net(num_classes=7)\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "net = net.to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8db71dc-60b5-45f9-841e-926d6b99c4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474, 30, 16, 480)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The discrepancy between len(dataset_train) and len(dataloader_train)*dataloader_train.batch_size arises because the total number of samples (474) is not exactly divisible by the batch size (16). Therefore, the last batch contains fewer than 16 samples. This is why len(dataloader_train)*dataloader_train.batch_size (480) is slightly larger than len(dataset_train) (474). The difference (480 - 474 = 6) is the number of “empty” spots in the last batch.\n",
    "\n",
    "len(dataset_train), len(dataloader_train), dataloader_train.batch_size, len(dataloader_train)*dataloader_train.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb66fba3-ccf1-441d-98b9-a8af3a4e4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_batch = None\n",
    "# for batch in dataloader_train:\n",
    "#     last_batch = batch\n",
    "\n",
    "# # Visualize the last batch\n",
    "# print(\"Last batch:\")\n",
    "# print(last_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36daae77-44b1-42ed-9a97-250100672611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.9727\n",
      "Epoch 2, Loss: 1.5846\n",
      "Epoch 3, Loss: 1.3529\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    # Iterate over training batches\n",
    "    for images, labels in dataloader_train: # iterate over the number of batches\n",
    "        images = images.to(device)  # Move images to GPU\n",
    "        labels = labels.to(device)  # Move labels to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(images) # or net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader_train) # divides the epoch loss by the number of batches (average loss per batch per epoch)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96d9d8e5-56d5-49cd-bf76-a304a4891332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cpu\n",
      "Data is on: cpu\n"
     ]
    }
   ],
   "source": [
    "# For your model\n",
    "print('Model is on:', next(net.parameters()).device)\n",
    "\n",
    "# For your data\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "print('Data is on:', images.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e402b6f-a778-4c27-a77f-a9a777f652bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "    #\n",
    "    # NO DATA AUGMENTATION AT TEST TIME \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a7fce05-1da7-46ea-a8cd-29d4bd6a36ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 486\n",
       "    Root location: clouds_test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = ImageFolder(\"clouds_test\",   transform=test_transforms,)\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc86c91d-adac-47de-8c34-51591fe80d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2421103f2e0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_test = DataLoader(\n",
    "  dataset_test, shuffle=False, batch_size=16\n",
    ")\n",
    "\n",
    "dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "692b6c3a-e6f2-48d6-8c39-762ae8c80d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MulticlassPrecision(), MulticlassRecall())"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "# Define metrics\n",
    "# Macro-Averaging: Computes the metric independently for each class and then takes the average\n",
    "metric_precision = Precision(task=\"multiclass\", num_classes=7, average='macro').to(device)\n",
    "metric_recall = Recall(task=\"multiclass\", num_classes=7, average='macro').to(device)\n",
    "\n",
    "metric_precision, metric_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95ca7522-9ecc-4b54-9525-a7748d590cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4560917615890503\n",
      "Recall: 0.4724655747413635\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad(): # doesn't compute gradients at all\n",
    "    for images, labels in dataloader_test:\n",
    "        images = images.to(device)  # Move your inputs to the chosen device\n",
    "        labels = labels.to(device)  # Move your labels to the chosen device\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        metric_precision(preds, labels)\n",
    "        metric_recall(preds, labels)\n",
    "\n",
    "precision = metric_precision.compute()\n",
    "recall = metric_recall.compute()\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ebbd16b-10d3-456c-803b-0bb55b38caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4161,  1.0124, -2.8504, -0.6890, -0.9558,  1.7521,  0.6637],\n",
      "        [ 0.3019,  0.3727, -2.4635,  0.0087, -0.4197,  0.7939,  0.6597],\n",
      "        [-0.1302, -0.0694, -1.8696,  0.1094, -0.3269,  1.3380,  0.5191],\n",
      "        [ 0.5027,  1.3931, -3.2849, -1.5226, -1.1727,  2.3170,  1.1403],\n",
      "        [ 0.2341,  0.0656, -1.1781,  0.3789, -0.4234,  0.6444,  0.0041],\n",
      "        [ 0.4972,  0.6649, -2.3763, -1.1618, -0.9696,  1.7976,  1.0623]])\n",
      "tensor([5, 5, 5, 5, 5, 5])\n",
      "tensor([6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# torch.max (above) Returns: Two tensors:\n",
    "# Values: The maximum values along the specified dimension (not used here, hence the _).\n",
    "# Indices: The indices of the maximum values, which correspond to the predicted class labels.\n",
    "# example with the last outputs and preds retrieved from the code above\n",
    "print(outputs)\n",
    "print(preds)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2dabe28-bd5b-44d6-921e-6ccd37c2325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "# When you initialize the precision metric with average=None in torchmetrics, it calculates the precision per class.\n",
    "metric_precision = Precision(task=\"multiclass\", num_classes=7, average=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac504b32-800b-40cc-a2c4-83f78bd529f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1429, 0.3299, 0.5714, 0.5378, 0.8333, 0.2773, 0.5000])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_test:\n",
    "        images = images.to(device)  # Move your inputs to the chosen device\n",
    "        labels = labels.to(device)  # Move your labels to the chosen device\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        metric_precision(preds, labels)\n",
    "        \n",
    "precision = metric_precision.compute()\n",
    "precision # visualize the precision for all 7 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2242477d-2bae-415c-9fd1-d79c85cacffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1429), 0.1428571492433548)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision[0], precision[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8b5156e-fd32-4dc6-852a-d3786a0155c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('cirriform clouds', 0), ('clear sky', 1), ('cumulonimbus clouds', 2), ('cumulus clouds', 3), ('high cumuliform clouds', 4), ('stratiform clouds', 5), ('stratocumulus clouds', 6)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.class_to_idx.items() # used to access the mapping of class names to their corresponding indices in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90accb0f-c583-4b6b-9fa9-198c07288f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cirriform clouds': 0.1428571492433548, 'clear sky': 0.3298968970775604, 'cumulonimbus clouds': 0.5714285969734192, 'cumulus clouds': 0.5378151535987854, 'high cumuliform clouds': 0.8333333134651184, 'stratiform clouds': 0.27731093764305115, 'stratocumulus clouds': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Get precision per class\n",
    "precision_per_class = {\n",
    "    k: precision[v].item()\n",
    "    for k, v \n",
    "    in dataset_test.class_to_idx.items()\n",
    "}\n",
    "\n",
    "print(precision_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec087a48-9e9d-46a8-a3af-99de04318023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df) - seq_length):\n",
    "        # Define inputs\n",
    "        x = df.iloc[i:(i+seq_length), 1]\n",
    "        # Define target\n",
    "        y = df.iloc[i+seq_length, 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "861afe7c-6405-49ba-9643-4c7f1811d017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:15:00</td>\n",
       "      <td>-0.704319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 00:30:00</td>\n",
       "      <td>-0.704319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 00:45:00</td>\n",
       "      <td>-0.678983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>-0.653647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 01:15:00</td>\n",
       "      <td>-0.704319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105210</th>\n",
       "      <td>2013-12-31 22:45:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105211</th>\n",
       "      <td>2013-12-31 23:00:00</td>\n",
       "      <td>-0.907259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105212</th>\n",
       "      <td>2013-12-31 23:15:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105213</th>\n",
       "      <td>2013-12-31 23:30:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105214</th>\n",
       "      <td>2013-12-31 23:45:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105215 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  consumption\n",
       "0       2011-01-01 00:15:00    -0.704319\n",
       "1       2011-01-01 00:30:00    -0.704319\n",
       "2       2011-01-01 00:45:00    -0.678983\n",
       "3       2011-01-01 01:00:00    -0.653647\n",
       "4       2011-01-01 01:15:00    -0.704319\n",
       "...                     ...          ...\n",
       "105210  2013-12-31 22:45:00    -0.932595\n",
       "105211  2013-12-31 23:00:00    -0.907259\n",
       "105212  2013-12-31 23:15:00    -0.932595\n",
       "105213  2013-12-31 23:30:00    -0.932595\n",
       "105214  2013-12-31 23:45:00    -0.932595\n",
       "\n",
       "[105215 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('electricity_consump/electricity_train.csv')\n",
    "\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ab7255e-f685-4c93-b559-2697bb8ea93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01 00:15:00</td>\n",
       "      <td>-0.957931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01 00:30:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01 00:45:00</td>\n",
       "      <td>-0.907259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>-0.881923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35035</th>\n",
       "      <td>2014-12-31 22:45:00</td>\n",
       "      <td>-0.070415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35036</th>\n",
       "      <td>2014-12-31 23:00:00</td>\n",
       "      <td>-0.045079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35037</th>\n",
       "      <td>2014-12-31 23:15:00</td>\n",
       "      <td>-0.045079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35038</th>\n",
       "      <td>2014-12-31 23:30:00</td>\n",
       "      <td>-0.045079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35039</th>\n",
       "      <td>2014-12-31 23:45:00</td>\n",
       "      <td>-0.095751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35040 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp  consumption\n",
       "0      2014-01-01 00:00:00    -0.932595\n",
       "1      2014-01-01 00:15:00    -0.957931\n",
       "2      2014-01-01 00:30:00    -0.932595\n",
       "3      2014-01-01 00:45:00    -0.907259\n",
       "4      2014-01-01 01:00:00    -0.881923\n",
       "...                    ...          ...\n",
       "35035  2014-12-31 22:45:00    -0.070415\n",
       "35036  2014-12-31 23:00:00    -0.045079\n",
       "35037  2014-12-31 23:15:00    -0.045079\n",
       "35038  2014-12-31 23:30:00    -0.045079\n",
       "35039  2014-12-31 23:45:00    -0.095751\n",
       "\n",
       "[35040 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('electricity_consump/electricity_test.csv')\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dc25a33-6d07-40ca-9daf-01b218e3a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105119, 96) (105119,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Use create_sequences to create train inputs and targets\n",
    "X_train, y_train = create_sequences(train_data, 24*4) # a record every 15 minutes\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e862f630-4ff4-4a78-96aa-fade0a72d756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34944, 96) (34944,)\n"
     ]
    }
   ],
   "source": [
    "# Use create_sequences to create test inputs and targets\n",
    "X_test, y_test = create_sequences(test_data, 24*4) # a record every 15 minutes\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b71b364e-ad57-41a3-a34a-1e0220b9fc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.70431852, -0.70431852, -0.67898263, ..., -0.65364675,\n",
       "         -0.72990776, -0.70431852],\n",
       "        [-0.70431852, -0.67898263, -0.65364675, ..., -0.72990776,\n",
       "         -0.70431852, -0.70431852],\n",
       "        [-0.67898263, -0.65364675, -0.70431852, ..., -0.70431852,\n",
       "         -0.70431852, -0.70431852],\n",
       "        ...,\n",
       "        [ 0.79759271,  0.82292859,  0.84826447, ..., -0.93259484,\n",
       "         -0.93259484, -0.90725895],\n",
       "        [ 0.82292859,  0.84826447,  0.77200346, ..., -0.93259484,\n",
       "         -0.90725895, -0.93259484],\n",
       "        [ 0.84826447,  0.77200346,  0.72133169, ..., -0.90725895,\n",
       "         -0.93259484, -0.93259484]]),\n",
       " array([-0.70431852, -0.70431852, -0.65364675, ..., -0.93259484,\n",
       "        -0.93259484, -0.93259484]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TensorDataset train\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    ")\n",
    "print(len(dataset_train))\n",
    "\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f322429c-b316-4639-bb2b-747cdbe8a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.93259484, -0.95793072, -0.93259484, ..., -0.93259484,\n",
       "         -0.93259484, -0.93259484],\n",
       "        [-0.95793072, -0.93259484, -0.90725895, ..., -0.93259484,\n",
       "         -0.93259484, -0.93259484],\n",
       "        [-0.93259484, -0.90725895, -0.88192307, ..., -0.93259484,\n",
       "         -0.93259484, -0.93259484],\n",
       "        ...,\n",
       "        [ 0.92452549,  0.95011473,  0.8991896 , ..., -0.07041469,\n",
       "         -0.07041469, -0.04507881],\n",
       "        [ 0.95011473,  0.8991896 ,  0.95011473, ..., -0.07041469,\n",
       "         -0.04507881, -0.04507881],\n",
       "        [ 0.8991896 ,  0.95011473,  0.97545061, ..., -0.04507881,\n",
       "         -0.04507881, -0.04507881]]),\n",
       " array([-0.93259484, -0.93259484, -0.93259484, ..., -0.04507881,\n",
       "        -0.04507881, -0.09575058]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TensorDataset test\n",
    "dataset_test = TensorDataset(\n",
    "    torch.from_numpy(X_test).float(),\n",
    "    torch.from_numpy(y_test).float(),\n",
    ")\n",
    "print(len(dataset_test))\n",
    "\n",
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "302b2809-51b9-422d-9dfa-d9eee27e1e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x24217fc16c0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataLoader based on dataset_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dataloader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b673c63-3b66-430b-a7d7-a4d6f425881d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x24217fc0ca0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataLoader based on dataset_test\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39632695-d2f9-410c-b78b-0f1487fda296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape:  torch.Size([16, 96])\n",
      "Labels shape:  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for seqs1, labels1 in dataloader_test:\n",
    "    print(\"Sequences shape: \", seqs1.shape)\n",
    "    print(\"Labels shape: \", labels1.shape)\n",
    "    break  # remove this line if you want to print shapes for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "baabdcab-8662-44a9-ae82-994bd8b5f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape:  torch.Size([16, 96])\n",
      "Labels shape:  torch.Size([16])\n",
      "16 96\n"
     ]
    }
   ],
   "source": [
    "for seqs1, labels1 in dataloader_train:\n",
    "    print(\"Sequences shape: \", seqs1.shape)\n",
    "    print(\"Labels shape: \", labels1.shape)\n",
    "    print(seqs1.size(0), seqs1.size(1))\n",
    "    break  # remove this line if you want to print shapes for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1c9562d-5cb2-4b78-b48a-0926b229560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Net class is a simple implementation of a Recurrent Neural Network (RNN) using PyTorch. It’s designed to take a sequence of numbers as input and return a single output number. The RNN layer processes the sequence, and the final output of this sequence is then passed through a fully connected (Linear) layer to get the final output. The number of features in the hidden state of the RNN is 32, and there are 2 stacked RNNs (i.e., num_layers=2). The batch size is determined by the size of the input x. The initial hidden state is initialized with zeros. The output of the RNN layer is the sequence of hidden states at each time step. The last hidden state of this sequence (i.e., out[:, -1, :]) is passed through the fully connected layer to get the final output. This is a common pattern in sequence-to-one RNNs, where we only care about the output at the final time step. The fully connected layer transforms the RNN output to the desired output size (in this case, 1).\n",
    "\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the class 'Net' which inherits from 'nn.Module'\n",
    "class Net_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the parent constructor. Necessary for PyTorch to detect this class as a custom module.\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a Recurrent Neural Network (RNN) layer\n",
    "        # input_size=1: The number of expected features in the input x\n",
    "        # hidden_size=32: The number of features in the hidden state h\n",
    "        # num_layers=2: Number of recurrent layers (i.e., 2 stacked RNNs)\n",
    "        # batch_first=True: Whether the first dimension of input represents batch size\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1, \n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True, # In PyTorch, the input to an RNN is a 3D tensor with dimensions defined as follows:\n",
    "                            # If batch_first=True, the dimensions are (batch_size, sequence_length, input_size).\n",
    "                        # If batch_first=False (which is the default), the dimensions are (sequence_length, batch_size, input_size).\n",
    "        )\n",
    "\n",
    "        # Define a fully connected layer (Linear layer)\n",
    "        # It will connect the RNN layer to the output layer\n",
    "        # 32: The size of each input sample (i.e., the hidden size of the RNN)\n",
    "        # 1: The size of each output sample\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation function\n",
    "\n",
    "        # Initialize the first hidden state with zeros\n",
    "        # 2: The number of layers in the RNN\n",
    "        # x.size(0): The batch size (i.e., the number of samples per batch)\n",
    "        # 32: The hidden size of the RNN\n",
    "        h0 = torch.zeros(2, x.size(0), 32) # x.size(0) returns the size of the first dimension of x, which is the batch size (i.e., the number of sequences in the batch). \n",
    "\n",
    "        # Pass the input 'x' and initial hidden states 'h0' through the RNN layer\n",
    "        # The RNN layer returns the output and the last hidden state\n",
    "        # We don't need the last hidden state, so we ignore it with '_'\n",
    "        out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Pass the output of the RNN layer through the fully connected (Linear) layer\n",
    "        # We only want the last time step, so we use 'out[:, -1, :]'\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        # Return the final output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "535e71ca-02ef-45ae-a574-8f3e129895e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_RNN(\n",
       "  (rnn): RNN(1, 32, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of your class\n",
    "net = Net_RNN()\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fc319d6-c690-4825-8168-5ed28338a721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4678],\n",
       "         [-0.9641],\n",
       "         [-0.5087],\n",
       "         [-1.1072],\n",
       "         [-0.2058]],\n",
       "\n",
       "        [[ 1.2287],\n",
       "         [ 0.6235],\n",
       "         [ 1.1977],\n",
       "         [-0.4022],\n",
       "         [ 1.0314]],\n",
       "\n",
       "        [[-0.2731],\n",
       "         [ 1.0176],\n",
       "         [-0.1079],\n",
       "         [-0.9429],\n",
       "         [-1.6757]],\n",
       "\n",
       "        [[-1.3871],\n",
       "         [-0.2041],\n",
       "         [ 0.3361],\n",
       "         [-1.7139],\n",
       "         [ 1.0958]],\n",
       "\n",
       "        [[ 0.6988],\n",
       "         [ 0.1840],\n",
       "         [-0.2139],\n",
       "         [-1.7711],\n",
       "         [-0.9848]],\n",
       "\n",
       "        [[-1.0870],\n",
       "         [-0.8225],\n",
       "         [ 1.7676],\n",
       "         [-0.9892],\n",
       "         [-1.6532]],\n",
       "\n",
       "        [[ 0.7608],\n",
       "         [ 0.2252],\n",
       "         [ 1.2579],\n",
       "         [ 2.1632],\n",
       "         [-0.7416]],\n",
       "\n",
       "        [[ 0.7417],\n",
       "         [-0.0930],\n",
       "         [ 0.7516],\n",
       "         [-1.3077],\n",
       "         [ 1.3170]],\n",
       "\n",
       "        [[-0.3934],\n",
       "         [ 0.9828],\n",
       "         [ 3.4031],\n",
       "         [-0.5741],\n",
       "         [-2.4618]],\n",
       "\n",
       "        [[-0.1632],\n",
       "         [-0.1909],\n",
       "         [ 1.2150],\n",
       "         [ 2.3923],\n",
       "         [-0.4713]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some random input data\n",
    "x = torch.randn(10, 5, 1)  # batch_size=10, sequence_length=5, input_size=1\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63310744-e036-450d-9af3-5e394908c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2247],\n",
       "        [-0.2545],\n",
       "        [-0.2386],\n",
       "        [-0.2062],\n",
       "        [-0.2311],\n",
       "        [-0.1954],\n",
       "        [-0.2852],\n",
       "        [-0.2320],\n",
       "        [-0.2049],\n",
       "        [-0.2750]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the input data through your network\n",
    "output = net(x)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5abe9cc-cf23-473d-a1a5-611a9454c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU\n",
    "\n",
    "class Net_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1011445a-a29d-4569-92b4-eee88f3121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "\n",
    "class Net_LSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0,c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "313863f1-83af-42d9-af45-4127567e078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_l = Net_LSTM(input_size=1)\n",
    "# Set up MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "  net.parameters(), lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d93f189a-7940-4191-afe6-9f515f82b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2591896057128906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1): # few epochs because slow\n",
    "    for seqs, labels in dataloader_train:\n",
    "        # Reshape model inputs\n",
    "        # seqs = seqs.view(16, 96, 1)\n",
    "        seqs = seqs.view(seqs.size(0), seqs.size(1), 1)\n",
    "        # Get model outputs\n",
    "        outputs = net_l(seqs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "698876d9-1d89-4c08-8c74-56a84a365e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_g = Net_GRU()\n",
    "# # Set up MSE loss\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(\n",
    "#   net.parameters(), lr=0.0001\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4966dc2e-8a52-48ae-9d95-a3f567265823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(3):\n",
    "#     for seqs, labels in dataloader_train:\n",
    "#         # Reshape model inputs\n",
    "#         # seqs = seqs.view(16, 96, 1)\n",
    "#         seqs = seqs.view(seqs.size(0), seqs.size(1), 1)\n",
    "#         # Get model outputs\n",
    "#         outputs = net_g(seqs)\n",
    "#         # Compute loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adeb0918-4652-47d5-8464-58b1659b1326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanSquaredError()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "# Define MSE metric\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c693fc5-5e81-48ff-be4b-d6b17e936652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0616, -0.0610, -0.0605, -0.0599, -0.0592, -0.0584, -0.0575, -0.0546,\n",
      "        -0.0513, -0.0485, -0.0486, -0.0496, -0.0506, -0.0516, -0.0524, -0.0529]) tensor(0.0827)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in dataloader_test:\n",
    "        # seqs = seqs.view(16, 96, 1)\n",
    "        seqs = seqs.view(seqs.size(0), seqs.size(1), 1)\n",
    "        # Pass seqs to net and squeeze the result\n",
    "        outputs = net_l(seqs).squeeze()\n",
    "        mse_calc = mse(outputs, labels)\n",
    "        \n",
    "print(outputs, mse_calc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e02ba696-9837-4a71-84fa-4ac31b6987fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.6752153038978577\n"
     ]
    }
   ],
   "source": [
    "# Compute final metric value\n",
    "test_mse = mse.compute()\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c22087fd-f587-4d8b-994e-fed49cc4af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with 2 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31e3b0b0-96b0-41f5-8e35-2039e716c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "\n",
    "my_path= os.getcwd()\n",
    "\n",
    "# Load the Omniglot dataset\n",
    "omniglot_dataset = datasets.Omniglot(\n",
    "    root=my_path, \n",
    "    download=True,\n",
    "    # the transform argument is used to ensure that the images from the Omniglot dataset are in the appropriate format and size for input into a PyTorch model.\n",
    "    transform=transforms.Compose([ \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79cab6ed-29b7-49e6-85af-4f0b9f361208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Omniglot\n",
       "    Number of datapoints: 19280\n",
       "    Root location: G:\\My Drive\\Ingegneria\\Data Science GD\\My-Practice\\my models\\NLP\\LLM\\2.Intermediate Deep Learning Pytorch\\omniglot-py\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "           )"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omniglot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0bb10631-8837-4ec3-b86a-e5d10463cbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('omniglot-py\\\\images_background\\\\Mkhedruli_(Georgian)\\\\character12\\\\0740_01.png',\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " 750)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = r'omniglot-py\\images_background'\n",
    "\n",
    "# Initialize an empty list to store the tuples\n",
    "data = []\n",
    "\n",
    "# Initialize a LabelBinarizer for one-hot encoding of the alphabets\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Get list of all items in the dataset path\n",
    "all_items = os.listdir(dataset_path)\n",
    "\n",
    "# Filter out only directories\n",
    "all_alphabets = [item for item in all_items if os.path.isdir(os.path.join(dataset_path, item))] # Check if it's a directory (to avoid desktop.ini files)\n",
    "\n",
    "lb.fit(all_alphabets)\n",
    "\n",
    "# Initialize a dictionary to store unique labels for each character of every alphabet\n",
    "char_label_dict = {}\n",
    "label_counter = 0\n",
    "\n",
    "# Loop over the directories in the dataset path\n",
    "for alphabet_dir in all_alphabets:  # alphabet folders (folders inside images_background)\n",
    "    if os.path.isdir(os.path.join(dataset_path, alphabet_dir)):  # Check if it's a directory (to avoid desktop.ini files)\n",
    "        alphabet_path = os.path.join(dataset_path, alphabet_dir)\n",
    "    \n",
    "    # Loop over the character directories in each alphabet directory\n",
    "    for character_dir in os.listdir(alphabet_path):\n",
    "        if os.path.isdir(os.path.join(alphabet_path, character_dir)):  # Check if it's a directory (to avoid desktop.ini files)\n",
    "            character_path = os.path.join(alphabet_path, character_dir)\n",
    "    \n",
    "        # Assign a unique label to each character of every alphabet\n",
    "        char_label_key = f\"{alphabet_dir}_{character_dir}\"\n",
    "        if char_label_key not in char_label_dict:\n",
    "            char_label_dict[char_label_key] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        # Loop over the images in each character directory\n",
    "        for image_file in os.listdir(character_path):\n",
    "            if image_file.endswith('.ini'):\n",
    "                continue # skip desktop.ini files\n",
    "            image_path = os.path.join(character_path, image_file) # path of the image in the character folder in the alphabet folder in the image_background folder\n",
    "            \n",
    "            # Create the alphabet vector\n",
    "            alphabet_vector = lb.transform([alphabet_dir])[0] # OHE indicating the alphabet folder ordinal position in the images_backgroung folder\n",
    "            \n",
    "            # Get the target label\n",
    "            target_label = char_label_dict[char_label_key] # decimal number indicating the character folder ordinal position in the image_background folder (counting all the character folders in all alphabet folders)\n",
    "            \n",
    "            # Append the tuple to the data list\n",
    "            data.append((image_path, alphabet_vector, target_label))\n",
    "\n",
    "data[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1cc5df98-af6a-43ed-a6c3-416933e354dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0964_01.png',\n",
       " '0964_02.png',\n",
       " '0964_03.png',\n",
       " '0964_04.png',\n",
       " '0964_05.png',\n",
       " '0964_06.png',\n",
       " '0964_07.png',\n",
       " '0964_08.png',\n",
       " '0964_09.png',\n",
       " '0964_10.png',\n",
       " '0964_11.png',\n",
       " '0964_12.png',\n",
       " '0964_13.png',\n",
       " '0964_14.png',\n",
       " '0964_15.png',\n",
       " '0964_16.png',\n",
       " '0964_17.png',\n",
       " '0964_18.png',\n",
       " '0964_19.png',\n",
       " '0964_20.png']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[image_file for image_file in os.listdir(character_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8d7b1572-5385-478e-93be-2dbe5b5e0227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('omniglot-py\\\\images_background\\\\Anglo-Saxon_Futhorc\\\\character10\\\\0305_10.png',\n",
       " array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 29)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[589]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "12cd77a3-3904-406c-9cb1-0c923c59a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "\t\t# Assign transform and samples to class attributes\n",
    "        self.transform = transform\n",
    "        self.samples = samples\n",
    "                    \n",
    "    def __len__(self):\n",
    "\t\t# Return number of samples\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \t# Unpack the sample at index idx\n",
    "        img_path, alphabet, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L') # convert to grayscale\n",
    "        # Transform the image \n",
    "        img_transformed = self.transform(img) # self.transform(img) is transforming the image img according to the transformations defined in self.transform when the OmniglotDataset object is created\n",
    "        return img_transformed, alphabet, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ce857707-fd53-45ca-8d83-56de551fb2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "('omniglot-py\\\\images_background\\\\Asomtavruli_(Georgian)\\\\character40\\\\0107_16.png', array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 155)\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(hiragana)\\\\character52\\\\0539_17.png', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 585)\n",
      "('omniglot-py\\\\images_background\\\\Bengali\\\\character28\\\\0159_09.png', array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 207)\n",
      "('omniglot-py\\\\images_background\\\\Asomtavruli_(Georgian)\\\\character29\\\\0096_15.png', array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 144)\n",
      "('omniglot-py\\\\images_background\\\\Asomtavruli_(Georgian)\\\\character33\\\\0100_14.png', array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 148)\n",
      "\n",
      "Test Data:\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(hiragana)\\\\character16\\\\0503_05.png', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 549)\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(hiragana)\\\\character28\\\\0515_18.png', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 561)\n",
      "('omniglot-py\\\\images_background\\\\Braille\\\\character03\\\\0194_09.png', array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 242)\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(katakana)\\\\character01\\\\0596_01.png', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 586)\n",
      "('omniglot-py\\\\images_background\\\\Balinese\\\\character06\\\\0113_03.png', array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), 161)\n"
     ]
    }
   ],
   "source": [
    "# # Convert the data list to a numpy array\n",
    "# data = np.array(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first few items in the train_data and test_data to check\n",
    "print('Train Data:')\n",
    "for item in train_data[:5]:\n",
    "    print(item)\n",
    "\n",
    "print('\\nTest Data:')\n",
    "for item in test_data[:5]:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4b436b60-13e1-4604-a399-92d8378eb8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OmniglotDataset at 0x2421c532f80>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of your custom dataset\n",
    "\n",
    "# You’ve already applied the transformations when you loaded the Omniglot dataset using the datasets.Omniglot function. However, the code you’ve shown in the second part is creating a custom dataset class, OmniglotDataset.\n",
    "# If you’re using the same data (i.e., omniglot_dataset) for both the datasets.Omniglot function and your custom OmniglotDataset class, and you’re applying the same transformations in both places, then you’re effectively transforming the data twice, which is not necessary.\n",
    "\n",
    "dataset_train = OmniglotDataset(transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64)),\n",
    "]), samples=train_data)  \n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9929347a-5e36-4b8e-ab0c-662327c5d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train, shuffle=True, batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "de46e470-43a5-4033-8e90-79bd7b330b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]], dtype=torch.int32),\n",
       " tensor([697])]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e8072cef-5601-47e2-b06e-0b0e878946d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[0].shape # image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e4a4fa4a-0135-4f24-97e2-e656e15e9366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[1].shape # total alphabets (30 alphabets folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "98dce247-a48b-4450-8758-53e830d86945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[2].shape # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ea44e083-9916-4efb-9ceb-0b8241dd5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n"
     ]
    }
   ],
   "source": [
    "last_index = len(dataset_train) - 1\n",
    "last_batch = dataset_train[last_index]\n",
    "\n",
    "# Wrap it in a batch format if needed\n",
    "last_batch = [last_batch]\n",
    "\n",
    "print(last_batch[0][2]) # 789 labels (should they be 964 ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5c97dd27-a5c8-4a2f-9e8b-54daf5c9b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network model\n",
    "class Net_imal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_imal, self).__init__()\n",
    "        \n",
    "        # Define the image processing sub-network\n",
    "        # This network will process the image input\n",
    "        self.image_layer = nn.Sequential(\n",
    "            # Convolutional layer with 1 input channel, 16 output channels (filters), 3x3 kernel size (filters size), and 1 padding\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            # Calculating the resulting feature map size (given a input image 64x64): O = [(I-K+2P)/S]+1 = 64-3+2*1+1 = 64\n",
    "            # Max pooling layer with 2x2 window size\n",
    "            nn.MaxPool2d(kernel_size=2), # Reduces the spatial dimension of the feature maps by half. From 64x64 to 32x32, preserving the depth of 16 channels.\n",
    "            # ELU activation function\n",
    "            nn.ELU(),\n",
    "            # Flatten the tensor for the fully connected layer\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128) # 128 is a design choice\n",
    "        )\n",
    "        \n",
    "        # Define the alphabet processing sub-network\n",
    "        # This network will process the alphabet input\n",
    "        self.alphabet_layer = nn.Sequential(\n",
    "            # Fully connected layer with input size 30 and output size 8\n",
    "            nn.Linear(30, 8), # processing a secondary input (the 30 different alphabets), compressing it into a more compact representation (8 is a design choice)\n",
    "            # ELU activation function\n",
    "            nn.ELU(), \n",
    "        )\n",
    "        \n",
    "        # Define the classifier sub-network\n",
    "        # This network will classify the processed inputs\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Fully connected layer with input size 128 + 8 and output size 964 (number of total characters in all alphabets)\n",
    "            nn.Linear(128 + 8, 964), # it's set up to combine and process the features from both your image and alphabet sub-networks. The 128 + 8 input size comes from concatenating the output of the image processing network (128 features) and the alphabet processing network (8 features).\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_image, x_alphabet):\n",
    "        # Pass the image input through the image processing sub-network\n",
    "        x_image = self.image_layer(x_image)\n",
    "        # Pass the alphabet input through the alphabet processing sub-network\n",
    "        x_alphabet = self.alphabet_layer(x_alphabet)\n",
    "        # Concatenate the processed image and alphabet inputs\n",
    "        x = torch.cat((x_image, x_alphabet), dim=1)\n",
    "        # Pass the concatenated inputs through the classifier sub-network\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "13849cdc-a9b6-4cf8-a193-763726cae41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_imal(\n",
       "  (image_layer): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=16384, out_features=128, bias=True)\n",
       "  )\n",
       "  (alphabet_layer): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=8, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=136, out_features=964, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_im_al = Net_imal()\n",
    "net_im_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c75bdc8d-3030-44ef-afae-f90b19bce18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove all files which cause errors otherwise\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# def remove_ipynb_checkpoints(directory):\n",
    "#     for root, dirs, files in os.walk(directory):\n",
    "#         if '.ipynb_checkpoints' in dirs:\n",
    "#             shutil.rmtree(os.path.join(root, '.ipynb_checkpoints'))\n",
    "#         for file in files:\n",
    "#             if file == 'desktop.ini':\n",
    "#                 os.remove(os.path.join(root, file))\n",
    "\n",
    "# # Call the function with the directory path\n",
    "# remove_ipynb_checkpoints(my_path)\n",
    "# my_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8845336c-f1f0-4b5d-b568-25a55c460e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15424/15424 [03:48<00:00, 67.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "outputs = []\n",
    "for img, alpha, labels in tqdm(dataloader_train, desc=\"Training Progress\"):\n",
    "    # Ensure the tensors are of the same type\n",
    "    img = img.type(torch.FloatTensor)\n",
    "    alpha = alpha.type(torch.FloatTensor)\n",
    "\n",
    "    output = net_im_al(img, alpha)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "67f38e4a-121b-4c26-89f0-6f07487a73fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0683, -0.0540, -0.0947,  0.1913, -0.1449, -0.0353,  0.1017,  0.2641,\n",
       "        -0.2614, -0.0255], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(outputs))\n",
    "outputs[0][0][:10]\n",
    "\n",
    "# with a batch size of 1, each batch contains one image. So, 15,424 batches mean you have 15,424 individual pictures in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a6f72e6d-706f-414a-8d95-330fd84a7fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('omniglot-py\\\\images_background\\\\Mkhedruli_(Georgian)\\\\character12\\\\0740_01.png',\n",
       " 23,\n",
       " 750)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the below alternative version, the alphabet_vector is a digital number between 0 and 29, and not a OHE vector anymore\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = 'omniglot-py\\\\images_background'\n",
    "\n",
    "# Initialize an empty list to store the tuples\n",
    "data = []\n",
    "\n",
    "# Get list of all alphabet directories\n",
    "all_alphabets = os.listdir(dataset_path)\n",
    "\n",
    "# Initialize a dictionary to store unique labels for each character of every alphabet\n",
    "char_label_dict = {}\n",
    "label_counter = 0\n",
    "\n",
    "# Loop over the directories in the dataset path\n",
    "for alphabet_dir in all_alphabets:\n",
    "    alphabet_path = os.path.join(dataset_path, alphabet_dir)\n",
    "    \n",
    "    # Loop over the character directories in each alphabet directory\n",
    "    for character_dir in os.listdir(alphabet_path):\n",
    "        character_path = os.path.join(alphabet_path, character_dir)\n",
    "        \n",
    "        # Assign a unique label to each character of every alphabet\n",
    "        char_label_key = f\"{alphabet_dir}_{character_dir}\"\n",
    "        if char_label_key not in char_label_dict:\n",
    "            char_label_dict[char_label_key] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        # Loop over the images in each character directory\n",
    "        for image_file in os.listdir(character_path):\n",
    "            image_path = os.path.join(character_path, image_file)\n",
    "            \n",
    "            # Create the alphabet vector\n",
    "            alphabet_vector = all_alphabets.index(alphabet_dir)\n",
    "            \n",
    "            # Get the target label\n",
    "            target_label = char_label_dict[char_label_key]\n",
    "            \n",
    "            # Append the tuple to the data list\n",
    "            data.append((image_path, alphabet_vector, target_label))\n",
    "\n",
    "data[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0801ee38-c14b-4650-a19e-f03e0d17589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "('omniglot-py\\\\images_background\\\\Asomtavruli_(Georgian)\\\\character40\\\\0107_16.png', 4, 155)\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(hiragana)\\\\character52\\\\0539_17.png', 18, 585)\n",
      "('omniglot-py\\\\images_background\\\\Bengali\\\\character28\\\\0159_09.png', 6, 207)\n",
      "('omniglot-py\\\\images_background\\\\Asomtavruli_(Georgian)\\\\character29\\\\0096_15.png', 4, 144)\n",
      "('omniglot-py\\\\images_background\\\\Asomtavruli_(Georgian)\\\\character33\\\\0100_14.png', 4, 148)\n",
      "\n",
      "Test Data:\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(hiragana)\\\\character16\\\\0503_05.png', 18, 549)\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(hiragana)\\\\character28\\\\0515_18.png', 18, 561)\n",
      "('omniglot-py\\\\images_background\\\\Braille\\\\character03\\\\0194_09.png', 8, 242)\n",
      "('omniglot-py\\\\images_background\\\\Japanese_(katakana)\\\\character01\\\\0596_01.png', 19, 586)\n",
      "('omniglot-py\\\\images_background\\\\Balinese\\\\character06\\\\0113_03.png', 5, 161)\n"
     ]
    }
   ],
   "source": [
    "# # Convert the data list to a numpy array\n",
    "# data = np.array(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first few items in the train_data and test_data to check\n",
    "print('Train Data:')\n",
    "for item in train_data[:5]:\n",
    "    print(item)\n",
    "\n",
    "print('\\nTest Data:')\n",
    "for item in test_data[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "80b3770e-1146-4768-80dc-3c12d71a2f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OmniglotDataset at 0x2421c533f10>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset_train\n",
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "      \ttransforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=train_data,\n",
    ")\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "396d7dbe-9e59-4947-ac8c-71db7c854d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OmniglotDataset at 0x2421c533d90>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset_test\n",
    "dataset_test = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "      \ttransforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=test_data,\n",
    ")\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "87e47ecf-7ca1-4503-aa58-419e01e09f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2421c531810>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloader_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, shuffle=True, batch_size=32,\n",
    ")\n",
    "\n",
    "dataloader_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "76ee62fe-198e-4ff6-bc25-73f7fe5f78fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2421c532980>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloader_test\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test, shuffle=False, batch_size=32,\n",
    ")\n",
    "\n",
    "dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4d9df772-b532-42ee-b8ef-1e9eefbdee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 64, 64])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[0].shape # image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e20836b3-5b3c-4ac1-a62d-069437943ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[1].shape # alphabet (integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2941be8e-995a-4f24-9a48-170efca650b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_train))[2].shape # label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ef2eda34-8464-4b51-8913-0dd3be47567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time the model will have two classification layers: two predictions in parallel\n",
    "\n",
    "class Net_2o(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_2o, self).__init__()\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1), # 1 input channel (B/W), 16 filters\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        # Define the two classifier layers\n",
    "        self.classifier_alpha = nn.Linear(128, 30)\n",
    "        self.classifier_char = nn.Linear(128, 964)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_image = self.image_layer(x)\n",
    "        # Pass x_image through the classifiers and return both results\n",
    "        output_alpha = self.classifier_alpha(x_image)\n",
    "        output_char = self.classifier_char(x_image)\n",
    "        return output_alpha, output_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "73e2f657-7dfe-489d-9130-a585be84ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_weight = 0.9 # more importance to the loss of the characters classification\n",
    "\n",
    "net = Net_2o()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for images, labels_alpha, labels_char in dataloader_train:\n",
    "        # Check if any element in images or labels is NaN or Inf\n",
    "        if torch.isnan(images).any() or torch.isinf(images).any():\n",
    "            print(\"NaN or Inf found in images\")\n",
    "        if torch.isnan(labels_alpha).any() or torch.isinf(labels_alpha).any():\n",
    "            print(\"NaN or Inf found in labels_alpha\")\n",
    "        if torch.isnan(labels_char).any() or torch.isinf(labels_char).any():\n",
    "            print(\"NaN or Inf found in labels_char\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs_alpha, outputs_char = net(images)\n",
    "\n",
    "        # Check if any element in outputs is NaN or Inf\n",
    "        if torch.isnan(outputs_alpha).any() or torch.isinf(outputs_alpha).any():\n",
    "            print(\"NaN or Inf found in outputs_alpha\")\n",
    "        if torch.isnan(outputs_char).any() or torch.isinf(outputs_char).any():\n",
    "            print(\"NaN or Inf found in outputs_char\")\n",
    "\n",
    "        # Compute alphabet classification loss\n",
    "        loss_alpha = criterion(outputs_alpha, labels_alpha)\n",
    "        # Compute character classification loss\n",
    "        loss_char = criterion(outputs_char, labels_char)\n",
    "\n",
    "        # Check if any element in losses is NaN or Inf\n",
    "        if torch.isnan(loss_alpha).any() or torch.isinf(loss_alpha).any():\n",
    "            print(\"NaN or Inf found in loss_alpha\")\n",
    "        if torch.isnan(loss_char).any() or torch.isinf(loss_char).any():\n",
    "            print(\"NaN or Inf found in loss_char\")\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = ((1 - char_weight) * loss_alpha) + (char_weight * loss_char)\n",
    "\n",
    "        # Check if any element in total loss is NaN or Inf\n",
    "        if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "            print(\"NaN or Inf found in total loss\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1) # it scales the entire gradient vector if its norm exceeds 1\n",
    "\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f0ab43ca-5318-4f0b-8415-20ccc143bc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2374, -0.3910,  0.4025,  0.2014, -0.0599,  0.0740,  0.2678, -0.0855,\n",
       "        -0.1538,  0.1782,  0.3466, -0.2357,  0.0164,  0.0225, -0.1080,  0.0814,\n",
       "         0.0436, -0.1996,  0.1307,  0.1691,  0.2995, -0.3139,  0.1264, -0.0241,\n",
       "         0.1262, -0.4272,  0.2418, -0.2202, -0.4613,  0.0017],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_alpha[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "324f8dd9-f0bb-477d-a9cd-338ce4f7ed63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, pred_alpha = torch.max(outputs_alpha[0], 0)\n",
    "pred_alpha # position (2) of the max output value (0.4025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2c8c487a-bd00-4192-9e39-87eb6faf7fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([964])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_char[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1313d4c3-c37c-4d50-8abf-6f28cf64b68e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.4986, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss # with char_weight = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f85af502-d369-45f3-b127-3afe45cfac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Define accuracy metrics\n",
    "    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n",
    "    acc_char = Accuracy(task=\"multiclass\", num_classes=964)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels_alpha, labels_char) in enumerate(dataloader_test):\n",
    "            # Obtain model outputs\n",
    "            outputs_alpha, outputs_char = net(images)\n",
    "            _, pred_alpha = torch.max(outputs_alpha, 1)\n",
    "            _, pred_char = torch.max(outputs_char, 1)\n",
    "            # Update both accuracy metrics\n",
    "            acc_alpha(pred_alpha, labels_alpha)\n",
    "            acc_char(pred_char, labels_char)\n",
    "            \n",
    "            # Print the values for the first batch of data\n",
    "            if i == 0:\n",
    "                print('outputs_alpha', outputs_alpha.size())\n",
    "                print('pred_alpha', pred_alpha.size())\n",
    "                print('labels_alpha', labels_alpha.size())\n",
    "                print('outputs_char', outputs_char.size())\n",
    "                print('pred_char', pred_char.size())\n",
    "                print('labels_char', labels_char.size())\n",
    "                print('outputs_alpha',outputs_alpha)\n",
    "                print('pred_alpha',pred_alpha)\n",
    "                print('labels_alpha',labels_alpha)\n",
    "                print('outputs_char',outputs_char)\n",
    "                print('pred_char',pred_char)\n",
    "                print('labels_char',labels_char)\n",
    "\n",
    "    print(f\"Alphabet: {acc_alpha.compute()}\")\n",
    "    print(f\"Character: {acc_char.compute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8ffe32b7-6d71-4981-a8f5-c590c34a89d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs_alpha torch.Size([32, 30])\n",
      "pred_alpha torch.Size([32])\n",
      "labels_alpha torch.Size([32])\n",
      "outputs_char torch.Size([32, 964])\n",
      "pred_char torch.Size([32])\n",
      "labels_char torch.Size([32])\n",
      "outputs_alpha tensor([[-2.6742e-01, -4.0554e-01,  2.8217e-01,  7.0159e-02,  1.2705e-02,\n",
      "          1.1929e-01,  3.0639e-01, -1.3936e-02, -1.9476e-01,  2.3590e-01,\n",
      "          1.9044e-01, -2.7688e-01, -5.8969e-02, -2.7900e-03, -8.7662e-02,\n",
      "          1.7465e-01,  1.4619e-02, -2.9748e-01,  8.1040e-02,  1.6176e-01,\n",
      "          1.7500e-01, -3.3546e-01,  8.5505e-02, -1.3382e-01,  4.8451e-03,\n",
      "         -4.7684e-01,  3.4961e-01, -1.3498e-01, -2.6196e-01, -5.4593e-02],\n",
      "        [-2.9536e-01, -3.5471e-01,  3.6160e-01,  1.2498e-01, -6.3908e-02,\n",
      "          2.4774e-02,  2.9557e-01, -6.9437e-02, -1.7657e-01,  2.0017e-01,\n",
      "          3.3303e-01, -1.3460e-01,  3.7772e-02,  7.4509e-03, -4.9389e-02,\n",
      "          1.5985e-01, -3.8358e-02, -2.5983e-01,  2.8756e-01,  1.1999e-01,\n",
      "          2.1008e-01, -3.3969e-01,  4.9472e-02, -9.4889e-02,  9.1332e-02,\n",
      "         -4.3800e-01,  2.4397e-01, -1.4206e-01, -2.7647e-01, -9.0306e-02],\n",
      "        [-3.0876e-01, -4.0713e-01,  3.4564e-01,  1.5616e-01, -1.3012e-01,\n",
      "          3.6754e-03,  2.6648e-01, -3.6767e-02, -8.5007e-02,  2.0358e-01,\n",
      "          3.2661e-01, -1.7129e-01, -1.8277e-02,  4.5729e-02, -7.0543e-02,\n",
      "          8.8056e-02,  5.2959e-03, -2.0277e-01,  1.4312e-01,  1.1487e-01,\n",
      "          2.5462e-01, -4.1005e-01,  1.4701e-01, -8.8282e-02,  1.7229e-01,\n",
      "         -5.1682e-01,  2.5377e-01, -7.6485e-02, -3.3088e-01, -7.1025e-02],\n",
      "        [-3.3539e-01, -3.5684e-01,  3.4696e-01,  1.0049e-01, -6.9325e-02,\n",
      "          4.7524e-02,  3.0310e-01, -1.8726e-02, -1.5360e-01,  2.1175e-01,\n",
      "          2.1555e-01, -2.0319e-01, -9.1822e-04,  1.2225e-03, -1.5259e-02,\n",
      "          6.7112e-02, -5.6600e-03, -1.9771e-01,  2.7440e-01,  1.3966e-01,\n",
      "          2.1022e-01, -3.3072e-01,  1.3245e-01, -6.1553e-02,  6.1667e-02,\n",
      "         -4.8076e-01,  2.8877e-01, -1.5351e-01, -3.5458e-01, -6.9254e-02],\n",
      "        [-2.1288e-01, -3.4259e-01,  2.0718e-01, -2.9649e-02, -2.7594e-02,\n",
      "          1.5759e-01,  2.4945e-01, -3.5374e-02, -1.0298e-01,  1.4952e-01,\n",
      "          2.8052e-01, -1.3004e-01, -5.7846e-02,  1.2571e-01, -3.7743e-02,\n",
      "         -2.3615e-02, -8.9552e-02, -1.8180e-01,  2.4683e-01,  6.3829e-02,\n",
      "          1.3065e-01, -3.6001e-01,  1.3560e-01, -1.0385e-01,  1.2147e-01,\n",
      "         -4.2131e-01,  1.9368e-01, -1.2347e-01, -2.4093e-01, -1.2061e-01],\n",
      "        [-2.2967e-01, -3.3980e-01,  2.2000e-01, -3.5373e-03,  1.1024e-02,\n",
      "          6.5689e-02,  2.1238e-01, -2.7186e-02, -1.6232e-01,  2.1090e-01,\n",
      "          3.3146e-01, -1.2115e-01, -5.4225e-02,  2.5053e-02, -9.0209e-02,\n",
      "          5.6214e-02,  9.2790e-03, -1.9724e-01,  2.7373e-01,  8.9297e-02,\n",
      "          2.1370e-01, -3.8111e-01,  1.6196e-01, -1.8176e-01,  8.3758e-02,\n",
      "         -3.9247e-01,  1.2718e-01, -7.2869e-02, -2.4416e-01, -1.8065e-01],\n",
      "        [-2.9171e-01, -2.7364e-01,  2.9633e-01,  7.8518e-02, -8.8915e-02,\n",
      "          1.3223e-02,  3.0432e-01, -5.4180e-02, -1.3454e-01,  1.7500e-01,\n",
      "          3.5783e-01, -1.5873e-01,  1.0441e-02,  4.4459e-03, -5.0197e-02,\n",
      "          5.2591e-02, -1.4047e-02, -1.0244e-01,  2.8827e-01,  1.1698e-01,\n",
      "          2.1919e-01, -2.4748e-01,  1.0627e-01, -5.4461e-02,  1.7902e-02,\n",
      "         -5.1521e-01,  2.1347e-01, -1.5058e-01, -2.5559e-01, -8.3287e-02],\n",
      "        [-2.6183e-01, -4.4588e-01,  3.2715e-01,  1.9587e-01, -1.4769e-01,\n",
      "          2.8395e-02,  2.4006e-01, -1.2689e-01, -1.6911e-01,  1.7828e-01,\n",
      "          3.6384e-01, -1.5507e-01,  4.1082e-03,  2.8621e-02, -4.1211e-02,\n",
      "          1.0653e-01,  1.5819e-03, -2.2715e-01,  1.9801e-01,  1.6003e-01,\n",
      "          1.9312e-01, -4.1129e-01,  1.6701e-01, -5.0544e-02,  1.5180e-01,\n",
      "         -4.9750e-01,  2.4949e-01, -1.0560e-01, -3.1576e-01, -4.0413e-02],\n",
      "        [-3.0857e-01, -3.2079e-01,  3.9110e-01,  1.5383e-01, -2.3563e-02,\n",
      "         -2.5327e-02,  3.0167e-01, -1.1423e-01, -1.3101e-01,  1.1480e-01,\n",
      "          2.2784e-01, -2.0691e-01,  6.6810e-02,  5.4577e-02, -2.5693e-03,\n",
      "          2.1846e-02, -7.6235e-03, -2.7002e-01,  2.3843e-01,  1.6634e-01,\n",
      "          2.3044e-01, -3.8595e-01,  1.2873e-01, -5.1813e-02,  1.7183e-01,\n",
      "         -4.2792e-01,  2.0606e-01, -1.9676e-01, -2.6464e-01,  9.7092e-02],\n",
      "        [-2.7885e-01, -4.1365e-01,  3.1239e-01,  1.5209e-01, -1.5245e-01,\n",
      "         -6.0169e-04,  2.6204e-01, -7.7784e-02, -4.5799e-02,  1.2503e-01,\n",
      "          3.1120e-01, -1.6434e-01,  1.2903e-02,  1.6559e-01,  1.2414e-02,\n",
      "          9.4348e-03, -1.6912e-02, -1.8348e-01,  1.6445e-01,  1.0566e-01,\n",
      "          1.9176e-01, -3.5717e-01,  1.0335e-01, -8.5797e-02,  1.8346e-01,\n",
      "         -5.4163e-01,  3.1421e-01, -1.2943e-01, -2.7178e-01,  1.0741e-02],\n",
      "        [-2.3585e-01, -4.6093e-01,  4.6498e-01,  3.1391e-02, -1.6004e-01,\n",
      "          9.0366e-02,  1.6114e-01, -9.8354e-02, -2.2762e-01,  1.8929e-01,\n",
      "          2.7409e-01, -2.0352e-01,  3.2387e-02,  7.3569e-02, -7.7018e-03,\n",
      "          1.5569e-02, -9.1699e-02, -2.2857e-01,  1.8539e-01,  6.3564e-02,\n",
      "          2.1438e-01, -4.0532e-01,  1.9116e-01,  1.0442e-02,  1.1235e-01,\n",
      "         -4.8778e-01,  2.1744e-01, -1.0120e-01, -3.2275e-01,  1.2166e-04],\n",
      "        [-3.2152e-01, -4.1609e-01,  4.6164e-01,  1.4518e-01, -1.3077e-01,\n",
      "          9.5020e-03,  1.9139e-01, -1.2834e-01, -2.3981e-01,  1.3581e-01,\n",
      "          2.7991e-01, -1.1195e-01,  5.2634e-02,  3.6997e-02, -5.4215e-02,\n",
      "          7.7906e-02, -2.5331e-02, -2.4637e-01,  2.1413e-01,  1.0694e-01,\n",
      "          2.3315e-01, -4.2200e-01,  1.3806e-01, -7.9039e-02,  1.1566e-01,\n",
      "         -4.5611e-01,  2.2439e-01, -1.5461e-01, -3.4323e-01,  2.2700e-02],\n",
      "        [-3.2215e-01, -4.3028e-01,  3.5593e-01, -3.6237e-04, -2.8428e-02,\n",
      "          6.7884e-02,  3.1292e-01, -1.3279e-01, -2.3157e-01,  1.4804e-01,\n",
      "          2.2654e-01, -2.0786e-01, -6.1487e-02,  8.6249e-02, -1.1259e-01,\n",
      "          1.1708e-01,  3.9935e-02, -2.7200e-01,  1.2939e-01,  1.5201e-01,\n",
      "          1.6552e-01, -3.1913e-01,  1.0973e-01, -7.1899e-02,  6.5169e-02,\n",
      "         -4.2318e-01,  2.8508e-01, -1.0210e-01, -3.6480e-01, -2.3778e-03],\n",
      "        [-2.9400e-01, -4.7104e-01,  3.1551e-01,  5.2770e-02,  1.6413e-03,\n",
      "          8.7295e-02,  2.1126e-01, -5.6202e-02, -1.5532e-01,  1.6192e-01,\n",
      "          2.6725e-01, -2.4777e-01, -5.7000e-02,  4.4466e-02, -7.0491e-02,\n",
      "          2.0756e-02,  5.1636e-02, -2.4480e-01,  1.3273e-01,  8.2716e-02,\n",
      "          2.2804e-01, -4.2081e-01,  2.0589e-01, -1.0586e-01,  1.5758e-01,\n",
      "         -4.3708e-01,  2.2937e-01, -1.0620e-01, -3.2495e-01, -9.5502e-02],\n",
      "        [-2.1842e-01, -3.8771e-01,  3.1588e-01,  1.4483e-01, -5.4944e-02,\n",
      "          1.4373e-01,  2.1743e-01, -9.2109e-02, -2.4245e-01,  9.7962e-02,\n",
      "          2.4879e-01, -1.5870e-01,  6.7836e-02,  6.1623e-02, -3.3676e-02,\n",
      "          7.2054e-02, -3.3184e-02, -2.2049e-01,  2.4378e-01,  1.2164e-01,\n",
      "          1.1790e-01, -4.1529e-01,  1.0630e-01, -8.2219e-02,  2.5047e-04,\n",
      "         -4.2963e-01,  1.6499e-01, -1.1368e-01, -2.2271e-01,  4.2196e-02],\n",
      "        [-2.3242e-01, -3.8152e-01,  3.0584e-01,  6.1650e-02, -1.0210e-01,\n",
      "         -1.1240e-02,  3.0346e-01, -1.4710e-01, -1.0308e-01,  1.9473e-01,\n",
      "          3.1352e-01, -1.8747e-01, -5.3840e-02,  1.1520e-01, -1.2604e-01,\n",
      "          2.0545e-02, -6.4607e-02, -1.5654e-01,  2.1177e-01,  9.4161e-02,\n",
      "          1.2845e-01, -3.7622e-01,  5.3347e-02, -1.3896e-01,  7.4223e-02,\n",
      "         -4.6742e-01,  2.5048e-01, -6.5079e-03, -2.0025e-01, -1.1632e-01],\n",
      "        [-2.0248e-01, -3.9778e-01,  2.3318e-01, -3.1584e-02,  3.8454e-03,\n",
      "          1.0765e-01,  2.6795e-01, -1.3907e-01, -1.4846e-01,  1.3808e-01,\n",
      "          3.7996e-01, -1.6063e-01, -4.4206e-02,  1.1222e-01, -6.3498e-02,\n",
      "          6.5888e-02, -1.1400e-01, -1.6693e-01,  2.5434e-01,  2.5275e-02,\n",
      "          1.5035e-01, -3.0563e-01,  1.7936e-01, -1.0323e-01,  6.8781e-02,\n",
      "         -3.9236e-01,  1.6104e-01, -8.2971e-02, -2.1843e-01, -1.4695e-01],\n",
      "        [-2.3960e-01, -4.1305e-01,  3.3131e-01,  1.4928e-01, -1.4665e-01,\n",
      "          8.8848e-02,  2.7388e-01, -8.5113e-02, -1.7947e-01,  1.9184e-01,\n",
      "          2.8874e-01, -1.9845e-01,  1.7124e-02, -2.4212e-02, -5.1386e-03,\n",
      "          1.8271e-01, -4.0858e-02, -2.3031e-01,  2.4800e-01,  1.7603e-01,\n",
      "          1.3766e-01, -4.0998e-01,  1.4544e-01, -1.0663e-01,  6.8744e-02,\n",
      "         -5.0629e-01,  2.3972e-01, -3.4027e-02, -2.8936e-01, -6.8454e-02],\n",
      "        [-3.2675e-01, -3.9528e-01,  3.6721e-01,  2.1066e-01, -7.1433e-02,\n",
      "         -2.0614e-02,  3.0092e-01, -8.7975e-02, -1.5805e-01,  2.1484e-01,\n",
      "          2.2340e-01, -1.6598e-01,  1.2610e-01,  1.9179e-02, -2.2151e-02,\n",
      "          1.3773e-01,  9.9836e-02, -2.3775e-01,  1.3274e-01,  2.0829e-01,\n",
      "          2.8009e-01, -4.3844e-01,  1.4993e-01, -4.6781e-02,  1.5795e-01,\n",
      "         -4.4732e-01,  2.2198e-01, -1.7760e-01, -3.3088e-01, -1.0018e-02],\n",
      "        [-1.9401e-01, -3.1891e-01,  2.2096e-01, -3.7652e-02,  8.8139e-03,\n",
      "          1.4628e-01,  2.6387e-01, -2.0067e-02, -1.6236e-01,  1.8531e-01,\n",
      "          3.9161e-01, -1.2409e-01, -7.9983e-02,  9.3361e-02, -1.3644e-01,\n",
      "          3.6859e-02, -7.4274e-02, -1.8120e-01,  2.3606e-01,  5.6426e-02,\n",
      "          2.3588e-01, -2.9243e-01,  1.7742e-01, -1.4585e-01,  1.2434e-01,\n",
      "         -3.4715e-01,  2.0819e-01, -1.0830e-01, -2.6891e-01, -1.5378e-01],\n",
      "        [-2.1251e-01, -4.5362e-01,  2.6630e-01, -3.4777e-02,  2.2335e-02,\n",
      "          2.0980e-01,  2.0508e-01, -4.8878e-02, -2.1328e-01,  1.3619e-01,\n",
      "          2.9931e-01, -1.6347e-01, -2.7330e-02,  4.0676e-02, -7.8953e-02,\n",
      "          4.3603e-02,  5.1592e-04, -1.8859e-01,  2.2335e-01,  8.5018e-02,\n",
      "          2.0622e-01, -3.4360e-01,  1.9635e-01, -8.8134e-02,  1.3420e-02,\n",
      "         -3.8652e-01,  1.7883e-01, -1.1683e-01, -2.3407e-01, -6.6145e-02],\n",
      "        [-2.5233e-01, -5.0662e-01,  2.7985e-01,  1.9039e-02, -1.0412e-01,\n",
      "          1.0858e-01,  1.8710e-01, -9.1421e-02, -2.3670e-01,  1.3235e-01,\n",
      "          3.7455e-01, -1.1793e-01, -3.5686e-02,  2.4547e-02, -5.8882e-02,\n",
      "          3.3645e-02,  3.3370e-02, -1.5826e-01,  2.4779e-01,  1.3329e-01,\n",
      "          2.3880e-01, -3.4583e-01,  1.7335e-01, -1.4910e-01,  6.2284e-02,\n",
      "         -3.6789e-01,  2.5226e-01, -1.3471e-01, -3.0805e-01, -4.2612e-02],\n",
      "        [-2.6154e-01, -4.5363e-01,  2.9326e-01,  4.3428e-02, -9.9054e-02,\n",
      "          5.0274e-02,  2.3562e-01, -9.5798e-02, -8.9766e-02,  1.0035e-01,\n",
      "          3.6939e-01, -1.2200e-01, -4.2607e-02,  1.4342e-01, -1.0374e-01,\n",
      "         -1.8276e-02, -3.9054e-02, -2.1584e-01,  2.3467e-01,  7.4844e-02,\n",
      "          1.6463e-01, -3.8145e-01,  9.1994e-02, -1.2526e-01,  1.2601e-01,\n",
      "         -4.7098e-01,  2.2903e-01, -9.2775e-02, -2.4258e-01, -1.1820e-01],\n",
      "        [-3.1393e-01, -3.8005e-01,  3.0979e-01,  9.5105e-02, -1.1228e-02,\n",
      "          4.1600e-02,  2.6423e-01, -1.4994e-02, -1.3105e-01,  2.0705e-01,\n",
      "          2.0656e-01, -2.2024e-01, -2.9323e-02,  9.1629e-04, -8.0063e-02,\n",
      "          3.2821e-02,  2.9135e-02, -2.2687e-01,  1.6978e-01,  1.2200e-01,\n",
      "          2.4503e-01, -3.9995e-01,  1.4559e-01, -6.3077e-02,  1.0173e-01,\n",
      "         -4.2117e-01,  1.6051e-01, -6.7369e-02, -2.4512e-01, -8.9839e-02],\n",
      "        [-2.9853e-01, -3.1505e-01,  3.6981e-01,  5.4507e-02,  3.2131e-02,\n",
      "          6.8270e-02,  2.5142e-01, -6.6143e-02, -1.6592e-01,  2.0615e-01,\n",
      "          2.8345e-01, -1.6423e-01,  2.7073e-02, -3.3320e-02, -6.8188e-02,\n",
      "          6.7557e-02, -1.9415e-02, -2.2182e-01,  2.5022e-01,  1.4265e-01,\n",
      "          1.9777e-01, -3.6296e-01,  1.8519e-01, -9.9912e-02,  9.9780e-02,\n",
      "         -4.1687e-01,  1.9429e-01, -1.6603e-01, -2.2636e-01, -7.6165e-02],\n",
      "        [-2.0808e-01, -4.8748e-01,  3.3633e-01,  7.1169e-02, -5.2918e-02,\n",
      "          1.5923e-01,  1.6599e-01, -8.2094e-02, -2.7701e-01,  1.9908e-01,\n",
      "          3.0265e-01, -2.4681e-01, -9.6441e-02,  5.2497e-02, -2.8650e-02,\n",
      "          7.3940e-02, -2.4823e-03, -2.4770e-01,  1.6685e-01,  1.1055e-01,\n",
      "          1.2443e-01, -4.2912e-01,  1.6984e-01, -9.6061e-02,  7.7192e-02,\n",
      "         -4.6565e-01,  2.8121e-01, -1.4243e-01, -3.2775e-01, -4.0104e-02],\n",
      "        [-2.4615e-01, -4.4508e-01,  2.6450e-01, -2.7428e-02,  3.9607e-02,\n",
      "          9.8956e-02,  2.5962e-01, -9.7973e-02, -2.9943e-01,  1.7508e-01,\n",
      "          2.7419e-01, -1.8204e-01, -5.9603e-02,  2.3015e-02, -8.9368e-02,\n",
      "          3.1320e-02, -1.8826e-02, -2.7307e-01,  2.6550e-01,  5.7369e-02,\n",
      "          2.6965e-01, -4.0344e-01,  1.5467e-01, -1.3744e-01,  4.3945e-02,\n",
      "         -4.3118e-01,  1.6890e-01, -1.5522e-01, -2.5617e-01, -8.8250e-02],\n",
      "        [-2.8290e-01, -3.6923e-01,  3.7970e-01,  7.8952e-02, -1.1517e-01,\n",
      "          1.6605e-02,  2.6714e-01, -1.0743e-01, -2.1798e-01,  2.4825e-01,\n",
      "          2.0734e-01, -2.1511e-01,  7.3966e-02, -3.3457e-02, -8.7253e-02,\n",
      "          1.4947e-01, -3.0920e-02, -2.0580e-01,  2.4452e-01,  1.4951e-01,\n",
      "          2.2710e-01, -3.9596e-01,  9.7225e-02, -1.7200e-02,  8.9534e-02,\n",
      "         -4.0639e-01,  2.0363e-01, -1.8104e-02, -3.3032e-01, -9.4315e-02],\n",
      "        [-3.3128e-01, -3.4632e-01,  2.4950e-01, -2.2012e-02, -2.0835e-02,\n",
      "          6.9804e-02,  3.4444e-01, -1.5089e-01, -2.3986e-01,  1.3630e-01,\n",
      "          2.5343e-01, -8.6069e-02, -6.3213e-03, -2.0229e-02, -1.5786e-02,\n",
      "          1.4355e-01,  1.4471e-02, -2.1892e-01,  2.6813e-01,  1.0345e-01,\n",
      "          1.8221e-01, -3.2034e-01,  1.8854e-02, -1.2728e-01,  1.0916e-02,\n",
      "         -2.8299e-01,  2.9446e-01, -1.6656e-01, -2.6090e-01, -1.2111e-01],\n",
      "        [-2.7141e-01, -3.4054e-01,  3.4748e-01,  6.2944e-02, -7.8515e-02,\n",
      "          1.0167e-01,  3.2451e-01, -1.1817e-01, -2.3840e-01,  1.6904e-01,\n",
      "          2.9407e-01, -1.4006e-01,  9.8586e-03, -8.1413e-03, -1.1207e-01,\n",
      "          4.9790e-02,  2.6598e-02, -1.9526e-01,  2.7389e-01,  1.1650e-01,\n",
      "          1.7850e-01, -3.0738e-01,  7.6290e-02, -1.2130e-01,  1.1138e-01,\n",
      "         -3.8804e-01,  2.3727e-01, -1.9246e-01, -3.0492e-01, -5.3538e-02],\n",
      "        [-2.5130e-01, -3.4316e-01,  3.3605e-01,  2.0846e-01, -1.6876e-01,\n",
      "          3.0069e-02,  2.6707e-01, -3.5518e-02, -8.3132e-02,  2.0679e-01,\n",
      "          2.9757e-01, -1.8228e-01,  8.2059e-02, -6.2392e-03, -5.8889e-02,\n",
      "          1.7897e-01,  7.3826e-03, -2.2269e-01,  1.7621e-01,  1.7406e-01,\n",
      "          2.3572e-01, -3.9731e-01,  1.3933e-01, -6.9467e-02,  8.2241e-02,\n",
      "         -5.2627e-01,  2.4325e-01, -9.0519e-02, -3.4143e-01, -2.7180e-02],\n",
      "        [-2.8972e-01, -4.6233e-01,  2.5770e-01,  1.7639e-01, -5.3174e-02,\n",
      "          3.3730e-02,  2.6773e-01, -1.0576e-01, -5.9294e-02,  1.4846e-01,\n",
      "          3.1521e-01, -2.8073e-01, -4.7577e-02,  1.1212e-01, -2.9228e-03,\n",
      "          3.4970e-02,  4.5154e-03, -2.4862e-01,  1.4977e-01,  8.9802e-02,\n",
      "          1.7854e-01, -4.0862e-01,  1.4545e-01, -5.8805e-02,  1.1969e-01,\n",
      "         -5.5094e-01,  2.4022e-01, -7.7052e-02, -2.4741e-01, -3.0209e-02]])\n",
      "pred_alpha tensor([26,  2,  2,  2, 10, 10, 10, 10,  2, 26,  2,  2,  2,  2,  2, 10, 10,  2,\n",
      "         2, 10, 10, 10, 10,  2,  2,  2, 10,  2,  6,  2,  2, 10])\n",
      "labels_alpha tensor([18, 18,  8, 19,  5, 15,  4,  9, 19,  0, 14, 15, 26, 20, 26, 12, 13, 18,\n",
      "         1, 28, 15,  5, 13,  4, 20,  6, 15,  0,  6, 18,  3,  7])\n",
      "outputs_char tensor([[-0.3954, -0.0772,  0.0933,  ..., -0.2210,  0.3494, -0.2956],\n",
      "        [-0.4331, -0.0319,  0.0084,  ..., -0.1097,  0.3317, -0.3750],\n",
      "        [-0.4524, -0.0089, -0.0644,  ..., -0.0901,  0.4404, -0.3287],\n",
      "        ...,\n",
      "        [-0.4515,  0.0159, -0.0746,  ..., -0.0793,  0.3616, -0.4236],\n",
      "        [-0.4109,  0.0029,  0.0402,  ..., -0.1223,  0.3733, -0.3467],\n",
      "        [-0.4590, -0.0522, -0.0213,  ..., -0.1594,  0.3774, -0.3278]])\n",
      "pred_char tensor([352, 425, 347, 425, 425, 425, 577, 425, 347, 347, 425, 347, 825, 347,\n",
      "        195, 425, 425, 425, 680, 425, 195, 425, 425, 347, 425, 425, 195, 425,\n",
      "        195, 195, 425, 347])\n",
      "labels_char tensor([549, 561, 242, 586, 161, 477, 127, 293, 622,  12, 440, 473, 854, 637,\n",
      "        849, 374, 386, 585,  48, 904, 476, 162, 414, 125, 659, 186, 475,   5,\n",
      "        198, 574, 115, 237])\n",
      "Alphabet: 0.0516078844666481\n",
      "Character: 0.0012966805370524526\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92daa857-512a-41e7-9a39-18dc52e41315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f1093-6ebf-4e8e-8608-0a26b317408d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb7842-13c4-454c-9a96-00e73d83c7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1efb3-8337-4eff-8249-096e8cada50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6abbb-0e9c-4dbe-abaf-f7d58e7a4d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdb549-493a-4971-af3e-b68c5c21f572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09771f0d-a16b-426d-b4fb-e9eceb853a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
