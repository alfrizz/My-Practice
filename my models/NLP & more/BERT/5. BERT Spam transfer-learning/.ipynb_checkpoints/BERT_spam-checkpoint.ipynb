{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933d5b39-984f-4197-8f4c-95287da2c961",
   "metadata": {},
   "source": [
    "Import Required Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87192d-c99f-4a7f-9b65-9effa5c8cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, BertConfig, BertTokenizer, BertModel\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6ee13-c551-4216-a054-1409bb721898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spamdata_v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d609f3-0f28-44a8-be75-9de3fe6d8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee60259-7fca-4805-be14-53482cff5a50",
   "metadata": {},
   "source": [
    "Split the Dataset into train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ff530-a8c6-4c87-a81b-21092d7904fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)\n",
    "\n",
    "print(train_text.shape)\n",
    "print(temp_text.shape)\n",
    "print(val_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56922227-46e4-4652-92b6-1f01e00e2f46",
   "metadata": {},
   "source": [
    "Import Bert - base- uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794b5ab-dd78-46be-b8f4-3d5093a9f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa702a-f4f1-48ec-baa1-f4588f25375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4661ae1-de3c-4b06-802b-f2a51ca78260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fcdfa4-0b72-4d0f-b7c1-9a3bbd900c16",
   "metadata": {},
   "source": [
    "Tokenize & Encode the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9d95c-11a6-42ed-b1eb-7e604336b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(len(tokens_train['input_ids']))\n",
    "# 3900 sentences in the training set, that will be divided in 121 batches of size 32 plus 1 last batch of size 28 (see code below)\n",
    "\n",
    "print(tokens_train.keys())\n",
    "print(train_text.tolist()[44])\n",
    "print(tokens_train['input_ids'][44])\n",
    "print(tokens_train['token_type_ids'][44])\n",
    "print(tokens_train['attention_mask'][44])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf6dc9-03de-476e-8ea2-387024514d49",
   "metadata": {},
   "source": [
    "Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4179bb5-db1c-4204-8752-b00bf72e6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train text:\", train_text.tolist()[8])\n",
    "for key, value in tokens_train.items():\n",
    "    print( 'KEY: {}\\nVALUE: {}\\nDECODE: {}\\nCONVERTtoTOKENS: {}\\nLABEL: {}'.\n",
    "          format(key, value[8], tokenizer.decode(value[8]),tokenizer.convert_ids_to_tokens(value[8]),train_labels.values[8])) \n",
    "    \n",
    "print(\"----\")    \n",
    "\n",
    "similar = [\"Sorry, I'lll call later\"]\n",
    "print(\"similar text:\", similar)\n",
    "token_similar = tokenizer.batch_encode_plus(list(similar), max_length = 25, pad_to_max_length=True, truncation=True) #padding = True\n",
    "for key, value in token_similar.items():\n",
    "    print( 'KEY: {}\\nVALUE: {}\\nDECODE: {}\\nCONVERTtoTOKENS: {}'.\n",
    "          format(key, value[0], tokenizer.decode(value[0]),tokenizer.convert_ids_to_tokens(value[0]))) \n",
    "\n",
    "print(\"----------------------------------------------------------------------------------\")    \n",
    "\n",
    "Himalayas = \"where is Himalayas in the world map?\"\n",
    "print(\"Himalayas text:\", Himalayas)\n",
    "token_Himalayas = tokenizer.encode_plus(Himalayas)\n",
    "for key, value in token_Himalayas.items():\n",
    "    print( 'KEY: {}\\nVALUE: {}\\nDECODE: {}\\nCONVERTtoTOKENS: {}'.\n",
    "          format(key, value, tokenizer.decode(value),tokenizer.convert_ids_to_tokens(value))) \n",
    "    \n",
    "print(\"----\")    \n",
    "\n",
    "Himalayass = \"where is Himalayass in the world map?\"\n",
    "print(\"Himalayass text:\", Himalayass)\n",
    "token_Himalayass = tokenizer.encode_plus(Himalayass)\n",
    "for key, value in token_Himalayass.items():\n",
    "    print( 'KEY: {}\\nVALUE: {}\\nDECODE: {}\\nCONVERTtoTOKENS: {}'.\n",
    "          format(key, value, tokenizer.decode(value),tokenizer.convert_ids_to_tokens(value))) \n",
    "    \n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "q1 = 'Who was Tony Stark?'\n",
    "c1 = 'Anthony Edward Stark known as Tony Stark is a fictional character in Avengers'\n",
    "print(\"double text:\", q1, c1)\n",
    "encoding = tokenizer.encode_plus(q1, c1)\n",
    "for key, value in encoding.items():\n",
    "    print( 'KEY: {}\\nVALUE: {}\\nDECODE: {}\\nCONVERTtoTOKENS: {}'.\n",
    "          format(key, value, tokenizer.decode(value),tokenizer.convert_ids_to_tokens(value))) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ea873-a872-404a-9fbf-71e035519abc",
   "metadata": {},
   "source": [
    "List to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc380861-d02b-4664-b426-4917d5dc7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids']).cuda()\n",
    "train_ids = torch.tensor(tokens_train['token_type_ids']).cuda()\n",
    "train_mask = torch.tensor(tokens_train['attention_mask']).cuda()\n",
    "train_y = torch.tensor(train_labels.tolist()).cuda()\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids']).cuda()\n",
    "val_ids = torch.tensor(tokens_val['token_type_ids']).cuda()\n",
    "val_mask = torch.tensor(tokens_val['attention_mask']).cuda()\n",
    "val_y = torch.tensor(val_labels.tolist()).cuda()\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids']).cuda()\n",
    "test_ids = torch.tensor(tokens_test['token_type_ids']).cuda()\n",
    "test_mask = torch.tensor(tokens_test['attention_mask']).cuda()\n",
    "test_y = torch.tensor(test_labels.tolist()).cuda()\n",
    "\n",
    "#example\n",
    "\n",
    "sent_idx = 7\n",
    "print(\"train text:\", train_text.tolist()[sent_idx])\n",
    "for key, value in tokens_train.items():\n",
    "    print( 'KEY: {}\\nVALUE: {}\\nDECODE: {}\\nCONVERTtoTOKENS: {}\\nLABEL: {}'.\n",
    "          format(key, value[sent_idx], tokenizer.decode(value[sent_idx]),tokenizer.convert_ids_to_tokens(value[sent_idx]),test_labels.values[sent_idx]))     \n",
    "print('-------------')    \n",
    "print(test_seq[sent_idx])\n",
    "print(test_ids[sent_idx])\n",
    "print(test_mask[sent_idx])\n",
    "print(test_y[sent_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a6d2a-31ab-46a0-928a-4f560d9f77b7",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe26e0-cadb-4cc1-b694-a118284411bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "#Every DataLoader has a Sampler which is used internally to get the indices for each batch\n",
    "\n",
    "# sampler for sampling the data during training (shuffled random indexes)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "count = 0\n",
    "for i in train_sampler:\n",
    "    # iterating over the RandomSampler\n",
    "        print('count:',count)\n",
    "        print(i)\n",
    "        count +=1\n",
    "        if (count >= 5):\n",
    "            break\n",
    "    \n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during validation (non shuffled sequential indexes)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "count = 0\n",
    "for i in val_sampler:\n",
    "    # iterating over the SequentialSampler\n",
    "        print('count:',count)\n",
    "        print(i)\n",
    "        count +=1\n",
    "        if (count >= 5):\n",
    "            break\n",
    "\n",
    "\n",
    "    \n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "\n",
    "print(train_data)\n",
    "print(train_sampler)\n",
    "print(train_dataloader)\n",
    "print(val_data)\n",
    "print(val_sampler)\n",
    "print(val_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60aef02-0042-4296-9219-e64b94538aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids, mask, label in val_dataloader: #sequential indexes\n",
    "    print('index:',ids[0],'\\nmask:',mask[0],'\\nlabel:',label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01af21-b85c-41ce-88a0-48bf676e646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids, mask, label in train_dataloader: #shuffled indexes\n",
    "    print('index:',ids[0],'\\nmask:',mask[0],'\\nlabel:',label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2707c2-38eb-4c01-a5f9-630e1de48c9b",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ea1e5-90b5-4e7e-9bb8-501a1028fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "#In some cases, you might be interested in keeping the weights of the pre-trained encoder frozen \n",
    "#and optimizing only the weights of the head layers\n",
    "\n",
    "#Take layers from a previously trained model. \n",
    "#Freeze them, so as to avoid destroying any of the information they contain during future training rounds. \n",
    "#Add some new, trainable layers on top of the frozen layers. \n",
    "#They will learn to turn the old features into predictions on a new dataset.\n",
    "#Freezing reduces training time as the backward passes go down in number\n",
    "#you only need to backpropagate the gradient and update the weights of the not freezed layers\n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c0ac0-3d71-4f2d-b4bd-0e374cc647cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12605397-4743-4dd4-9fea-aff8876852de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our defined architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191c4eb-dee2-483b-a314-0e685afbf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a8c7f-0602-4f81-b5c1-f110d880cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in model.parameters():\n",
    "#     print(i.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4a1c0-8751-4d6c-8040-5833c394d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37c1da-d1cc-4d9e-9864-dadd534b8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight(class_weight ='balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd8076-3b0f-47da-b8f8-d624d6a2fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd975d-5a9c-43ef-a1f1-5d584f4423ec",
   "metadata": {},
   "source": [
    "Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e77d0-f8ca-4de9-be1d-821e2aab6039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train() # To train the model, you should first set it back in training mode with model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 30 batches.\n",
    "        if step % 30 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch - average per epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02011913-2df7-4b88-aa24-63d35949dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval() # is a kind of switch for some specific layers/parts of the model \n",
    "                 # that behave differently during training and inference (evaluating) time. \n",
    "                 # For example, Dropouts Layers, BatchNorm Layers etc. \n",
    "                 # You need to turn them off during model evaluation, and .eval() will do it for you\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 30 batches.\n",
    "        if step % 30 == 0 and not step == 0:\n",
    "            \n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad(): #we don't compute or use gradients during evaluation, so turning off the autograd will speed up execution and will reduce memory usage\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c760c-bd52-489e-9bd6-e3403f444555",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perc_list = []\n",
    "\n",
    "for step,batch in enumerate(train_dataloader):\n",
    "    sent_id = batch[0]\n",
    "    # print(sent_id)\n",
    "    mask = batch[1]\n",
    "    # print(mask)\n",
    "    label = batch[2]\n",
    "    # print(label)\n",
    "    \n",
    "# prediction_percent are in the form of:      no. of batches (122), size of batch (32) , no. of classes (2).\n",
    "# reshape the predictions_percent in form of: number of samples (3900), no. of classes(2).\n",
    "    \n",
    "    pred_perc = model(sent_id, mask) # tensor of 32, as batch size\n",
    "    pred_perc = pred_perc.detach().cpu().numpy()\n",
    "    print('batch #', len(pred_perc_list), pred_perc.shape)    \n",
    "    pred_perc_list.append(pred_perc)  \n",
    "    # break    \n",
    "    \n",
    "pred_perc_list_reshaped  = np.concatenate(pred_perc_list, axis=0)\n",
    "print('pred percent list length:',len(pred_perc_list))\n",
    "print('pred percent list reshaped:', pred_perc_list_reshaped.shape)\n",
    "print('last pred perc:', pred_perc.shape, '\\n', pred_perc) #last one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec6842-a50a-4369-b8d9-9f298ff162ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa856bb-4bd1-4965-b134-7b457df5b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce68bd1-0ebf-499a-b6ef-f627d92e036c",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfd4de-d15c-43b1-8c6f-72a65d391a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad(): #no gradients computation needed\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2304c15-897c-4e2b-9e5e-49e6311afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's performance\n",
    "test_y = test_y.detach().cpu().numpy()\n",
    "preds_01 = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds_01))\n",
    "\n",
    "print(type(test_y))\n",
    "print(type(preds_01))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
