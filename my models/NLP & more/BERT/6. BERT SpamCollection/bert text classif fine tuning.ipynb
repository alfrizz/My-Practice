{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a222552b-237d-49f5-925c-9df01cb412ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5966f291-2287-45e7-919c-0cb1c26218be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 0,\n",
       "  'text': 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n'},\n",
       " {'label': 0, 'text': 'Ok lar... Joking wif u oni...\\n'},\n",
       " {'label': 1,\n",
       "  'text': \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\"}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('SMSSpamCollection') as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        split = line.split('\\t')\n",
    "        data.append({'label': 1 if split[0] == 'spam' else 0,\n",
    "                     'text': split[1]},)\n",
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0de070a-f2e6-44dd-8f6c-5ad2ff2ed492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ã¼ b going to esplanade fr home?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                    Ok lar... Joking wif u oni...\\n\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro...\n",
       "...     ...                                                ...\n",
       "5569      1  This is the 2nd time we have tried 2 contact u...\n",
       "5570      0            Will Ã¼ b going to esplanade fr home?\\n\n",
       "5571      0  Pity, * was in mood for that. So...any other s...\n",
       "5572      0  The guy did some bitching but I acted like i'd...\n",
       "5573      0                       Rofl. Its true to its name\\n\n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f247b87-bbc6-4a42-83ce-5ad2054203d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n',\n",
       "       'Ok lar... Joking wif u oni...\\n',\n",
       "       \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\",\n",
       "       ..., 'Pity, * was in mood for that. So...any other suggestions?\\n',\n",
       "       \"The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\\n\",\n",
       "       'Rofl. Its true to its name\\n'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.text.values\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb2f95cc-af1b-44f3-8273-35cf51f11253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.label.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16a57343-ff99-4cd3-baf3-2d69792b4bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to preprocess the text source before feeding it to BERT. To do so, we download the BertTokenizer:\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e44decb8-0717-4dc3-90b6-cc5b7bbed385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3636\n",
      "['it', \"'\", 's', 'not', 'that', 'you', 'make', 'me', 'cry', '.', 'it', \"'\", 's', 'just', 'that', 'when', 'all', 'our', 'stuff', 'happens', 'on', 'top', 'of', 'everything', 'else', ',', 'it', 'pushes', 'me', 'over', 'the', 'edge', '.', 'you', 'don', \"'\", 't', 'under', '##dt', '##and', 'how', 'often', 'i', 'cry', 'over', 'my', 'sorry', ',', 'sorry', 'life', '.']\n",
      "[2009, 1005, 1055, 2025, 2008, 2017, 2191, 2033, 5390, 1012, 2009, 1005, 1055, 2074, 2008, 2043, 2035, 2256, 4933, 6433, 2006, 2327, 1997, 2673, 2842, 1010, 2009, 13956, 2033, 2058, 1996, 3341, 1012, 2017, 2123, 1005, 1056, 2104, 11927, 5685, 2129, 2411, 1045, 5390, 2058, 2026, 3374, 1010, 3374, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "rand_index = random.randint(0, len(text)-1) # to select one random SMS index\n",
    "print(rand_index)\n",
    "sms_tokens = tokenizer.tokenize(text[rand_index])\n",
    "print(sms_tokens)\n",
    "sms_ids = tokenizer.convert_tokens_to_ids(sms_tokens)\n",
    "print(sms_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9fa3554c-84ae-4eaf-a9b0-e56ea1fddb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╕\n",
      "│ Tokens   │   Token IDs │\n",
      "╞══════════╪═════════════╡\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ happy    │        3407 │\n",
      "├──────────┼─────────────┤\n",
      "│ birthday │        5798 │\n",
      "├──────────┼─────────────┤\n",
      "│ .        │        1012 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "├──────────┼─────────────┤\n",
      "│ hi       │        7632 │\n",
      "╘══════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence():\n",
    "    '''Displays the tokens and respective IDs of a random text sample'''\n",
    "    index = random.randint(0, len(text)-1) \n",
    "    table = np.array([tokenizer.tokenize(text[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "    print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "46e3e054-4c18-45d4-b95e-dc9310035989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7929,  2474,  2099,  1012,  1012,  1012, 16644, 15536,  2546,\n",
       "          1057,  2006,  2072,  1012,  1012,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "    '''\n",
    "    Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "    '''\n",
    "    # the encode_plus method from the tokenizer object is commonly used in transformers for Natural Language Processing tasks.\n",
    "    return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 32,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "\n",
    "preprocessing(text[1], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d11225f-af0c-4c35-b6c6-c52a9a552483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  7929,  2474,  2099,  1012,  1012,  1012, 16644, 15536,  2546,\n",
       "          1057,  2006,  2072,  1012,  1012,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in text:\n",
    "    encoding_dict = preprocessing(sample, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "# We can observe the token IDs for a text sample and recognize the presence of the special tokens 101 [CLS] and 102 [SEP], as well as the padding 0 [PAD] up to the desired max_length:\n",
    "    \n",
    "print(type(token_id[0]), type(attention_masks[0])) # lists of tensors generated\n",
    "token_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb3db44c-c162-4f42-b5a7-26b5bbfbf57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2175,  2127,  ..., 28194,  1012,   102],\n",
       "        [  101,  7929,  2474,  ...,     0,     0,     0],\n",
       "        [  101,  2489,  4443,  ...,  2000,  4374,   102],\n",
       "        ...,\n",
       "        [  101, 12063,  1010,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  3124,  ...,  2489,   102,     0],\n",
       "        [  101, 20996, 10258,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # This line concatenates the list of tensors in token_id along the first dimension (dimension 0). The result is a single tensor.\n",
    "    \n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "print(type(token_id))\n",
    "token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f5f441dd-aa7e-4c58-9657-9b8819669d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above: converting the 'attention_mask' list of tensors to a  single tensor\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a9831146-8f86-480a-a0c9-ff73fb3b55e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\AppData\\Local\\Temp\\ipykernel_44516\\2896184176.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line converts the list of labels into a PyTorch tensor.\n",
    "labels = torch.tensor(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bb9e4626-8333-4ac1-8333-be7a7207d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  7929,  2474,  2099,  1012,  1012,  1012, 16644, 15536,  2546,\n",
      "         1057,  2006,  2072,  1012,  1012,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "[CLS] ok lar... joking wif u oni... [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "['[CLS]', 'ok', 'la', '##r', '.', '.', '.', 'joking', 'wi', '##f', 'u', 'on', '##i', '.', '.', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(token_id[1])\n",
    "print(tokenizer.decode(token_id[1]))\n",
    "print(tokenizer.tokenize(tokenizer.decode(token_id[1])))\n",
    "print(attention_masks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "703660b2-11ad-4d13-b86a-daf5cf8f0622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]    │         101 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ he       │        2002 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ says     │        2758 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ hi       │        7632 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ and      │        1998 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ to       │        2000 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ get      │        2131 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ your     │        2115 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ass      │        4632 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ back     │        2067 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ to       │        2000 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ south    │        2148 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ tampa    │        9925 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ (        │        1006 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ prefer   │        9544 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##ably   │        8231 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ at       │        2012 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ a        │        1037 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ke       │       17710 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##gger   │       13327 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ )        │        1007 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]    │         102 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "    '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "    index = random.randint(0, len(text) - 1)\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "    token_ids = [i.numpy() for i in token_id[index]]\n",
    "    attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "    table = np.array([tokens, token_ids, attention]).T\n",
    "    print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c385f49f-601a-4559-8ae3-35eb86be95bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4459,) (1115,)\n"
     ]
    }
   ],
   "source": [
    "# We split the dataset into train (80%) and validation (20%) sets, and wrap them around a torch.utils.data.DataLoader object. With its intuitive syntax, DataLoader provides an iterable over the given dataset.\n",
    "\n",
    "val_ratio = 0.2\n",
    "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "batch_size = 16\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "print(train_idx.shape, val_idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0c1de98-b2d5-4757-bdd8-6074a10f9dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x1dd170a7880>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8532163d-2a81-4b44-add5-1f90dfa925f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1dd5b0f95a0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "12291896-225d-4673-864e-2ab06a1a2c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101,  8840,  2140,  2085,  1045,  1005,  1049,  2044,  2008,  2980,\n",
      "          2250, 13212,   999,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2305,  2305,  1010,  2156,  2017,  4826,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 13433, 19903, 23778, 20689,  1010, 12849, 20051,  6979, 19636,\n",
      "         14852,  1010,  4895, 25787,  3126,  2695,  1010,  9413, 10244,  4487,\n",
      "          2015,  1010,  1004,  8318,  1025,  1001,  1004, 14181,  1025,  1012,\n",
      "           102,     0],\n",
      "        [  101,  2129,  2003,  2115,  6134,  2279,  2733,  1029,  1045,  2572,\n",
      "          2041,  1997,  2237,  2023,  5353,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2821,  1024,  1011,  1007,  2069,  1018,  2648,  2867,  3039,\n",
      "          2000,  2377,  2113,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2031,  2115,  6265,  1998,  2272,  2855,  1998,  2330,  1996,\n",
      "          2341,  1024,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2053,  6203,  1045,  2079,  2031,  2489,  7696,  2302,  2151,\n",
      "         28667,  8167,  3351,  1012,  7632,  7632,  7632,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  8840,  2140,  2115,  2467,  2061, 13359,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2074,  3773,  2115,  4771,  2655,  2026,  6203,  2567,  1012,\n",
      "          2079,  2031,  1037, 24665,  2620,  2154,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 22434, 10470,  2017,  2031,  2180,  1037, 22098,  5824, 12376,\n",
      "          2072,  1012,  2023,  2003,  2054,  2017,  2131,  2043,  2017,  2663,\n",
      "          2256,  2489, 10470,  1012,  2000,  2202,  2112,  4604, 22098,  2000,\n",
      "          6564,   102],\n",
      "        [  101,  8744,  2003,  8744,  1012,  2021, 28194,  2003,  8744,  1029,\n",
      "          8840,  2140,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2005, 24471,  3382,  2000,  2663,  1037,  1037, 29646, 17788,\n",
      "          2692,  5356,  2296,  1059,  2243, 19067,  2102,  1024,  2895,  2000,\n",
      "          3770, 16086,  2620,  1012,  1056,  1005,  1055,  1004,  1039,  1005,\n",
      "          1055,   102],\n",
      "        [  101, 12098,  5603,  2339,  1996,  6616,  2003,  6343,  1999,  2237,\n",
      "          1025,  1035,  1025,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2025,  3892,  6775,  1012,  9105,  2039,  2006,  2070,  3637,\n",
      "          1012,  2023,  2003,  2026,  2047,  2193,  2011,  1996,  2126,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  3913,  2158,   999,  2074,  2000,  2292,  2017,  2113,\n",
      "          2017,  2024,  4771,  1998,  2245,  2125,  1012,  2079,  2031,  1037,\n",
      "          2307,  2154,  1012,  1998,  2065,  2017,  2064,  4604,  2033, 12170,\n",
      "         13344,   102],\n",
      "        [  101,  2092,  1045,  2097,  3422, 14021, 16816,  1999,  7605,   999,\n",
      "           999,  1038,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Attention Masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Loop over the DataLoader\n",
    "for batch in train_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    print(\"Input IDs:\", b_input_ids)\n",
    "    print(\"Attention Masks:\", b_input_mask)\n",
    "    print(\"Labels:\", b_labels)\n",
    "    \n",
    "    # Break after the first batch for brevity\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "81f2a536-88ff-4830-8c37-e294149ed2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "    '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "    return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "    '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "    return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "    '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "    return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "    '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "    return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "    '''\n",
    "    Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "    '''\n",
    "    preds = np.argmax(preds, axis = 1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    tp = b_tp(preds, labels)\n",
    "    tn = b_tn(preds, labels)\n",
    "    fp = b_fp(preds, labels)\n",
    "    fn = b_fn(preds, labels)\n",
    "    b_accuracy = (tp + tn) / len(labels)\n",
    "    b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "    b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "    b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "    return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7aac1bdb-7733-4a86-b2b4-4d69361fcfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "# This code snippet is initializing the BertForSequenceClassification model with the bert-base-uncased pre-trained weights.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "'''\n",
    "model.parameters(): This is passing all of the model’s parameters to the optimizer, so they can be updated during training.\n",
    "lr = 5e-5: This is the learning rate, which controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "eps = 1e-08: This is a very small number to prevent any division by zero in the implementation (also known as epsilon).\n",
    "'''\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n",
    "# Run on GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "775f060a-5b7b-4225-9a38-5e8f60159dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ca7aa-d450-4bdd-8cc0-5d1002246b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training and validation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9f7a2e2f-b188-4f6a-86df-cca3a23509a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|████████████████████████████████████████████████████████████▌                                                            | 1/2 [00:28<00:28, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.0953\n",
      "\t - Validation Accuracy: 0.9732\n",
      "\t - Validation Precision: 0.8709\n",
      "\t - Validation Recall: 0.9627\n",
      "\t - Validation Specificity: 0.9724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:55<00:00, 27.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.0322\n",
      "\t - Validation Accuracy: 0.9893\n",
      "\t - Validation Precision: 0.9874\n",
      "\t - Validation Recall: 0.9279\n",
      "\t - Validation Specificity: 0.9980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 2\n",
    "\n",
    "# trange: This is a function from the tqdm module, which is a fast, extensible progress bar for Python. It’s used here to display a progress bar for the epochs in the console.\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    # model.train(): This sets the model to training mode. This is important because some layers like dropout and batch normalization behave differently during training and evaluation.\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad() # This clears old gradients from the last step (otherwise you’d just accumulate the gradients from all loss.backward() calls)\n",
    "        # Forward pass\n",
    "        # train_output = model(...): This performs a forward pass through the model, which computes predictions based on the input ids, attention masks, and labels.\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward() # This computes the gradient of the loss with respect to the model parameters.\n",
    "        optimizer.step() #  This updates the model parameters based on the current gradient.\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # By requiring explicit calls to these functions, PyTorch gives you more control over the training process, which can be very helpful for implementing more complex training regimes.\n",
    "        \n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader: # This starts a loop over the validation data. Each batch is a tuple containing input ids, attention masks, and labels for a batch of examples.\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad(): # This context manager tells PyTorch not to construct the computation graph during this block, which reduces memory consumption and speeds up computation. The computation graph is used for automatic differentiation, which is essential for backpropagation during training. However, during evaluation, we’re not updating the model’s parameters, so we don’t need to compute gradients.\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy() # detach the logits from the computation graph, move them to the CPU, and convert them to numpy arrays. The reason we move logits and labels to the CPU is because many operations in PyTorch, especially those that involve converting to numpy or native Python types, are not supported on the GPU.\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ae0d7-a772-45a1-a2c4-d97507b414e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predicting\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2f42d625-350e-4380-91dd-69572bbc529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  call me at 123343453 to give you your prize of one million euro\n",
      "Predicted Class:  Spam\n",
      "-------------\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8679,  2.1930]], device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor([[-1.8679,  2.1930]], device='cuda:0')\n",
      "[[-1.8678647  2.1930404]]\n",
      "1\n",
      "[1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# new_sentence = 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'\n",
    "\n",
    "new_sentence = 'call me at 123343453 to give you your prize of one million euro'\n",
    "\n",
    "\n",
    "# We need Token IDs and Attention Mask for inference on the new sentence\n",
    "test_ids = []\n",
    "test_attention_mask = []\n",
    "\n",
    "# Apply the tokenizer\n",
    "encoding = preprocessing(new_sentence, tokenizer)\n",
    "\n",
    "# Extract IDs and Attention Mask\n",
    "test_ids.append(encoding['input_ids'])\n",
    "test_attention_mask.append(encoding['attention_mask'])\n",
    "test_ids = torch.cat(test_ids, dim = 0)\n",
    "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "# These lines are concatenating the lists of input IDs and attention masks into PyTorch tensors. The dim = 0 argument means that the tensors are concatenated along the first dimension (i.e., they are stacked vertically).\n",
    "# After this snippet, test_ids and test_attention_mask should be PyTorch tensors ready to be fed into your model.\n",
    "\n",
    "# Forward pass, calculate logit predictions\n",
    "with torch.no_grad():\n",
    "    output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "\n",
    "prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n",
    "\n",
    "print('Input Sentence: ', new_sentence)\n",
    "print('Predicted Class: ', prediction)\n",
    "print('-------------')\n",
    "print(output)\n",
    "print(output.logits)\n",
    "print(output.logits.cpu().numpy())\n",
    "print(np.argmax(output.logits.cpu().numpy()))\n",
    "print(np.argmax(output.logits.cpu().numpy()).flatten())\n",
    "print(np.argmax(output.logits.cpu().numpy()).flatten().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70fb82-9e85-4608-a32a-b17e625ec6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e4c2a-d929-49c0-beff-8b2256751dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75edb1-6018-4a7a-a64a-78b30f811b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9cd0a-9e28-4af0-9b2c-f90fd61f0d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503374b-3867-465e-9078-f57ca063b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339eb98-5f46-4b2e-9558-5e915cecd986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
