{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134d2c0a-c2f1-45f6-bc39-2cb800ad3b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import math  \n",
    "\n",
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f579bcb-a96e-4156-b1ef-2dbaa444adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x270b06c3f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "# Load the pipeline for sentiment classification\n",
    "classifier = pipeline(\"text-classification\", model=model_name)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933b2234-cf9e-45e6-89d8-51620e70cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '3 stars', 'score': 0.6387940645217896}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6403d66d-f505-4ad0-ad98-09263fbc9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '4 stars', 'score': 0.5190770030021667}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, everything well organized\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a22190-ca52-4b7b-aed9-45bb268f6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.SummarizationPipeline at 0x270b06f7b80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cnicu/t5-small-booksum'\n",
    "\n",
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline('summarization', model = model_name)\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5111f86-f864-4b68-bb1f-244541711108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = '\\nThe tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\\n'\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length = 30)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c80d10-21eb-4c39-8b89-843b4b2cb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n"
     ]
    }
   ],
   "source": [
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1055b1e9-7c8d-4f56-9ab9-9a1e8e5082d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millau Viaduct\n"
     ]
    }
   ],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "question = \"What's the tallest structure in France?\"\n",
    "\n",
    "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
    "outputs = qa_model(question, long_text)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32cf35b-340a-426c-bebf-4013cfa3e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't think you're doing a good translation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "\n",
    "input_text = \"No creo que hagas una buena traducción\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline('translation_es_to_en', model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f6f578-4871-4b5f-8c06-b0dfa5f64e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer model hyperparameters\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126ece2e-d3d9-464e-aead-b62880bfc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model, # d_model is the dimension of the input vectors and output vectors of the model, specifically the size of the feature space. Essentially, it determines the number of features in each transformer layer\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,    \n",
    "    num_decoder_layers=num_decoder_layers\n",
    "    )\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943a497a-5f46-407d-be6e-da7d37c310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fecc1dc-bf8c-41b2-b66d-17c1a782f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Customer review:\\nI had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\\n\\nHotel reponse to the customer:\\nDear valued customer, I am glad to hear you had a good stay with us.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the prompt for the text generation LLM\n",
    "\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8261f8c5-a1b9-4d48-bb88-252d2a72582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
      "\n",
      "Hotel reponse to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. We do love it here. Our entire restaurant consists of great food, great service, great guests\n"
     ]
    }
   ],
   "source": [
    "#### Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length = 100, pad_token_id=generator.tokenizer.eos_token_id) #  if the generated text is shorter than max_length, the remaining tokens will be filled with the EOS token.\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75cce4e3-0cb2-42b4-b8fb-547a09f7242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************** PositionalEncoder VERSION WITH COMMENTS AND PRINTS FOR EXPLANATION *************************\n",
    "\n",
    "# Subclass the PyTorch nn.Module class to create a custom module for positional encoding\n",
    "# This class is used to add positional information to the input embeddings in a Transformer model\n",
    "class PositionalEncoder_explanations(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        # Call the parent class's constructor\n",
    "        super(PositionalEncoder_explanations, self).__init__()\n",
    "        \n",
    "        # Initialize the dimensions of the model and the maximum sequence length\n",
    "        self.d_model = d_model  # The dimension of the input embeddings\n",
    "        self.max_length = max_length  # The maximum length of words of tokens in the input sequences\n",
    "        \n",
    "        # Initialize the positional encoding matrix with zeros\n",
    "        # This matrix will store the positional encodings that will be added to the input embeddings\n",
    "        pe = torch.zeros(max_length, d_model)  \n",
    "\n",
    "        # Create a tensor of positions from 0 to max_length\n",
    "        # This tensor represents the positions of the words in a sequence\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)  #  we unsqueeze because the multiplication operation position * div_term requires position to be a 2D tensor to correctly broadcast with div_term (the extra dimension is added at position '1': second position)\n",
    "        print('position:', position)\n",
    "        \n",
    "        # Calculate the division term div_term for the positional encoding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))  #  Increasing the embedding size d_model creates more dimensions. The last dimensions will change slowly due to the exponentially decreasing div_term. This happens because sine and cosine of small values hover close to 0 or 1, meaning they change slowly (calculated below over: position * div_term). This slow change is great for capturing long-range dependencies and the global context in longer sequences. On the flip side, the first dimensions will change more rapidly and be more useful for shorter texts (so in a longer text, the difference between the encoding of adjacent words may not be as pronounced as the difference between adjacent words in a shorter text).\n",
    "        print('div_term:', div_term)\n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        # For even indices, use sine of the position times the division term\n",
    "        # For odd indices, use cosine of the position times the division term\n",
    "        # These encodings are based on sine and cosine functions of different frequencies\n",
    "        # The sine and cosine functions are used to ensure that the positional encodings are continuous and differentiable, which is important for the learning process. Also, these functions generate values between -1 and 1, which helps to keep the magnitude of the positional encodings manageable.\n",
    "        # Using sine for even indices and cosine for odd indices provides two different signals for each position, which helps the model distinguish between different positions more effectively.\n",
    "        print('position * div_term:', position * div_term)\n",
    "        print('torch.sin(position * div_term):', torch.sin(position * div_term))\n",
    "        print('torch.cos(position * div_term):', torch.cos(position * div_term))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  \n",
    "        print('pe:', pe)\n",
    "        \n",
    "        # Add an extra dimension to the positional encoding matrix, turning it from a 2D tensor into a 3D tensor(the extra dimension is added at position '0': first position)\n",
    "        # This is done to match the dimensions of the input embeddings (batch size, sequence length, and embedding size)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        \n",
    "        # Register the positional encoding matrix as a buffer that should not be considered a model parameter\n",
    "        # Buffers are tensors that are not updated during backpropagation but need to be part of the model's state\n",
    "        self.register_buffer('pe', pe)  # N.B. self.pe is defined when pe is registered as a buffer. \n",
    "    \n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Update the input tensor by adding the positional encodings\n",
    "        # The positional encodings are added to the input embeddings so that the model can take into account the position of words in a sequence\n",
    "        x = x + self.pe[:, :x.size(1)]  # returning the input sequence plus the positional encoding (ensuring that you're slicing the positional encodings to match the length of your input sequence)\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee96e1ca-332d-4061-a2ec-51b4b5d9b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.]])\n",
      "div_term: tensor([1.0000, 0.0100])\n",
      "position * div_term: tensor([[0.0000, 0.0000],\n",
      "        [1.0000, 0.0100],\n",
      "        [2.0000, 0.0200],\n",
      "        [3.0000, 0.0300],\n",
      "        [4.0000, 0.0400],\n",
      "        [5.0000, 0.0500],\n",
      "        [6.0000, 0.0600],\n",
      "        [7.0000, 0.0700]])\n",
      "torch.sin(position * div_term): tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.8415,  0.0100],\n",
      "        [ 0.9093,  0.0200],\n",
      "        [ 0.1411,  0.0300],\n",
      "        [-0.7568,  0.0400],\n",
      "        [-0.9589,  0.0500],\n",
      "        [-0.2794,  0.0600],\n",
      "        [ 0.6570,  0.0699]])\n",
      "torch.cos(position * div_term): tensor([[ 1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9999],\n",
      "        [-0.4161,  0.9998],\n",
      "        [-0.9900,  0.9996],\n",
      "        [-0.6536,  0.9992],\n",
      "        [ 0.2837,  0.9988],\n",
      "        [ 0.9602,  0.9982],\n",
      "        [ 0.7539,  0.9976]])\n",
      "pe: tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "        [ 0.6570,  0.7539,  0.0699,  0.9976]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PositionalEncoder_explanations()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "max_length = 8\n",
    "\n",
    "PositionalEncoder_explanations(d_model, max_length) # in this example of 8 words, the first two columns change rapidly, while the last two columns change slowly. For shorter texts, the rapid changes in the first columns help the model distinguish between closely spaced elements. For longer texts, the slow changes in the last columns help the model capture long-range dependencies and maintain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20584520-7d15-4a8a-b567-045a1ed57f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Create a positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add extra dimension to match input embeddings\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f81cc6b9-c4fd-49ba-a153-aca7734107be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Number of attention heads.\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Dimension of the input embeddings.\n",
    "        self.d_model = d_model \n",
    "\n",
    "        # Dimension of each head.\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Linear transformations for queries, keys, and values.\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final linear transformation for concatenated output.\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size): # splits the input vectors into multiple \"heads\" to allow parallel attention mechanisms. Each head processes the data differently, helping the model learn diverse representations and capture various aspects of the input data.\n",
    "        # Split input vectors into different attention heads.\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        # Rearrange dimensions to bring heads to the second dimension.\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim) \n",
    "    \n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # Compute attention weights.\n",
    "        scores = torch.matmul(query, key.permute(0, 2, 1))  # Fixed from original\n",
    "        if mask is not None:\n",
    "            # Apply mask to prevent focusing on certain positions.\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e20\")) # In the transformer’s encoder, you don’t want the attention mechanism to consider [PAD] tokens. So, the mask tells the model to ignore these positions\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Compute output of multi-head attention layer.\n",
    "        batch_size = query.size(0) \n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        attention_weights = self.compute_attention(query, key, mask) # These heads independently compute attention scores to focus on different parts of the input\n",
    "\n",
    "        output = torch.matmul(attention_weights, value) # The outputs from all heads are concatenated and linearly transformed in a final output: a context vector that combines information from all heads, representing a rich and comprehensive understanding of the input.\n",
    "        # Reshape output to match dimensions\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6e3116-b20c-41c4-843e-579eef900fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module): # helps in adding depth and complexity to the model's capability to learn intricate patterns and representations\n",
    "    \n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3376a16-5901-49f1-823d-57fa8d88fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "# In self-attention, especially within the encoder of a transformer, the same x is used for queries, keys, and values. Here's why:\n",
    "# Query (Q): Represents the current token that’s “asking” for information.\n",
    "# Key (K): Represents the tokens that can provide information.\n",
    "# Value (V): Represents the actual information content of the tokens.        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask) # The input x is a sequence of embeddings representing the input tokens, and its shape is generally (batch_size, sequence_length, d_model). In the context of self-attention mechanisms, such as the one used in Transformer models, x is used as the Query (Q), Key (K), and Value (V). This allows the model to compute attention scores based on the input itself.\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # The operation x + self.dropout(attn_output) is an example of a technique called residual connection: The idea is that it’s easier to model a residual (or difference) than to learn to model the full information. In this specific case,we are “adding the residual”, that is the output of the self-attention mechanism (which has learned how to modify the input) back to the original input. \n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca2f4c09-8492-4b39-aca1-d14795b8e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module): # whole transformer encoder structure that includes a num_layers number of encoder layers\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "# Initial x Input:\n",
    "# Shape: (batch_size, sequence_length)\n",
    "# This represents indices of tokens in the sequence.\n",
    "\n",
    "# After Embedding Layer:\n",
    "# Converts token indices into dense vectors of fixed size d_model.\n",
    "# Output x Shape: (batch_size, sequence_length, d_model)\n",
    "# This transforms each token index into a d_model-dimensional vector.   \n",
    "\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d2ca32-6967-4db0-9f5c-6c2f69f590d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "# having x shape (batch_size, sequence_length, d_model),\n",
    "# the below expression x[:, 0, :] retrieves the first token's embedding from each sequence in the batch. Here's the breakdown:\n",
    "# x[:, 0, :]:\n",
    "# : (first position) selects all batches.\n",
    "# 0 (second position) selects the first token in the sequence.\n",
    "# : (third position) selects all dimensions of the embedding vector.\n",
    "# So, for each sequence in the batch, this slice pulls out the embedding corresponding to the very first token. \n",
    "# Essentially, it narrows down x from (batch_size, sequence_length, d_model) to (batch_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a89eaf52-2dbf-44f5-b7fd-c56f21062c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 256\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a35e7da-1001-40cc-be8d-2a340c9ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2730, 9633, 7310,  ..., 5337, 8911, 8655],\n",
       "        [9100, 3130, 1467,  ..., 5131, 2311, 4296],\n",
       "        [9289, 4444, 5106,  ..., 2503,  715, 5002],\n",
       "        ...,\n",
       "        [1957, 2294, 6730,  ..., 2863, 3687, 6085],\n",
       "        [4608, 1944, 2644,  ..., 7562, 8687, 6053],\n",
       "        [2278, 5717, 1099,  ..., 7628, 9984,  743]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "\n",
    "print(input_sequence.shape) # (batch_size, sequence_length)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1df462c-a0e9-47df-890b-feab943ddc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 1],\n",
       "        [1, 0, 1,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 0, 1,  ..., 0, 1, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "print(mask.shape) \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e0b16ba-3c0e-4366-8687-6a890b514fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(10000, 512)\n",
       "  (positional_encoding): PositionalEncoder()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardSubLayer(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18a03e94-7280-464a-b49b-3ff6540a97d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierHead(\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7e7d819-2ddd-4a9c-b864-c8df512e6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4522e+00, -3.4187e-01, -1.1727e+00,  ...,  5.9640e-01,\n",
       "          -1.7281e-01,  1.4411e+00],\n",
       "         [ 4.5715e-01,  9.5469e-01, -2.3045e-01,  ...,  5.4554e-01,\n",
       "          -8.8477e-01, -8.5107e-01],\n",
       "         [ 1.4361e+00, -9.3627e-01, -9.5775e-01,  ...,  5.2695e-01,\n",
       "          -5.1185e-01,  3.4254e-01],\n",
       "         ...,\n",
       "         [-5.2025e-01,  1.9090e+00, -8.0050e-02,  ...,  8.1839e-01,\n",
       "           7.3602e-01,  1.2990e+00],\n",
       "         [-1.3942e-01,  1.0294e+00, -6.3529e-01,  ...,  2.0018e-01,\n",
       "           5.9061e-01,  1.7946e-01],\n",
       "         [-1.9647e+00, -1.3317e+00,  6.3102e-02,  ..., -2.7807e-01,\n",
       "           6.7340e-01, -2.0837e-01]],\n",
       "\n",
       "        [[-5.5525e-02,  1.5098e+00, -1.1182e+00,  ...,  1.4036e+00,\n",
       "           8.9546e-01,  1.3795e+00],\n",
       "         [ 1.1859e+00,  7.6336e-01,  8.3842e-01,  ..., -5.1712e-01,\n",
       "          -1.4016e+00,  3.0985e-01],\n",
       "         [-5.4757e-01,  9.7382e-01, -1.7918e+00,  ..., -1.3475e+00,\n",
       "          -4.6099e-01,  2.1250e+00],\n",
       "         ...,\n",
       "         [ 6.0065e-01,  3.5893e-01,  1.8632e-01,  ...,  1.5503e+00,\n",
       "          -3.7654e-01, -1.2042e-01],\n",
       "         [ 1.1044e+00, -1.1162e+00, -5.3181e-01,  ...,  7.5545e-01,\n",
       "           1.6823e+00,  5.0773e-01],\n",
       "         [ 1.6943e-01,  6.3132e-01,  4.4071e-01,  ..., -3.0011e-01,\n",
       "          -4.2009e-01,  9.0276e-01]],\n",
       "\n",
       "        [[ 3.2471e-01,  8.5660e-01, -8.8103e-02,  ..., -1.2370e+00,\n",
       "           4.6351e-01,  1.2804e+00],\n",
       "         [ 2.5663e-01,  4.5846e-01,  1.3518e+00,  ...,  8.1903e-01,\n",
       "          -1.0215e+00, -1.4652e-01],\n",
       "         [-5.2976e-01, -7.7487e-02, -7.4126e-01,  ...,  1.1811e+00,\n",
       "           1.7245e-01,  2.0322e+00],\n",
       "         ...,\n",
       "         [-8.2322e-01, -5.1942e-01, -1.5480e-01,  ...,  5.6543e-01,\n",
       "           1.0409e+00,  1.3227e+00],\n",
       "         [ 6.3972e-02,  1.7902e-01, -1.6712e+00,  ..., -3.6952e-02,\n",
       "           2.7715e-01,  2.4336e+00],\n",
       "         [-1.2828e+00,  2.4607e-01,  2.8447e-01,  ..., -3.4998e-01,\n",
       "          -6.2219e-01,  7.9946e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.6675e-01,  4.0291e-01,  7.6506e-01,  ..., -3.6636e-01,\n",
       "           1.0786e-01,  1.8686e-01],\n",
       "         [-9.7675e-03,  1.8259e+00,  3.3491e-01,  ..., -3.9438e-01,\n",
       "           3.6158e-01,  9.3607e-01],\n",
       "         [-1.6380e-01, -3.5157e-01, -1.5311e+00,  ...,  1.1132e+00,\n",
       "           4.1309e-01,  2.1520e+00],\n",
       "         ...,\n",
       "         [-8.1012e-01, -6.1900e-01, -1.8471e+00,  ...,  9.9002e-01,\n",
       "          -4.2265e-01,  1.0064e+00],\n",
       "         [ 8.3416e-01, -1.1875e+00, -1.0641e-01,  ...,  4.6387e-01,\n",
       "          -2.0068e-02,  9.6973e-01],\n",
       "         [-1.0950e-01, -7.4617e-01,  1.2949e+00,  ..., -7.7464e-01,\n",
       "          -2.7720e-01,  3.0131e-01]],\n",
       "\n",
       "        [[-1.6751e+00,  2.3622e+00,  2.4974e-01,  ..., -4.9683e-01,\n",
       "           1.9784e+00, -1.3246e+00],\n",
       "         [-3.5429e-01,  4.9401e-02,  8.2144e-01,  ...,  1.2421e-01,\n",
       "           7.8471e-04,  1.0332e+00],\n",
       "         [-4.0783e-01,  1.1231e-01,  1.5881e+00,  ..., -6.2458e-01,\n",
       "           9.6578e-01,  2.4223e+00],\n",
       "         ...,\n",
       "         [ 1.0940e+00,  3.4453e-01, -3.7541e-01,  ...,  1.0090e+00,\n",
       "           1.2906e-02,  5.6693e-01],\n",
       "         [ 9.8885e-02,  9.0957e-01,  3.6516e-01,  ...,  3.9895e-01,\n",
       "           1.6566e+00,  8.2136e-01],\n",
       "         [ 7.1924e-01,  6.3802e-01, -7.0847e-01,  ...,  3.4820e-01,\n",
       "          -6.9814e-01,  6.1786e-01]],\n",
       "\n",
       "        [[ 1.4755e+00,  8.8484e-01,  1.4055e+00,  ...,  5.5402e-01,\n",
       "           2.3978e-02,  1.2767e+00],\n",
       "         [-4.9022e-01,  4.5870e-01,  3.2969e-01,  ...,  9.4438e-01,\n",
       "           1.5590e+00,  1.9998e+00],\n",
       "         [ 9.6723e-01, -2.9651e-01,  1.3972e+00,  ...,  5.1410e-01,\n",
       "           9.1972e-01, -2.2443e-01],\n",
       "         ...,\n",
       "         [ 7.8737e-01, -7.0230e-01,  4.5860e-01,  ..., -6.0281e-01,\n",
       "           2.0263e-01,  3.0694e-01],\n",
       "         [ 1.4270e+00,  4.9601e-01, -5.7685e-01,  ...,  8.9695e-01,\n",
       "           8.4389e-01,  4.2800e-01],\n",
       "         [-8.4969e-01,  5.4488e-01,  5.2860e-01,  ..., -8.2274e-01,\n",
       "           3.9652e-02,  1.0281e+00]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the forward pass \n",
    "enc_output = encoder(input_sequence, mask)\n",
    "\n",
    "print(enc_output.shape) # #(batch_size, sequence_length, d_model)\n",
    "enc_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2056424-9171-414b-9011-5b26f1acbe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification outputs for a batch of  8 sequences:\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8574, -1.4040, -1.1083],\n",
       "        [-1.0385, -1.3379, -0.9580],\n",
       "        [-1.3063, -1.0490, -0.9705],\n",
       "        [-2.1504, -1.0816, -0.6079],\n",
       "        [-1.3866, -0.9245, -1.0403],\n",
       "        [-1.2048, -0.6321, -1.7791],\n",
       "        [-2.1419, -1.1900, -0.5476],\n",
       "        [-1.0430, -1.0729, -1.1856]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification = classifier(enc_output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print (enc_output[:,0,:].shape) # (batch_size, d_model)\n",
    "print(classification.shape) # (batch_size, n_classes)\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44e16b75-3f55-4d58-a1b9-a10be7fc1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "# 1) Self-Attention in Encoder: Captures intra-sequence dependencies within the input (original language).\n",
    "# 2) Self-Attention in Decoder: Captures intra-sequence dependencies within the generated sequence (target language).\n",
    "# 3) Cross-Attention in Decoder: Integrates information from the input sequence to guide the generation of the output sequence.                \n",
    "        \n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask) # this causal mask hides future tokens to prevent the model from \"cheating\" by looking ahead (in the masked multihead attention) the tokens of the target language\n",
    "        x = self.norm1(x + self.dropout(self_attn_output)) # residual connection\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask) # it's a Padding Mask: Like the encoder, it ignores padding tokens\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output)) # residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fdcfa-d55c-413c-baa5-1ef645e1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module): # whole transformer decoder structure that includes a num_layers number of decoder layers\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) # using EncoderLayer, but it should be DecoderLayer\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # def forward(self, x, self_mask):\n",
    "    #     x = self.embedding(x)\n",
    "    #     x = self.positional_encoding(x)\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x, self_mask)\n",
    "            \n",
    "    def forward(self, x, self_mask, encoder_output, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask, encoder_output, cross_mask)\n",
    "        # return x\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        # When you apply F.log_softmax(x, dim=-1), the softmax function is applied to the d_model dimension. This means that the softmax function is applied independently to each sequence in each batch, and the output tensor will have the same shape as the input tensor.\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bcc5c-d3b7-480c-a651-b94bdb9b4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "print(input_sequence.shape)\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0b6e6-0bc1-4a04-a52e-2acc89e660dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c46fb-b6d5-4541-bbd8-3ec1025ebd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu(torch.ones(1, 8, 8), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f0f76-af6f-4023-8ffb-4e46d86b2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75930524-056e-4a59-96c4-8d868e92f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()  # Upper triangular mask\n",
    "\n",
    "print(self_attention_mask.shape)\n",
    "self_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a217c58-6b86-492d-85b9-4e919a54c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffcd35-d9f0-4055-ad88-7482e1c559a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder cross_mask\n",
    "\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018a3cb-a64a-48e0-9f14-1e956a794d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why the output is a 3D tensor rather than a 2D one is because the model is processing multiple sequences at once (the batch size is greater than 1) and it’s predicting a probability distribution over all possible words for each position in each sequence.\n",
    "# So, for each sequence in the batch (8 sequences), and for each position in each sequence (256 positions), you have a vector of length 10000 (the size of your vocabulary) representing the probability distribution over all possible next words.\n",
    "\n",
    "dec_output = decoder(input_sequence, self_attention_mask, enc_output, padding_mask)\n",
    "print(dec_output.shape)\n",
    "print(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4b5a5-409b-45bf-95a6-2e4dad5472ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ef05c-f507-4470-b3be-e43b0dcd2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c8829-7de1-457b-8106-1d4f2204f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fe889-4fdb-460e-a474-a413ebdb40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e4513-4aca-4f7e-88cd-0b5debfc8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b5ebc-e84b-4947-bc91-05e3f343eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a294c-f96d-416e-b1b0-5ede64da8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41652f3-395f-4e35-9188-4253114fb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face's dataset hub\n",
    "dataset = load_dataset('opinosis', trust_remote_code=True)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4af29-1973-4998-8729-05d4cb433fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of instances: {len(dataset['train'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc46a3-f0a1-49fc-be25-756cc67c2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c21f6-5636-4f8e-b78d-972a85b9fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d5b6c-d2dc-400a-92f9-298af290b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][-2]['review_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ac89b-0714-4100-8875-fec54785af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14010bce-ddff-4d7b-986c-b095399d5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c225539-d3a1-423f-af36-98eac35410f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa7054-04bc-4d68-a154-8146a9cd133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b5825-25c6-43a0-bc14-e99996465bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd820626-ebb8-45c8-a330-c77d65785125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why the input and translated IDs vectors have more elements than the corresponding word inputs is due to the way the tokenizer works.\n",
    "# In your code, the tokenizer.encode function is used to convert the input text into a sequence of IDs, which represent the tokens in the text. These tokens can be individual words, but they can also be smaller units depending on the tokenizer. For example, a word might be split into multiple subwords, each with its own ID.\n",
    "# Additionally, special tokens are often added to the sequence. For instance, a common practice is to add a special token at the beginning and end of the sequence. In your case, the 0 at the end of each input_ids and translated_ids tensor is likely a special token, such as an end-of-sequence token\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    print('english_input', english_input)\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
    "    print('input_ids', input_ids)\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    print('translated_ids', translated_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a180b31-33ec-4984-9b7c-e9cdd955d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "mlqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671edd08-a89d-4458-87a9-d983f359f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea420d-7e1f-4798-831e-3f013e593b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a96dda-066d-4279-adf9-857af82a91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dcdce-caea-4b31-9139-4dc4dc0485fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd781f-c2c5-4388-ac12-1e3511634879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cf16e-e65e-4dbd-a9a7-626e7b1516b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2161c-bb10-4245-8a2c-62be2ed387b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "answer_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1c323-1b76-473a-81c0-69bd1a7ab90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8b999-8f13-47a1-8250-cab93c023205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb9446-576e-4166-bc80-9853aefda005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163c51c-1724-4570-b838-6b4093a61e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "tokenized_datasets = []\n",
    "\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928e249-2aaa-40c9-b3c0-7aac99a0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b817ae-9314-484f-94c0-9f3c07106629",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774792d6-ea1d-488d-9030-75fade5c2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "dataset = load_dataset('emotion', trust_remote_code=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280e72a-80ef-4fc7-b518-8d72e086dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode your dataset\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "emotions_encoded = dataset.map(encode, batched=True)\n",
    "\n",
    "emotions_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80aee8-81ce-4070-8fd1-d9a544a9b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n",
    "    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee495e68-4020-4df7-b410-a52280e040b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys of the first example in the training dataset\n",
    "print(emotions_encoded[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661d92a-9e65-4877-8cdf-8a156b75494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for example in emotions_encoded[\"train\"]:\n",
    "    unique_labels.add(example[\"label\"])\n",
    "print(f\"Unique labels: {sorted(list(unique_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffd65a-a5ff-45ca-9b28-cb7b89a65b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop to fine-tune the model\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221a882-ffd6-40ad-84c0-df2bf9c33c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597455a9-ff17-434d-9e61-b902021d03d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e38d8-6a31-4441-97ba-33476121482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain class labels from raw predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd845ce-4035-4633-bbd9-ce458944a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b5687-013e-4ada-8145-323df266236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "test_examples = [{'text': 'I love this product!', 'label': 1},\n",
    "                 {'text': 'The service was terrible.', 'label': 0},\n",
    "                 {'text': 'This movie is amazing.', 'label': 1},\n",
    "                 {'text': \"I'm disappointed with the quality.\", 'label': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be27ae-994b-496d-999f-55115b8ad3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e602e-ef39-4c73-a089-2bba48d0b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1baf2e-7d14-416a-9838-454af1507e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8c898-4d6e-4403-9aa9-538ad6823bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assuming true_labels and predicted_labels are defined\n",
    "result = accuracy_score(true_labels, predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95310d6-52fe-4d68-be6e-cb76a4a3be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08298bec-b7c1-4d01-92cd-75964b962d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy, precision, recall and F1 score metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385dad1c-cea7-4381-b19b-0d0f198e0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Pass the examples to the pipeline, and obtain a list predicted labels\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf56cc-d82f-43c4-abe8-7151fcd42113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"Fantastic hotel, exceeded expectations!\",\n",
    "    \"Quiet despite central location, great stay.\",\n",
    "    \"Friendly staff, welcoming atmosphere.\",\n",
    "    \"Spacious, comfy room—a perfect retreat.\",\n",
    "    \"Cleanliness could improve, overall decent stay.\",\n",
    "      \"Disappointing stay, noisy and unclean room.\",\n",
    "    \"Terrible service, unfriendly staff, won't return.\"\n",
    "]\n",
    "\n",
    "test_labels = [1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49785c-9eff-4c15-a056-5768d5dca1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sentiment_analysis([example for example in test_examples])\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac1d45-969b-4a01-b29d-70c105e8bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(f1.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=test_labels, predictions=predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abff60-d03c-403a-823e-b7a63f887ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dadc3-aaaa-45e6-b876-3baeff5a0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Current trends show that by 2030 \"\n",
    "\n",
    "# Encode the prompt, generate text and decode it\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e52113-d928-4378-b974-91b02f1ea68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(prompt_ids, max_length=50)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48beed3e-4f7b-458c-a4be-bdf80b607cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625510fd-e877-4f02-91cb-c86754fc900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id=model_name,\n",
    "                             predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731511c-2b79-4b53-b346-4f0311d5cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rouge_score\n",
    "\n",
    "# Load the rouge metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions,references=references)\n",
    "print(\"ROUGE results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c4aa1-4079-4cd1-b577-16624aed5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
    "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=llm_outputs, references=references)\n",
    "print(\"Meteor: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e40f7-ead6-48ba-89cc-92381bee75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(\"EM results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc18bd-fae3-4578-bbdc-002f53b694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
    "\n",
    "reference_1 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
    "     ]\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd083c-774c-4f2b-919e-c4a7cf655fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why there are multiple reference sentences for each input sentence is because of the inherent ambiguity and variability in translation. There can be several equally correct translations for a given sentence, depending on factors like context, tone, and style. By providing multiple reference translations, we can capture some of this variability and get a more robust estimate of the model’s performance.\n",
    "\n",
    "# In the code you posted, the BLEU score is being calculated for the translations. The BLEU score is a metric that measures the quality of a translation by comparing it to one or more reference translations. It does this by counting the number of n-gram matches between the translation and the reference(s), and then normalizing by the total number of n-grams in the translation. The more the translation resembles the reference(s), the higher the BLEU score will be.\n",
    "\n",
    "# In your example, the first input sentence “Hola, ¿cómo estás?” is translated and then the translation is compared to two reference translations: “Hello, how are you?” and “Hi, how are you?”. The BLEU score is then computed for this translation.\n",
    "\n",
    "# The same process is repeated for the second set of input sentences and references. The final BLEU score is a measure of how well the translations match the reference translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49442a73-3f15-403b-9025-7b088cbfb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the first input sentence\n",
    "translated_output = translator(input_sentence_1)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "# Calculate BLEU metric for translation quality\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824753e7-3f19-493b-b701-15d12927718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translated_outputs = translator(input_sentences_2)\n",
    "\n",
    "translated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e780d-2c9f-4a28-bec9-3bf185cb4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7ab11-2129-46c5-823e-ca6874125b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb9c7a-cd48-4bcd-86e6-0073e6c38e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trl\n",
    "from trl import PPOTrainer, PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787e0ec-e281-424f-93d2-f4f279cac82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a reference model\n",
    "\n",
    "# When you call create_reference_model(model), it creates a copy of the model and freezes its parameters. This means that the weights of the reference model will not be updated during training.\n",
    "# This reference model is then used to compare with the updated model at each step of the training process. The idea is to ensure that the policy (i.e., the behavior of the model) does not change too drastically from one update to the next\n",
    "\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47816608-d588-4f58-83ef-b6ab1aa9bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if the parameters of a model are frozen, you can iterate over the parameters and check their requires_grad attribute. Here’s a small function that can do this:\n",
    "\n",
    "def check_if_frozen(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is not frozen\")\n",
    "        else:\n",
    "            print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d672e16-34e3-4e81-90d2-140eb381a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_frozen(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e860b1-5b6e-4241-8732-149251c6f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_frozen(model_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d53836-3a01-4e6d-b898-a5e2be57acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa787b53-013d-4599-95c6-16da82810d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer configuration\n",
    "# this code is setting up a configuration for a PPO trainer with specific batch and mini-batch sizes. This configuration would be used when training a model using the PPO algorithm. \n",
    "\n",
    "ppo_config = PPOConfig(batch_size=1, mini_batch_size=1)\n",
    "\n",
    "ppo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863c35b-4fb9-443f-b302-c44d4be31764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PPOTrainer instance\n",
    "# this line of code is setting up a PPO trainer with a specific configuration, model, reference model, and tokenizer. The trainer can then be used to train the model using the PPO algorithm.Typically, the trainer would have a method like train() that you can call to start the training process. The training process involves repeatedly sampling data, using the data to update the model, and then evaluating the performance of the model. The goal is to improve the model’s performance on some task, such as generating text. The PPO algorithm is particularly well-suited to tasks where the data is sequential or temporal in nature. It’s also known for its stability and efficiency, which makes it a popular choice for many reinforcement learning tasks.\n",
    "\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "\n",
    "ppo_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fbe5d-f5d9-4976-8b43-bddbe8a6dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Next year, I \"\n",
    "\n",
    "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792a051-b10a-4f39-a1ed-1c49a1d8e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is using a pretrained language model to generate a response to a given prompt. The response is calculated by feeding the encoded input into the model and then decoding the model’s output back into text. The response represents what the model thinks is the most likely continuation of the input prompt. The exact details of how the response is calculated depend on the specifics of the model and the respond_to_batch function. \n",
    "\n",
    "from trl.core import respond_to_batch\n",
    "\n",
    "response  = respond_to_batch(model, input) # function to generate a response from the model. The function takes the model and the encoded input as arguments.\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091fd2f1-bb64-4b24-a6d6-288f9d8e0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a more complex scenario, you might want to design a reward function that gives higher rewards for better responses and lower rewards for worse ones. This would require a way to evaluate the quality of the responses, which could be based on various factors such as the relevance of the response to the input, the grammatical correctness of the response, etc. This is typically the challenging part in reinforcement learning - designing a good reward function.\n",
    "\n",
    "import torch\n",
    "reward = [torch.tensor(1.0)]\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf05e00-7038-4045-a53b-1e5ca26448af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LLM for one step with PPO\n",
    "# while step() is used for a single step of training, train() is used for full-scale training over multiple epochs. The code is likely a simplified example or a debugging scenario where only a single step of training is being performed. For training a model to completion, you would generally use a train() function or similar.\n",
    "\n",
    "train_stats = ppo_trainer.step([input[0]], [response[0]], reward) # The step function is used to perform one step of training, where the model’s parameters are updated to maximize the expected reward.\n",
    "\n",
    "train_stats\n",
    "\n",
    "# The train_stats dictionary contains various statistics and metrics that are calculated during the training step. Here’s a brief explanation of some of the key metrics:\n",
    "# ‘objective/kl’: This is the Kullback-Leibler (KL) divergence, which measures how one probability distribution diverges from a second, expected probability distribution. In this case, it’s 0.0, indicating no divergence.\n",
    "# ‘objective/logprobs’: These are the log probabilities of the actions taken by the model. They are used in the calculation of the policy gradient.\n",
    "# ‘ppo/mean_scores’: This is the mean of the scores (rewards) obtained during the training step.\n",
    "# ‘tokens/queries_len_mean’: This is the average length of the queries processed in the training step.\n",
    "# ‘ppo/loss/policy’, ‘ppo/loss/value’, ‘ppo/loss/total’: These are the losses for the policy, value function, and the total loss respectively. The policy loss is related to how well the model is doing in terms of taking the right actions. The value loss is related to how well the model is predicting the expected future rewards.\n",
    "# ‘ppo/policy/entropy’: This is the entropy of the policy. It’s a measure of the randomness of the policy. A higher entropy means the policy is more random, while a lower entropy means the policy is more deterministic.\n",
    "# ‘ppo/returns/mean’: This is the mean of the returns (sum of rewards) obtained during the training step.\n",
    "# ‘ppo/val/vpred’: This is the predicted value of the state by the model.\n",
    "# ‘time/ppo/total’: This is the total time taken for the training step.\n",
    "# The warnings about degrees of freedom being less than or equal to 0 are due to the standard deviation (std()) function being called on a dataset with insufficient size. This can happen when the batch size or mini-batch size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4396e-4852-44e9-bb28-81079b28b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training steps\n",
    "num_steps = 100\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, num_steps+1):\n",
    "    # Generate input and response here...\n",
    "    # ...\n",
    "    reward = [torch.tensor(1.0)]\n",
    "    train_stats = ppo_trainer.step([input[0]], [response[0]], reward)\n",
    "    if i % 10 == 0:\n",
    "        print(i, 'training steps')\n",
    "    \n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163c3a8-bf7e-4b63-9b65-f63fd3a6bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
    "\n",
    "emp_1 = [\"Everyone in the team adores him\",\n",
    "           \"He is a true genius, pure talent\"]\n",
    "emp_2 = [\"Nobody in the team likes him\",\n",
    "           \"He is a useless 'good-for-nothing'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f9f82-b535-41a2-aeab-2547d3b8b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "toxicity_metric = load(\"toxicity\")\n",
    "\n",
    "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
    "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549b980-b7a6-4f9c-b3d2-358f07499d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation=\"maximum\")\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation=\"maximum\")\n",
    "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c292b-8fc6-466d-a3e3-9b9bdc8964b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation=\"ratio\")\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_2, aggregation=\"ratio\")\n",
    "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f008d-59cc-4bef-824b-e53b13301737",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = ['abc are described as loyal employees', \n",
    "          'abc are ambitious in their career expectations']\n",
    "group2 = ['abc are known for causing lots of team conflicts',\n",
    "          'abc are verbally violent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981893d6-721b-4f83-9c6d-046b534b82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the regard and regard-comparison metrics\n",
    "\n",
    "regard = evaluate.load(\"regard\")\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "# regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f5daa-a99e-478d-a380-14b17fa238ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the regard (polarities) of each group separately\n",
    "\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f12334-abb2-4cdc-b03c-3eba57609dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the relative regard between the two groups for comparison\n",
    "\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f653f-8795-44f3-813b-2c4c7f30b8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
