{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "44da563b-806a-4b95-ae6d-585217e9d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.training import Example\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eef713-2cc3-498d-a232-5f8c559e36e8",
   "metadata": {},
   "source": [
    "Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c704d4b-7c67-4652-9848-56626daf337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLP is becoming increasingly popular for providing business solutions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b7f1b9-84b5-4a71-b8e2-6b3dc9c0c98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x247522ffa60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load en_core_web_sm and create an nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b363ff-3bd9-4324-90f0-21cbebb58056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLP is becoming increasingly popular for providing business solutions."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Doc container for the text object\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70294f12-561e-4de1-a83b-0c1a11765cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'becoming', 'increasingly', 'popular', 'for', 'providing', 'business', 'solutions', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a list containing the text of each token in the Doc container\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74293462-019e-46db-9612-47fd16d01d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First text tokens:\n",
      " ['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', ' ', 'most', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'\n",
    "\n",
    "# Create a Doc container of the given text\n",
    "document = nlp(text)\n",
    "    \n",
    "# Store and review the token text values of tokens for the Doc container\n",
    "first_text_tokens = [token.text for token in document]\n",
    "print(\"First text tokens:\\n\", first_text_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b689082-524b-46d5-a500-8f43c22b4c88",
   "metadata": {},
   "source": [
    "Running a spaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2ae534-c7d6-403d-81bd-fe210f77de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['A loaded spaCy model can be used to compile documents list!',\n",
    " 'Tokenization is the first step in any spacy pipeline.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8311df20-fb8f-462f-bf6d-10fa9db0a589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A loaded spaCy model can be used to compile documents list!,\n",
       " Tokenization is the first step in any spacy pipeline.]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run an nlp model on each item of texts and append the Doc container to documents\n",
    "documents = []\n",
    "for text in texts:\n",
    "    documents.append(nlp(text))\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e670b0-7795-4fce-bfc5-dcc72cc0de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'loaded', 'spaCy', 'model', 'can', 'be', 'used', 'to', 'compile', 'documents', 'list', '!']\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'any', 'spacy', 'pipeline', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the token texts for each Doc container\n",
    "for doc in documents:\n",
    "    print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf57d7-1937-425e-87c6-92037827c82c",
   "metadata": {},
   "source": [
    "Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1c479e-1356-4ecf-bf1a-e7560c9737e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1c6fd4-f4b8-4d41-a3dd-ef4ee80cea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " ['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', ' ', 'most', '.']\n",
      "Lemmas:\n",
      " ['I', 'have', 'buy', 'several', 'of', 'the', 'Vitality', 'can', 'dog', 'food', 'product', 'and', 'have', 'find', 'they', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'the', 'product', 'look', 'more', 'like', 'a', 'stew', 'than', 'a', 'process', 'meat', 'and', 'it', 'smell', 'well', '.', 'my', 'Labrador', 'be', 'finicky', 'and', 'she', 'appreciate', 'this', 'product', 'well', 'than', ' ', 'most', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in document]\n",
    "print(\"Tokens:\\n\", tokens)\n",
    "\n",
    "lemmas = [token.lemma_ for token in document]\n",
    "print(\"Lemmas:\\n\", lemmas, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f7e0f-8e67-4ab0-8126-6a0d86bfcb54",
   "metadata": {},
   "source": [
    "Sentence segmentation with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cda23c9-236d-4c68-b67c-c8671e36ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    " 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".',\n",
    " 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.',\n",
    " 'If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.',\n",
    " 'Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.',\n",
    " 'I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.',\n",
    " \"This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!\",\n",
    " 'This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!',\n",
    " \"Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\",\n",
    " 'This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8cc4fcb-83d5-4ad4-9787-eb4f7773430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 7, 3, 4, 5, 5, 5, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Generating a documents list of all Doc containers\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Iterate through documents and append sentences in each doc to the sentences list\n",
    "sentences = []\n",
    "for doc in documents:\n",
    "    sentences.append([s for s in doc.sents])\n",
    "    \n",
    "# Find number of sentences per each doc container\n",
    "print([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e79724-1f0c-4648-aa86-62e3c137afb6",
   "metadata": {},
   "source": [
    "POS tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8c301f1-a95c-40e1-bb01-8b8128944a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['What is the arrival time in San francisco for the 7:55 AM flight leaving Washington?',\n",
    " 'Cheapest airfare from Tacoma to Orlando is 650 dollars.',\n",
    " 'Round trip fares from Pittsburgh to Philadelphia are under 1000 dollars!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ac358b-821e-4ccb-8838-36eaa539ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  What | POS tag:  PRON\n",
      "Text:  is | POS tag:  AUX\n",
      "Text:  the | POS tag:  DET\n",
      "Text:  arrival | POS tag:  NOUN\n",
      "Text:  time | POS tag:  NOUN\n",
      "Text:  in | POS tag:  ADP\n",
      "Text:  San | POS tag:  PROPN\n",
      "Text:  francisco | POS tag:  PROPN\n",
      "Text:  for | POS tag:  ADP\n",
      "Text:  the | POS tag:  DET\n",
      "Text:  7:55 | POS tag:  NUM\n",
      "Text:  AM | POS tag:  PROPN\n",
      "Text:  flight | POS tag:  NOUN\n",
      "Text:  leaving | POS tag:  VERB\n",
      "Text:  Washington | POS tag:  PROPN\n",
      "Text:  ? | POS tag:  PUNCT\n",
      "\n",
      "\n",
      "Text:  Cheapest | POS tag:  ADJ\n",
      "Text:  airfare | POS tag:  NOUN\n",
      "Text:  from | POS tag:  ADP\n",
      "Text:  Tacoma | POS tag:  PROPN\n",
      "Text:  to | POS tag:  ADP\n",
      "Text:  Orlando | POS tag:  PROPN\n",
      "Text:  is | POS tag:  AUX\n",
      "Text:  650 | POS tag:  NUM\n",
      "Text:  dollars | POS tag:  NOUN\n",
      "Text:  . | POS tag:  PUNCT\n",
      "\n",
      "\n",
      "Text:  Round | POS tag:  ADJ\n",
      "Text:  trip | POS tag:  NOUN\n",
      "Text:  fares | POS tag:  NOUN\n",
      "Text:  from | POS tag:  ADP\n",
      "Text:  Pittsburgh | POS tag:  PROPN\n",
      "Text:  to | POS tag:  ADP\n",
      "Text:  Philadelphia | POS tag:  PROPN\n",
      "Text:  are | POS tag:  AUX\n",
      "Text:  under | POS tag:  ADP\n",
      "Text:  1000 | POS tag:  NUM\n",
      "Text:  dollars | POS tag:  NOUN\n",
      "Text:  ! | POS tag:  PUNCT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print token texts and POS tags for each Doc container\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        print(\"Text: \", token.text, \"| POS tag: \", token.pos_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc0d04-5730-47bc-84f0-ce135a0b2dfb",
   "metadata": {},
   "source": [
    "NER with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5de4cb5-7c78-419b-9a43-ba4e49d178a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I want to fly from Boston at 8:38 am and arrive in Denver at 11:10 in the morning',\n",
    " 'What flights are available from Pittsburgh to Baltimore on Thursday morning?',\n",
    " 'What is the arrival time in San francisco for the 7:55 AM flight leaving Washington?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6632677f-acae-456c-bdfc-b4c118251834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Boston', 'GPE'), ('8:38 am', 'TIME'), ('Denver', 'GPE'), ('11:10 in the morning', 'TIME')]\n",
      "[('Pittsburgh', 'GPE'), ('Baltimore', 'GPE'), ('Thursday', 'DATE'), ('morning', 'TIME')]\n",
      "[('San francisco', 'GPE'), ('7:55 AM', 'TIME'), ('Washington', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print the entity text and label for the entities in each document\n",
    "for doc in documents:\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22aa5aa1-c6ad-47cc-8e9b-aff0e1192e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Pittsburgh | Entity type:  GPE\n"
     ]
    }
   ],
   "source": [
    "# Print the 6th token's text and entity type of the second document\n",
    "print(\"\\nText:\", documents[1][5].text, \"| Entity type: \", documents[1][5].ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a02a6e-4179-47f6-8aba-8e5c16422ff3",
   "metadata": {},
   "source": [
    "Text processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68587246-bc4a-437d-bf47-17e7aa9ea8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    " 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".',\n",
    " 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.',\n",
    " 'If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.',\n",
    " 'Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.',\n",
    " 'I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.',\n",
    " \"This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!\",\n",
    " 'This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!',\n",
    " \"Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\",\n",
    " 'This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbd61a5e-743e-4b3f-9082-ed57449cb54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [nlp(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aed8c35-4d67-474a-b22e-911cfb7ed4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I have bought several of the Vitality canned dog food products and have found them all to be of good quality.,\n",
       " The product looks more like a stew than a processed meat and it smells better.,\n",
       " My Labrador is finicky and she appreciates this product better than  most.]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list to store sentences of each Doc container in documents\n",
    "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "436dee6e-2153-44e4-875d-1f1ac6893449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in documents:\n",
      " [3, 2, 7, 3, 4, 5, 5, 5, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Print number of sentences in each Doc container in documents\n",
    "num_sentences = [len(s) for s in sentences]\n",
    "print(\"Number of sentences in documents:\\n\", num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79b4c12-8a4a-45ff-9f98-d61f3ab05db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third text entities:\n",
      " [('citrus gelatin', 'PERSON'), ('Filberts', 'PERSON'), (\"C.S. Lewis'\", 'ORG'), ('The Lion, The Witch', 'WORK_OF_ART'), ('The Wardrobe', 'WORK_OF_ART'), ('Edmund', 'GPE'), ('Sisters', 'PERSON'), ('Witch', 'LOC')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record entities text and corresponding label of the third Doc container\n",
    "third_text_entities = [(ent.text, ent.label_) for ent in documents[2].ents]\n",
    "print(\"Third text entities:\\n\", third_text_entities, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6d0eb22-447e-4124-a5d4-56e3e68e8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten tokens of third text:\n",
      " [('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('confection', 'NOUN'), ('that', 'PRON'), ('has', 'AUX'), ('been', 'AUX'), ('around', 'ADP'), ('a', 'DET'), ('few', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "# Record first ten tokens and corresponding POS tag for the third Doc container\n",
    "third_text_10_pos = [(token.text, token.pos_) for token in documents[2]][:10]\n",
    "print(\"First ten tokens of third text:\\n\", third_text_10_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41359fd-f30f-4e3d-a121-78c5337d10c0",
   "metadata": {},
   "source": [
    "Word-sense disambiguation with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa9e7244-fcaa-4ecc-b788-a990cfffdc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:  [('jam', 'VERB')] \n",
      "\n",
      "Sentence 2:  [('jam', 'NOUN')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\"This device is used to jam the signal.\",\n",
    "         \"I am stuck in a traffic jam\"]\n",
    "\n",
    "# Create a list of Doc containers in the texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print a token's text and POS tag if the word jam is in the token's text\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Sentence {i+1}: \", [(token.text, token.pos_) for token in doc if \"jam\" in token.text], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175cd96b-72b3-41b9-8d59-beab2506eb47",
   "metadata": {},
   "source": [
    "Dependency parsing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5995edbd-e1b9-4965-91e9-ca0359da381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I want to fly from Boston at 8:38 am and arrive in Denver at 11:10 in the morning',\n",
    " 'What flights are available from Pittsburgh to Baltimore on Thursday morning?',\n",
    " 'What is the arrival time in San francisco for the 7:55 AM flight leaving Washington?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc4af279-3359-4218-888f-bc2f7e35a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'nsubj', 'nominal subject'), ('want', 'ROOT', 'root'), ('to', 'aux', 'auxiliary'), ('fly', 'xcomp', 'open clausal complement'), ('from', 'prep', 'prepositional modifier'), ('Boston', 'pobj', 'object of preposition'), ('at', 'prep', 'prepositional modifier'), ('8:38', 'nummod', 'numeric modifier'), ('am', 'pobj', 'object of preposition'), ('and', 'cc', 'coordinating conjunction'), ('arrive', 'conj', 'conjunct'), ('in', 'prep', 'prepositional modifier'), ('Denver', 'pobj', 'object of preposition'), ('at', 'prep', 'prepositional modifier'), ('11:10', 'pobj', 'object of preposition'), ('in', 'prep', 'prepositional modifier'), ('the', 'det', 'determiner'), ('morning', 'pobj', 'object of preposition')] \n",
      "\n",
      "[('What', 'det', 'determiner'), ('flights', 'nsubj', 'nominal subject'), ('are', 'ROOT', 'root'), ('available', 'acomp', 'adjectival complement'), ('from', 'prep', 'prepositional modifier'), ('Pittsburgh', 'pobj', 'object of preposition'), ('to', 'prep', 'prepositional modifier'), ('Baltimore', 'pobj', 'object of preposition'), ('on', 'prep', 'prepositional modifier'), ('Thursday', 'compound', 'compound'), ('morning', 'pobj', 'object of preposition'), ('?', 'punct', 'punctuation')] \n",
      "\n",
      "[('What', 'attr', 'attribute'), ('is', 'ROOT', 'root'), ('the', 'det', 'determiner'), ('arrival', 'compound', 'compound'), ('time', 'nsubj', 'nominal subject'), ('in', 'prep', 'prepositional modifier'), ('San', 'compound', 'compound'), ('francisco', 'pobj', 'object of preposition'), ('for', 'prep', 'prepositional modifier'), ('the', 'det', 'determiner'), ('7:55', 'nummod', 'numeric modifier'), ('AM', 'compound', 'compound'), ('flight', 'pobj', 'object of preposition'), ('leaving', 'acl', 'clausal modifier of noun (adjectival clause)'), ('Washington', 'dobj', 'direct object'), ('?', 'punct', 'punctuation')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of Doc containts of texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print each token's text, dependency label and its explanation\n",
    "for doc in documents:\n",
    "    print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b7be0-587c-4e21-b007-c385a4710a96",
   "metadata": {},
   "source": [
    "spaCy vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a02b09eb-0d7b-4852-a535-f9e9755e4941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 300,\n",
       " 'vectors': 514157,\n",
       " 'keys': 514157,\n",
       " 'name': 'en_vectors',\n",
       " 'mode': 'default'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the en_core_web_md model\n",
    "lg_nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "lg_nlp.meta[\"vectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28e17356-266f-471f-bce7-df7729f59015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  514157 \n",
      "\n",
      "Dimension of word vectors:  300\n"
     ]
    }
   ],
   "source": [
    "# Print the number of words in the model's vocabulary\n",
    "print(\"Number of words: \", lg_nlp.meta[\"vectors\"][\"vectors\"], \"\\n\")\n",
    "\n",
    "# Print the dimensions of word vectors in en_core_web_md model\n",
    "print(\"Dimension of word vectors: \", lg_nlp.meta[\"vectors\"][\"width\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c16b7-6753-4fef-b075-67dee9d619fa",
   "metadata": {},
   "source": [
    "Word vectors in spaCy vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45133b4f-515c-4495-bc07-7bd9640164d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18194338103975822726, 3702023516439754181]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"like\", \"love\"]\n",
    "\n",
    "# IDs of all the given words\n",
    "ids = [lg_nlp.vocab.strings[w] for w in words]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07462fdc-3a4c-4594-9a5a-c9e4589db32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-2.3334 , -1.3695 , -1.133  , -0.68461, -1.8482 , -0.63712,\n",
       "         2.6791 ,  4.1433 , -2.5616 , -1.8061 ], dtype=float32),\n",
       " array([ 2.0565 , -3.2259 , -5.7364 , -6.146  ,  0.15748, -2.4284 ,\n",
       "         7.658  ,  2.7064 , -2.211  , -0.8999 ], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the first ten elements of the word vectors for each word\n",
    "word_vectors = [lg_nlp.vocab.vectors[i][:10] for i in ids]\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651a187-5860-451f-bffa-a1f88f12455a",
   "metadata": {},
   "source": [
    "Word vectors projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f196f4a5-9a2a-4184-965c-e515bd1e2b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5423999730010037932, 10103162543233135282]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"tiger\", \"bird\"]\n",
    "\n",
    "# Extract word IDs of given words\n",
    "word_ids = [lg_nlp.vocab.strings[w] for w in words]\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0efe00b4-72c3-4660-8413-61e7744e3807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.032863,  1.8007  , -1.3854  , -3.5269  , -0.24236 ,  0.41086 ,\n",
       "         0.26883 ,  0.26619 ,  2.2089  ,  0.5561  ],\n",
       "       [ 4.8752  , -1.9177  , -1.3281  , -5.278   ,  2.2977  , -0.40337 ,\n",
       "        -2.4936  ,  0.63511 , -2.1338  ,  2.1657  ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract word vectors and stack the first five (or ten) elements vertically\n",
    "word_vectors = np.vstack([lg_nlp.vocab.vectors[i][:10] for i in word_ids])\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2eb5d3e-1796-4e8f-b1ee-023af055c0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.3782477e+00,  4.1429936e-07],\n",
       "       [-4.3782473e+00,  4.1429939e-07]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the transformed word vectors using the pca object\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_transformed = pca.fit_transform(word_vectors)\n",
    "word_vectors_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eab21994-e928-4b93-bf72-941885aeda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.3782477 -4.3782473]\n"
     ]
    }
   ],
   "source": [
    "# Print the first component of the transformed word vectors\n",
    "print(word_vectors_transformed[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aef97b-9104-4108-bc87-bf452ca6b683",
   "metadata": {},
   "source": [
    "Similar words in a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df366fe8-5adb-400e-8157-40de49b7fca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4912942957612137283"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_comp = lg_nlp.vocab.strings['computer']\n",
    "id_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55135cdc-f600-42a8-8a80-a90244c674d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67285 , -0.86102 ,  1.6945  , -0.60859 ,  0.13298 ,  1.5135  ,\n",
       "        3.8222  ,  5.4456  , -2.8585  , -1.2448  ,  6.2764  ,  4.2784  ,\n",
       "       -4.589   ,  5.011   ,  1.015   ,  2.0452  ,  3.8958  ,  0.33248 ,\n",
       "       -3.322   ,  1.1575  ,  2.4343  ,  0.31949 , -0.33847 , -1.6331  ,\n",
       "       -3.269   , -4.7326  , -2.4177  , -4.63    ,  1.3962  ,  3.7998  ,\n",
       "       -0.787   , -0.85938 , -3.0182  ,  0.28905 ,  0.64035 , -0.45379 ,\n",
       "        3.4345  , -0.22517 ,  3.8027  ,  2.6739  ,  0.97571 , -0.76596 ,\n",
       "        0.53999 ,  2.2714  ,  0.87652 , -0.84191 ,  0.24501 , -3.1094  ,\n",
       "        1.3224  , -2.7244  , -0.30636 ,  2.9063  ,  0.26466 , -3.5191  ,\n",
       "       -1.0453  ,  2.3392  , -1.3542  ,  3.7928  , -1.4989  , -0.82555 ,\n",
       "        3.2697  ,  4.2062  , -5.0726  , -1.458   ,  0.82807 ,  2.1958  ,\n",
       "       -1.1805  , -4.4558  , -0.26838 ,  3.1355  , -0.60561 ,  1.8562  ,\n",
       "       -0.35967 , -0.53501 ,  2.6208  , -2.8202  , -2.2845  , -1.1683  ,\n",
       "       -4.325   , -2.5027  , -2.8288  , -0.52345 ,  2.8522  ,  2.5518  ,\n",
       "        1.5887  , -4.7376  ,  1.1592  ,  1.0966  ,  1.9461  , -2.4293  ,\n",
       "       -6.9027  ,  3.7767  ,  1.9932  , -1.1892  , -1.3401  , -0.77041 ,\n",
       "        1.0825  ,  1.3028  ,  1.6872  ,  0.92636 ,  0.67587 ,  1.029   ,\n",
       "        1.6173  , -0.90632 ,  0.85986 ,  3.5184  , -1.5297  , -1.2081  ,\n",
       "        1.9314  ,  3.0564  ,  4.0105  , -0.68713 , -0.47882 ,  0.9113  ,\n",
       "       -0.48548 ,  2.6505  , -3.5706  ,  2.7736  ,  0.10338 , -6.7501  ,\n",
       "       -6.5766  , -3.3489  , -0.17284 ,  0.16847 ,  0.11285 , -4.2019  ,\n",
       "        0.21072 , -4.3188  ,  0.17319 , -3.1244  ,  2.3181  ,  2.8323  ,\n",
       "       -0.99637 ,  1.2639  , -1.0739  , -0.63108 , -2.7523  , -3.0308  ,\n",
       "        2.1712  , -0.16248 ,  2.436   ,  0.79294 , -0.658   ,  0.55868 ,\n",
       "        2.6284  ,  2.1707  , -1.896   ,  1.5094  ,  0.21675 ,  3.3289  ,\n",
       "        0.37263 ,  0.16115 ,  3.8921  ,  0.09997 , -2.1095  , -1.0789  ,\n",
       "        3.8803  ,  2.4588  , -2.4947  , -0.7958  , -1.278   , -1.3421  ,\n",
       "        0.12149 ,  0.66381 , -2.1778  , -0.045254, -1.0635  ,  3.2142  ,\n",
       "       -0.28847 , -1.2409  , -1.8334  , -0.022577,  0.8096  ,  2.9832  ,\n",
       "        5.1596  ,  2.7613  ,  1.514   , -1.6926  , -1.7996  , -2.1606  ,\n",
       "       -2.4388  ,  1.7222  ,  1.456   , -6.0544  , -0.80842 ,  0.94397 ,\n",
       "       -1.2394  , -4.0843  ,  3.9125  , -2.4146  , -0.049345, -0.95309 ,\n",
       "       -1.9327  , -4.1914  ,  0.9084  ,  4.4195  , -4.6422  , -0.063438,\n",
       "        3.9064  ,  0.99776 , -1.9775  , -0.55331 ,  2.5938  , -3.9547  ,\n",
       "        5.9973  ,  4.05    , -3.7917  ,  0.024585, -0.88672 , -4.7738  ,\n",
       "       -0.78481 , -2.4802  , -1.266   ,  4.7329  , -0.97555 , -0.29834 ,\n",
       "        5.6206  , -1.1295  , -0.45455 , -1.232   , -0.33758 ,  1.8956  ,\n",
       "        0.10203 , -0.34735 , -0.88857 , -0.94469 ,  3.0194  , -1.5947  ,\n",
       "        1.2475  , -0.13178 ,  0.25714 , -4.8687  , -1.5399  ,  1.0118  ,\n",
       "        1.2967  ,  1.671   ,  3.5908  , -0.6228  ,  0.18844 , -0.41971 ,\n",
       "       -6.1943  ,  0.72438 , -0.087838,  1.7838  ,  0.64545 , -0.25147 ,\n",
       "       -1.269   ,  1.2807  , -1.4543  , -0.45343 ,  1.6351  , -1.983   ,\n",
       "       -6.0014  , -4.9101  , -1.5271  , -1.2483  ,  2.5238  , -4.5502  ,\n",
       "       -3.6479  , -0.74559 , -1.7338  ,  7.1456  , -2.3592  , -2.9088  ,\n",
       "        0.51177 , -0.76631 ,  1.6664  ,  2.5737  , -4.1096  , -1.4449  ,\n",
       "        0.51208 , -1.0336  ,  0.066038,  0.070377, -1.5003  , -2.7288  ,\n",
       "        0.6446  , -2.2793  , -3.0031  ,  1.1464  ,  2.5979  ,  0.048356,\n",
       "        4.0535  ,  1.9545  ,  2.7701  , -0.24374 ,  0.35203 ,  3.7871  ,\n",
       "       -1.5185  ,  2.7488  ,  0.76511 , -2.2421  , -2.1311  ,  0.51683 ,\n",
       "       -3.161   ,  0.79062 ,  1.6554  , -1.3156  , -0.20369 ,  2.8584  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector = lg_nlp.vocab.vectors[id_comp]\n",
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "307a285c-5ef2-4d18-9430-fe6f26bbdb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4912942957612137283,  6749792856493933245,  7262238544365424156,\n",
       "          3596863613112108743, 18005194085716360258,  2048265920074810992,\n",
       "         17343804685088620554, 14132390025522728554, 14099521314249290004,\n",
       "         17778691834923080873]], dtype=uint64),\n",
       " array([[  1306, 344354, 439921, 429316,   5438, 159166,  95645, 476453,\n",
       "         138676, 314483]], dtype=int32),\n",
       " array([[1.    , 0.9592, 0.9515, 0.9069, 0.8993, 0.8867, 0.8786, 0.862 ,\n",
       "         0.8492, 0.8385]], dtype=float32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar word to the word computer\n",
    "most_similar_words = lg_nlp.vocab.vectors.most_similar(np.asarray([word_vector]), n = 10)\n",
    "most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df3a65bf-20f3-4cfd-8e30-540914b7c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'computer-', 'computer--', 'Batcomputer', 'computers', 'minicomputer', 'microcomputer', 'Komputer', 'microcomputers', 'computerize']\n"
     ]
    }
   ],
   "source": [
    "# Find the list of similar words given the word IDs\n",
    "words = [lg_nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009e97e-de38-43a9-b27a-ee85f54a106f",
   "metadata": {},
   "source": [
    "Doc similarity with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acdc1f68-c1ba-4486-a4ba-f05cd27b1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I like the Vitality canned dog food products.',\n",
    " 'The peanuts were actually small sized unsalted. Not sure if this was an error.',\n",
    " 'It is a light, pillowy citrus gelatin with nuts - in this case Filberts.',\n",
    " 'the Root Beer Extract I ordered is very medicinal.',\n",
    " 'Great taffy at a great price.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1348fc6-ec18-427d-af77-626ec13b5004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I like the Vitality canned dog food products.,\n",
       " The peanuts were actually small sized unsalted. Not sure if this was an error.,\n",
       " It is a light, pillowy citrus gelatin with nuts - in this case Filberts.,\n",
       " the Root Beer Extract I ordered is very medicinal.,\n",
       " Great taffy at a great price.]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a documents list containing Doc containers\n",
    "documents = [lg_nlp(t) for t in texts]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "303c3119-cb69-40a7-9829-68cbb424f476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "canned dog food"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Doc container of the category\n",
    "category = \"canned dog food\"\n",
    "category_document = lg_nlp(category)\n",
    "category_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bedd04bd-0c80-478b-b2a4-af8e8cd8ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity with document 1: 0.761\n",
      "Semantic similarity with document 2: 0.365\n",
      "Semantic similarity with document 3: 0.336\n",
      "Semantic similarity with document 4: 0.382\n",
      "Semantic similarity with document 5: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Print similarity scores of each Doc container and the category_document\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Semantic similarity with document {i+1}:\", round(doc.similarity(category_document), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d3999-8bdc-4e32-af85-e65dca3a6f9e",
   "metadata": {},
   "source": [
    "Span similarity with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcfe167f-63db-4d61-b63b-f7513cbb499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity with I like the : 0.32\n"
     ]
    }
   ],
   "source": [
    "# Print similarity score of a given Span and category_document\n",
    "document_span = documents[0][0:3]\n",
    "print(f\"Semantic similarity with\", document_span.text, \":\", round(document_span.similarity(category_document), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bb7c5-dc22-441f-81e7-93b8599afb0c",
   "metadata": {},
   "source": [
    "Semantic similarity for categorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67f50aed-a359-4931-8349-6d6afd724d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = 'This hot sauce is amazing! We picked up a bottle on a trip! '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52d08ec8-8f7e-4b21-ac56-054da1d6dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate Doc containers for the word \"sauce\" and for \"texts\" string\n",
    "key = lg_nlp('sauce')\n",
    "sentences = lg_nlp(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f4b7e42-0c19-4cda-aff2-560c1ba4ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This hot sauce is amazing! 0.49996989411361104\n",
      "We picked up a bottle on a trip! 0.16954893952092578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5}, {'score': 0.17}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity score of each sentence and a Doc container for the word sauce\n",
    "semantic_scores = []\n",
    "for sent in sentences.sents:\n",
    "    semantic_scores.append({\"score\": round(sent.similarity(key), 2)})\n",
    "    print(sent, sent.similarity(key))\n",
    "semantic_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe57539-d622-484f-9cd7-4bed1bc88017",
   "metadata": {},
   "source": [
    "Adding pipes in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81f8a559-3ea6-4969-b022-5492e1ff320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts ='I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most. Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\". This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch. If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal. Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd26a5b1-fda9-4e04-9ceb-41305f20c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  19 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[I have bought several of the Vitality canned dog food products and have found them all to be of good quality.,\n",
       " The product looks more like a stew than a processed meat and it smells better.,\n",
       " My Labrador is finicky and she appreciates this product better than  most.,\n",
       " Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted.,\n",
       " Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".,\n",
       " This is a confection that has been around a few centuries.,\n",
       "  It is a light, pillowy citrus gelatin with nuts - in this case Filberts.,\n",
       " And it is cut into tiny squares and then liberally coated with powdered sugar.,\n",
       "  And it is a tiny mouthful of heaven.,\n",
       "  Not too chewy, and very flavorful.,\n",
       "  I highly recommend this yummy treat.,\n",
       "  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.,\n",
       " If you are looking for the secret ingredient in Robitussin I believe I have found it.,\n",
       "  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.,\n",
       "  The flavor is very medicinal.,\n",
       " Great taffy at a great price.,\n",
       "  There was a wide assortment of yummy taffy.,\n",
       "  Delivery was very quick.,\n",
       "  If your a taffy lover, this is a deal.]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a blank spaCy English model and add a sentencizer component\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create Doc containers, store sentences and print its number of sentences\n",
    "doc = nlp(texts)\n",
    "sentences = [s for s in doc.sents]\n",
    "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2608fa65-588e-4d45-b924-ba88c8343e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence tokens:  [The, product, looks, more, like, a, stew, than, a, processed, meat, and, it, smells, better, .]\n"
     ]
    }
   ],
   "source": [
    "# Print the list of tokens in the second sentence\n",
    "print(\"Second sentence tokens: \", [token for token in sentences[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62fe71-ca1c-4f34-9fa4-56050f65f6cc",
   "metadata": {},
   "source": [
    "Analyzing pipelines in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77e79bbf-b5f8-4e07-bdce-a8b9d2894b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component       Assigns           Requires         Scores        Retokenizes\n",
      "-   -------------   ---------------   --------------   -----------   -----------\n",
      "0   tagger          token.tag                          tag_acc       False      \n",
      "                                                                                \n",
      "1   entity_linker   token.ent_kb_id   doc.ents         nel_micro_f   False      \n",
      "                                      doc.sents        nel_micro_r              \n",
      "                                      token.ent_iob    nel_micro_p              \n",
      "                                      token.ent_type                            \n",
      "\n",
      "\u001b[1m\n",
      "================================ Problems (4) ================================\u001b[0m\n",
      "\u001b[38;5;3m⚠ 'entity_linker' requirements not met: doc.ents, doc.sents,\n",
      "token.ent_iob, token.ent_type\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load a blank spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add tagger and entity_linker pipeline components\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "# Analyze the pipeline\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "\n",
    "# In this instance, the pipeline is missing sentence segmentation and named entity recognition components before entity_linker component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd746a-8bb3-4a73-8dda-9b600fdd4ddd",
   "metadata": {},
   "source": [
    "EntityRuler with blank spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9faa0f7-d9bc-434a-a12e-a8f15661dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OpenAI', 'ORG'), ('Microsoft', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
    "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"microsoft\"}]}]\n",
    "text = \"OpenAI has joined forces with Microsoft.\"\n",
    "\n",
    "# Add EntityRuler component to the model\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add given patterns to the EntityRuler component\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model on a given text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities text and type for all entities in the Doc container\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d590c52-e83d-4e6f-9fa4-2116c280648f",
   "metadata": {},
   "source": [
    "EntityRuler for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2aafc482-613e-41c3-a80a-b54646bed547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New York Group', 'ORG'), ('1987', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New York Group was built in 1987.\"\n",
    "\n",
    "# Add an EntityRuler to the nlp before NER component\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define a pattern to classify lower cased new york group as ORG\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"new york group\"}]}]\n",
    "\n",
    "# Add the patterns to the EntityRuler component\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model and print entities text and type for all the entities\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009b8ef-f99c-44b2-89db-f33340b242db",
   "metadata": {},
   "source": [
    "EntityRuler with multi-patterns in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8b53376-4aed-4b54-941e-ddaee6e9e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'This is a confection. In this case Filberts. And it is cut into tiny squares. This is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1515afeb-fea0-4251-8e4c-e9e15e8934f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before EntityRuler:  [('Filberts', 'PERSON'), ('Edmund', 'PERSON'), ('Sisters', 'ORG')] \n",
      "\n",
      "After EntityRuler:  [('Filberts', 'PERSON'), ('Edmund', 'PERSON'), ('Brother', 'PERSON'), ('Sisters', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print a list of tuples of entities text and types in the example_text\n",
    "print(\"Before EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents], \"\\n\")\n",
    "\n",
    "# Define pattern to add a label PERSON for lower cased sisters and brother entities\n",
    "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"brother\"}]},\n",
    "            {\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"sisters\"}]}]\n",
    "\n",
    "# Add an EntityRuler component and add the patterns to the ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before = \"ner\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print a list of tuples of entities text and types\n",
    "print(\"After EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af5d51-f129-4189-bb3e-d2d134730e20",
   "metadata": {},
   "source": [
    "RegEx in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad474a95-5179-43bf-855a-c47e0327c81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x2475eff39a0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Our phone number is (425)-123-4567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "pattern = r\"\\((\\d){3}\\)-(\\d){3}-(\\d){4}\"\n",
    "\n",
    "# Find all the matching patterns in the text\n",
    "phones = re.finditer(pattern, text)\n",
    "phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f94d6331-5258-47f4-b070-3ccf211b8ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start character:  20 | End character:  34 | Matching text:  (425)-123-4567\n"
     ]
    }
   ],
   "source": [
    "# Print start and end characters and matching section of the text\n",
    "for match in phones:\n",
    "    start_char = match.start()\n",
    "    end_char = match.end()\n",
    "    print(\"Start character: \", start_char, \"| End character: \", end_char, \"| Matching text: \", text[start_char:end_char])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a283459-1941-43c4-97f5-82b27f91014e",
   "metadata": {},
   "source": [
    "RegEx with EntityRuler in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fa06aba-38b6-4615-95d9-9564f1bcb0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4251234567', 'PHONE_NUMBERS')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Our phone number is 4251234567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "patterns = [{\"label\": \"PHONE_NUMBERS\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"(\\d){10}\"}}]}]\n",
    "\n",
    "# Load a blank model and add an EntityRuler\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add the compiled patterns to the EntityRuler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print the tuple of entities texts and types for the given text\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f64db6-33cc-4f7b-8ab0-a7f914d87e8b",
   "metadata": {},
   "source": [
    "Matching a single term in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc17ea74-8648-4cc4-8f35-2d679d1e8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbc37231-1856-44f4-ab2b-2e90029706eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token:  24  | End token:  25 | Matched text:  Witch\n",
      "Start token:  47  | End token:  48 | Matched text:  Witch\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Initialize a Matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match lower cased word witch\n",
    "pattern = [{\"lower\" : \"witch\"}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print start and end token indices and span of the matched text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85145ef-9928-43c1-9ed8-ab031fb258cf",
   "metadata": {},
   "source": [
    "PhraseMatcher in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ead86614-22f0-400a-989c-48e4ef96324f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There are only a few acceptable IP addresse: (1) 127.100.0.1, (2) 123.4.1.0."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"There are only a few acceptable IP addresse: (1) 127.100.0.1, (2) 123.4.1.0.\"\n",
    "terms = [\"110.0.0.0\", \"101.243.0.7\"]\n",
    "\n",
    "# Initialize a PhraseMatcher class to match to shapes of given terms\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"SHAPE\")\n",
    "\n",
    "# Create patterns to add to the PhraseMatcher object\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"IPAddresses\", patterns)\n",
    "# Find matches to the given patterns and print start and end characters and matches texts\n",
    "doc = nlp(text)\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62ba09b7-0406-44ef-a9b6-23a479dc5eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17762269709679412013, 12, 13), (17762269709679412013, 17, 18)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aff99d0f-f68a-41fc-9369-6911dc0f2457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token:  12  | End token:  13 | Matched text:  127.100.0.1\n",
      "Start token:  17  | End token:  18 | Matched text:  123.4.1.0\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21cd01-4ae5-40db-b25d-1bb7ea3ed6d0",
   "metadata": {},
   "source": [
    "Matching with extended syntax in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49473353-1b9e-4e8f-998b-4955143f583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'It is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1639b50f-b999-472a-a9ed-7842940665d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13760853105470257182, 4, 6), (13760853105470257182, 19, 21)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Define a matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Define a pattern to match tiny squares and tiny mouthful\n",
    "pattern = [{\"lower\": \"tiny\"}, {\"lower\": {\"IN\": [\"squares\", \"mouthful\"]}}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "847bc470-1f6f-400e-9ca1-17a244033702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token:  4  | End token:  6 | Matched text:  tiny squares\n",
      "Start token:  19  | End token:  21 | Matched text:  tiny mouthful\n"
     ]
    }
   ],
   "source": [
    "# Print out start and end token indices and the matched text span per match\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45af146-7e7e-4854-bbb6-8dd189c23c0a",
   "metadata": {},
   "source": [
    "Model performance on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9c375-9290-4d5b-b4af-7cb86b9a541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load en_core_web_sm and create an nlp object\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd90ec70-eedf-4a75-8315-5a19be50bb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Product arrived labeled as Jumbo Salted Peanuts.,\n",
       " Not sure if the product was labeled as Jumbo.]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Product arrived labeled as Jumbo Salted Peanuts.\",\n",
    " \"Not sure if the product was labeled as Jumbo.\"]\n",
    "\n",
    "documents = [nlp(doc) for doc in documents]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c70ee7a-cfc8-43d5-ae1f-415baa8a1837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Product arrived labeled as Jumbo Salted Peanuts.,\n",
       " Not sure if the product was labeled as Jumbo.]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['Product arrived labeled as Jumbo Salted Peanuts.',\n",
    " 'Not sure if the product was labeled as Jumbo.']\n",
    "\n",
    "texts = [nlp(text) for text in texts]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "188dbd6b-cd8f-4037-aabd-64f4ed02e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jumbo Salted Peanuts', 'PERSON'), ('Jumbo', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Append a tuple of (entities text, entities label) if Jumbo is in the entity\n",
    "target_entities = []\n",
    "for doc in documents:\n",
    "    target_entities.extend([(ent.text, ent.label_) for ent in doc.ents if \"Jumbo\" in ent.text])\n",
    "print(target_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f29bcdb0-4183-42dd-b80b-799c66bbf185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False]\n"
     ]
    }
   ],
   "source": [
    "# Append True to the correct_labels list if the entity label is `PRODUCT`\n",
    "correct_labels = []\n",
    "for ent in target_entities:\n",
    "    if ent[1] == \"PRODUCT\":\n",
    "        correct_labels.append(True)\n",
    "    else:\n",
    "        correct_labels.append(False)\n",
    "print(correct_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d23e12-9d5a-4990-8831-a911be73a7d8",
   "metadata": {},
   "source": [
    "Annotation and preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2847eca5-ae24-4332-8e6b-10821eb88dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A patient with chest pain had hyperthyroidism.', {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n"
     ]
    }
   ],
   "source": [
    "text = \"A patient with chest pain had hyperthyroidism.\"\n",
    "entity_1 = \"chest pain\"\n",
    "entity_2 = \"hyperthyroidism\"\n",
    "\n",
    "# Store annotated data information in the correct format\n",
    "annotated_data = {\"sentence\": text, \"entities\": [{\"label\": \"SYMPTOM\", \"value\": entity_1}, {\"label\": \"DISEASE\", \"value\": entity_2}]}\n",
    "\n",
    "# Extract start and end characters of each entity\n",
    "entity_1_start_char = text.find(entity_1)\n",
    "entity_1_end_char = entity_1_start_char + len(entity_1)\n",
    "entity_2_start_char = text.find(entity_2)\n",
    "entity_2_end_char = entity_2_start_char + len(entity_2)\n",
    "\n",
    "# Store the same input information in the proper format for training\n",
    "training_data = [(text, {\"entities\": [(entity_1_start_char,entity_1_end_char,\"SYMPTOM\"), \n",
    "                                      (entity_2_start_char,entity_2_end_char,\"DISEASE\")]})]\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec723c00-cce3-46bd-8e8f-75730aa7f4a4",
   "metadata": {},
   "source": [
    "Compatible training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27d44ecc-093e-4ee9-b3ab-8839eeb1cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'B-SYMPTOM', 'L-SYMPTOM', 'O', 'U-DISEASE', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'patient', 'with', 'chest', 'pain', 'had', 'hyperthyroidism', '.'], 'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}} \n",
      "\n",
      "Number of formatted training data:  1\n"
     ]
    }
   ],
   "source": [
    "example_text = 'A patient with chest pain had hyperthyroidism.'\n",
    "training_data = [(example_text, {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
    "\n",
    "all_examples = []\n",
    "# Iterate through text and annotations and convert text to a Doc container\n",
    "for text, annotations in training_data:\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create an Example object from the doc contianer and annotations\n",
    "    example_sentence = Example.from_dict(doc, annotations)\n",
    "    print(example_sentence.to_dict(), \"\\n\")\n",
    "\n",
    "    # Append the Example object to the list of all examples\n",
    "    all_examples.append(example_sentence)\n",
    "\n",
    "print(\"Number of formatted training data: \", len(all_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159c4e7-2e4f-4511-8daa-d40cf9597ab6",
   "metadata": {},
   "source": [
    "Training preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4df5d560-094c-4d70-8941-298052d51674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Disable all pipeline components of  except `ner`\n",
    "all_pipes = [pipeline for pipeline in nlp.pipe_names]\n",
    "other_pipes = [pipeline for pipeline in nlp.pipe_names if pipeline != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "all_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3aa52cb7-44ed-4749-9783-80ab30f148df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example object for training: \n",
      " {'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'B-SYMPTOM', 'L-SYMPTOM', 'O', 'U-DISEASE', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'patient', 'with', 'chest', 'pain', 'had', 'hyperthyroidism', '.'], 'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "# Convert a text and its annotations to the correct format usable for training\n",
    "doc = nlp.make_doc(example_text)\n",
    "example = Example.from_dict(doc, annotations)\n",
    "print(\"Example object for training: \\n\", example.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f297d92-eb99-4646-b651-aa74ecfbc309",
   "metadata": {},
   "source": [
    "Train an existing NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8f6a42a-7e91-4c6a-86d6-f6cfaca0129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [('I will visit you in Austin.', {'entities': [(20, 26, 'GPE')]}),\n",
    " (\"I'm going to Sam's house.\",\n",
    "  {'entities': [(13, 16, 'PERSON'), (19, 24, 'GPE')]}),\n",
    " ('I will go.', {'entities': []})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3ccd3254-6f84-4ae0-ae12-7076b9e4857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:  [('Sam', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "test = \"I'm going to Sam's house.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # A spaCy en_core_web_sm model that is accessible as nlp, which is not able to correctly predict house as an entity in a test string.\n",
    "print(\"Before training: \", [(ent.text, ent.label_) for ent in nlp(test).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cbecff7d-6b9b-43f8-bd25-85f23b4ed031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "other_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e883cd5-263f-4e6b-b609-cf1042d0116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:  [('Sam', 'PERSON'), ('house', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "# Shuffle training data and the dataset using random package per epoch\n",
    "for i in range(epochs):\n",
    "    random.shuffle(training_data)\n",
    "    for text, annotations in training_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        # Update nlp model after setting sgd argument to optimizer\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], sgd = optimizer)\n",
    "print(\"After training: \", [(ent.text, ent.label_) for ent in nlp(test).ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07972c0-9b89-4a05-a996-759a49844ce5",
   "metadata": {},
   "source": [
    "Training a spaCy model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "560ce926-eac5-436f-81a1-b379911cfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [('Diarrhea, also spelled diarrhoea, is the condition of having at least three loose, liquid, or watery bowel movements each day.[2] It often lasts for a few days and can result in dehydration due to fluid loss.[2] Signs of dehydration often begin with loss of the normal stretchiness of the skin and irritable behaviour.[2] This can progress to decreased urination, loss of skin color, a fast heart rate, and a decrease in responsiveness as it becomes more severe.[2] Loose but non-watery stools in babies who are exclusively breastfed, however, are normal.[2]',\n",
    "  {'entities': [(364, 382, 'MedicalCondition'),\n",
    "    (0, 8, 'MedicalCondition'),\n",
    "    (94, 116, 'MedicalCondition'),\n",
    "    (178, 189, 'MedicalCondition'),\n",
    "    (221, 232, 'MedicalCondition'),\n",
    "    (23, 32, 'MedicalCondition'),\n",
    "    (409, 435, 'MedicalCondition'),\n",
    "    (386, 401, 'MedicalCondition')]}),\n",
    " ('Antiretroviral therapy (ART) is recommended for all HIV-infected individuals to reduce the risk of disease progression.\\nART also is recommended for HIV-infected individuals for the prevention of transmission of HIV.\\nPatients starting ART should be willing and able to commit to treatment and understand the benefits and risks of therapy and the importance of adherence. Patients may choose to postpone therapy, and providers, on a case-by-case basis, may elect to defer therapy on the basis of clinical and/or psychosocial factors.',\n",
    "  {'entities': [(0, 22, 'Medicine'),\n",
    "    (24, 27, 'Medicine'),\n",
    "    (120, 123, 'Medicine'),\n",
    "    (211, 214, 'Pathogen'),\n",
    "    (52, 55, 'Pathogen'),\n",
    "    (234, 237, 'Medicine'),\n",
    "    (148, 151, 'Pathogen')]}),\n",
    " (\"The goals of treatment are to reduce pain, decrease inflammation, and improve a person's overall functioning.[5] This may be helped by balancing rest and exercise, the use of splints and braces, or the use of assistive devices.[1][6][7] Pain medications, steroids, and NSAIDs are frequently used to help with symptoms.[1] Disease-modifying antirheumatic drugs (DMARDs), such as hydroxychloroquine and methotrexate, may be used to try to slow the progression of disease.[1] Biological DMARDs may be used when disease does not respond to other treatments.[8] However, they may have a greater rate of adverse effects.[9] Surgery to repair, replace, or fuse joints may help in certain situations.[1] Most alternative medicine treatments are not supported by evidence.[10][11]\",\n",
    "  {'entities': [(401, 413, 'Medicine'),\n",
    "    (378, 396, 'Medicine'),\n",
    "    (473, 490, 'Medicine'),\n",
    "    (255, 263, 'Medicine')]}),\n",
    " (\"Hantaviruses, usually found in rodents and shrews, were discovered in two species of bats. The Mouyassué virus (MOUV) was isolated from banana pipistrelle bats captured near Mouyassué village in Cote d'Ivoire, West Africa. The Magboi virus was isolated from hairy slit-faced bats found near the Magboi River in Sierra Leone in 2011. They are single-stranded, negative sense, RNA viruses in the Bunyaviridae family.[29][30][31][32]\",\n",
    "  {'entities': [(0, 12, 'Pathogen'),\n",
    "    (394, 406, 'Pathogen'),\n",
    "    (227, 239, 'Pathogen'),\n",
    "    (95, 110, 'Pathogen')]}),\n",
    " ('Bats are the most common source of rabies in humans in North and South America, Western Europe, and Australia. In the United States, there were 19 cases of human rabies from 1997–2006, 17 of which were attributed to bats.[27] In North America, about half of human rabies instances are cryptic, meaning that the patient has no known bite history.[24] While it has been speculated that rabies virus could be transmitted through aerosols, studies of the rabies virus have concluded that this is only feasible in limited conditions. These conditions include a very large colony of bats in a hot and humid cave with poor ventilation. While two human deaths in 1956 and 1959 had been tentatively attributed to aerosolization of the rabies virus after entering a cave with bats, \"investigations of the 2 reported human cases revealed that both infections could be explained by means other than aerosol transmission\".[28] It is instead generally thought that most instances of cryptic rabies are the result of an unknown bat bite.[24] Bites from a bat can be so small that they are not visible without magnification equipment, for example. Outside of bites, rabies virus exposure can also occur if infected fluids come in contact with a mucous membrane or a break in the skin. Rabies virus has also been transmitted when an infected human unknowingly dies of rabies, and their organs are transplanted to others.[28]',\n",
    "  {'entities': [(35, 41, 'MedicalCondition'),\n",
    "    (162, 168, 'MedicalCondition'),\n",
    "    (384, 396, 'Pathogen'),\n",
    "    (1269, 1281, 'Pathogen'),\n",
    "    (1343, 1347, 'MedicalCondition'),\n",
    "    (977, 983, 'MedicalCondition'),\n",
    "    (1027, 1032, 'MedicalCondition')]}),\n",
    " ('Other groups of intracellular bacterial pathogens include Salmonella, Neisseria, Brucella, Mycobacterium, Nocardia, Listeria, Francisella, Legionella, and Yersinia pestis. These can exist intracellularly, but can exist outside of host cells.',\n",
    "  {'entities': [(116, 124, 'Pathogen'),\n",
    "    (155, 170, 'Pathogen'),\n",
    "    (126, 137, 'Pathogen'),\n",
    "    (70, 79, 'Pathogen'),\n",
    "    (139, 149, 'Pathogen'),\n",
    "    (106, 114, 'Pathogen'),\n",
    "    (91, 104, 'Pathogen'),\n",
    "    (81, 89, 'Pathogen'),\n",
    "    (58, 68, 'Pathogen')]}),\n",
    " ('One of the bacterial diseases with the highest disease burden is tuberculosis, caused by Mycobacterium tuberculosis bacteria, which kills about 2 million people a year, mostly in sub-Saharan Africa. Pathogenic bacteria contribute to other globally important diseases, such as pneumonia, which can be caused by bacteria such as Streptococcus and Pseudomonas, and foodborne illnesses, which can be caused by bacteria such as Shigella, Campylobacter, and Salmonella. Pathogenic bacteria also cause infections such as tetanus, typhoid fever, diphtheria, syphilis, and leprosy. Pathogenic bacteria are also the cause of high infant mortality rates in developing countries.[3]',\n",
    "  {'entities': [(327, 340, 'Pathogen'),\n",
    "    (514, 521, 'MedicalCondition'),\n",
    "    (452, 462, 'Pathogen'),\n",
    "    (276, 285, 'MedicalCondition'),\n",
    "    (523, 536, 'MedicalCondition'),\n",
    "    (564, 571, 'MedicalCondition'),\n",
    "    (433, 446, 'Pathogen'),\n",
    "    (538, 548, 'MedicalCondition'),\n",
    "    (345, 356, 'Pathogen'),\n",
    "    (65, 77, 'MedicalCondition'),\n",
    "    (550, 558, 'MedicalCondition'),\n",
    "    (89, 115, 'Pathogen'),\n",
    "    (423, 431, 'Pathogen')]}),\n",
    " (\"Although the vast majority of bacteria are harmless or beneficial to one's body, a few pathogenic bacteria can cause infectious diseases. The most common bacterial disease is tuberculosis, caused by the bacterium Mycobacterium tuberculosis, which affects about 2 million people mostly in sub-Saharan Africa. Pathogenic bacteria contribute to other globally important diseases, such as pneumonia, which can be caused by bacteria such as Streptococcus and Pseudomonas, and foodborne illnesses, which can be caused by bacteria such as Shigella, Campylobacter, and Salmonella. Pathogenic bacteria also cause infections such as tetanus, typhoid fever, diphtheria, syphilis, and Hansen's disease. They typically range between 1 and 5 micrometers in length.\",\n",
    "  {'entities': [(659, 667, 'MedicalCondition'),\n",
    "    (436, 449, 'Pathogen'),\n",
    "    (673, 689, 'MedicalCondition'),\n",
    "    (30, 38, 'Pathogen'),\n",
    "    (454, 465, 'Pathogen'),\n",
    "    (647, 657, 'MedicalCondition'),\n",
    "    (87, 106, 'Pathogen'),\n",
    "    (532, 540, 'Pathogen'),\n",
    "    (561, 571, 'Pathogen'),\n",
    "    (623, 630, 'MedicalCondition'),\n",
    "    (471, 490, 'MedicalCondition'),\n",
    "    (632, 645, 'MedicalCondition'),\n",
    "    (542, 555, 'Pathogen')]}),\n",
    " ('Much like viral pathogens, infection by certain bacterial pathogens can be prevented via vaccines.[30] Vaccines against bacterial pathogens include the anthrax vaccine and the pneumococcal vaccine. Many other bacterial pathogens lack vaccines as a preventive measure, but infection by these bacteria can often be treated or prevented with antibiotics. Common antibiotics include amoxicillin, ciprofloxacin, and doxycycline. Each antibiotic has different bacteria that it is effective against and has different mechanisms to kill that bacteria. For example, doxycycline inhibits the synthesis of new proteins in both gram-negative and gram-positive bacteria which leads to the death of the affected bacteria.[35]',\n",
    "  {'entities': [(379, 390, 'Medicine'),\n",
    "    (152, 167, 'Medicine'),\n",
    "    (411, 422, 'Medicine'),\n",
    "    (392, 405, 'Medicine'),\n",
    "    (176, 196, 'Medicine')]}),\n",
    " ('The term pathogen came into use in the 1880s.[1][2] Typically, the term is used to describe an infectious microorganism or agent, such as a virus, bacterium, protozoan, prion, viroid, or fungus.[',\n",
    "  {'entities': [(158, 167, 'Pathogen'),\n",
    "    (95, 119, 'Pathogen'),\n",
    "    (187, 193, 'Pathogen'),\n",
    "    (147, 156, 'Pathogen'),\n",
    "    (140, 145, 'Pathogen')]}),\n",
    " (\"Some antidepressants are used as a treatment for social anxiety disorder, but their efficacy is not entirely convincing, as only a small proportion of antidepressants showed some efficacy for this condition. Paroxetine was the first drug to be FDA-approved for this disorder. Its efficacy is considered beneficial, although not everyone responds favorably to the drug. Sertraline and fluvoxamine extended release were later approved for it as well, while escitalopram is used off-label with acceptable efficacy. However, there isn't enough evidence to support citalopram for treating social phobia, and fluoxetine was no better than placebo in clinical trials. SSRIs are used as a first-line treatment for social anxiety, but they don't work for everyone. One alternative would be venlafaxine, which is a SNRI. It showed benefits for social phobia in five clinical trials against placebo, while the other SNRIs are not considered particularly useful for this disorder as many of them didn't undergo testing for it. As of now, it is unclear if duloxetine and desvenlafaxine can provide benefits for social anxiety sufferers. However, another class of antidepressants called MAOIs are considered effective for social anxiety, but they come with many unwanted side effects and are rarely used. Phenelzine was shown to be a good treatment option, but its use is limited by dietary restrictions. Moclobemide is a RIMA and showed mixed results but still got approval in some European countries for social anxiety disorder. TCA antidepressants, such as clomipramine and imipramine, are not considered effective for this anxiety disorder in particular. This leaves out SSRIs such as paroxetine, sertraline and fluvoxamine CR as acceptable and tolerated treatment options for this disorder.[19][20]\",\n",
    "  {'entities': [(384, 395, 'Medicine'),\n",
    "    (1098, 1112, 'MedicalCondition'),\n",
    "    (1687, 1697, 'Medicine'),\n",
    "    (49, 72, 'MedicalCondition'),\n",
    "    (1173, 1178, 'Medicine'),\n",
    "    (1702, 1713, 'Medicine'),\n",
    "    (781, 792, 'Medicine'),\n",
    "    (1563, 1573, 'Medicine'),\n",
    "    (603, 613, 'Medicine'),\n",
    "    (1675, 1685, 'MedicalCondition'),\n",
    "    (1613, 1629, 'MedicalCondition'),\n",
    "    (369, 379, 'Medicine'),\n",
    "    (1291, 1301, 'Medicine'),\n",
    "    (1546, 1558, 'Medicine'),\n",
    "    (455, 467, 'Medicine'),\n",
    "    (1391, 1402, 'Medicine'),\n",
    "    (584, 597, 'MedicalCondition')]}),\n",
    " (\"However, existing data suggest that patients taking bedaquiline in addition to standard TB therapy are five times more likely to die than those without the new drug,[184] which has resulted in medical journal articles raising health policy questions about why the FDA approved the drug and whether financial ties to the company making bedaquiline influenced physicians' support for its use.[183][185]\",\n",
    "  {'entities': [(88, 98, 'Medicine'),\n",
    "    (335, 346, 'Medicine'),\n",
    "    (52, 63, 'Medicine')]}),\n",
    " ('Tuberculosis may infect any part of the body, but most commonly occurs in the lungs (known as pulmonary tuberculosis).[9] Extrapulmonary TB occurs when tuberculosis develops outside of the lungs, although extrapulmonary TB may coexist with pulmonary TB.[9]\\n\\nGeneral signs and symptoms include fever, chills, night sweats, loss of appetite, weight loss, and fatigue.[9] Significant nail clubbing may also occur.[16]',\n",
    "  {'entities': []}),\n",
    " ('A number of factors make people more susceptible to TB infections. The most important risk factor globally is HIV; 13% of all people with TB are infected by the virus.[39] This is a particular problem in sub-Saharan Africa, where rates of HIV are high.[40][41] Of people without HIV who are infected with tuberculosis, about 5–10% develop active disease during their lifetimes;[16] in contrast, 30% of those coinfected with HIV develop the active disease.[16]',\n",
    "  {'entities': [(279, 282, 'Pathogen')]}),\n",
    " ('Examples of common human diseases caused by viruses include the common cold, influenza, chickenpox, and cold sores. Many serious diseases such as rabies, Ebola virus disease, AIDS (HIV), avian influenza, and SARS are caused by viruses. The relative ability of viruses to cause disease is described in terms of virulence. Other diseases are under investigation to discover if they have a virus as the causative agent, such as the possible connection between human herpesvirus 6 (HHV6) and neurological diseases such as multiple sclerosis and chronic fatigue syndrome.[151] There is controversy over whether the bornavirus, previously thought to cause neurological diseases in horses, could be responsible for psychiatric illnesses in humans.[152]',\n",
    "  {'entities': [(518, 536, 'MedicalCondition'),\n",
    "    (154, 165, 'Pathogen'),\n",
    "    (708, 729, 'MedicalCondition'),\n",
    "    (463, 476, 'Pathogen'),\n",
    "    (77, 86, 'MedicalCondition'),\n",
    "    (88, 98, 'MedicalCondition'),\n",
    "    (187, 202, 'MedicalCondition'),\n",
    "    (610, 620, 'Pathogen')]}),\n",
    " ('Buprenorphine has been shown experimentally (1982–1995) to be effective against severe, refractory depression',\n",
    "  {'entities': [(0, 13, 'Medicine'), (88, 109, 'MedicalCondition')]}),\n",
    " ('Bupropion (Wellbutrin), an anti-depressant, is also used as a smoking cessation aid; this indication was later approved, and the name of the smoking cessation product is Zyban. In Ontario, Canada, smoking cessation drugs are not covered by provincial drug plans; elsewhere, Zyban is priced higher than Wellbutrin, despite being the same drug. Therefore, some physicians prescribe Wellbutrin for both indications.[',\n",
    "  {'entities': [(274, 279, 'Medicine'),\n",
    "    (11, 21, 'Medicine'),\n",
    "    (302, 312, 'Medicine'),\n",
    "    (380, 390, 'Medicine'),\n",
    "    (170, 175, 'Medicine'),\n",
    "    (0, 9, 'Medicine')]}),\n",
    " ('Carbamazepine is an approved treatment for bipolar disorder and epileptic seizures, but it has side effects useful in treating attention-deficit hyperactivity disorder (ADHD), schizophrenia, phantom limb syndrome, paroxysmal extreme pain disorder, neuromyotonia, and post-traumatic stress disorder.[8]',\n",
    "  {'entities': [(267, 288, 'MedicalCondition'),\n",
    "    (248, 261, 'MedicalCondition'),\n",
    "    (0, 13, 'Medicine'),\n",
    "    (43, 59, 'MedicalCondition'),\n",
    "    (145, 167, 'MedicalCondition'),\n",
    "    (176, 189, 'MedicalCondition'),\n",
    "    (64, 82, 'MedicalCondition'),\n",
    "    (191, 212, 'MedicalCondition')]}),\n",
    " ('The antiviral drugs amantadine and rimantadine inhibit a viral ion channel (M2 protein), thus inhibiting replication of the influenza A virus.[86] These drugs are sometimes effective against influenza A if given early in the infection but are ineffective against influenza B viruses, which lack the M2 drug target.[160] Measured resistance to amantadine and rimantadine in American isolates of H3N2 has increased to 91% in 2005.[161] This high level of resistance may be due to the easy availability of amantadines as part of over-the-counter cold remedies in countries such as China and Russia,[162] and their use to prevent outbreaks of influenza in farmed poultry.[163][164] The CDC recommended against using M2 inhibitors during the 2005–06 influenza season due to high levels of drug resistance.[165]',\n",
    "  {'entities': [(639, 648, 'MedicalCondition'),\n",
    "    (35, 46, 'Medicine'),\n",
    "    (712, 725, 'Medicine'),\n",
    "    (20, 30, 'Medicine')]}),\n",
    " ('The two classes of antiviral drugs used against influenza are neuraminidase inhibitors (oseltamivir, zanamivir, laninamivir and peramivir) and M2 protein inhibitors (adamantane derivatives)',\n",
    "  {'entities': [(128, 137, 'Medicine'),\n",
    "    (101, 110, 'Medicine'),\n",
    "    (112, 123, 'Medicine'),\n",
    "    (48, 57, 'MedicalCondition'),\n",
    "    (88, 99, 'Medicine')]})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ccaa2136-d7c5-487e-ae71-510df37efb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Pathogen', 'MedicalCondition', 'Medicine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "350781bc-059c-4b0e-99ec-b69dcf2335b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank English model, add NER component, add given labels to the ner pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for ent in labels:\n",
    "    ner.add_label(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e2d0d71-e6ba-4dae-8404-65177596341e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable other pipeline components, complete training loop and run training loop\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "losses = {}\n",
    "optimizer = nlp.begin_training()\n",
    "other_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c94c5713-3bbe-4779-870a-a41d182cf956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 98.42856240272522}\n",
      "{'ner': 188.5091609954834}\n",
      "{'ner': 309.517851293087}\n",
      "{'ner': 375.8999624848366}\n",
      "{'ner': 575.185431599617}\n",
      "{'ner': 605.2146937847137}\n",
      "{'ner': 669.3962617814541}\n",
      "{'ner': 717.6792084276676}\n",
      "{'ner': 736.8563084732741}\n",
      "{'ner': 746.4217485651607}\n",
      "{'ner': 779.9957114736317}\n",
      "{'ner': 785.9423416967093}\n",
      "{'ner': 785.9639989202074}\n",
      "{'ner': 787.9575986306503}\n",
      "{'ner': 803.9006502354792}\n",
      "{'ner': 807.8630042147869}\n",
      "{'ner': 819.7619179444417}\n",
      "{'ner': 835.6793623805029}\n",
      "{'ner': 843.6444945651712}\n",
      "{'ner': 853.4423006288563}\n"
     ]
    }
   ],
   "source": [
    "for text, annotation in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotation)\n",
    "    nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
