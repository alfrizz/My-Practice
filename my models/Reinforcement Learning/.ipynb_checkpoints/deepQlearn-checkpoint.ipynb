{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db57e3cf-0a26-467c-8936-061ddce14cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/a-beginners-guide-to-deep-reinforcement-learning/\n",
    "\n",
    "# Solving the CartPole Problem using Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33a5664-a579-42ef-962a-ddda2777869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562f427f-03ab-45aa-bc49-ba5990bf3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN Model\n",
    "class DQN(tf.keras.Model): # creating a child class DQN that inherits from the parent class tf.keras.Model\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__() # to call the __init__ of the parent class, before overriding it. It can be also written \"super().__init__()\"\n",
    "        self.dense1 = tf.keras.layers.Dense(24, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_actions, activation='linear') #  is the part of the model that predicts the Q-values for each possible action given the current state of the environment. The activation function for this layer is ‘linear’, meaning that the layer will output the raw values produced by its neurons without applying any additional function to them. This is common in Q-learning models like DQN, where the goal is to predict a set of arbitrary real-valued numbers (the Q-values).\n",
    "\n",
    " \n",
    "    def call(self, inputs): # here the model takes the current state of the environment (the inputs) and produces Q-values for each possible action. The agent can then use these Q-values to select its next action\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e377b29-d111-4d31-aa04-90ef225767cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DQN at 0x24585026310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CartPole has 2 possible actions: push left or push right\n",
    "\n",
    "num_actions = 2 \n",
    "dqn_agent = DQN(num_actions)\n",
    "dqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f034bc4-7eb6-4c60-a8b2-0a541ee38f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN Algorithm Parameters\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.99\n",
    "\n",
    "# Initial exploration probability\n",
    "exploration_prob = 1.0\n",
    "# Decay rate of exploration probability\n",
    "exploration_decay = 0.995\n",
    "# Minimum exploration probability\n",
    "min_exploration_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c13737-df80-4b3a-a1cb-b53628e4be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CartPole Environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#if we want to render\n",
    "# env = gym.make('CartPole-v1', render_mode='human')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f2e77e-6c37-4190-932f-dbe830ab4603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03587867, -0.04158823,  0.00895021,  0.00234133], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "# it returns an array containing, in order:\n",
    "# 1)position of the cart along the linear surface\n",
    "# 2)vertical angle of the pole on the cart\n",
    "# 3)linear velocity of the cart\n",
    "# 4)angular velocity of the pole on the cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3794d0e-87da-48ba-b934-b9851b1ece70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00649007,  0.01811054,  0.04681893, -0.01777743], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_reset = env.reset()[0]\n",
    "state_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e248b8-7496-475b-8b3f-254e63151334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.00304406, -0.00302192]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#The main reason for adding the extra dimension in this case is to match the input shape that the model expects. The model is designed to process batches of states, so even when you’re only predicting the Q-values for a single state (state_reset.reshape(1,-1) or state[np.newaxis, :] below), that state needs to be in the form of a batch.\n",
    "dqn_agent(state_reset.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d227626-bb5a-4428-a404-1ad90785a1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.003021919"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_agent(state_reset.reshape(1,-1)).numpy()[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1593fb16-0753-4218-8291-3a6f472c81fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(dqn_agent(state_reset.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e864091f-bea6-4ecf-8a6c-fa120d6e7bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py309\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57885c67-5576-498f-baf4-b6a26d7c6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError() # The MSE loss function measures the average squared differences between the estimated and true Q-values. This is appropriate for your problem because it effectively penalizes the model when its predictions are far from the actual values\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe774bdd-7dce-4384-a10f-ae62bb8f9144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4184e453-6ccc-4d60-8dad-5a3013bea6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py309\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.00685228, -0.1776505 ,  0.04646339,  0.28930208], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d835c334-8a12-476b-bf9b-c2e9a1d95312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Reward = 27.0\n",
      "Episode 200: Reward = 16.0\n",
      "Episode 300: Reward = 21.0\n",
      "Episode 400: Reward = 26.0\n",
      "Episode 500: Reward = 104.0\n"
     ]
    }
   ],
   "source": [
    "# Training the DQN\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 500\n",
    " \n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    " \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if np.random.rand() < exploration_prob:\n",
    "            action = env.action_space.sample()  # Explore randomly\n",
    "        else:\n",
    "            action = np.argmax(dqn_agent(state[np.newaxis, :])) # Given the current state of the environment, pass that state to the DQN model, get the predicted Q-values for each possible action, and choose the action with the highest Q-value ------ np.newaxis adds a extra dimension to the state array (same as state.reshape(1, -1)) to match the input shape that the model expects (see above)\n",
    " \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    " \n",
    "        # Update the Q-values using Bellman equation\n",
    "        with tf.GradientTape() as tape: # context manager that allows TensorFlow to trace the computation of Q-values, the Bellman equation, and the calculation of the loss, and automatically compute the gradients of the loss with respect to the trainable variables (dqn_agent.trainable_variables)\n",
    "            current_q_values = dqn_agent(state[np.newaxis, :])\n",
    "            next_q_values = dqn_agent(next_state[np.newaxis, :])\n",
    "            max_next_q = tf.reduce_max(next_q_values, axis=-1)\n",
    "            target_q_values = current_q_values.numpy()\n",
    "            target_q_values[0, action] = reward + discount_factor * max_next_q * (1 - done) # updating the q_value for the specific action taken\n",
    "            loss = loss_fn(current_q_values, target_q_values)\n",
    " \n",
    "        gradients = tape.gradient(loss, dqn_agent.trainable_variables) # calculated the gradients using the context manager above\n",
    "        optimizer.apply_gradients(zip(gradients, dqn_agent.trainable_variables)) # applies the gradients computed during backpropagation to update the parameters of the neural network (trainable variables). The zip function combines the gradients and trainable variables into pairs. Each pair consists of a specific gradient and the corresponding trainable variable. This pairing is necessary to specify which gradients should be applied to which variables during the parameter update.\n",
    " \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    " \n",
    "        if done:\n",
    "            break\n",
    " \n",
    "    # Decay exploration probability\n",
    "    exploration_prob = max(min_exploration_prob, exploration_prob * exploration_decay)\n",
    "    if (episode + 1)%100==0:\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7127bb3-8a46-49b9-8671-9665d12667b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Reward: 110.3\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Trained DQN\n",
    "num_eval_episodes = 10\n",
    "eval_rewards = []\n",
    " \n",
    "for _ in range(num_eval_episodes):\n",
    "    state = env.reset()[0]\n",
    "    eval_reward = 0\n",
    " \n",
    "    for _ in range(max_steps_per_episode):\n",
    "        action = np.argmax(dqn_agent(state[np.newaxis, :]))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        eval_reward += reward\n",
    "        state = next_state\n",
    " \n",
    "        if done:\n",
    "            break\n",
    " \n",
    "    eval_rewards.append(eval_reward)\n",
    " \n",
    "average_eval_reward = np.mean(eval_rewards)\n",
    "print(f\"Average Evaluation Reward: {average_eval_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad3d59-164b-43e6-928b-5b89386f5e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea980e3-7636-4600-a992-8346adbcfc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e996df8-af72-4948-bcad-80f6f9807bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72377383-3e09-41cb-b42f-4b0d26b91f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7c691-ce2e-4447-bcab-d6d4c423f9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c43a77-e8d4-46f2-ade3-4e45055afcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a126be0-500b-44d2-9dc2-6f6d7114048d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af545533-4cd2-4292-bc91-7cc6c51dd727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py309",
   "language": "python",
   "name": "py309"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
