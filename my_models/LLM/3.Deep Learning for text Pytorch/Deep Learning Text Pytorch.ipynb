{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9030c42-92d4-49f0-aefa-558e6c5e316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Torch version: 2.2.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fac74d0-7035-413c-b649-d488c00f7233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchtext version: 0.17.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(\"Torchtext version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f57ac7-9680-43fc-8996-efcaef8e8201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'dataville',\n",
       " ',',\n",
       " 'a',\n",
       " 'data',\n",
       " 'analyst',\n",
       " 'named',\n",
       " 'alex',\n",
       " 'explores',\n",
       " 'hidden',\n",
       " 'insights',\n",
       " 'within',\n",
       " 'vast',\n",
       " 'data',\n",
       " '.',\n",
       " 'with',\n",
       " 'determination',\n",
       " ',',\n",
       " 'alex',\n",
       " 'uncovers',\n",
       " 'patterns',\n",
       " ',',\n",
       " 'cleanses',\n",
       " 'the',\n",
       " 'data',\n",
       " ',',\n",
       " 'and',\n",
       " 'unlocks',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'join',\n",
       " 'this',\n",
       " 'adventure',\n",
       " 'to',\n",
       " 'unleash',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'data-driven',\n",
       " 'decisions',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "# !pip install torchtext\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
    "\n",
    "# Initialize the tokenizer and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b598c044-848b-4ba1-b3fd-0be5364e5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', ',', 'data', 'alex', 'data', '.', ',', 'alex', ',', 'the', 'data', ',', '.', 'the', 'of', '.']\n"
     ]
    }
   ],
   "source": [
    "threshold = 1\n",
    "# Remove rare words and print common tokens\n",
    "freq_dist = FreqDist(tokens)\n",
    "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b0188f-6d9c-4f56-9e39-e92d5da93df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The moor is very sparsely inhabited, and those who live near each other are thrown very much together. For this reason I saw a good deal of Sir Charles Baskerville. With the exception of Mr. Frankland, of Lafter Hall, and Mr. Stapleton, the naturalist, there are no other men of education within many miles. Sir Charles was a retiring man, but the chance of his illness brought us together, and a community of interests in science kept us so. He had brought back much scientific information from South Africa, and many a charming evening we have spent together discussing the comparative anatomy of the Bushman and the Hottentot.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aaadaa8-26ff-44f9-9d38-358c1a266eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'moor',\n",
       " 'is',\n",
       " 'very',\n",
       " 'sparsely',\n",
       " 'inhabited',\n",
       " ',',\n",
       " 'and',\n",
       " 'those',\n",
       " 'who']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38359376-f1dc-4f32-b487-5fea63229b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moor',\n",
       " 'sparsely',\n",
       " 'inhabited',\n",
       " ',',\n",
       " 'live',\n",
       " 'near',\n",
       " 'thrown',\n",
       " 'much',\n",
       " 'together',\n",
       " '.',\n",
       " 'reason',\n",
       " 'saw',\n",
       " 'good',\n",
       " 'deal',\n",
       " 'sir',\n",
       " 'charles',\n",
       " 'baskerville',\n",
       " '.',\n",
       " 'exception',\n",
       " 'mr',\n",
       " '.',\n",
       " 'frankland',\n",
       " ',',\n",
       " 'lafter',\n",
       " 'hall',\n",
       " ',',\n",
       " 'mr',\n",
       " '.',\n",
       " 'stapleton',\n",
       " ',',\n",
       " 'naturalist',\n",
       " ',',\n",
       " 'men',\n",
       " 'education',\n",
       " 'within',\n",
       " 'many',\n",
       " 'miles',\n",
       " '.',\n",
       " 'sir',\n",
       " 'charles',\n",
       " 'retiring',\n",
       " 'man',\n",
       " ',',\n",
       " 'chance',\n",
       " 'illness',\n",
       " 'brought',\n",
       " 'us',\n",
       " 'together',\n",
       " ',',\n",
       " 'community',\n",
       " 'interests',\n",
       " 'science',\n",
       " 'kept',\n",
       " 'us',\n",
       " '.',\n",
       " 'brought',\n",
       " 'back',\n",
       " 'much',\n",
       " 'scientific',\n",
       " 'information',\n",
       " 'south',\n",
       " 'africa',\n",
       " ',',\n",
       " 'many',\n",
       " 'charming',\n",
       " 'evening',\n",
       " 'spent',\n",
       " 'together',\n",
       " 'discussing',\n",
       " 'comparative',\n",
       " 'anatomy',\n",
       " 'bushman',\n",
       " 'hottentot',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694b05e7-d1c8-4a23-883e-63853163f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moor', 'spars', 'inhabit', ',', 'live', 'near', 'thrown', 'much', 'togeth', '.', 'reason', 'saw', 'good', 'deal', 'sir', 'charl', 'baskervil', '.', 'except', 'mr', '.', 'frankland', ',', 'lafter', 'hall', ',', 'mr', '.', 'stapleton', ',', 'naturalist', ',', 'men', 'educ', 'within', 'mani', 'mile', '.', 'sir', 'charl', 'retir', 'man', ',', 'chanc', 'ill', 'brought', 'us', 'togeth', ',', 'commun', 'interest', 'scienc', 'kept', 'us', '.', 'brought', 'back', 'much', 'scientif', 'inform', 'south', 'africa', ',', 'mani', 'charm', 'even', 'spent', 'togeth', 'discuss', 'compar', 'anatomi', 'bushman', 'hottentot', '.']\n"
     ]
    }
   ],
   "source": [
    "# Perform stemming on the filtered tokens\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b20c427-e426-44c3-98a3-635f4b1c3529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fiction': tensor([1., 0., 0., 0., 0.]),\n",
       " 'Non-fiction': tensor([0., 1., 0., 0., 0.]),\n",
       " 'Biography': tensor([0., 0., 1., 0., 0.]),\n",
       " 'Children': tensor([0., 0., 0., 1., 0.]),\n",
       " 'Mystery': tensor([0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(genres)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "\n",
    "# Create a dictionary mapping genres to their one-hot vectors\n",
    "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
    "\n",
    "one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaccda2-61ba-40fa-b4a0-41778ec2b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction: [1. 0. 0. 0. 0.]\n",
      "Non-fiction: [0. 1. 0. 0. 0.]\n",
      "Biography: [0. 0. 1. 0. 0.]\n",
      "Children: [0. 0. 0. 1. 0.]\n",
      "Mystery: [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "for genre, vector in one_hot_dict.items():\n",
    "    print(f'{genre}: {vector.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7997b88d-cfa0-4b48-a313-09ed4a050ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
    "\n",
    "# Initialize Bag-of-words with the list of book titles\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
    "\n",
    "bow_encoded_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64da5efb-52b8-4a96-963b-87b599c9814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract and print the first five features\n",
    "# print(vectorizer.get_feature_names_out()[:5])\n",
    "# print(bow_encoded_titles.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76a31454-eb7c-41f1-86ae-de8517e9a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1984' 'catcher' 'expectations' 'gatsby' 'great' 'hobbit' 'in' 'kill'\n",
      " 'mockingbird' 'rye' 'the' 'to']\n",
      "[[0 0 0 1 1 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 1 2 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# features and occurrencies\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow_encoded_titles.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93edcf01-6ef9-4b0d-89d5-89bdbce2db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = ['A portrait of the Jazz Age in all of its decadence and excess.',\n",
    " 'A gripping, heart-wrenching, and wholly remarkable tale of coming-of-age in a South poisoned by virulent prejudice.',\n",
    " 'A startling and haunting vision of the world.',\n",
    " 'A story of lost innocence.',\n",
    " 'A timeless adventure story.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80d77b5-4564-4a9a-a6dd-cbc530e60938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x32 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 41 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing TF-IDF from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF encoding vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded_descriptions = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "tfidf_encoded_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51848571-5abf-47b2-81e9-b1726889ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract and print the first five features\n",
    "# print(vectorizer.get_feature_names_out()[:5])\n",
    "# print(tfidf_encoded_descriptions.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b2bbd6f-a299-4c24-aeae-a9ca0e209b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure' 'age' 'all' 'and' 'by' 'coming' 'decadence' 'excess'\n",
      " 'gripping' 'haunting' 'heart' 'in' 'innocence' 'its' 'jazz' 'lost' 'of'\n",
      " 'poisoned' 'portrait' 'prejudice' 'remarkable' 'south' 'startling'\n",
      " 'story' 'tale' 'the' 'timeless' 'virulent' 'vision' 'wholly' 'world'\n",
      " 'wrenching']\n",
      "[[0.         0.25943581 0.321564   0.21535516 0.         0.\n",
      "  0.321564   0.321564   0.         0.         0.         0.25943581\n",
      "  0.         0.321564   0.321564   0.         0.36232709 0.\n",
      "  0.321564   0.         0.         0.         0.         0.\n",
      "  0.         0.25943581 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.20817488 0.         0.17280396 0.2580274  0.2580274\n",
      "  0.         0.         0.2580274  0.         0.2580274  0.20817488\n",
      "  0.         0.         0.         0.         0.29073627 0.2580274\n",
      "  0.         0.2580274  0.2580274  0.2580274  0.         0.\n",
      "  0.2580274  0.         0.         0.2580274  0.         0.2580274\n",
      "  0.         0.2580274 ]\n",
      " [0.         0.         0.         0.28774996 0.         0.\n",
      "  0.         0.         0.         0.42966246 0.         0.\n",
      "  0.         0.         0.         0.         0.24206433 0.\n",
      "  0.         0.         0.         0.         0.42966246 0.\n",
      "  0.         0.34664897 0.         0.         0.42966246 0.\n",
      "  0.42966246 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.58042343 0.         0.         0.58042343 0.32700044 0.\n",
      "  0.         0.         0.         0.         0.         0.46828197\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.61418897 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.49552379\n",
      "  0.         0.         0.61418897 0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(tfidf_encoded_descriptions.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6641b9b4-3aac-47c5-8865-da0f2092aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc86eeeb-be89-4952-a742-ba024ad84984",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r', encoding='utf-8') as file:\n",
    "    shakespeare = file.read().split('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca533d5-61dc-40dd-8248-92bfbd8fd62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever',\n",
       " ' You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww',\n",
       " 'gutenberg',\n",
       " 'org',\n",
       " ' If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "253447bb-a0c5-4de1-9834-c52ea2ade990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project gutenberg ebook complet work william shakespear , william shakespear ebook use anyon anywher unit state part world cost almost restrict whatsoev', 'may copi , give away re-us term project gutenberg licens includ ebook onlin www', 'gutenberg', 'org', 'locat unit state , check law countri locat use ebook']\n"
     ]
    }
   ],
   "source": [
    "processed_shakespeare = preprocess_sentences(shakespeare)\n",
    "print(processed_shakespeare[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ceff7d9-fc73-4de4-bb85-f58d273b23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define your Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    # it is mandatory to define these three methods when you extend the Dataset class in PyTorch.\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a1e0ea3-b85e-4044-874e-5c8e040a0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold to remove all tokens that appear less than or equal to threshold times in the sentence\n",
    "\n",
    "filter_threshod = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b2d36a-f98e-4da0-a1d7-c0d0718d40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        threshold = filter_threshod\n",
    "        tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12298595-3718-4cf3-817b-cd716fe26dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdfc4dea-1ee6-4dd0-bf4a-6cf7af3ce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(data):\n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "921164d7-cf60-4588-b025-585e4e765b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dff82d6a-1835-426e-8995-d1ab66dc1c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_benedictu' '_ergo_' '_hic' '_le' '_molli' '_mulier_' '_solus_' '_the'\n",
      " '_to' 'aaron' 'abat' 'abbot' 'abhor' 'abhorson' 'abl' 'aboard' 'abu'\n",
      " 'access' 'accommod' 'accost' 'account' 'accu' 'accur' 'achil' 'acquaint'\n",
      " 'act' 'action' 'actor' 'ad' 'adam' 'adder' 'adieu' 'admir' 'adoni'\n",
      " 'adramadio' 'adulteri' 'advanc' 'advantag' 'advic' 'aemiliu' 'aenea'\n",
      " 'aer_' 'afterward' 'agamemnon' 'age' 'agreement' 'agrippa' 'ah' 'aid'\n",
      " 'aim' 'air' 'ajax' 'ala' 'alack' 'alban' 'alcibiad' 'alençon' 'alexa'\n",
      " 'alexand' 'alia' 'alik' 'aliv' 'all' 'allegi' 'allon' 'allow' 'almost'\n",
      " 'alon' 'along' 'alow' 'altar' 'alter' 'although' 'altogeth' 'amaz'\n",
      " 'amber' 'ambiti' 'amen' 'amend' 'amiss' 'amiti' 'among' 'amurath'\n",
      " 'anchor' 'ancient' 'andrew' 'andronicu' 'angel' 'angelo' 'angl' 'angri'\n",
      " 'ann' 'anon' 'anoth' 'another' 'answer' 'antigonu' 'antipholu' 'antoni'\n",
      " 'antonio']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names and the first 100 components of the first item\n",
    "print(vectorizer.get_feature_names_out()[:100]) \n",
    "next(iter(dataloader))[0, :100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1e85600-bc20-4221-9dca-a87b8a1ff5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 0,\n",
       " 'book': 1,\n",
       " 'was': 12,\n",
       " 'fantastic': 3,\n",
       " 'I': 4,\n",
       " 'really': 5,\n",
       " 'love': 6,\n",
       " 'science': 7,\n",
       " 'fiction': 8,\n",
       " 'but': 9,\n",
       " 'the': 10,\n",
       " 'protagonist': 11,\n",
       " 'rude': 13,\n",
       " 'sometimes': 14}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map a unique index to each word\n",
    "words = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1552e8ec-5228-4ea4-87d4-cbbd6e4d4bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1, 12,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert word_to_idx to a tensor\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42269666-1b30-4517-b8e8-de6c78dbf7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9155e-01, -4.2846e-02,  7.4626e-01, -7.2732e-01,  4.1755e-01,\n",
      "          5.8797e-01,  2.9430e-01,  1.0333e+00, -3.7366e-02, -3.5645e-01],\n",
      "        [ 4.4465e-04,  2.6341e+00, -6.0215e-01, -6.1211e-01, -1.1978e+00,\n",
      "         -1.1391e+00, -1.6949e+00,  8.8291e-01,  1.6486e+00, -8.7134e-01],\n",
      "        [-3.5535e-01, -6.2705e-01, -1.0797e+00,  1.1430e+00,  1.0941e+00,\n",
      "         -8.2658e-01, -1.0178e-01, -5.2812e-01,  2.9619e-01, -1.5035e+00],\n",
      "        [ 1.2312e+00, -9.7742e-01, -1.3229e-01,  7.1796e-01,  1.7844e+00,\n",
      "         -1.0645e+00,  7.5272e-01,  4.2596e-01, -7.8369e-01,  3.3566e-01],\n",
      "        [ 1.5683e+00,  1.6907e-02,  1.3010e+00, -1.8566e+00, -4.3566e-01,\n",
      "          2.0464e-01, -3.7057e-01,  1.3641e+00, -3.5580e-01, -1.4706e+00],\n",
      "        [ 2.8564e-01, -4.7004e-01, -1.5656e+00, -5.8641e-01, -7.5532e-01,\n",
      "          1.2489e+00, -1.1991e+00,  5.0140e-01,  2.3359e-01, -1.3675e-01],\n",
      "        [-1.1918e+00, -1.7489e+00, -1.5809e+00, -2.8796e-01,  5.5131e-01,\n",
      "          1.3875e-01,  1.0814e+00, -4.4488e-01,  1.0245e+00, -4.5666e-01],\n",
      "        [-9.0428e-01, -7.8774e-01, -7.8573e-01,  1.4671e+00, -1.3468e-01,\n",
      "         -1.0297e+00,  3.8306e-01, -2.2191e+00, -3.0697e-01, -1.5979e+00],\n",
      "        [-1.2893e+00, -1.0576e+00, -4.0237e-01,  2.3374e-01, -6.2903e-01,\n",
      "         -7.7567e-01,  1.4894e+00, -2.3016e+00,  4.0303e-01,  1.8483e-01],\n",
      "        [ 1.1435e+00, -2.9948e-01,  5.9382e-01, -2.2389e-01,  1.1507e+00,\n",
      "         -9.9329e-01,  6.5336e-02,  7.0169e-01,  1.2503e+00,  4.7531e-03],\n",
      "        [-1.7011e-01,  8.3241e-01,  2.7311e-01, -4.7342e-01,  7.5508e-01,\n",
      "         -1.0500e+00,  3.1610e-01, -1.5558e+00,  2.8832e-01, -1.3373e-01],\n",
      "        [-5.7208e-01,  1.4855e+00, -2.2442e+00, -4.3859e-01,  1.1474e-01,\n",
      "         -1.6443e+00, -3.9267e-02,  8.1205e-01,  1.7862e+00, -4.0635e-01],\n",
      "        [-3.5535e-01, -6.2705e-01, -1.0797e+00,  1.1430e+00,  1.0941e+00,\n",
      "         -8.2658e-01, -1.0178e-01, -5.2812e-01,  2.9619e-01, -1.5035e+00],\n",
      "        [-9.0759e-01, -9.7822e-02,  1.3345e+00, -6.7140e-01, -1.3163e-01,\n",
      "         -1.6573e+00,  8.7071e-02, -5.5149e-01,  1.7144e+00, -1.5952e+00],\n",
      "        [-1.6285e+00,  1.7361e+00, -1.0140e+00,  3.5958e-01, -8.2373e-01,\n",
      "          7.2726e-01,  5.0856e-01,  4.9263e-02,  9.5102e-01,  9.4558e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the embedding layer with the number of embeddings and the size of each embedding\n",
    "embedding = nn.Embedding(15, 10) # creates an embedding of 10 values (embedding dimension) for each of the 15 words (number of embeddings)\n",
    "\n",
    "# Pass the tensor to the embedding layer \n",
    "output = embedding(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27b130f4-d395-4d33-8f16-9d9291dfc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data labeled positively 1 or negatively 0\n",
    "\n",
    "data = [(['I', 'love', 'this', 'first','book'], 1),\n",
    "     (['This', 'is', 'an', 'amazing', 'novel'], 1),\n",
    "     (['I', 'really', 'like', 'this', 'story'], 1),\n",
    "     (['I', 'do', 'not', 'like', 'this', 'book'], 0),\n",
    "     (['I', 'hate', 'this', 'novel'], 0),\n",
    "     (['This', 'is', 'a', 'terrible', 'story'], 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53c79a6f-fd2a-4a3c-a80f-c0c9cc1211f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " ['love',\n",
       "  'story',\n",
       "  'I',\n",
       "  'really',\n",
       "  'this',\n",
       "  'amazing',\n",
       "  'like',\n",
       "  'is',\n",
       "  'an',\n",
       "  'hate',\n",
       "  'terrible',\n",
       "  'not',\n",
       "  'do',\n",
       "  'a',\n",
       "  'novel',\n",
       "  'first',\n",
       "  'book',\n",
       "  'This'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_words = []\n",
    "\n",
    "# for sub_list, _ in data:\n",
    "#     for word in sub_list:\n",
    "#         unique_words.append(word)\n",
    "\n",
    "# list(set(unique_words))\n",
    "\n",
    "unique_words = list(set(word for sub_list, _ in data for word in sub_list))\n",
    "\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "vocab_size, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83ca5824-7f8a-47fe-9502-05e47a2d3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassificationCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextClassificationCNN, self).__init__() # initializes the base class nn.Module\n",
    "        # Initialize the embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # Transforms input words into dense vectors of a fixed size (embed_dim).\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1) # Applies a 1D convolution over the embedding vectors to detect patterns.\n",
    "        self.fc = nn.Linear(embed_dim, 2) # binary classification\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1) # reshaping the tensor to match the expected input format of the convolutional layer, inverting position 2 with position 1, changing from [batch_size, sequence_length, embed_dim] to [batch_size, embed_dim, sequence_length].\n",
    "        # Pass the embedded text through the convolutional layer and apply a ReLU\n",
    "        conved = F.relu(self.conv(embedded)) # Passes the embeddings through the convolution layer and applies the ReLU activation function to extract features.\n",
    "        conved = conved.mean(dim=2) # Mean Pooling: Reduces the dimensions by averaging across one dimension.\n",
    "        return self.fc(conved) # Outputs the logits for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c66d883c-5d6d-406a-8ed3-872166a314ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(18, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embed_dim = 10\n",
    "\n",
    "# Initialize embedding layer with input and output dimensions\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c445f89-ce89-43c1-ade5-c7cc4edc1b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0,\n",
       " 'story': 1,\n",
       " 'I': 2,\n",
       " 'really': 3,\n",
       " 'this': 4,\n",
       " 'amazing': 5,\n",
       " 'like': 6,\n",
       " 'is': 7,\n",
       " 'an': 8,\n",
       " 'hate': 9,\n",
       " 'terrible': 10,\n",
       " 'not': 11,\n",
       " 'do': 12,\n",
       " 'a': 13,\n",
       " 'novel': 14,\n",
       " 'first': 15,\n",
       " 'book': 16,\n",
       " 'This': 17}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfbb42a6-27fb-440f-850a-67c28367a520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  0,  4, 15, 16, 17,  7,  8,  5, 14,  2,  3,  6,  4,  1,  2, 12, 11,\n",
       "          6,  4, 16,  2,  9,  4, 14, 17,  7, 13, 10,  1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each word w in the sentence is replaced by its index in word_to_ix. If a word is not found in word_to_ix, the get method returns 0 as a default value. The result is a list of indices that represent the sentence (applied later in the model training)\n",
    "\n",
    "torch.LongTensor([word_to_ix.get(w, 0) for sentence, _ in data for w in sentence]).unsqueeze(0) # gets the indexed in the word_to_ix vocabulary of the words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96dc21d2-659e-48df-83e8-eb9ff325200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationCNN(\n",
       "  (embedding): Embedding(18, 10)\n",
       "  (conv): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextClassificationCNN(vocab_size, embed_dim)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c75deaa6-48e6-40b8-b0e0-7f83df5a6ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.3550,  0.2004]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.3968,  0.0740]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.3561,  0.3760]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[-0.4681,  0.3257]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[-0.2896,  0.1875]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[-0.1506, -0.0615]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.2319,  0.0779]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.3221, -0.0128]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.2911,  0.3110]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[-0.3139,  0.1654]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[-0.1079,  0.0210]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[-0.0492, -0.1752]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.2107,  0.0485]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.3143, -0.0290]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.3075,  0.3279]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[-0.2246,  0.0758]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 0.0286, -0.1006]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.0279, -0.2620]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.2378,  0.0686]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.3633,  0.0054]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.3572,  0.3965]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[-0.1515,  0.0008]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 0.1623, -0.2205]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.1035, -0.3429]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.3009,  0.1211]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.4437,  0.0770]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.4437,  0.4996]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[-0.0735, -0.0752]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 0.3068, -0.3483]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.1905, -0.4311]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.3936,  0.2107]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.5580,  0.1842]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.5355,  0.6236]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[ 0.0083, -0.1462]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 0.4688, -0.4930]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.2875, -0.5214]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.5042,  0.3222]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.6880,  0.3075]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.6612,  0.7570]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[ 0.1154, -0.2464]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 0.6493, -0.6464]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.4008, -0.6332]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.6312,  0.4594]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.8057,  0.4331]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.7923,  0.9046]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[ 0.2368, -0.3514]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 0.8321, -0.8100]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.5047, -0.7252]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.7484,  0.5864]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-0.9276,  0.5603]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-0.9169,  1.0480]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[ 0.3722, -0.4648]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 1.0167, -0.9784]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.6341, -0.8456]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  0,  4, 15, 16]]) tensor([[-0.8554,  0.7025]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[17,  7,  8,  5, 14]]) tensor([[-1.0407,  0.6773]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[2, 3, 6, 4, 1]]) tensor([[-1.0280,  1.1803]], grad_fn=<AddmmBackward0>) tensor([1])\n",
      "tensor([[ 2, 12, 11,  6,  4, 16]]) tensor([[ 0.5060, -0.5832]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[ 2,  9,  4, 14]]) tensor([[ 1.1692, -1.1135]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "tensor([[17,  7, 13, 10,  1]]) tensor([[ 0.7518, -0.9491]], grad_fn=<AddmmBackward0>) tensor([0])\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for sentence, label in data:     \n",
    "        # Clear the gradients\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_ix.get(w, 0) for w in sentence]).unsqueeze(0) # provides the indexed in the word_to_ix vocabulary of the sentences in data (sentences encoding). Necessary before applying the self.embedding to the text in the model\n",
    "        label = torch.LongTensor([int(label)])\n",
    "        outputs = model(sentence)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        # print(loss)\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        print(sentence, outputs, label) # just last one (outputs are logits)\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f12a28f3-a342-452c-bdbd-868a10eca956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'this', 'book'], ['I', 'do', 'not', 'like', 'this', 'book']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_reviews = [\n",
    "    \"I love this book\".split(),\n",
    "    \"I do not like this book\".split()\n",
    "]\n",
    "\n",
    "book_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "979a7d85-d163-465d-95b8-d91c97516119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor tensor([[ 2,  0,  4, 16]])\n",
      "outputs tensor([[-0.9109,  0.9031]], grad_fn=<AddmmBackward0>)\n",
      "predicted_label tensor([1])\n",
      "Book Review: I love this book\n",
      "Sentiment: Positive\n",
      "\n",
      "input_tensor tensor([[ 2, 12, 11,  6,  4, 16]])\n",
      "outputs tensor([[ 0.7609, -0.8040]], grad_fn=<AddmmBackward0>)\n",
      "predicted_label tensor([0])\n",
      "Book Review: I do not like this book\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for review in book_reviews:\n",
    "    # Convert the review words into tensor form\n",
    "    input_tensor = torch.tensor([word_to_ix[w] for w in review], dtype=torch.long).unsqueeze(0) \n",
    "    print('input_tensor',input_tensor)\n",
    "    \n",
    "    # Get the model's output\n",
    "    outputs = model(input_tensor)\n",
    "    print('outputs',outputs)\n",
    "    # Find the index of the most likely sentiment category\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    print('predicted_label',predicted_label)\n",
    "    \n",
    "    # Convert the predicted label into a sentiment string\n",
    "    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n",
    "    print(f\"Book Review: {' '.join(review)}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16287c11-e6fc-42af-82f4-a5f99fd9c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the RNN class\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size # Sets the size of the hidden state in the RNN.\n",
    "        self.num_layers = num_layers # Sets the number of stacked RNN layers.\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # When batch_first=True is set in an RNN, it means that the input and output tensors will have the shape [batch_size, sequence_length, input_size]. By default, PyTorch expects the input shape to be [sequence_length, batch_size, input_size]\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) # Initializes the hidden state with zeros. Its shape is [num_layers, batch_size, hidden_size]. If x has a shape of [batch_size, sequence_length, input_size], then: x.size(0) returns the batch_size, which is the number of sequences in the batch.\n",
    "        out, _ = self.rnn(x, h0) # The RNN outputs out (all hidden states for all time steps) and _ (the final hidden state, which isn't used here).\n",
    "        out = out[:, -1, :] # Selects the hidden state at the last time step for each sequence with out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfff31ba-bde6-41c0-8ee9-401906d52122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(6, 32, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 6\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef9d9fc1-07e6-454b-98df-eb1bfb17b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 6])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq = torch.tensor([[[0., 0., 0., 1., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.]]])\n",
    "\n",
    "X_train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81b3c662-912e-444e-a7d7-970c0e6d08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq = torch.tensor([2, 2, 2, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a5dbded-eea6-458a-9262-bc9a9a100c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.1248911619186401\n",
      "Epoch: 2, Loss: 0.973309338092804\n",
      "Epoch: 3, Loss: 0.8396531939506531\n",
      "Epoch: 4, Loss: 0.7184687256813049\n",
      "Epoch: 5, Loss: 0.6190042495727539\n",
      "Epoch: 6, Loss: 0.5575698614120483\n",
      "Epoch: 7, Loss: 0.5377610921859741\n",
      "Epoch: 8, Loss: 0.5386293530464172\n",
      "Epoch: 9, Loss: 0.5369535684585571\n",
      "Epoch: 10, Loss: 0.5235108137130737\n"
     ]
    }
   ],
   "source": [
    "# Train the model for ten epochs and zero the gradients\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "799d5d11-ba8f-4d0f-8480-8c78f665c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LSTM and the output layer with parameters\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c030fbe5-9807-405a-8e65-4999d5d05124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with required parameters\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2e3e07a-e19b-4c47-a8cc-07dfb3566da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.0723886489868164\n",
      "Epoch: 2, Loss: 1.0383002758026123\n",
      "Epoch: 3, Loss: 1.0030786991119385\n",
      "Epoch: 4, Loss: 0.9654237031936646\n",
      "Epoch: 5, Loss: 0.9242860674858093\n",
      "Epoch: 6, Loss: 0.8789087533950806\n",
      "Epoch: 7, Loss: 0.8290559649467468\n",
      "Epoch: 8, Loss: 0.7754516005516052\n",
      "Epoch: 9, Loss: 0.7201984524726868\n",
      "Epoch: 10, Loss: 0.6669596433639526\n"
     ]
    }
   ],
   "source": [
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f8a5037-4879-4d8e-8d35-a160d6ae3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db744b5e-43d8-4a55-9939-493672f69198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "008670bd-d9eb-4f4f-aca9-93ba1ab62ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.0797009468078613\n",
      "Epoch: 2, Loss: 1.0256575345993042\n",
      "Epoch: 3, Loss: 0.9725996255874634\n",
      "Epoch: 4, Loss: 0.9161521792411804\n",
      "Epoch: 5, Loss: 0.8538802862167358\n",
      "Epoch: 6, Loss: 0.7856529951095581\n",
      "Epoch: 7, Loss: 0.7140219807624817\n",
      "Epoch: 8, Loss: 0.644810140132904\n",
      "Epoch: 9, Loss: 0.586836040019989\n",
      "Epoch: 10, Loss: 0.549041748046875\n",
      "Epoch: 11, Loss: 0.5341728329658508\n",
      "Epoch: 12, Loss: 0.5343982577323914\n",
      "Epoch: 13, Loss: 0.5372315049171448\n",
      "Epoch: 14, Loss: 0.534077525138855\n",
      "Epoch: 15, Loss: 0.5223044157028198\n"
     ]
    }
   ],
   "source": [
    "# Train the model and backpropagate the loss after initialization\n",
    "for epoch in range(15): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = gru_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ef89b42-9ec1-4e48-ab8e-73641fa83601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "\n",
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c53487a1-52d6-4b34-bb0d-231fb5599e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_seq = torch.tensor([[[0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[1., 0., 0., 1., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.]]])\n",
    "\n",
    "X_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c984085-aa49-461b-a9f2-52a52abf6ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2006, -2.3813,  1.7712],\n",
       "         [-0.4732, -2.4382,  1.9004],\n",
       "         [-0.2006, -2.3813,  1.7712],\n",
       "         [-0.2006, -2.3813,  1.7712],\n",
       "         [-0.4813, -2.6254,  2.0863],\n",
       "         [-0.2006, -2.3813,  1.7712]], grad_fn=<AddmmBackward0>),\n",
       " tensor([2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the predictions\n",
    "outputs = rnn_model(X_test_seq)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "outputs,predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14ff76fc-39ca-4b7f-8421-2c68cfe23549",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_seq = torch.tensor([2, 1, 0, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "308c20eb-5c64-4b00-af6c-6d7b66178fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Model - Accuracy: 0.5, Precision: 0.5, Recall: 0.5, F1 Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate the metrics\n",
    "accuracy_score = accuracy(predicted, y_test_seq)\n",
    "precision_score = precision(predicted, y_test_seq)\n",
    "recall_score = recall(predicted, y_test_seq)\n",
    "f1_score = f1(predicted, y_test_seq)\n",
    "print(\"RNN Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_score, precision_score, recall_score, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0295684b-7b20-4136-a265-fd21a39e1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = lstm_model(X_test_seq)\n",
    "_, y_pred_lstm = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac3f4f58-4fc6-4209-98c6-eb877ad8fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model - Accuracy: 0.5, Precision: 0.5, Recall: 0.5, F1 Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate metrics for the LSTM model\n",
    "accuracy_1 = accuracy(y_pred_lstm, y_test_seq)\n",
    "precision_1 = precision(y_pred_lstm, y_test_seq)\n",
    "recall_1 = recall(y_pred_lstm, y_test_seq)\n",
    "f1_1 = f1(y_pred_lstm, y_test_seq)\n",
    "print(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b750b620-d3ac-4d9c-ad2c-51e538c92a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gru_model(X_test_seq)\n",
    "_, y_pred_gru = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9804be32-8f0c-4e04-b1fe-03cea76749c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Model - Accuracy: 0.5, Precision: 0.5, Recall: 0.5, F1 Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for the GRU model\n",
    "accuracy_2 = accuracy(y_pred_gru, y_test_seq)\n",
    "precision_2 = precision(y_pred_gru, y_test_seq)\n",
    "recall_2 = recall(y_pred_gru, y_test_seq)\n",
    "f1_2 = f1(y_pred_gru, y_test_seq)\n",
    "print(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30495648-c485-48fc-8e10-a5cefe97661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27e88252-5416-4d56-b440-97538c309082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include an RNN layer and linear layer in RNNmodel class\n",
    "\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])  # out[:, -1, :] means “for each sequence in the batch, take the last hidden state” (In the context of RNNs, the last hidden state often contains information about the entire sequence because it has seen all the previous inputs in the sequence. This is why it’s commonly used for sequence classification tasks.)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78c418c8-6ac3-4b5b-aa30-9aa231a1fe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = ['y', ',','b','w','a','d','e','p','o','k','n','v','s','T','l','m','.','g','c','h','u','f','-','A',' ','r','t','i']\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df1d907c-51af-4efd-92e1-a98afe7cfc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNmodel(\n",
       "  (rnn): RNN(28, 16, batch_first=True)\n",
       "  (fc): Linear(in_features=16, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the RNN model\n",
    "model = RNNmodel(len(chars), 16, len(chars))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae5c3c8e-0387-415e-9118-80095113e68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " ',': 1,\n",
       " 'b': 2,\n",
       " 'w': 3,\n",
       " 'a': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'p': 7,\n",
       " 'o': 8,\n",
       " 'k': 9,\n",
       " 'n': 10,\n",
       " 'v': 11,\n",
       " 's': 12,\n",
       " 'T': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " '.': 16,\n",
       " 'g': 17,\n",
       " 'c': 18,\n",
       " 'h': 19,\n",
       " 'u': 20,\n",
       " 'f': 21,\n",
       " '-': 22,\n",
       " 'A': 23,\n",
       " ' ': 24,\n",
       " 'r': 25,\n",
       " 't': 26,\n",
       " 'i': 27}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = {chars[i]:i for i in range(len(chars))}\n",
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96d845a0-4142-450d-a14f-1f51901a1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9ed715e-4ac3-40cb-83c8-b1a84af8cf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b3388dc-9b89-4330-bf57-0dcedd7e1edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([218, 1, 28])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape # batch_size is 218, sequence_length is 1, and number_of_features (characters) is 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "897b1113-97e4-4fb2-bedf-e683c7b590a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([218])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.tensor([19,  6, 24, 25,  4,  2,  2, 27, 26, 22, 19,  8, 14,  6, 24,  3,  6, 10,\n",
    "        26, 24, 12, 26, 25,  4, 27, 17, 19, 26, 24,  8, 10, 24, 14, 27,  9,  6,\n",
    "        24,  4, 24, 26, 20, 10, 10,  6, 14, 24, 21,  8, 25, 24, 12,  8, 15,  6,\n",
    "        24,  3,  4,  0,  1, 24,  4, 10,  5, 24, 26, 19,  6, 10, 24,  5, 27,  7,\n",
    "         7,  6,  5, 24, 12, 20,  5,  5,  6, 10, 14,  0, 24,  5,  8,  3, 10,  1,\n",
    "        24, 12,  8, 24, 12, 20,  5,  5,  6, 10, 14,  0, 24, 26, 19,  4, 26, 24,\n",
    "        23, 14, 27, 18,  6, 24, 19,  4,  5, 24, 10,  8, 26, 24,  4, 24, 15,  8,\n",
    "        15,  6, 10, 26, 24, 26,  8, 24, 26, 19, 27, 10,  9, 24,  4,  2,  8, 20,\n",
    "        26, 24, 12, 26,  8,  7,  7, 27, 10, 17, 24, 19,  6, 25, 12,  6, 14, 21,\n",
    "        24,  2,  6, 21,  8, 25,  6, 24, 12, 19,  6, 24, 21,  8, 20, 10,  5, 24,\n",
    "        19,  6, 25, 12,  6, 14, 21, 24, 21,  4, 14, 14, 27, 10, 17, 24,  5,  8,\n",
    "         3, 10, 24,  4, 24, 11,  6, 25,  0, 24,  5,  6,  6,  7, 24,  3,  6, 14,\n",
    "        14, 16])\n",
    "\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd6d4021-a7c5-468c-bece-2398b89f9fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 2.976134777069092\n",
      "Epoch 20/100, Loss: 2.677084445953369\n",
      "Epoch 30/100, Loss: 2.4725914001464844\n",
      "Epoch 40/100, Loss: 2.2726950645446777\n",
      "Epoch 50/100, Loss: 2.1078569889068604\n",
      "Epoch 60/100, Loss: 1.9847338199615479\n",
      "Epoch 70/100, Loss: 1.9022361040115356\n",
      "Epoch 80/100, Loss: 1.8479633331298828\n",
      "Epoch 90/100, Loss: 1.8115605115890503\n",
      "Epoch 100/100, Loss: 1.7871347665786743\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9a95e71-965c-4243-8bcc-1357d2f7734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([218, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8395, -0.5993,  0.9741,  ...,  1.5718, -0.7799, -0.6828],\n",
       "        [-1.5890, -3.1118, -0.4486,  ..., -1.5477,  2.6892,  2.7753],\n",
       "        [-1.8929, -2.5851, -3.3455,  ...,  3.0360,  0.2332, -1.0040],\n",
       "        ...,\n",
       "        [-1.8929, -2.5851, -3.3455,  ...,  3.0360,  0.2332, -1.0040],\n",
       "        [ 3.0976, -1.5643, -0.1192,  ..., -0.1872, -1.4737,  3.5292],\n",
       "        [ 3.0976, -1.5643, -0.1192,  ..., -0.1872, -1.4737,  3.5292]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.shape) # outputs the logits of all possible characters\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50bc81c6-125c-498e-8a00-bf328b49e75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'y',\n",
       " 1: ',',\n",
       " 2: 'b',\n",
       " 3: 'w',\n",
       " 4: 'a',\n",
       " 5: 'd',\n",
       " 6: 'e',\n",
       " 7: 'p',\n",
       " 8: 'o',\n",
       " 9: 'k',\n",
       " 10: 'n',\n",
       " 11: 'v',\n",
       " 12: 's',\n",
       " 13: 'T',\n",
       " 14: 'l',\n",
       " 15: 'm',\n",
       " 16: '.',\n",
       " 17: 'g',\n",
       " 18: 'c',\n",
       " 19: 'h',\n",
       " 20: 'u',\n",
       " 21: 'f',\n",
       " 22: '-',\n",
       " 23: 'A',\n",
       " 24: ' ',\n",
       " 25: 'r',\n",
       " 26: 't',\n",
       " 27: 'i'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char = {i:chars[i] for i in range(len(chars))}\n",
    "ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "422a15e3-8e2c-40fe-a318-974eff036834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "test_input = char_to_ix['r'] # 25\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
    "print(test_input.shape)\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0cf6c12-356f-409f-a5d7-b7a6978c3d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: 'r', Predicted Output: 'a'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.4213e+00, -1.5242e+00, -4.7428e-02, -1.0359e+00,  3.3683e+00,\n",
       "           2.7207e-01,  1.3397e+00, -2.2774e+00, -1.5252e-03, -3.5113e+00,\n",
       "           3.7883e-01, -1.6907e+00,  3.0331e+00, -2.5844e+00, -5.7443e-01,\n",
       "          -2.2052e+00, -1.1477e+00, -3.6155e+00, -2.9645e+00, -1.0199e+00,\n",
       "          -2.8242e+00,  4.3883e-01, -1.2937e+00, -1.3535e+00,  2.6196e+00,\n",
       "           1.5258e-01,  7.7540e-02,  6.4389e-01]], grad_fn=<AddmmBackward0>),\n",
       " 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "print(f\"Test Input: 'r', Predicted Output: '{ix_to_char[predicted_char_ix]}'\")\n",
    "predicted_output, predicted_char_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1141eb6f-2a8f-4b8f-98f4-ee4ba8133253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length,seq_length), nn.Sigmoid()) # Linear Layer: Takes input of size seq_length and outputs the same size (seq_length serves as both the size of the input noise vector and the size of the output generated data.).Sigmoid Activation: Squashes the output values to be between 0 and 1, that is particularly useful when the generated data needs to be in a specific range, such as pixel values for images (which range from 0 to 1)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the discriminator networks\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length, 1), nn.Sigmoid()) # Linear Layer: Takes input of size seq_length and outputs a single value. Sigmoid Activation: Outputs a value between 0 and 1, representing the probability that the input is real.\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9f3fccd-bbc5-460b-a236-2dbd7427aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 5 #: Length of each synthetic data sequence\n",
    "num_sequences = 100 #: Total number of sequences generated\n",
    "num_epochs = 50 #: Number of complete passes through the dataset\n",
    "print_every = 10 #: Output display frequency, showing results every 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3255c219-b8ad-4e31-a3b4-af1bbe82c1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=5, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d18df4e2-9b9d-466f-ba32-e5ca86ed9514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eb49d87-e810-49ae-a3a9-743b47e1a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Since Generator and discriminator inherit from nn.Module, they also inherits the parameters() method.\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "77d62c63-3aae-43e9-8cae-bb8053f24661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[0., 1., 0., 0., 1.],\n",
    "                    [0., 0., 1., 0., 1.],\n",
    "                    [0., 0., 1., 1., 0.],\n",
    "                    [0., 1., 0., 0., 1.],\n",
    "                    [0., 1., 1., 1., 0.],\n",
    "                    [0., 0., 1., 0., 1.],\n",
    "                    [0., 0., 0., 1., 1.],\n",
    "                    [0., 1., 0., 1., 1.],\n",
    "                    [1., 1., 1., 0., 0.],\n",
    "                    [0., 1., 1., 1., 0.],\n",
    "                    [1., 0., 0., 0., 1.],\n",
    "                    [0., 0., 1., 0., 1.],\n",
    "                    [0., 1., 1., 0., 1.],\n",
    "                    [1., 0., 1., 1., 0.],\n",
    "                    [0., 0., 1., 0., 0.],\n",
    "                    [1., 0., 1., 0., 1.],\n",
    "                    [0., 0., 0., 1., 0.],\n",
    "                    [1., 1., 0., 1., 1.],\n",
    "                    [1., 1., 1., 0., 0.],\n",
    "                    [1., 1., 0., 0., 0.],\n",
    "                    [0., 1., 0., 1., 1.],\n",
    "                    [1., 1., 1., 0., 1.],\n",
    "                    [1., 1., 0., 1., 0.],\n",
    "                    [0., 1., 1., 1., 1.],\n",
    "                    [0., 0., 1., 0., 1.],\n",
    "                    [0., 0., 1., 0., 0.],\n",
    "                    [1., 0., 0., 0., 0.],\n",
    "                    [0., 1., 1., 0., 0.],\n",
    "                    [1., 0., 0., 1., 0.],\n",
    "                    [0., 1., 1., 1., 1.],\n",
    "                    [1., 0., 0., 0., 0.],\n",
    "                    [0., 0., 0., 0., 0.],\n",
    "                    [1., 1., 0., 1., 0.],\n",
    "                    [1., 0., 0., 0., 1.],\n",
    "                    [0., 0., 1., 0., 0.],\n",
    "                    [0., 0., 1., 0., 0.],\n",
    "                    [0., 1., 0., 1., 0.],\n",
    "                    [0., 1., 0., 0., 1.],\n",
    "                    [1., 0., 1., 0., 1.],\n",
    "                    [1., 0., 1., 1., 0.],\n",
    "                    [1., 1., 1., 0., 0.],\n",
    "                    [0., 0., 0., 0., 1.],\n",
    "                    [1., 1., 1., 0., 1.],\n",
    "                    [0., 0., 1., 0., 0.],\n",
    "                    [1., 1., 0., 1., 1.],\n",
    "                    [0., 1., 1., 0., 1.],\n",
    "                    [0., 0., 1., 0., 0.],\n",
    "                    [1., 0., 0., 1., 0.],\n",
    "                    [1., 0., 0., 1., 0.],\n",
    "                    [1., 0., 1., 0., 1.],\n",
    "                    [0., 0., 1., 0., 1.],\n",
    "                    [1., 0., 1., 0., 1.],\n",
    "                    [0., 0., 1., 1., 0.],\n",
    "                    [1., 0., 0., 0., 1.],\n",
    "                    [0., 1., 0., 1., 1.],\n",
    "                    [1., 0., 1., 1., 0.],\n",
    "                    [1., 1., 1., 0., 1.],\n",
    "                    [0., 0., 0., 0., 1.],\n",
    "                    [1., 1., 0., 1., 0.],\n",
    "                    [1., 1., 1., 0., 0.],\n",
    "                    [0., 1., 0., 1., 0.],\n",
    "                    [1., 0., 0., 1., 0.],\n",
    "                    [1., 1., 1., 1., 1.],\n",
    "                    [1., 0., 0., 1., 1.],\n",
    "                    [1., 0., 1., 0., 0.],\n",
    "                    [0., 0., 0., 0., 1.],\n",
    "                    [0., 1., 0., 1., 1.],\n",
    "                    [1., 0., 1., 1., 0.],\n",
    "                    [0., 0., 0., 1., 1.],\n",
    "                    [0., 1., 1., 1., 1.],\n",
    "                    [1., 0., 1., 1., 1.],\n",
    "                    [0., 1., 0., 1., 0.],\n",
    "                    [1., 0., 1., 0., 1.],\n",
    "                    [1., 1., 0., 1., 0.],\n",
    "                    [0., 0., 0., 1., 1.],\n",
    "                    [0., 1., 1., 1., 0.],\n",
    "                    [0., 0., 0., 0., 0.],\n",
    "                    [0., 0., 1., 1., 1.],\n",
    "                    [1., 1., 1., 0., 1.],\n",
    "                    [1., 0., 0., 0., 0.],\n",
    "                    [1., 1., 1., 1., 1.],\n",
    "                    [0., 0., 1., 0., 1.],\n",
    "                    [0., 1., 0., 1., 0.],\n",
    "                    [1., 1., 1., 1., 1.],\n",
    "                    [0., 0., 0., 0., 0.],\n",
    "                    [1., 0., 0., 1., 0.],\n",
    "                    [1., 0., 0., 1., 1.],\n",
    "                    [0., 0., 0., 0., 0.],\n",
    "                    [0., 1., 0., 1., 1.],\n",
    "                    [0., 0., 0., 0., 0.],\n",
    "                    [1., 1., 1., 1., 0.],\n",
    "                    [1., 1., 0., 1., 0.],\n",
    "                    [1., 1., 1., 0., 1.],\n",
    "                    [1., 0., 0., 0., 1.],\n",
    "                    [0., 1., 0., 1., 1.],\n",
    "                    [0., 1., 0., 1., 0.],\n",
    "                    [1., 0., 1., 0., 1.],\n",
    "                    [1., 0., 1., 1., 0.],\n",
    "                    [1., 0., 1., 1., 1.],\n",
    "                    [0., 1., 1., 0., 1.]])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9326c6f-8f8f-47c2-9237-15fd9c0977c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.tensor([1., 0., 1., 1., 0.])\n",
    "test_real = test_data.unsqueeze(0)\n",
    "test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f83ea8b-7d9e-4a5b-ad60-031d6ed79d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1546, 0.5740, 0.6748, 0.7829, 0.6753]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982, 0.5545, 0.3936, 0.2869, 0.4244]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_noise = torch.rand((1, seq_length))\n",
    "print(test_noise)\n",
    "test_noise = generator(test_noise)\n",
    "test_noise.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f33a157d-cf6b-4432-8174-7cd1a52ef2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3595]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.4690]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_test_real = discriminator(test_real)\n",
    "disc_test_noise = discriminator(test_noise.detach())\n",
    "disc_test_real, disc_test_noise # a output close to 1 represents a real image, while an output closer to 0 represents a fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d541acc-d2e1-41e6-bcad-b0e3d9fded14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.]]), tensor([[0.]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(disc_test_real), torch.zeros_like(disc_test_noise) # expected outputs (1:real, 0:fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7ae9db1-4f32-461d-9780-ced8a75d1440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50:\t Generator loss: 0.6854485273361206\t Discriminator loss: 1.424321174621582\n",
      "Epoch 20/50:\t Generator loss: 0.6872369050979614\t Discriminator loss: 1.4275445938110352\n",
      "Epoch 30/50:\t Generator loss: 0.6769758462905884\t Discriminator loss: 1.4055067300796509\n",
      "Epoch 40/50:\t Generator loss: 0.6922941207885742\t Discriminator loss: 1.4285491704940796\n",
      "Epoch 50/50:\t Generator loss: 0.6925503611564636\t Discriminator loss: 1.4118452072143555\n"
     ]
    }
   ],
   "source": [
    "# Start of the training process\n",
    "for epoch in range(num_epochs):\n",
    "    # For each epoch, the model goes through the entire dataset\n",
    "    for real_data in data:\n",
    "        # The real data is unsqueezed to add an extra dimension: unsqueeze(0) is used to create a batch with size 1 from the single sample of real_data. This ensures that the data can be correctly processed by the discriminator and generator, which are designed to handle batches of data.\n",
    "        real_data = real_data.unsqueeze(0)\n",
    "        # A random noise vector is generated\n",
    "        noise = torch.rand((1, seq_length))\n",
    "        # This noise vector is then passed through the generator to create the fake data\n",
    "        fake_data = generator(noise)\n",
    "        # The discriminator is then used to classify the real and fake data (outputs '1' if considers the input real, and outputs '0' if considers the input fake: ideally, disc_real should be always classified to 1, and disc_fake should be always classified to 0)\n",
    "        disc_real = discriminator(real_data)\n",
    "        disc_fake = discriminator(fake_data.detach()) # we want to use the output of the generator 'fake_data' to feed into the discriminator but don’t want these operations to influence the gradients of the generator (because here we're training the discriminator, and not the generator)\n",
    "        # The loss for the discriminator is calculated as the sum of the losses for the real and fake data\n",
    "        loss_disc = criterion(disc_real, torch.ones_like(disc_real)) + criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        # The gradients are then backpropagated through the discriminator\n",
    "        optimizer_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        # The discriminator's weights are updated\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # The generator is trained here\n",
    "        disc_fake = discriminator(fake_data) # In this case (when we train the generator), we need to calculate gradients for the generator because we want to update its parameters to minimize this loss. So, we don’t use detach() when passing the fake data through the discriminator.\n",
    "        # The loss for the generator is calculated\n",
    "        loss_gen = criterion(disc_fake, torch.ones_like(disc_fake)) # the generator loss is small if it's able to fool the discriminator (so if the outputs of the discriminator for the fake data is 1)\n",
    "        # The gradients are backpropagated through the generator\n",
    "        optimizer_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        # The generator's weights are updated\n",
    "        optimizer_gen.step()\n",
    "\n",
    "    # The losses for the generator and the discriminator are printed every 'print_every' epochs\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\\t Generator loss: {loss_gen.item()}\\t Discriminator loss: {loss_disc.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1fc0e573-8477-464b-9b89-d71758be98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real data: \n",
      "tensor([[0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [0., 1., 1., 1., 0.]])\n",
      "\n",
      "Generated data: \n",
      "tensor([[0.9740, 0.5491, 0.9054, 0.0567, 0.3241]])\n",
      "tensor([[0., 0., 1., 1., 1.]])\n",
      "tensor([[0.3442, 0.5166, 0.1226, 0.4327, 0.9537]])\n",
      "tensor([[0., 0., 1., 1., 1.]])\n",
      "tensor([[0.0865, 0.1589, 0.0019, 0.7451, 0.0364]])\n",
      "tensor([[0., 0., 1., 1., 0.]])\n",
      "tensor([[0.7152, 0.3166, 0.8678, 0.7232, 0.9533]])\n",
      "tensor([[0., 0., 1., 1., 1.]])\n",
      "tensor([[0.8748, 0.9489, 0.2699, 0.1841, 0.7344]])\n",
      "tensor([[0., 0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# print real data\n",
    "print(\"\\nReal data: \")\n",
    "print(data[:5])\n",
    "\n",
    "# print generated fake data, by the trained generator\n",
    "print(\"\\nGenerated data: \")\n",
    "for _ in range(5):\n",
    "    noise = torch.rand((1, seq_length))\n",
    "    generated_data = generator(noise)\n",
    "    # The generated data is detached from its computation graph and rounded before printing\n",
    "    print(noise)\n",
    "    print(torch.round(generated_data).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7a7e964f-8cf0-4237-9b81-8b56c92557d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am usually called a \"good guy\" by my friends and family. I am a good guy, and I'm not a bad guy.\n",
      "\n",
      "I'm a very good person. And I don't want to be a jerk. But I do want people to know that I have a lot of respect for them. That I care about them, that they care, because I know they're going to love me. They're not going away.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel \n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "seed_text = \"I am usually called\"\n",
    "\n",
    "# Encode the seed text to get input tensors\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, max_length=100, temperature=0.7, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id) \n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5aed1e5-87a6-4d61-8697-aff11e9ea227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Jo, donnez-moi le stylo\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initalize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_prompt = \"translate English to French: 'Hello, give me the pen'\"\n",
    "\n",
    "# Encode the input prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the translated ouput\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\",generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "864a4441-513a-472e-847b-1f2a3a0f0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.08170417696237564\n",
      "ROUGE Score: {'rouge1_fmeasure': tensor(0.2692), 'rouge1_precision': tensor(0.2000), 'rouge1_recall': tensor(0.4118), 'rouge2_fmeasure': tensor(0.1600), 'rouge2_precision': tensor(0.1176), 'rouge2_recall': tensor(0.2500), 'rougeL_fmeasure': tensor(0.2692), 'rougeL_precision': tensor(0.2000), 'rougeL_recall': tensor(0.4118), 'rougeLsum_fmeasure': tensor(0.2692), 'rougeLsum_precision': tensor(0.2000), 'rougeLsum_recall': tensor(0.4118)}\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore, ROUGEScore\n",
    "\n",
    "reference_text = \"Once upon a time, there was a little girl who lived in a village near the forest.\"\n",
    "generated_text = \"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\"\n",
    "\n",
    "# Initialize BLEU and ROUGE scorers\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "# Calculate the BLEU and ROUGE scores\n",
    "bleu_score = bleu([generated_text], [[reference_text]])\n",
    "rouge_score = rouge([generated_text], [[reference_text]])\n",
    "\n",
    "# Print the BLEU and ROUGE scores\n",
    "print(\"BLEU Score:\", bleu_score.item())\n",
    "print(\"ROUGE Score:\", rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4fc3920-6ac5-41e4-9199-579ba8909fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e0808ea0-aa99-4751-9fcc-139126bc4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Setup the optimizer using model parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6093bb95-059b-4a66-81b1-a9e05204f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I love this!',\n",
    " 'This is terrible.',\n",
    " 'Amazing experience!',\n",
    " 'Not my cup of tea.']\n",
    "\n",
    "labels = [1, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41a9e4c0-116a-4db6-81bd-8492d1f3df7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2293, 2023,  999,  102,    0,    0],\n",
       "        [ 101, 2023, 2003, 6659, 1012,  102,    0,    0],\n",
       "        [ 101, 6429, 3325,  999,  102,    0,    0,    0],\n",
       "        [ 101, 2025, 2026, 2452, 1997, 5572, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([1, 0, 1, 0])}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize your data and return PyTorch tensors\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=32)\n",
    "inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "inputs # The tokenizer also adds these special tokens ([CLS] and [SEP]) to each sentence, which is why the maximum number of tokens is 8 (in the sencence 'Not my cup of tea.' --> [ 101, 2025, 2026, 2452, 1997, 5572, 1012,  102]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f29eae0a-62a7-4904-aa5d-4d3156c3e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6837122440338135, Outputs: SequenceClassifierOutput(loss=tensor(0.6837, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3516, -0.3675],\n",
      "        [-0.4386, -0.2354],\n",
      "        [-0.6778, -0.0728],\n",
      "        [-0.4461, -0.2462]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch: 2, Loss: 0.571395993232727, Outputs: SequenceClassifierOutput(loss=tensor(0.5714, grad_fn=<NllLossBackward0>), logits=tensor([[-0.3580, -0.2193],\n",
      "        [-0.3531, -0.3552],\n",
      "        [-0.6342,  0.0609],\n",
      "        [-0.2216, -0.5026]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Epoch: 3, Loss: 0.6573758125305176, Outputs: SequenceClassifierOutput(loss=tensor(0.6574, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2009, -0.1965],\n",
      "        [-0.2347, -0.1050],\n",
      "        [-0.2827,  0.0024],\n",
      "        [-0.0879, -0.2450]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    outputs = model(**inputs) # during the training, the model takes the inputs, including the labels\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}, Outputs: {outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "55ca8d32-416f-40e4-8619-4bb2d980cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_eval {'input_ids': tensor([[ 101, 1045, 2018, 1037, 2919, 2154,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "outputs_eval SequenceClassifierOutput(loss=None, logits=tensor([[-0.3731, -0.0702]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text = \"I had a bad day!\"\n",
    "\n",
    "# Tokenize the text and return PyTorch tensors\n",
    "input_eval = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=32)\n",
    "print('input_eval',input_eval)\n",
    "outputs_eval = model(**input_eval)\n",
    "print('outputs_eval',outputs_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "488b87ff-439f-4780-8ff8-3d7abe535e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3731, -0.0702]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_eval.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21851a7e-f61f-4a21-872f-923104838865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4078, 0.4824]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the output logits to probabilities (with sigmoid, not summing up to 1)\n",
    "predictions = torch.nn.functional.sigmoid(outputs_eval.logits)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0b24b312-805a-46fd-9a8b-c035fead9efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4248, 0.5752]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the output logits to probabilities (with softmax, summing up to 1)\n",
    "predictions = torch.nn.functional.softmax(outputs_eval.logits, dim=-1)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "04b152be-a11a-4584-bd66-c5116e9ea559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I had a bad day!\n",
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Display the sentiments\n",
    "predicted_label = 'positive' if torch.argmax(predictions) > 0 else 'negative'\n",
    "print(f\"Text: {text}\\nSentiment: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3c3183e8-1047-4276-b08c-89ef85da9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, heads, num_layers, dropout): # heads is the number of parallel attention layers (or “heads”) in the multi-head attention mechanism. Each head learns a different type of attention and then the model combines the results from all heads.\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # Initialize the encoder \n",
    "        self.encoder = nn.TransformerEncoder( # nn.TransformerEncoder Stacks num_layers of the below TransformerEncoderLayer to form the complete Transformer encoder\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads), # Creates a single layer of the Transformer encoder, which uses multi-head self-attention with embed_size dimensions and heads parallel attention layers.\n",
    "            num_layers=num_layers)\n",
    "        \n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the transformer encoder \n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1) # mean often calculated when you want to convert a sequence output (for example, a sequence of word embeddings) into a single vector that can be used for classification. The mean operation effectively captures the ‘average’ representation of all the words in the sequence.\n",
    "        return self.fc(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "110cc366-88de-47a0-8493-836fe277a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.3)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f78ed09b-7cdd-476e-abfa-9d58ae821657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.05\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3df203b7-ab6c-4bdc-b89c-dae7df9d9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Regret buying product, Label: 0\n",
      "Sentence: So happy with quality, Label: 1\n",
      "Sentence: Extremely unsatisfied quality, Label: 0\n",
      "Sentence: This is great service, Label: 1\n",
      "Sentence: This is amazing experience, Label: 1\n",
      "Sentence: Money wasted service, Label: 0\n",
      "Sentence: So dissatisfied service, Label: 0\n",
      "Sentence: Five stars service, Label: 1\n",
      "Sentence: This is amazing quality, Label: 1\n",
      "Sentence: One star purchase, Label: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Base positive and negative phrases\n",
    "positive_bases = [\n",
    "    \"I love\", \"This is amazing\", \"Fantastic\", \"Highly recommend\", \n",
    "    \"Best\", \"Absolutely wonderful\", \"Exceeded my expectations\", \"So happy with\",\n",
    "    \"Incredible\", \"Very satisfied\", \"Couldn't be happier\", \"Top-notch\", \"Great value\",\n",
    "    \"Awesome\", \"Just perfect\", \"Five stars\", \"This is great\", \"So delighted\", \"Worth every penny\",\n",
    "    \"Pleasantly surprised\"\n",
    "]\n",
    "\n",
    "negative_bases = [\n",
    "    \"This is terrible\", \"Worst\", \"Not worth\", \"Very disappointed\", \n",
    "    \"Absolutely awful\", \"I hate\", \"So dissatisfied\", \"Would not recommend\", \n",
    "    \"Terrible\", \"Regret buying\", \"Awful\", \"Such a waste\", \"One star\",\n",
    "    \"Horrible\", \"Not what I expected\", \"Highly disappointing\", \"Unhappy with\",\n",
    "    \"Doesn't work as expected\", \"Extremely unsatisfied\", \"Money wasted\"\n",
    "]\n",
    "\n",
    "# Add some variations to these base phrases\n",
    "suffixes = [\"product\", \"experience\", \"purchase\", \"service\", \"quality\"]\n",
    "\n",
    "# Generate dataset with variations\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "for _ in range(40):\n",
    "    positive_sentence = random.choice(positive_bases) + \" \" + random.choice(suffixes)\n",
    "    negative_sentence = random.choice(negative_bases) + \" \" + random.choice(suffixes)\n",
    "    train_sentences.append(positive_sentence)\n",
    "    train_labels.append(1)\n",
    "    train_sentences.append(negative_sentence)\n",
    "    train_labels.append(0)\n",
    "\n",
    "# Shuffle the dataset\n",
    "combined = list(zip(train_sentences, train_labels))\n",
    "random.shuffle(combined)\n",
    "train_sentences[:], train_labels[:] = zip(*combined)\n",
    "\n",
    "# Print first 10 examples to check\n",
    "for i in range(10):\n",
    "    print(f\"Sentence: {train_sentences[i]}, Label: {train_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1154c136-6c92-4de5-8e5d-7a2c007aa81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.3).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3478ee1-cc88-4091-a1eb-169b899d93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The token_embeddings are used to convert each token (word) in the sentence into a high-dimensional vector representation that captures its semantic meaning. These embeddings are typically learned from large amounts of text data and are useful for many natural language processing tasks. By starting with these precomputed embeddings, the model can more easily learn to make accurate predictions.\n",
    "\n",
    "def tokens_encoded(word, encod_model):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    word_tokenized = tokenizer(word, return_tensors='pt', truncation=True, padding='max_length', max_length=512)['input_ids'].float().to(device)\n",
    "    return encod_model.encoder(word_tokenized)  # Get the embeddings\n",
    "\n",
    "# tokens_encoded('example', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84cfc836-356c-4dd1-9b00-cc923af77947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, aver_loss_1: 0.39282071590423584, aver_loss_0: 1.0661193132400513\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for epoch in range(1): # increase\n",
    "    \n",
    "    losses_0 = 0\n",
    "    losses_1 = 0\n",
    "    labels_0 = 0\n",
    "    labels_1 = 0\n",
    "    \n",
    "    print('epoch:', epoch)\n",
    "    for i, (sentence, label) in enumerate(zip(train_sentences, train_labels)):        \n",
    "        tokens = sentence.split()\n",
    "\n",
    "        # Get token embeddings and ensure proper shape\n",
    "        data = torch.stack([tokens_encoded(token, model) for token in tokens]).view(1, len(tokens), 512).to(device)\n",
    "        label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "        # Generate predictions and compute loss\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label_tensor)\n",
    "        \n",
    "        if label == 0:\n",
    "            losses_0 += loss\n",
    "            labels_0 += 1\n",
    "        else:\n",
    "            losses_1 += loss\n",
    "            labels_1 += 1\n",
    "\n",
    "        # Backpropagation and optimizer steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, aver_loss_1: {losses_1/labels_1}, aver_loss_0: {losses_0/labels_0}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d7e6a09-5307-4452-9cdb-312ae13220b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens ['This', 'is', 'good']\n",
      "data.shape torch.Size([1, 3, 512])\n",
      "output tensor([[-1.1916, -0.4394]], device='cuda:0')\n",
      "predicted tensor([1], device='cuda:0')\n",
      "'This is good' is Positive\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    # Deactivate the gradient computations and get the sentiment prediction.\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        print('tokens', tokens)\n",
    "        data = torch.stack([tokens_encoded(token, model) for token in tokens]).view(1, len(tokens), 512)\n",
    "        print('data.shape', data.shape)\n",
    "        output = model(data)\n",
    "        print('output', output)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        print('predicted', predicted)\n",
    "        return \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
    "\n",
    "sample_sentence = \"This is good\"\n",
    "print(f\"'{sample_sentence}' is {predict(sample_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a1cabda-2059-49a1-9a28-033aa5be1c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  7,  8,  6,  2],\n",
       "         [16, 14,  4, 12,  0],\n",
       "         [ 9, 14, 13, 15,  0],\n",
       "         [ 3, 14,  2, 10,  0]], device='cuda:0'),\n",
       " tensor([11,  5,  0,  1], device='cuda:0'))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_data = [[2, 7, 8, 6, 2], [16, 14, 4, 12], [9, 14, 13, 15], [3, 14, 2, 10]]\n",
    "train_target_data = [11, 5, 0, 1]\n",
    "\n",
    "ix_to_word = {0: 'noisy',\n",
    "             1: 'mammals',\n",
    "             2: 'the',\n",
    "             3: 'whales',\n",
    "             4: 'very',\n",
    "             5: 'animals',\n",
    "             6: 'on',\n",
    "             7: 'cat',\n",
    "             8: 'sat',\n",
    "             9: 'parrots',\n",
    "             10: 'largest',\n",
    "             11: 'mat',\n",
    "             12: 'loyal',\n",
    "             13: 'colorful',\n",
    "             14: 'are',\n",
    "             15: 'and',\n",
    "             16: 'dogs'}\n",
    "\n",
    "# The pad_sequences function is used to ensure that all sequences in a batch have the same length so they can be processed together. \n",
    "\n",
    "def pad_sequences(batch):\n",
    "    max_len = max([len(seq) for seq in batch])\n",
    "    return torch.stack([torch.cat([torch.tensor(seq), torch.zeros(max_len-len(seq)).long()]) for seq in batch])\n",
    "\n",
    "# Pad sequences and move to device\n",
    "train_input_data = pad_sequences(train_input_data).to(device)\n",
    "train_target_data = torch.tensor(train_target_data).to(device)\n",
    "train_input_data, train_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "401bbbd5-3f47-46e1-8f8a-532efecd6d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(ix_to_word)\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae56cb27-4a4f-4523-bdfc-fe560463d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNWithAttentionModel, self).__init__()\n",
    "        # Create an embedding layer for the vocabulary\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Linear layer for attention mechanism\n",
    "        # The purpose of the attention mechanism is to allow the model to focus on different parts of the input sequence when making predictions.\n",
    "        # Here, the linear layer takes the hidden state from the RNN and transforms it to produce attention scores for each time step in the input sequence.\n",
    "        # The dimension of the output is set to `hidden_dim` to ensure that the attention scores align with the hidden states' dimension.\n",
    "        self.attention = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Linear layer for classification\n",
    "        # This layer maps the context vector (which is a weighted sum of hidden states) to the output vocabulary.\n",
    "        # This is used to predict the next word in the sequence.\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)  # Convert word indices to embeddings\n",
    "        out, _ = self.rnn(x)  # Process embeddings with RNN to get hidden states for each time step\n",
    "        \n",
    "        # Attention mechanism: \n",
    "        # Step 1: Compute attention scores\n",
    "        # The attention layer takes the hidden states from the RNN and produces attention scores.\n",
    "        # These scores represent the importance of each time step in the input sequence for the current prediction.\n",
    "        attn_scores = self.attention(out)\n",
    "        \n",
    "        # Step 2: Apply softmax to get attention weights\n",
    "        # Softmax is applied to the attention scores to normalize them to a probability distribution.\n",
    "        # This means the attention weights sum to 1 and can be interpreted as the model's confidence in each time step's importance.\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=1)\n",
    "        \n",
    "        # Step 3: Compute context vector\n",
    "        # The context vector is a weighted sum of the hidden states, where the weights are the attention scores.\n",
    "        # This effectively summarizes the entire input sequence, focusing more on the important parts as determined by the attention mechanism.\n",
    "        context = torch.sum(attn_weights * out, dim=1)\n",
    "        \n",
    "        # Step 4: Classification\n",
    "        # The context vector is passed through a fully connected layer to produce the final output.\n",
    "        # This output is used to predict the next word in the sequence.\n",
    "        out = self.fc(context)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2211a8a1-5321-473f-8605-8f3585d56702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNWithAttentionModel(\n",
       "  (embeddings): Embedding(17, 10)\n",
       "  (rnn): RNN(10, 16, batch_first=True)\n",
       "  (attention): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc): Linear(in_features=16, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model = RNNWithAttentionModel().to(device)\n",
    "attention_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4a87eff0-878f-415a-9f4c-d6ad57ff7dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc126281-126d-4e9c-b919-99f16b07f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([ 2, 10, 16,  2], device='cuda:0')\n",
      "loss tensor(2.7768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([ 1,  5, 16,  1], device='cuda:0')\n",
      "loss tensor(2.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([1, 5, 0, 1], device='cuda:0')\n",
      "loss tensor(2.4335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([1, 5, 0, 1], device='cuda:0')\n",
      "loss tensor(2.2631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([1, 5, 0, 1], device='cuda:0')\n",
      "loss tensor(2.0926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([1, 5, 0, 1], device='cuda:0')\n",
      "loss tensor(1.9220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([1, 5, 0, 1], device='cuda:0')\n",
      "loss tensor(1.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([1, 5, 0, 1], device='cuda:0')\n",
      "loss tensor(1.5858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([11,  5,  0,  1], device='cuda:0')\n",
      "loss tensor(1.4258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "train_input_data tensor([[ 2,  7,  8,  6,  2],\n",
      "        [16, 14,  4, 12,  0],\n",
      "        [ 9, 14, 13, 15,  0],\n",
      "        [ 3, 14,  2, 10,  0]], device='cuda:0')\n",
      "train_target_data tensor([11,  5,  0,  1], device='cuda:0')\n",
      "training prediction: tensor([11,  5,  0,  1], device='cuda:0')\n",
      "loss tensor(1.2755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "----------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TRAIN RNN ATT\n",
    "\n",
    "epochs = 10 # change to 300\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    attention_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('train_input_data', train_input_data)\n",
    "    # print('train_input_data.shape', train_input_data.shape)\n",
    "    print('train_target_data', train_target_data)\n",
    "    # print('train_target_data.shape', train_target_data.shape)\n",
    "    \n",
    "    train_outputs = attention_model(train_input_data)\n",
    "    # print('train_outputs', train_outputs)\n",
    "    # print('train_outputs.shape', train_outputs.shape)\n",
    "    \n",
    "    probabilities = F.softmax(train_outputs, dim=1)  # Apply softmax to the outputs to get probabilities (just for understanding, because the loss criterion uses the outputs logits direct, instead of the probabilities)\n",
    "    # print('probabilities:', probabilities)  \n",
    "    print('training prediction:', probabilities.argmax(dim=1))\n",
    "    \n",
    "    loss = criterion(train_outputs, train_target_data)\n",
    "    print('loss', loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('----------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1bdc4237-6f86-41ea-942b-7b54ae6ed4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input and target data\n",
    "test_input_data = [[2, 7, 8, 6, 2], [16, 14, 4, 12], [9, 14, 13, 15], [3, 14, 2, 10], [2, 10, 1, 14, 2]]\n",
    "test_target_data = [11, 5, 0, 1, 3]\n",
    "\n",
    "# ix_to_word = {0: 'noisy',\n",
    "#              1: 'mammals',\n",
    "#              2: 'the',\n",
    "#              3: 'whales',\n",
    "#              4: 'very',\n",
    "#              5: 'animals',\n",
    "#              6: 'on',\n",
    "#              7: 'cat',\n",
    "#              8: 'sat',\n",
    "#              9: 'parrots',\n",
    "#              10: 'largest',\n",
    "#              11: 'mat',\n",
    "#              12: 'loyal',\n",
    "#              13: 'colorful',\n",
    "#              14: 'are',\n",
    "#              15: 'and',\n",
    "#              16: 'dogs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e105c281-3ffe-4330-951f-87ddc0532c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_test tensor([[2, 7, 8, 6, 2]], device='cuda:0')\n",
      "['the', 'cat', 'sat', 'on', 'the']\n",
      "real output mat\n",
      "attention_prediction mat\n",
      "input_test tensor([[16, 14,  4, 12]], device='cuda:0')\n",
      "['dogs', 'are', 'very', 'loyal']\n",
      "real output animals\n",
      "attention_prediction animals\n",
      "input_test tensor([[ 9, 14, 13, 15]], device='cuda:0')\n",
      "['parrots', 'are', 'colorful', 'and']\n",
      "real output noisy\n",
      "attention_prediction noisy\n",
      "input_test tensor([[ 3, 14,  2, 10]], device='cuda:0')\n",
      "['whales', 'are', 'the', 'largest']\n",
      "real output mammals\n",
      "attention_prediction mammals\n",
      "input_test tensor([[ 2, 10,  1, 14,  2]], device='cuda:0')\n",
      "['the', 'largest', 'mammals', 'are', 'the']\n",
      "real output whales\n",
      "attention_prediction mammals\n"
     ]
    }
   ],
   "source": [
    "# TEST RNN ATT\n",
    "\n",
    "# Process each input sequence and target\n",
    "for input_seq, target in zip(test_input_data, test_target_data):\n",
    "    input_test = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    print('input_test', input_test)\n",
    "    print([ix_to_word[index.item()] for index in input_test[0]])\n",
    "\n",
    "    # Set the attention model to evaluation mode\n",
    "    attention_model.eval()\n",
    "\n",
    "    # Get the attention output by passing the appropriate input\n",
    "    attention_output = attention_model(input_test)\n",
    "\n",
    "    # Get the target word\n",
    "    target_output = ix_to_word[target]\n",
    "    print('real output', target_output)\n",
    "\n",
    "    # Get the predicted word\n",
    "    attention_prediction = ix_to_word[torch.argmax(attention_output).item()]\n",
    "    print('attention_prediction', attention_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "596102ed-6297-4a75-bab8-32d85e3878e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel_2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNModel_2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)  # Convert input indices to embeddings\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)  # Pass the input and initial hidden state through the RNN\n",
    "        out = out[:, -1, :]  # Select the last output of each sequence\n",
    "        out = self.fc(out)  # Pass the last outputs through the fully connected layer\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "998b9cec-e0e7-4a92-b0bc-72320d853f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rnn_model = RNNModel_2(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a27f6c42-e976-474e-a600-f06dbe9f8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 100\n",
      "Epoch: 200\n",
      "Epoch: 300\n",
      "Epoch: 400\n",
      "Epoch: 500\n",
      "Epoch: 600\n",
      "Epoch: 700\n",
      "Epoch: 800\n",
      "Epoch: 900\n"
     ]
    }
   ],
   "source": [
    "# TRAN RNN SIMP\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    rnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_outputs = rnn_model(train_input_data)\n",
    "    loss = criterion(train_outputs, train_target_data)\n",
    "    \n",
    "    # print('training prediction:', train_outputs.argmax(dim=1))\n",
    "    # print('loss', loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('Epoch:', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd743cce-f8b5-4a2e-b714-1ba764831cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_test tensor([[2, 7, 8, 6, 2]], device='cuda:0')\n",
      "['the', 'cat', 'sat', 'on', 'the']\n",
      "real output mat\n",
      "predicted output mat\n",
      "-------------------\n",
      "input_test tensor([[16, 14,  4, 12]], device='cuda:0')\n",
      "['dogs', 'are', 'very', 'loyal']\n",
      "real output animals\n",
      "predicted output sat\n",
      "-------------------\n",
      "input_test tensor([[ 9, 14, 13, 15]], device='cuda:0')\n",
      "['parrots', 'are', 'colorful', 'and']\n",
      "real output noisy\n",
      "predicted output noisy\n",
      "-------------------\n",
      "input_test tensor([[ 3, 14,  2, 10]], device='cuda:0')\n",
      "['whales', 'are', 'the', 'largest']\n",
      "real output mammals\n",
      "predicted output mammals\n",
      "-------------------\n",
      "input_test tensor([[ 2, 10,  1, 14,  2]], device='cuda:0')\n",
      "['the', 'largest', 'mammals', 'are', 'the']\n",
      "real output whales\n",
      "predicted output mat\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# TEST RNN SIMP\n",
    "\n",
    "# Process each input sequence and target\n",
    "for input_seq, target in zip(test_input_data, test_target_data):\n",
    "    input_test = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    print('input_test', input_test)\n",
    "    print([ix_to_word[index.item()] for index in input_test[0]])\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    rnn_model.eval()\n",
    "\n",
    "    # Get the model output\n",
    "    rnn_model_output = rnn_model(input_test)\n",
    "\n",
    "    # Convert target tensor to Python integer\n",
    "    target_output = ix_to_word[target]\n",
    "    print('real output', target_output)\n",
    "\n",
    "    # Get the predicted word\n",
    "    rnn_model_prediction = ix_to_word[rnn_model_output.argmax(dim=1).item()]\n",
    "    print('predicted output', rnn_model_prediction)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab099c-8290-4742-89d7-7cdc2b91d033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f7424-cf94-4fd2-92c6-8590905ac46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
