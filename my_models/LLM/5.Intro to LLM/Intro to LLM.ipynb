{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134d2c0a-c2f1-45f6-bc39-2cb800ad3b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import math  \n",
    "\n",
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f579bcb-a96e-4156-b1ef-2dbaa444adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x165904cf070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "\n",
    "# Load the pipeline for sentiment classification\n",
    "classifier = pipeline(\"text-classification\", model=model_name)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933b2234-cf9e-45e6-89d8-51620e70cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '3 stars', 'score': 0.6387940645217896}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6403d66d-f505-4ad0-ad98-09263fbc9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '4 stars', 'score': 0.5190770030021667}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The food was good, everything well organized\"\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a22190-ca52-4b7b-aed9-45bb268f6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.SummarizationPipeline at 0x165cdddac80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'cnicu/t5-small-booksum'\n",
    "\n",
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline('summarization', model = model_name)\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5111f86-f864-4b68-bb1f-244541711108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = '\\nThe tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\\n'\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length = 30)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c80d10-21eb-4c39-8b89-843b4b2cb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey\n"
     ]
    }
   ],
   "source": [
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1055b1e9-7c8d-4f56-9ab9-9a1e8e5082d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Alienware\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millau Viaduct\n"
     ]
    }
   ],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "question = \"What's the tallest structure in France?\"\n",
    "\n",
    "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
    "outputs = qa_model(question, long_text)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32cf35b-340a-426c-bebf-4013cfa3e3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't think you're doing a good translation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "\n",
    "input_text = \"No creo que hagas una buena traducci√≥n\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline('translation_es_to_en', model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f6f578-4871-4b5f-8c06-b0dfa5f64e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer model hyperparameters\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126ece2e-d3d9-464e-aead-b62880bfc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model, # d_model is the dimension of the input vectors and output vectors of the model, specifically the size of the feature space. Essentially, it determines the number of features in each transformer layer\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,    \n",
    "    num_decoder_layers=num_decoder_layers\n",
    "    )\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943a497a-5f46-407d-be6e-da7d37c310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fecc1dc-bf8c-41b2-b66d-17c1a782f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Customer review:\\nI had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\\n\\nHotel reponse to the customer:\\nDear valued customer, I am glad to hear you had a good stay with us.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the prompt for the text generation LLM\n",
    "\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8261f8c5-a1b9-4d48-bb88-252d2a72582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
      "\n",
      "Hotel reponse to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. There were several people there and, as always, the food was fresh and the service was nice\n"
     ]
    }
   ],
   "source": [
    "#### Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length = 100, pad_token_id=generator.tokenizer.eos_token_id) #  if the generated text is shorter than max_length, the remaining tokens will be filled with the EOS token.\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa4b5a5-409b-45bf-95a6-2e4dad5472ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='textattack/distilbert-base-uncased-SST-2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e08ef05c-f507-4470-b3be-e43b0dcd2324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a4c8829-7de1-457b-8106-1d4f2204f0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 2190, 3185, 1045, 1005, 2310, 2412, 3427,  999,  102,    0],\n",
       "        [ 101, 2054, 2019, 9643, 3185, 1012, 1045, 9038, 3666, 2009, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037fe889-4fdb-460e-a474-a413ebdb40fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03e4513-4aca-4f7e-88cd-0b5debfc8f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0542,  0.2731],\n",
       "        [ 0.9809, -0.7639]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "809b5ebc-e84b-4947-bc91-05e3f343eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d5a294c-f96d-416e-b1b0-5ede64da8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for \"The best movie I've ever watched!\": 1\n",
      "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
     ]
    }
   ],
   "source": [
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b41652f3-395f-4e35-9188-4253114fb1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_sents', 'summaries'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from Hugging Face's dataset hub\n",
    "dataset = load_dataset('opinosis', trust_remote_code=True)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1af4af29-1973-4998-8729-05d4cb433fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of instances: {len(dataset['train'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcbc46a3-f0a1-49fc-be25-756cc67c2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['review_sents', 'summaries']\n"
     ]
    }
   ],
   "source": [
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d85c21f6-5636-4f8e-b78d-972a85b9fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 2)\n",
      "dict_keys(['review_sents', 'summaries'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review_sents': \", and is very, very accurate .\\r\\n but for the most part, we find that the Garmin software provides accurate directions, whereever we intend to go .\\r\\n This function is not accurate if you don't leave it in battery mode say, when you stop at the Cracker Barrell for lunch and to play one of those trangle games with the tees .\\r\\n It provides immediate alternatives if the route from the online map program was inaccurate or blocked by an obstacle .\\r\\n I've used other GPS units, as well as GPS built into cars   and to this day NOTHING beats the accuracy of a Garmin GPS .\\r\\n It got me from point A to point B with 100% accuracy everytime .\\r\\n It has yet to disappoint, getting me everywhere with 100% accuracy .\\r\\n0 out of 5 stars Honest, accurate review, , PLEASE READ !\\r\\n Aside from that, every destination I've thrown at has been 100% accurate .\\r\\nIn closing, this is a fantastic GPS with some very nice features and is very accurate in directions .\\r\\n Plus, I've always heard that there are  quirks  with any GPS being accurate, having POIs, etc .\\r\\n DESTINATION TIME, , This is pretty accurate too .\\r\\n But, it's always very accurate .\\r\\n The map is pretty accurate and the Point of interest database also is good .\\r\\n Most of the times, this info was very accurate .\\r\\nI've even used it in the  pedestrian  mode, and it's amazing how accurate it is .\\r\\n  ONLY  is only accurate when an ad says,  Top sirloin steak, ONLY $1 .\\r\\n The most accurate review stated that these machines are adjunct to a good map and signs on the interstate .\\r\\n The directions are highly accurate down to a  T  .\\r\\n Depending on what you are using it for, it is a nice adjunct to a travel trip and the directions are accurate and usually the quickest, but not always .\\r\\n The screen is easy to see, the voice tells you where you are and it's very accurate .\\r\\n It was accurate to the minute when it told me when I would arrive home .\\r\\n0 out of 5 stars GPS Navigator doesn't navigate accurately on a straight road .\\r\\n I was familiar with the streets and only used the Nuvi to get an accurate arrival time estimate .\\r\\n but after that it is very easy and quite accurate to use .\\r\\n The accuracy at this point is very good .\\r\\nWhile the 255W routing seems generally accurate and logical, on my first use I discovered that it does have some errors in its internal map .\\r\\n Bottom line is I wanted a unit that is accurate and had reliable satellite connection .\\r\\n I've used it around town and find it to be extremely accurate .\\r\\nI found the maps to be inaccurate at first, but after I updated them from Garmin's website everything is golden .\\r\\n A lot of my friends' addresses are inaccurate by any GPS .\\r\\n It loads quickly, have pretty accurate directions, and can recalculate quickly when I miss a turn .\\r\\n Because the accuracy is good to the street address level, it may not be able to guide you to the exact location if your destination is inside a shopping mall .\\r\\nI updated to the latest 2010 map soon after I received the unit, so the map is accurate to me .\\r\\n I was blown away at the accuracy and routing capability this thing had .\\r\\n I used it the day I bought it,   and then this morning, and as soon as it comes on it is  ready to navigate  The only downfall of this product, and the only reason I did not give it 5 stars is the fact that the speed limit it displays for the road you are on isn't 100% accurate .\\r\\n If your looking for a nice, accurate GPS for not so much money, got with this one .\\r\\n0 out of 5 stars Inexpensive, accurate, plenty of features, August 6, 2009\\r\\n The only glitch I have found so far is that the speed limits are not 100% accurate, although the GPS, amazingly, is able to very accurately tell you how fast your vehicle is moving .\\r\\n I was a little disappointed in the inaccuracy of the posted speed limit, as I'm guilty of not paying close enough attention to those signs, especially w  interstate speed traps that are constantly changing up and down .\\r\\n The closest one that gives the most accurate route that I usually take is the Navigon .\\r\\n After 2 weeks, it has yet to make a mistake, and is always completely accurate ,  even to the point of telling me which side of the street my destination is on .\\r\\n It has worked well for local driving giving accurate directions for roads and streets .\\r\\nThe estimated time to arrival does not seem to calculate the travelling time accurately .\\r\\nAccuracy is as good as any other unit, they all sometimes tell you you have arrived when you haven't, or continue to tell you to turn when you're already there .\\r\\n Accuracy is determined by the maps .\\r\\n Less traveled rural roads will not be accurate on any unit .\\r\\n Accuracy is within a few yards .\\r\\nWhat the 255w does best is find a street address, business, point of interest, hospital or airport and give you turn, by, turn directions with amazing accuracy .\\r\\n The Garmin is loaded with very accurate maps that generally know the roads in even the remotest areas .\\r\\nI'm really glad I bought it though, and like the easy to read graphics, the voice used to tell you the name of the street you are to turn on, the uncannily accurate estimates of mileage and time of arrival at your destination .\\r\\nMy new Garmin 255w had very Easy Set Up, Accurate Directions to locations, User Friendly Unit to anyone in my vehicle who tried it .\\r\\n I had a GPS 10, years ago when I owned a boat that was difficult to use and with very poor accuracy so I had assumed that the road GPS wasn't any better .\\r\\n Practiced visiting places I already knew to see how accurate the directions and maps would be .\\r\\n Easy to use, excellent accuracy, nice and intuitive interface .\\r\\n The directions provided have all been quite accurate thus far .\\r\\n,  Very Accurate but with one small glitch I found ,  I'll explain in the CONS\\r\\nThis is a great GPS, it is so easy to use and it is always accurate .\\r\\nVery easy to operate and pretty accurate as well, only led me astray once and that was in northern Maine where roads are few and paved ones fewer .\\r\\n Easy to use and amazed at how accurate this item is .\\r\\nTo date it's been a very easy to use and accurate .\\r\\n Mounted really easily and has been very accurate .\\r\\n seems to be rather accurate .\\r\\n It was accurate on determing original directions and recalculating when necessary .\\r\\nHighly accurate, POIs are great .\\r\\n I can't believe how accurate and detailed the information estimated time of arrival,speed limits along the way,and detailed map of my route, to name a few .\\r\\n Speed of calculation, accuracy, and simplicity of operation are top notch .\\r\\n\",\n",
       " 'summaries': ['This unit is generally quite accurate.  \\r\\nSet-up and usage are considered to be very easy. \\r\\nThe maps can be updated, and tend to be reliable.',\n",
       "  \"The Garmin seems to be generally very accurate.\\r\\nIt's easy to use with an intuitive interface.\",\n",
       "  'It is very accurate, even in destination time.',\n",
       "  'Very accurate with travel and destination time.\\r\\nNegatives are not accurate with speed limits and rural roads.',\n",
       "  'Its accurate, fast and its simple operations make this a for sure buy.']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'].shape)\n",
    "print(dataset['train'][0].keys())\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404d5b6c-d2dc-400a-92f9-298af290b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][0]['review_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8ac89b-0714-4100-8875-fec54785af1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14010bce-ddff-4d7b-986c-b095399d5b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21603,    10,     3,     6,    11,    19,   182,     6,   182,  4034,\n",
       "             3,     5,    68,    21,     8,   167,   294,     6,    62,   253,\n",
       "            24,     8,  3121,  1109,   889,   795,  4034,  7943,     6,   213,\n",
       "          3258,    62,  8286,    12,   281,     3,     5,   100,  1681,    19,\n",
       "            59,  4034,     3,    99,    25,   278,    31,    17,  1175,    34,\n",
       "            16,  3322,  2175,   497,     6,   116,    25,  1190,    44,     8,\n",
       "         23291,    49, 13384,  3820,    21,  3074,    11,    12,   577,    80,\n",
       "            13,   273,     3, 11665,  3537,  1031,    28,     8,     3,    17,\n",
       "            15,    15,     7,     3,     5,    94,   795,  5299, 10336,     3,\n",
       "            99,     8,  2981,    45,     8,   367,  2828,   478,    47, 27801,\n",
       "            42, 12468,    57,    46, 22820,     3,     5,    27,    31,   162,\n",
       "           261,   119,  9679,  3173,     6,    38,   168,    38,  9679,  1192,\n",
       "           139,  2948,    11,    12,    48,   239,  4486,   566,  2365,  3853,\n",
       "             7,     8,  7452,    13,     3,     9,  3121,  1109,  9679,     3,\n",
       "             5,    94,   530,   140,    45,   500,    71,    12,   500,   272,\n",
       "            28,  2349,  7452,   334,   715,     3,     5,    94,    65,   780,\n",
       "            12, 26963,     6,   652,   140,  6531,    28,  2349,  7452,     3,\n",
       "             5,     3,   632,    91,    13,   305,  4811, 11772,   222,     6,\n",
       "          4034,  1132,     6,     3,     6, 26573,   391, 19552,     3,    55,\n",
       "            71,  1583,    45,    24,     6,   334,  3954,    27,    31,   162,\n",
       "             3, 12618,    44,    65,   118,  2349,  4034,     3,     5,    86,\n",
       "          6733,     6,    48,    19,     3,     9,  2723,  9679,    28,   128,\n",
       "           182,  1245,   753,    11,    19,   182,  4034,    16,  7943,     3,\n",
       "             5,  2477,     6,    27,    31,   162,   373,  1943,    24,   132,\n",
       "            33,   546, 12546,     7,    28,   136,  9679,   271,  4034,     6,\n",
       "           578,  9915,   196,     7,     6,   672,     3,     5,     3, 18284,\n",
       "         25424,  8015,   332, 15382,     6,     3,     6,   100,    19,  1134,\n",
       "          4034,   396,     3,     5,   299,     6,    34,    31,     7,   373,\n",
       "           182,  4034,     3,     5,    37,  2828,    19,  1134,  4034,    11,\n",
       "             8,  4564,    13,  1046,  3501,    92,    19,   207,     3,     5,\n",
       "          1377,    13,     8,   648,     6,    48,  2845,    47,   182,  4034,\n",
       "             3,     5,    27,    31,   162,   237,   261,    34,    16,     8,\n",
       "         16400,  2175,     6,    11,    34,    31,     7,  1237,   149,  4034,\n",
       "            34,    19,     3,     5, 20728,    19,   163,  4034,   116,    46,\n",
       "             3,     9,    26,   845,     6,  2224,   108,    52,    40,    32,\n",
       "            77, 17718,     6, 20728,  1970,     3,     5,    37,   167,  4034,\n",
       "          1132,  4568,    24,   175,  4096,    33, 28542,    12,     3,     9,\n",
       "           207,  2828,    11,  3957,    30,     8,  1413,  5540,     3,     5,\n",
       "            37,  7943,    33,  1385,  4034,   323,    12,     3,     9,   332,\n",
       "             3,     5,     3, 11333,    30,   125,    25,    33,   338,    34,\n",
       "            21,     6,    34,    19,     3,     9,  1245, 28542,    12,     3,\n",
       "             9,  1111,  1469,    11,     8,  7943,    33,  4034,    11,  1086,\n",
       "             8,     3, 31051,     6,    68,    59,   373,     3,     5,    37,\n",
       "          1641,    19,   514,    12,   217,     6,     8,  2249,   817,     7,\n",
       "            25,   213,    25,    33,    11,    34,    31,     7,   182,  4034,\n",
       "             3,     5,    94,    47,  4034,    12,     8,  1962,   116,    34,\n",
       "          1219,   140,   116,    27,   133,  3658,   234,     3,     5,     3,\n",
       "           632,    91,    13,   305,  4811,  9679, 19261, 19306,   744,    31,\n",
       "            17,  7939, 12700,    30,     3,     9,  2541,  1373,     3,     5,\n",
       "            27,    47,  3324,    28,     8,  6162,    11,   163,   261,     8,\n",
       "          1174,  2099,    12,   129,    46,  4034,  6870,    97,  7037,     3,\n",
       "             5,     1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c225539-d3a1-423f-af36-98eac35410f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    48,  1681,    19,    59,  4034,     3,    99,    25,   278,\n",
       "            31,    17,  1175,    34,    16,  3322,  2175,   497,     6,   116,\n",
       "            25,  1190,    44,     8, 23291,    49, 13384,  3820,    21,  3074,\n",
       "            11,    12,   577,    80,    13,   273,     3, 11665,  3537,  1031,\n",
       "            28,     8,     3,    17,    15,    15,     7,     3,     5,    34,\n",
       "           795,  5299, 10336,     3,    99,     8,  2981,    45,     8,   367,\n",
       "          2828,   478,    47, 27801,    42, 12468,    57,    46, 22820,     3,\n",
       "             5,     1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2fa7054-04bc-4d68-a154-8146a9cd133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text (first 400 characters): \n",
      " , and is very, very accurate .\n",
      " but for the most part, we find that the Garmin software provides accurate directions, whereever we intend to go .\n",
      " This function is not accurate if you don't leave it in battery mode say, when you stop at the Cracker Barrell for lunch and to play one of those trangle games with the tees .\n",
      " It provides immediate alternatives if the route from the online map progra\n",
      "\n",
      "Generated Summary: \n",
      " this function is not accurate if you don't leave it in battery mode say, when you stop at the Cracker Barrell for lunch and to play one of those trangle games with the tees. it provides immediate alternatives if the route from the online map program was inaccurate or blocked by an obstacle.\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a59b5825-25c6-43a0-bc14-e99996465bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd820626-ebb8-45c8-a330-c77d65785125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_input Hello\n",
      "input_ids tensor([[3923,    0]])\n",
      "translated_ids tensor([[65000,  2119,     3,     0]])\n",
      "English: Hello | Spanish: Hola.\n",
      "---------------------------------\n",
      "english_input Thank you\n",
      "input_ids tensor([[1825,   40,    0]])\n",
      "translated_ids tensor([[65000,  1124,     3,     0]])\n",
      "English: Thank you | Spanish: Gracias.\n",
      "---------------------------------\n",
      "english_input How are you?\n",
      "input_ids tensor([[594,  53,  40,  21,   0]])\n",
      "translated_ids tensor([[65000,    50,  1102,  1221,    21,     0]])\n",
      "English: How are you? | Spanish: ¬øC√≥mo est√°s?\n",
      "---------------------------------\n",
      "english_input Sorry\n",
      "input_ids tensor([[5099,    0]])\n",
      "translated_ids tensor([[65000,   350,  1669,     3,     0]])\n",
      "English: Sorry | Spanish: Lo siento.\n",
      "---------------------------------\n",
      "english_input Goodbye\n",
      "input_ids tensor([[22191,     0]])\n",
      "translated_ids tensor([[65000,  8631,     3,     0]])\n",
      "English: Goodbye | Spanish: Adi√≥s.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    print('english_input', english_input)\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
    "    print('input_ids', input_ids)\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    print('translated_ids', translated_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")\n",
    "    print('---------------------------------')\n",
    "    \n",
    "# the additional tokens generated in the input_ids and translated_ids are start/end of sequences or special characters (like punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a180b31-33ec-4984-9b7c-e9cdd955d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11590\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "mlqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "671edd08-a89d-4458-87a9-d983f359f9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  who represented robert frost and walter kasza in their suit?\n",
      "Context:  In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"\n"
     ]
    }
   ],
   "source": [
    "question = mlqa[\"test\"][\"question\"][1]\n",
    "context = mlqa[\"test\"][\"context\"][1]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fea420d-7e1f-4798-831e-3f013e593b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='deepset/minilm-uncased-squad2', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89a96dda-066d-4279-adf9-857af82a91b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2040,  3421,  2728, 10097,  1998,  4787, 10556, 17112,  2050,\n",
       "          1999,  2037,  4848,  1029,   102,  1999,  2807,  1010,  2274, 13294,\n",
       "          6831, 16728,  1998,  1996, 24835,  1997, 16728,  4787, 10556, 17112,\n",
       "          2050,  1998,  2728, 10097, 12923,  1996, 18531,  1998,  1996,  2142,\n",
       "          2163,  4483,  3860,  4034,  1012,  2037,  4848,  1010,  1999,  2029,\n",
       "          2027,  2020,  3421,  2011,  2577,  2899,  2118,  2375,  2934,  5655,\n",
       "         10722, 12866,  1010,  6884,  2027,  2018,  2042,  2556,  2043,  2312,\n",
       "         12450,  1997,  4242, 12141,  2018,  2042,  5296,  1999,  2330, 14496,\n",
       "          1998, 19874,  2012, 18087,  1012, 16012,  4523,  3111,  2579,  2013,\n",
       "          1996, 17612, 11390,  2020, 16578,  2011, 18607,  2118, 16012, 24229,\n",
       "          2015,  1010,  2040,  2179,  2152,  3798,  1997,  4487, 11636,  2378,\n",
       "          1010,  4487, 10609,  6844, 27942,  2319,  1010,  1998, 13012,  2818,\n",
       "         10626,  8913, 16921, 11474,  1999,  2037,  2303,  6638,  1012,  1996,\n",
       "         17612, 11390,  6884,  2027,  2018,  8760,  3096,  1010, 11290,  1010,\n",
       "          1998, 16464,  6441,  2349,  2000,  2037,  2147,  2012, 18087,  1010,\n",
       "          1998,  2008,  2023,  2018,  5201,  2000,  1996,  6677,  1997, 10097,\n",
       "          1998, 10556, 17112,  2050,  1012,  1996,  4848,  4912,  9430,  2005,\n",
       "          1996,  6441,  2027,  2018,  8760,  1010,  6815,  1996, 18531,  2018,\n",
       "         17800,  8971, 11704,  4475,  1010,  1998,  2008,  1996, 19044,  2018,\n",
       "          3478,  1999,  2049,  4611,  2000, 16306,  1996,  7692,  5680,  1998,\n",
       "          7233,  2552,  1006,  2029, 21208,  2015,  8304,  1997,  4795,  4475,\n",
       "          1007,  1012,  2027,  2036,  4912,  6851,  2592,  2055,  1996, 12141,\n",
       "          2000,  2029,  2027,  2020,  9382,  6086,  1010,  5327,  2023,  2052,\n",
       "         10956,  1996,  2966,  3949,  1997,  8643,  1012, 12295,  3389,  1044,\n",
       "          1012,  5226,  1010,  2280,  3472,  1997,  1996,  2160,  4454,  2837,\n",
       "          1010,  2409,  3438,  2781,  6398, 23920,  2358, 28083,  1010,  1000,\n",
       "          1996,  2250,  2486,  2003, 26268,  2075,  2035,  2592,  2055,  2181,\n",
       "          4868,  1999,  2344,  2000,  4047,  3209,  2013,  1037,  9870,  1012,\n",
       "          1000,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "174dcdce-caea-4b31-9139-4dc4dc0485fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=384, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2dd781f-c2c5-4388-ac12-1e3511634879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.4820, -5.9404, -6.2120, -6.1143, -6.5224, -6.4951, -6.2917, -6.3820,\n",
       "         -6.5194, -6.7537, -6.2008, -6.3057, -6.4222, -6.4810, -0.4820, -5.5818,\n",
       "         -5.7127, -6.4838, -5.9010, -6.0856, -6.0827, -6.3097, -6.4831, -6.1578,\n",
       "         -6.2008, -6.4419, -5.9325, -5.5235, -5.9732, -6.4012, -6.6175, -6.5242,\n",
       "         -6.0292, -6.4287, -6.0237, -5.4262, -4.9336, -6.4591, -5.5458, -4.5965,\n",
       "         -6.0965, -5.3848, -6.1986, -5.7813, -5.3709, -0.0967, -3.6305, -5.5801,\n",
       "         -2.8863, -5.5696,  0.0861, -5.0735, -2.8041, -3.2378,  6.5888, -0.5763,\n",
       "         -1.1910,  2.4895,  0.1463,  4.1906, -1.8216, -0.0463, -5.4365, -5.1538,\n",
       "         -5.5951, -6.0921, -6.2618, -6.3710, -6.2747, -5.7789, -6.4970, -6.4495,\n",
       "         -6.2545, -6.6000, -6.4817, -6.4268, -6.4884, -6.3635, -6.2562, -6.5128,\n",
       "         -6.4415, -6.6112, -6.3148, -6.3446, -6.8477, -5.9043, -6.4874, -6.6654,\n",
       "         -6.4378, -6.4005, -6.3099, -6.2191, -6.7656, -6.4703, -6.4687, -6.4831,\n",
       "         -4.9352, -6.5111, -6.3306, -6.6558, -6.9831, -6.9680, -6.4465, -6.2893,\n",
       "         -6.0121, -6.4803, -6.4756, -6.0502, -6.5058, -6.6713, -6.5684, -6.1859,\n",
       "         -6.4539, -6.4879, -6.5260, -6.7620, -6.7823, -6.5785, -6.1419, -6.4833,\n",
       "         -6.4755, -6.5008, -6.5386, -6.7791, -6.4410, -6.4611, -6.4583, -6.7551,\n",
       "         -6.8670, -5.8719, -5.9943, -6.7303, -6.1312, -6.1893, -6.2868, -6.3172,\n",
       "         -6.2946, -6.5369, -6.4914, -6.7502, -6.6072, -6.4082, -6.7176, -6.3489,\n",
       "         -6.4275, -6.3015, -6.3812, -6.3948, -6.6457, -6.9567, -6.5195, -6.2427,\n",
       "         -6.1972, -6.3346, -6.3461, -6.4480, -6.3168, -6.3859, -6.5576, -5.9615,\n",
       "         -6.5165, -6.1290, -6.5967, -6.9680, -6.8173, -5.7011, -6.0831, -6.0428,\n",
       "         -6.2535, -6.4083, -6.3562, -6.4950, -6.5098, -6.5071, -6.7817, -6.7672,\n",
       "         -5.7865, -5.7569, -5.7132, -6.4003, -6.1213, -6.4398, -6.1772, -6.8230,\n",
       "         -6.9537, -6.5846, -6.1461, -5.7941, -5.8649, -6.5066, -6.3936, -6.5133,\n",
       "         -6.4720, -6.5814, -6.4607, -6.4804, -6.3157, -6.0253, -6.5988, -6.5927,\n",
       "         -6.6548, -6.9350, -6.5541, -6.4327, -6.4150, -6.5901, -6.4517, -6.5193,\n",
       "         -6.3497, -6.8527, -6.9698, -6.9144, -6.2016, -6.4439, -6.2456, -6.0947,\n",
       "         -6.5262, -6.4521, -6.4014, -6.5620, -6.4957, -6.4853, -6.4589, -6.5335,\n",
       "         -6.4319, -6.7173, -6.9058, -6.2722, -6.3455, -6.3468, -6.2674, -6.3380,\n",
       "         -6.3258, -6.5878, -6.5012, -6.6106, -6.8690, -6.0947, -6.0853, -6.5197,\n",
       "         -6.6816, -6.6372, -6.7403, -6.2364, -6.4089, -6.5629, -6.2013, -6.3653,\n",
       "         -6.5288, -6.8093, -6.8345, -6.4905, -6.0912, -6.6778, -6.4388, -6.1766,\n",
       "         -6.4428, -6.7380, -6.6223, -6.2700, -6.0169, -5.9708, -6.7105, -6.3784,\n",
       "         -6.1689, -6.4427, -6.2820, -6.5095, -6.4428, -6.2260, -6.6279, -6.3991,\n",
       "         -6.4478, -6.4181, -6.3223, -6.6457, -6.3996, -6.3713, -6.7171, -6.8474,\n",
       "         -6.7107, -0.4820]]), end_logits=tensor([[ 0.0376, -6.2967, -6.2359, -6.4589, -5.9785, -6.1539, -6.3142, -6.2074,\n",
       "         -6.0861, -5.5189, -6.2517, -6.3076, -5.9661, -6.1248,  0.0376, -6.4850,\n",
       "         -5.7961, -5.7732, -5.7663, -6.2239, -6.0937, -5.7001, -6.1083, -6.3125,\n",
       "         -5.8604, -6.1807, -5.8984, -6.3517, -6.3420, -6.0595, -5.4928, -6.1127,\n",
       "         -6.1434, -5.1780, -5.9419, -6.1952, -4.4741, -5.8974, -5.9913, -6.3248,\n",
       "         -5.2776, -5.6960, -5.3836, -2.0631,  0.3515, -5.4174, -5.2529, -4.7250,\n",
       "         -6.3011, -6.3327, -5.1048, -6.3885, -5.9280, -6.1813, -0.3886, -3.7012,\n",
       "         -1.2272, -2.0356, -0.5812, -2.0380, -2.2948,  7.0419,  0.8632, -6.2775,\n",
       "         -6.5706, -6.4826, -6.3699, -6.0517, -6.3156, -6.7207, -6.1173, -6.2938,\n",
       "         -6.4302, -5.6127, -6.2345, -6.2873, -5.7374, -6.3120, -6.3754, -5.8221,\n",
       "         -5.9909, -5.5348, -6.3016, -4.7683, -4.9178, -6.6841, -6.2987, -5.9292,\n",
       "         -6.2895, -6.4713, -6.4172, -6.4519, -5.4016, -6.3175, -6.1902, -6.3045,\n",
       "         -6.1986, -5.4807, -6.2536, -5.9987, -4.9704, -5.4392, -6.3272, -6.4349,\n",
       "         -6.6167, -6.2335, -6.3408, -6.6715, -6.3391, -6.0135, -6.2228, -6.6126,\n",
       "         -6.3855, -6.3628, -6.3491, -5.9183, -5.8877, -6.2987, -6.6032, -6.3407,\n",
       "         -6.3379, -6.3336, -6.3066, -5.6832, -6.3613, -6.3360, -6.2072, -5.3520,\n",
       "         -5.3779, -6.5915, -6.5643, -5.3621, -6.3190, -6.4054, -6.4220, -6.3324,\n",
       "         -6.2773, -6.1991, -6.1469, -5.9394, -6.2439, -6.2093, -5.5869, -6.3649,\n",
       "         -6.3994, -6.4797, -6.2519, -6.4040, -5.3590, -5.4726, -6.2399, -6.4268,\n",
       "         -6.4936, -6.3593, -6.2906, -6.2661, -6.2839, -5.9170, -6.1345, -6.2564,\n",
       "         -6.2478, -6.4886, -6.1180, -4.9785, -5.0158, -6.5933, -6.1853, -6.3398,\n",
       "         -5.9098, -6.3356, -6.3779, -5.8647, -6.1395, -6.2646, -5.5698, -5.5538,\n",
       "         -6.3436, -6.5360, -5.5192, -6.2436, -6.3176, -6.1347, -6.3544, -5.3626,\n",
       "         -5.4242, -6.1958, -6.4804, -6.5821, -5.1821, -6.1808, -6.1725, -6.3183,\n",
       "         -6.2941, -5.9685, -6.2344, -6.0960, -6.3557, -6.5778, -5.9815, -6.2085,\n",
       "         -5.8661, -5.0169, -6.0879, -6.4361, -6.4213, -6.1985, -6.2726, -6.3325,\n",
       "         -6.3884, -5.6664, -5.1706, -5.3379, -6.4083, -6.4136, -6.3974, -6.5017,\n",
       "         -6.0015, -6.3910, -6.4590, -5.9765, -6.3192, -6.3913, -6.3221, -6.3450,\n",
       "         -6.3638, -5.7460, -5.6399, -6.3804, -6.4513, -6.3788, -6.3869, -6.3232,\n",
       "         -6.2728, -5.9923, -6.3062, -5.5061, -5.6318, -6.2326, -6.3711, -6.1498,\n",
       "         -6.0018, -5.6580, -5.8831, -6.4441, -6.2402, -6.2480, -6.2194, -6.2966,\n",
       "         -6.1919, -5.7250, -5.8146, -6.2302, -6.3961, -5.7751, -6.0112, -6.3588,\n",
       "         -6.2726, -5.6104, -6.0433, -6.1631, -6.5377, -6.4589, -5.6546, -6.2930,\n",
       "         -6.4329, -6.2093, -6.3890, -6.1030, -6.3581, -6.3808, -5.3690, -6.3312,\n",
       "         -6.2747, -6.3519, -6.3387, -5.9879, -6.3820, -6.4370, -5.8003, -5.6793,\n",
       "         -5.8762,  0.0376]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e0cf16e-e65e-4dbd-a9a7-626e7b1516b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(54), tensor(62))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eed2161c-bb10-4245-8a2c-62be2ed387b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2577,  2899,  2118,  2375,  2934,  5655, 10722, 12866])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "answer_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccc1c323-1b76-473a-81c0-69bd1a7ab90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  george washington university law professor jonathan turley\n"
     ]
    }
   ],
   "source": [
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85e151e9-9b1d-4748-b1c4-489e51e803fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You have 1 GPU(s).\n",
      "GPU Device Name: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You have\", torch.cuda.device_count(), \"GPU(s).\")\n",
    "    print(\"GPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f718d03-b1cb-43b3-8fc4-91dcf364d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 20.3074 seconds\n",
      "GPU Time: 0.4351 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate a random matrix\n",
    "size = 15000 # recommended to set max 15000 to test on CPU, max 35000 on GPU (matrix multiplication computation = size^3)\n",
    "a = torch.randn(size, size)\n",
    "b = torch.randn(size, size)\n",
    "\n",
    "# Test on CPU \n",
    "start_time = time.time()\n",
    "c_cpu = torch.matmul(a, b)\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "# Generate a random matrix\n",
    "size = 35000 # recommended to set max 15000 to test on CPU, max 35000 on GPU (matrix multiplication computation = size^3)\n",
    "a = torch.randn(size, size)\n",
    "b = torch.randn(size, size)\n",
    "\n",
    "# Test on GPU\n",
    "if torch.cuda.is_available():\n",
    "    a_gpu = a.to('cuda')\n",
    "    b_gpu = b.to('cuda')\n",
    "    start_time = time.time()\n",
    "    c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "    gpu_time = time.time() - start_time\n",
    "    print(f\"GPU Time: {gpu_time:.4f} seconds\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23e8b999-8f13-47a1-8250-cab93c023205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96cb9446-576e-4166-bc80-9853aefda005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_steps=None,\n",
       "eval_strategy=no,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./smaller_bert_finetuned\\runs\\Oct12_12-26-30_alfrizz,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=5,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./smaller_bert_finetuned,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['mlflow'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./smaller_bert_finetuned,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8, # batch size for training on each device (e.g., GPU).\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8163c51c-1724-4570-b838-6b4093a61e46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x165dec32290>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "tokenized_datasets = []\n",
    "\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "# When using the Trainer class from Hugging Face Transformers with TrainingArguments, the model, data, and training steps are handled for you. If CUDA is available and detected, the Trainer will automatically move the model and data to the GPU. You don't need to manually set .to(device) on the model.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d928e249-2aaa-40c9-b3c0-7aac99a0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98b817ae-9314-484f-94c0-9f3c07106629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "774792d6-ea1d-488d-9030-75fade5c2712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "dataset = load_dataset('emotion', trust_remote_code=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "af570afc-ff25-4eb2-a2d8-724e3003eab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Function to split each subset into 10\n",
    "def get_subset(dataset, split_name, fraction=0.1): # it means 0.1 (10) used for the train\n",
    "    return dataset[split_name].train_test_split(train_size=fraction, seed=42)['train'] # so we use the 'train' subset (10%) of the dataset\n",
    "\n",
    "# Create smaller dataset with 10% of each original subset\n",
    "small_dataset = DatasetDict({\n",
    "    'train': get_subset(dataset, 'train'),\n",
    "    'validation': get_subset(dataset, 'validation'),\n",
    "    'test': get_subset(dataset, 'test')\n",
    "})\n",
    "\n",
    "print(small_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ad064e8a-3580-43db-ad2d-fdbfd09762d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 6\n",
      "Labels: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique labels\n",
    "unique_labels = small_dataset['train'].features['label'].names\n",
    "\n",
    "# Print the number of unique labels and their names\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f027f45f-ade8-4734-afc1-a7211dc77713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i feel like im the only one whos caring about whats good for me right now 2\n"
     ]
    }
   ],
   "source": [
    "# Print the first text string in the training set\n",
    "print(small_dataset['train'][0]['text'], small_dataset['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "daa338c6-66a5-4af4-989a-dc69bcbe5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Text: i feel like im the only one whos caring about whats good for me right now | Lavel_index: 2 Label: love\n",
      "Example 2: Text: im feeling determined now to push through any hiccups and reach my ultimate goal of being within the healthy weight range kg for my height | Lavel_index: 1 Label: joy\n",
      "Example 3: Text: i just feel more dazed and alone in the end | Lavel_index: 5 Label: surprise\n",
      "Example 4: Text: i feel there is also a difference between loving someone and being in love with someone | Lavel_index: 2 Label: love\n",
      "Example 5: Text: im home alone with my son and im feeling sad | Lavel_index: 0 Label: sadness\n"
     ]
    }
   ],
   "source": [
    "# Print a few examples with their text and label\n",
    "for i in range(5):\n",
    "    text = small_dataset['train'][i]['text']\n",
    "    label_index = small_dataset['train'][i]['label']\n",
    "    label_text = unique_labels[label_index]\n",
    "    print(f\"Example {i+1}: Text: {text} | Lavel_index: {label_index} Label: {label_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d280e72a-80ef-4fc7-b518-8d72e086dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 2361.42 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode your dataset\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "emotions_encoded = small_dataset.map(encode, batched=True) # by using the .map function, you're applying the encode function (tokenization) to every sample in your dataset.\n",
    "\n",
    "emotions_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "de80aee8-81ce-4070-8fd1-d9a544a9b46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x165f9f785b0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n",
    "    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ee495e68-4020-4df7-b410-a52280e040b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys of the first example in the training dataset\n",
    "print(emotions_encoded[\"train\"][0].keys())\n",
    "\n",
    "# Tokenizing single sequences doesn't require segment differentiation, hence no 'token_type_ids'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a661d92a-9e65-4877-8cdf-8a156b75494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_labels = set()\n",
    "# for example in emotions_encoded[\"train\"]:\n",
    "#     unique_labels.add(example[\"label\"])\n",
    "# print(f\"Unique labels: {sorted(list(unique_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "acffd65a-a5ff-45ca-9b28-cb7b89a65b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 08:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 15:54:30 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id 1b816ed5dc594b8aab83cb89da609b77: Failed to log run data: Exception: Changing param values is not allowed. Param with key='problem_type' was already logged with value='None' for run ID='1b816ed5dc594b8aab83cb89da609b77'. Attempted logging new value 'single_label_classification'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.15114902210235595, metrics={'train_runtime': 539.185, 'train_samples_per_second': 14.837, 'train_steps_per_second': 1.855, 'total_flos': 1059814785024000.0, 'train_loss': 0.15114902210235595, 'epoch': 5.0})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop to fine-tune the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1221a882-ffd6-40ad-84c0-df2bf9c33c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2009,  1005,  1055,  2601,  1998, 16373,  2648,   102],\n",
       "         [  101,  1045,  2293, 18134,   999,   102,     0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  6015,  2000,  4607,  2045,   102],\n",
       "         [  101,  2009,  1005,  1055,  2428,  9223,   102,     0,     0],\n",
       "         [  101,  2008,  3084,  2033,  2514,  2204,   102,     0,     0]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\", \"I'm scared to enter there\", \"It's really unexpected\", \"That makes me feel good\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()} # move tokenized tensors to device\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "597455a9-ff17-434d-9e61-b902021d03d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3327, -1.0188, -3.3314,  3.3021, -1.0555, -5.1176],\n",
       "        [-2.0071, -0.0838,  5.6982, -1.9928, -3.1754, -2.0236],\n",
       "        [-2.0161, -1.7759, -3.0611, -1.4618,  6.8023, -2.1666],\n",
       "        [-2.6330, -1.8602, -2.7156, -2.1896,  1.2098,  4.5404],\n",
       "        [-2.6366,  7.8942, -1.6927, -2.4569, -2.4308, -2.4874]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device) # Move model to the device (cuda)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e72e38d8-6a31-4441-97ba-33476121482d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 4, 5, 1]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain class labels from raw predictions\n",
    "predicted_label_index = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "predicted_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6fd845ce-4035-4633-bbd9-ce458944a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input Text 1: It's dark and rainy outside\n",
      "Predicted Label Index: 3\n",
      "Predicted Label: anger\n",
      "\n",
      " Input Text 2: I love penguins!\n",
      "Predicted Label Index: 2\n",
      "Predicted Label: love\n",
      "\n",
      " Input Text 3: I'm scared to enter there\n",
      "Predicted Label Index: 4\n",
      "Predicted Label: fear\n",
      "\n",
      " Input Text 4: It's really unexpected\n",
      "Predicted Label Index: 5\n",
      "Predicted Label: surprise\n",
      "\n",
      " Input Text 5: That makes me feel good\n",
      "Predicted Label Index: 1\n",
      "Predicted Label: joy\n"
     ]
    }
   ],
   "source": [
    "for i, predicted_label in enumerate(predicted_label_index):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label Index: {predicted_label_index[i]}\")\n",
    "    print(f\"Predicted Label: {unique_labels[predicted_label_index[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "959b5687-013e-4ada-8145-323df266236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "test_examples = [{'text': 'I love this product!', 'label': 1},\n",
    "                 {'text': 'The service was terrible.', 'label': 0},\n",
    "                 {'text': 'This movie is amazing.', 'label': 1},\n",
    "                 {'text': \"I'm disappointed with the quality.\", 'label': 0},\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "04be27ae-994b-496d-999f-55115b8ad3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998855590820312},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996507167816162},\n",
       " {'label': 'POSITIVE', 'score': 0.9998838901519775},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997726082801819}]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9a1e602e-ef39-4c73-a089-2bba48d0b60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ab1baf2e-7d14-416a-9838-454af1507e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c3a8c898-4d6e-4403-9aa9-538ad6823bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# assuming true_labels and predicted_labels are defined\n",
    "result = accuracy_score(true_labels, predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f95310d6-52fe-4d68-be6e-cb76a4a3be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "08298bec-b7c1-4d01-92cd-75964b962d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "\n",
      "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n",
      "Precision = TP / (TP + FP)\n",
      "where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
      "\n",
      "\n",
      "Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\n",
      "Recall = TP / (TP + FN)\n",
      "Where TP is the true positives and FN is the false negatives.\n",
      "\n",
      "\n",
      "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
      "F1 = 2 * (precision * recall) / (precision + recall)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the accuracy, precision, recall and F1 score metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "69cf56cc-d82f-43c4-abe8-7151fcd42113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"Fantastic hotel, exceeded expectations!\",\n",
    "    \"Quiet despite central location, great stay.\",\n",
    "    \"Friendly staff, welcoming atmosphere.\",\n",
    "    \"Spacious, comfy room‚Äîa perfect retreat.\",\n",
    "    \"Cleanliness could improve, overall decent stay.\",\n",
    "    \"Disappointing stay, noisy and unclean room.\",\n",
    "    \"Terrible service, unfriendly staff, won't return.\"\n",
    "]\n",
    "\n",
    "test_labels = [1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ba49785c-9eff-4c15-a056-5768d5dca1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sentiment_analysis([example for example in test_examples])\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "caac1d45-969b-4a01-b29d-70c105e8bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.888888888888889}\n",
      "{'precision': 0.8}\n",
      "{'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(f1.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=test_labels, predictions=predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "25abff60-d03c-403a-823e-b7a63f887ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "242dadc3-aaaa-45e6-b876-3baeff5a0989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11297,  4283,  1366,   905,   326,   416, 25054,   220]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Current stock data show that by 2030 \"\n",
    "\n",
    "# Encode the prompt, generate text and decode it\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b7e52113-d928-4378-b974-91b02f1ea68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11297,  4283,  1366,   905,   326,   416, 25054,   220,  1849,  1169,\n",
       "          2811,  1605,   481,   423,   257,  2010,  2861,   286,   720,    16,\n",
       "            13,    18, 12989,    11,   393,   720,    16,    13,    18, 12989,\n",
       "           517,   621,   262,  2811,  1605,   287,   262,   938,  5707,    13,\n",
       "           198,   464,  2811,  1605,   481,   423,   257,  2010,  2861,   286]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(prompt_ids, max_length=50)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "48beed3e-4f7b-458c-a4be-bdf80b607cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  Current stock data show that by 2030 ¬†the average American will have a net worth of $1.3 trillion, or $1.3 trillion more than the average American in the last decade.\n",
      "The average American will have a net worth of\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "625510fd-e877-4f02-91cb-c86754fc900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 40.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  3308.342305777208\n",
      "212\n",
      "212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[139.0388641357422,\n",
       " 2412.967041015625,\n",
       " 2127.130126953125,\n",
       " 2127.130126953125,\n",
       " 1623.478515625,\n",
       " 1117.8956298828125,\n",
       " 1159.115234375,\n",
       " 12869.310546875,\n",
       " 503.7154846191406,\n",
       " 1159.115234375]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id=model_name,\n",
    "                             predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])\n",
    "print(len(results['perplexities']))\n",
    "print(len(generated_text))\n",
    "results['perplexities'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ce34025a-2472-4407-bcdb-1c4fa65cde50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 33.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  8.184481620788574\n",
      "1\n",
      "212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.184481620788574]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if using squared brackets around 'generated_text' to compute the perplexity\n",
    "\n",
    "results = perplexity.compute(model_id=model_name, \n",
    "                             predictions=[generated_text])\n",
    "\n",
    "# Print results\n",
    "print(\"Perplexity: \", results['mean_perplexity'])\n",
    "print(len(results['perplexities']))\n",
    "print(len(generated_text))\n",
    "results['perplexities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8731511c-2b79-4b53-b346-4f0311d5cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE results:  {'rouge1': 0.7719298245614034, 'rouge2': 0.6181818181818182, 'rougeL': 0.736842105263158, 'rougeLsum': 0.736842105263158}\n"
     ]
    }
   ],
   "source": [
    "# ! pip install rouge_score\n",
    "\n",
    "# Load the rouge metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions,references=references)\n",
    "print(\"ROUGE results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "545c4aa1-4079-4cd1-b577-16624aed5ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.02k/7.02k [00:00<?, ?B/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Alienware\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteor:  {'meteor': 0.5350702240481536}\n"
     ]
    }
   ],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
    "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=llm_outputs, references=references)\n",
    "print(\"Meteor: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "118e40f7-ead6-48ba-89cc-92381bee75ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM results:  {'exact_match': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(\"EM results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "28dc18bd-fae3-4578-bbdc-002f53b694d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_1 = \"Hola, ¬øc√≥mo est√°s?\"\n",
    "\n",
    "reference_1 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
    "     ]\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¬øc√≥mo est√°s?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd083c-774c-4f2b-919e-c4a7cf655fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason why there are multiple reference sentences for each input sentence is because of the inherent ambiguity and variability in translation. There can be several equally correct translations for a given sentence, depending on factors like context, tone, and style. By providing multiple reference translations, we can capture some of this variability and get a more robust estimate of the model‚Äôs performance.\n",
    "\n",
    "# In the code you posted, the BLEU score is being calculated for the translations. The BLEU score is a metric that measures the quality of a translation by comparing it to one or more reference translations. It does this by counting the number of n-gram matches between the translation and the reference(s), and then normalizing by the total number of n-grams in the translation. The more the translation resembles the reference(s), the higher the BLEU score will be.\n",
    "\n",
    "# In your example, the first input sentence ‚ÄúHola, ¬øc√≥mo est√°s?‚Äù is translated and then the translation is compared to two reference translations: ‚ÄúHello, how are you?‚Äù and ‚ÄúHi, how are you?‚Äù. The BLEU score is then computed for this translation.\n",
    "\n",
    "# The same process is repeated for the second set of input sentences and references. The final BLEU score is a measure of how well the translations match the reference translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "49442a73-3f15-403b-9025-7b088cbfb875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: Hey, how are you?\n",
      "{'bleu': 0.7598356856515925, 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the first input sentence\n",
    "translated_output = translator(input_sentence_1)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "# Calculate BLEU metric for translation quality\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "824753e7-3f19-493b-b701-15d12927718b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hey, how are you?'},\n",
       " {'translation_text': \"I'm great, thanks.\"}]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translated_outputs = translator(input_sentences_2)\n",
    "\n",
    "translated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dd4e780d-2c9f-4a28-bec9-3bf185cb4bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey, how are you?', \"I'm great, thanks.\"]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "beb7ab11-2129-46c5-823e-ca6874125b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.8627788640890415, 'precisions': [0.9090909090909091, 0.8888888888888888, 0.8571428571428571, 0.8], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 11, 'reference_length': 11}\n"
     ]
    }
   ],
   "source": [
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4abb9c7a-cd48-4bcd-86e6-0073e6c38e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 2)\n",
       "      (wpe): Embedding(1024, 2)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-1): 2 x GPT2Block(\n",
       "          (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=2, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install trl # (Transformer Reinforcement Learning) library\n",
    "from trl import PPOTrainer, PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# PPOTrainer and PPOConfig are part of the trl (Transformer Reinforcement Learning) library. They facilitate training models using the Proximal Policy Optimization (PPO) algorithm, which is a reinforcement learning technique. PPO can fine-tune transformer models to optimize for specific objectives, such as generating more human-like text.\n",
    "\n",
    "# PPOTrainer:\n",
    "# Purpose: Manages the training process using PPO.\n",
    "# Functionality: Handles the policy updates and interactions between the agent (model) and environment (data).\n",
    "# Usage: Trains models to improve performance by optimizing rewards through a structured approach.\n",
    "\n",
    "# PPOConfig:\n",
    "# Purpose: Configures the PPO training parameters.\n",
    "# Components: Includes settings like learning rate, batch size, number of epochs, and more.\n",
    "# Usage: Customizes the PPO training process to suit specific needs and goals.\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0787e0ec-e281-424f-93d2-f4f279cac82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 2)\n",
       "      (wpe): Embedding(1024, 2)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-1): 2 x GPT2Block(\n",
       "          (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=2, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a reference model\n",
    "\n",
    "# When you call create_reference_model(model), it creates a copy of the model and freezes its parameters. This means that the weights of the reference model will not be updated during training.\n",
    "# This reference model is then used to compare with the updated model at each step of the training process. The idea is to ensure that the policy (i.e., the behavior of the model) does not change too drastically from one update to the next\n",
    "\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "47816608-d588-4f58-83ef-b6ab1aa9bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if the parameters of a model are frozen, you can iterate over the parameters and check their requires_grad attribute. Here‚Äôs a small function that can do this:\n",
    "\n",
    "def check_if_frozen(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} is not frozen\")\n",
    "        else:\n",
    "            print(f\"{name} is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0d672e16-34e3-4e81-90d2-140eb381a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model.transformer.wte.weight is not frozen\n",
      "pretrained_model.transformer.wpe.weight is not frozen\n",
      "pretrained_model.transformer.h.0.ln_1.weight is not frozen\n",
      "pretrained_model.transformer.h.0.ln_1.bias is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.weight is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.bias is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.h.0.ln_2.weight is not frozen\n",
      "pretrained_model.transformer.h.0.ln_2.bias is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.weight is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.bias is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.h.1.ln_1.weight is not frozen\n",
      "pretrained_model.transformer.h.1.ln_1.bias is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.weight is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.bias is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.h.1.ln_2.weight is not frozen\n",
      "pretrained_model.transformer.h.1.ln_2.bias is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.weight is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.bias is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.weight is not frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.bias is not frozen\n",
      "pretrained_model.transformer.ln_f.weight is not frozen\n",
      "pretrained_model.transformer.ln_f.bias is not frozen\n",
      "v_head.summary.weight is not frozen\n",
      "v_head.summary.bias is not frozen\n"
     ]
    }
   ],
   "source": [
    "check_if_frozen(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "06e860b1-5b6e-4241-8732-149251c6f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model.transformer.wte.weight is frozen\n",
      "pretrained_model.transformer.wpe.weight is frozen\n",
      "pretrained_model.transformer.h.0.ln_1.weight is frozen\n",
      "pretrained_model.transformer.h.0.ln_1.bias is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.weight is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_attn.bias is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.0.attn.c_proj.bias is frozen\n",
      "pretrained_model.transformer.h.0.ln_2.weight is frozen\n",
      "pretrained_model.transformer.h.0.ln_2.bias is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.weight is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_fc.bias is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.0.mlp.c_proj.bias is frozen\n",
      "pretrained_model.transformer.h.1.ln_1.weight is frozen\n",
      "pretrained_model.transformer.h.1.ln_1.bias is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.weight is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_attn.bias is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.1.attn.c_proj.bias is frozen\n",
      "pretrained_model.transformer.h.1.ln_2.weight is frozen\n",
      "pretrained_model.transformer.h.1.ln_2.bias is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.weight is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_fc.bias is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.weight is frozen\n",
      "pretrained_model.transformer.h.1.mlp.c_proj.bias is frozen\n",
      "pretrained_model.transformer.ln_f.weight is frozen\n",
      "pretrained_model.transformer.ln_f.bias is frozen\n",
      "v_head.summary.weight is frozen\n",
      "v_head.summary.bias is frozen\n"
     ]
    }
   ],
   "source": [
    "check_if_frozen(model_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "75d53836-3a01-4e6d-b898-a5e2be57acc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='sshleifer/tiny-gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "if tokenizer._pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fa787b53-013d-4599-95c6-16da82810d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with=None, task_name=None, model_name='gpt2', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=1, forward_batch_size=None, mini_batch_size=1, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, gradient_checkpointing=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=1, global_backward_batch_size=None, global_batch_size=None)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize trainer configuration\n",
    "# this code is setting up a configuration for a PPO trainer with specific batch and mini-batch sizes. This configuration would be used when training a model using the PPO algorithm. \n",
    "\n",
    "ppo_config = PPOConfig(batch_size=1, mini_batch_size=1)\n",
    "\n",
    "ppo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1863c35b-4fb9-443f-b302-c44d4be31764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:266: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<trl.trainer.ppo_trainer.PPOTrainer at 0x165ce525690>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOTrainer instance\n",
    "# this line of code is setting up a PPO trainer with a specific configuration, model, reference model, and tokenizer. The trainer can then be used to train the model using the PPO algorithm.Typically, the trainer would have a method like train() that you can call to start the training process. The training process involves repeatedly sampling data, using the data to update the model, and then evaluating the performance of the model. The goal is to improve the model‚Äôs performance on some task, such as generating text. The PPO algorithm is particularly well-suited to tasks where the data is sequential or temporal in nature. It‚Äôs also known for its stability and efficiency, which makes it a popular choice for many reinforcement learning tasks.\n",
    "# N.B. Even though PPO is on-policy (it can learn only from actions taken by current policy), the reference model is not used to gather new data but to stabilize and guide the policy updates during training.\n",
    "\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "\n",
    "ppo_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "771fbe5d-f5d9-4976-8b43-bddbe8a6dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10019,   614,    11,   314,   220]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Next year, I \"\n",
    "\n",
    "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2792a051-b10a-4f39-a1ed-1c49a1d8e5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15197, 24174, 12929, 42507,  4255, 19083,  7718, 30414, 30593, 48668,\n",
       "         46900, 32275, 24018, 46815, 18304, 28796,  3941,  8724, 31644, 19266]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.core import respond_to_batch\n",
    "\n",
    "response  = respond_to_batch(model.to(device), input.to(device)) # function to generate a response from the model. The function takes the model and the encoded input as arguments.\n",
    "\n",
    "# this code is using a pretrained language model to generate a response to a given prompt. The response is calculated by feeding the encoded input into the model and then decoding the model‚Äôs output back into text. The response represents what the model thinks is the most likely continuation of the input prompt. The exact details of how the response is calculated depend on the specifics of the model and the respond_to_batch function. \n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "091fd2f1-bb64-4b24-a6d6-288f9d8e0110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.)]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a more complex scenario, you might want to design a reward function that gives higher rewards for better responses and lower rewards for worse ones. This would require a way to evaluate the quality of the responses, which could be based on various factors such as the relevance of the response to the input, the grammatical correctness of the response, etc. This is typically the challenging part in reinforcement learning - designing a good reward function.\n",
    "\n",
    "import torch\n",
    "reward = [torch.tensor(1.0)]\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0cf05e00-7038-4045-a53b-1e5ca26448af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1304: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  std_scores = data[\"scores\"].std()\n",
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1331: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
      "C:\\Users\\Alienware\\miniconda3\\envs\\py310\\lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1334: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective/kl': 0.0,\n",
       " 'objective/kl_dist': 0.0,\n",
       " 'objective/logprobs': array([[-10.803115, -10.834714, -10.795107, -10.808006, -10.81736 ,\n",
       "         -10.846797, -10.826721, -10.803664, -10.758843, -10.889929,\n",
       "         -10.850676, -10.812679, -10.761826, -10.849661, -10.760221,\n",
       "         -10.799231, -10.85004 , -10.812998, -10.813044, -10.78842 ,\n",
       "         -10.863429, -10.875851, -10.839982, -10.794012]], dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-10.803115, -10.834714, -10.795107, -10.808006, -10.81736 ,\n",
       "         -10.846797, -10.826721, -10.803664, -10.758843, -10.889929,\n",
       "         -10.850676, -10.812679, -10.761826, -10.849661, -10.760221,\n",
       "         -10.799231, -10.85004 , -10.812998, -10.813044, -10.78842 ,\n",
       "         -10.863429, -10.875851, -10.839982, -10.794012]], dtype=float32),\n",
       " 'objective/kl_coef': 0.2,\n",
       " 'objective/entropy': 216.41537475585938,\n",
       " 'ppo/mean_non_score_reward': 0.0,\n",
       " 'ppo/mean_scores': 1.0,\n",
       " 'ppo/std_scores': nan,\n",
       " 'tokens/queries_len_mean': 5.0,\n",
       " 'tokens/queries_len_std': nan,\n",
       " 'tokens/queries_dist': 5.0,\n",
       " 'tokens/responses_len_mean': 20.0,\n",
       " 'tokens/responses_len_std': nan,\n",
       " 'tokens/responses_dist': 20.0,\n",
       " 'ppo/loss/policy': 4.275143055565422e-06,\n",
       " 'ppo/loss/value': 0.5741720795631409,\n",
       " 'ppo/loss/total': 0.0574214830994606,\n",
       " 'ppo/policy/entropy': 10.824563980102539,\n",
       " 'ppo/policy/approxkl': 2.458734194021872e-08,\n",
       " 'ppo/policy/policykl': -1.4424324490391882e-06,\n",
       " 'ppo/policy/clipfrac': 0.0,\n",
       " 'ppo/policy/advantages': array([-1.5005555 , -1.4941624 , -1.4874328 , -1.480349  , -1.4558209 ,\n",
       "         0.07201599, -1.7019345 ,  0.03634082,  0.2546579 ,  0.35631695,\n",
       "        -1.3619965 ,  0.47078466,  0.56332564,  0.6995626 , -1.0647857 ,\n",
       "        -1.0271426 , -0.9639559 ,  0.89558953, -0.38717458,  1.0299674 ,\n",
       "         1.2522172 , -0.48500133,  1.421842  ,  1.3951919 , -1.5005555 ,\n",
       "        -1.4941624 , -1.4874328 , -1.480349  , -1.4558209 ,  0.07201599,\n",
       "        -1.7019345 ,  0.03634082,  0.2546579 ,  0.35631695, -1.3619965 ,\n",
       "         0.47078466,  0.56332564,  0.6995626 , -1.0647857 , -1.0271426 ,\n",
       "        -0.9639559 ,  0.89558953, -0.38717458,  1.0299674 ,  1.2522172 ,\n",
       "        -0.48500133,  1.421842  ,  1.3951919 , -1.5005555 , -1.4941624 ,\n",
       "        -1.4874328 , -1.480349  , -1.4558209 ,  0.07201599, -1.7019345 ,\n",
       "         0.03634082,  0.2546579 ,  0.35631695, -1.3619965 ,  0.47078466,\n",
       "         0.56332564,  0.6995626 , -1.0647857 , -1.0271426 , -0.9639559 ,\n",
       "         0.89558953, -0.38717458,  1.0299674 ,  1.2522172 , -0.48500133,\n",
       "         1.421842  ,  1.3951919 , -1.5005555 , -1.4941624 , -1.4874328 ,\n",
       "        -1.480349  , -1.4558209 ,  0.07201599, -1.7019345 ,  0.03634082,\n",
       "         0.2546579 ,  0.35631695, -1.3619965 ,  0.47078466,  0.56332564,\n",
       "         0.6995626 , -1.0647857 , -1.0271426 , -0.9639559 ,  0.89558953,\n",
       "        -0.38717458,  1.0299674 ,  1.2522172 , -0.48500133,  1.421842  ,\n",
       "         1.3951919 ], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': 2.3841858265427618e-08,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99999803,\n",
       "        1.        , 1.0000563 , 0.9999438 , 1.0000162 , 1.0000591 ,\n",
       "        0.9999447 , 0.9999772 , 1.0000515 , 1.0000019 , 0.9999104 ,\n",
       "        1.0000553 , 1.0000391 , 1.0000572 , 0.9999447 , 0.99994564,\n",
       "        0.9999333 , 0.99999714, 1.0003825 , 0.99996287, 1.0000582 ,\n",
       "        1.        , 1.0000153 , 0.999692  , 0.99999714, 1.000001  ,\n",
       "        1.0001116 , 0.9998884 , 1.0000305 , 1.0001183 , 0.9998894 ,\n",
       "        0.9999524 , 1.0001049 , 1.0000048 , 0.99981976, 1.0001106 ,\n",
       "        1.0000772 , 1.0001135 , 0.9998894 , 0.9998913 , 0.99986655,\n",
       "        0.99999434, 1.0007498 , 0.9999209 , 1.0001163 , 0.99999905,\n",
       "        1.0000315 , 0.99937075, 0.99999523, 1.000001  , 1.0001659 ,\n",
       "        0.99983317, 1.0000439 , 1.0001774 , 0.9998341 , 0.9999257 ,\n",
       "        1.0001564 , 1.0000095 , 0.9997311 , 1.0001659 , 1.0001154 ,\n",
       "        1.0001707 , 0.9998341 , 0.9998369 , 0.99980074, 0.9999914 ,\n",
       "        1.0011011 , 0.999877  , 1.0001736 , 0.99999905, 1.0000477 ,\n",
       "        0.9990306 ], dtype=float32),\n",
       " 'ppo/returns/mean': 0.4815755784511566,\n",
       " 'ppo/returns/var': 0.07492037117481232,\n",
       " 'ppo/val/vpred': -0.43659132719039917,\n",
       " 'ppo/val/error': 1.1483441591262817,\n",
       " 'ppo/val/clipfrac': 0.0,\n",
       " 'ppo/val/mean': -0.4380952715873718,\n",
       " 'ppo/val/var': 0.22237072885036469,\n",
       " 'ppo/val/var_explained': -14.327528953552246,\n",
       " 'ppo/learning_rate': 1.41e-05,\n",
       " 'time/ppo/forward_pass': 0.04386758804321289,\n",
       " 'time/ppo/compute_rewards': 0.030444622039794922,\n",
       " 'time/ppo/compute_advantages': 0.05278754234313965,\n",
       " 'time/ppo/optimize_step': 0.4694826602935791,\n",
       " 'time/ppo/calc_stats': 0.03735947608947754,\n",
       " 'time/ppo/total': 0.6425437927246094}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LLM for one step with PPO\n",
    "\n",
    "# The below function ppo_trainer.step([input[0]], [response[0]], reward) is used to update the PPO model based on the inputs, generated responses, and the received rewards. By iteratively calling this function with various inputs, responses, and rewards, the PPOTrainer refines the model to generate better responses over time. \n",
    "\n",
    "# while step() is used for a single step of training, train() is used for full-scale training over multiple epochs. The code is likely a simplified example or a debugging scenario where only a single step of training is being performed. For training a model to completion, you would generally use a train() function or similar.\n",
    "\n",
    "train_stats = ppo_trainer.step([input[0]], [response[0]], reward) # The step function is used to perform one step of training, where the model‚Äôs parameters are updated to maximize the expected reward.\n",
    "\n",
    "train_stats\n",
    "\n",
    "# The train_stats dictionary contains various statistics and metrics that are calculated during the training step. Here‚Äôs a brief explanation of some of the key metrics:\n",
    "# ‚Äòobjective/kl‚Äô: This is the Kullback-Leibler (KL) divergence, which measures how one probability distribution diverges from a second, expected probability distribution. In this case, it‚Äôs 0.0, indicating no divergence.\n",
    "# ‚Äòobjective/logprobs‚Äô: These are the log probabilities of the actions taken by the model. They are used in the calculation of the policy gradient.\n",
    "# ‚Äòppo/mean_scores‚Äô: This is the mean of the scores (rewards) obtained during the training step.\n",
    "# ‚Äòtokens/queries_len_mean‚Äô: This is the average length of the queries processed in the training step.\n",
    "# ‚Äòppo/loss/policy‚Äô, ‚Äòppo/loss/value‚Äô, ‚Äòppo/loss/total‚Äô: These are the losses for the policy, value function, and the total loss respectively. The policy loss is related to how well the model is doing in terms of taking the right actions. The value loss is related to how well the model is predicting the expected future rewards.\n",
    "# ‚Äòppo/policy/entropy‚Äô: This is the entropy of the policy. It‚Äôs a measure of the randomness of the policy. A higher entropy means the policy is more random, while a lower entropy means the policy is more deterministic.\n",
    "# ‚Äòppo/returns/mean‚Äô: This is the mean of the returns (sum of rewards) obtained during the training step.\n",
    "# ‚Äòppo/val/vpred‚Äô: This is the predicted value of the state by the model.\n",
    "# ‚Äòtime/ppo/total‚Äô: This is the total time taken for the training step.\n",
    "# The warnings about degrees of freedom being less than or equal to 0 are due to the standard deviation (std()) function being called on a dataset with insufficient size. This can happen when the batch size or mini-batch size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "16a4396e-4852-44e9-bb28-81079b28b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 training steps\n",
      "20 training steps\n",
      "30 training steps\n",
      "40 training steps\n",
      "50 training steps\n",
      "60 training steps\n",
      "70 training steps\n",
      "80 training steps\n",
      "90 training steps\n",
      "100 training steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective/kl': -0.1894083023071289,\n",
       " 'objective/kl_dist': -0.1894083023071289,\n",
       " 'objective/logprobs': array([[-10.804715, -10.814219, -10.793684, -10.821551, -10.827355,\n",
       "         -10.82836 , -10.841099, -10.872051, -10.734868, -10.863733,\n",
       "         -10.8736  , -10.789813, -10.740524, -10.825425, -10.780062,\n",
       "         -10.819388, -10.859491, -10.851975, -10.810377, -10.876074,\n",
       "         -10.83882 , -10.882045, -10.817121, -10.872608]], dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-10.803115, -10.834714, -10.795107, -10.808006, -10.81736 ,\n",
       "         -10.846797, -10.826721, -10.803664, -10.758843, -10.889929,\n",
       "         -10.850676, -10.812679, -10.761826, -10.849661, -10.760221,\n",
       "         -10.799231, -10.85004 , -10.812998, -10.813044, -10.78842 ,\n",
       "         -10.863429, -10.875851, -10.839982, -10.794012]], dtype=float32),\n",
       " 'objective/kl_coef': 0.1996003957414051,\n",
       " 'objective/entropy': 216.6047821044922,\n",
       " 'ppo/mean_non_score_reward': 0.001890298561193049,\n",
       " 'ppo/mean_scores': 1.0,\n",
       " 'ppo/std_scores': nan,\n",
       " 'tokens/queries_len_mean': 5.0,\n",
       " 'tokens/queries_len_std': nan,\n",
       " 'tokens/queries_dist': 5.0,\n",
       " 'tokens/responses_len_mean': 20.0,\n",
       " 'tokens/responses_len_std': nan,\n",
       " 'tokens/responses_dist': 20.0,\n",
       " 'ppo/loss/policy': -6.613433652091771e-05,\n",
       " 'ppo/loss/value': 0.3904992341995239,\n",
       " 'ppo/loss/total': 0.0389837883412838,\n",
       " 'ppo/policy/entropy': 10.824533462524414,\n",
       " 'ppo/policy/approxkl': 4.249903984288039e-09,\n",
       " 'ppo/policy/policykl': 1.1718273526639678e-05,\n",
       " 'ppo/policy/clipfrac': 0.0,\n",
       " 'ppo/policy/advantages': array([-1.0480705 , -1.0210106 , -0.9925265 , -0.9625431 , -1.3140193 ,\n",
       "        -1.1138552 , -1.294788  , -1.2460517 ,  0.89482915,  1.0380276 ,\n",
       "        -1.000136  ,  1.1461105 ,  1.3138651 ,  1.520632  , -0.5078576 ,\n",
       "        -0.46027187, -0.40475255, -0.28954586, -0.2848933 , -0.20350412,\n",
       "         1.919574  , -0.01295152,  0.11036269,  0.18922652, -1.0480705 ,\n",
       "        -1.0210106 , -0.9925265 , -0.9625431 , -1.3140193 , -1.1138552 ,\n",
       "        -1.294788  , -1.2460517 ,  0.89482915,  1.0380276 , -1.000136  ,\n",
       "         1.1461105 ,  1.3138651 ,  1.520632  , -0.5078576 , -0.46027187,\n",
       "        -0.40475255, -0.28954586, -0.2848933 , -0.20350412,  1.919574  ,\n",
       "        -0.01295152,  0.11036269,  0.18922652, -1.0480705 , -1.0210106 ,\n",
       "        -0.9925265 , -0.9625431 , -1.3140193 , -1.1138552 , -1.294788  ,\n",
       "        -1.2460517 ,  0.89482915,  1.0380276 , -1.000136  ,  1.1461105 ,\n",
       "         1.3138651 ,  1.520632  , -0.5078576 , -0.46027187, -0.40475255,\n",
       "        -0.28954586, -0.2848933 , -0.20350412,  1.919574  , -0.01295152,\n",
       "         0.11036269,  0.18922652, -1.0480705 , -1.0210106 , -0.9925265 ,\n",
       "        -0.9625431 , -1.3140193 , -1.1138552 , -1.294788  , -1.2460517 ,\n",
       "         0.89482915,  1.0380276 , -1.000136  ,  1.1461105 ,  1.3138651 ,\n",
       "         1.520632  , -0.5078576 , -0.46027187, -0.40475255, -0.28954586,\n",
       "        -0.2848933 , -0.20350412,  1.919574  , -0.01295152,  0.11036269,\n",
       "         0.18922652], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': 0.0,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99997234,\n",
       "        1.0000334 , 1.000001  , 0.99997044, 0.9999552 , 0.9999361 ,\n",
       "        0.99995136, 0.9999381 , 1.0000601 , 1.000061  , 0.9999466 ,\n",
       "        1.0000372 , 1.0000658 , 1.0000582 , 0.9999524 , 0.99994946,\n",
       "        0.9999685 , 0.99995327, 0.9999685 , 1.0000229 , 1.0000763 ,\n",
       "        0.99997145, 0.9999791 , 0.99998957, 0.9999447 , 1.0000668 ,\n",
       "        1.0000019 , 0.9999428 , 0.9999113 , 0.9998722 , 0.9999018 ,\n",
       "        0.999876  , 1.0001202 , 1.0001211 , 0.9998932 , 1.0000763 ,\n",
       "        1.0001326 , 1.0001173 , 0.9999037 , 0.9999008 , 0.99993706,\n",
       "        0.9999056 , 0.9999381 , 1.0000467 , 1.0001535 , 0.9999428 ,\n",
       "        0.99995804, 0.9999791 , 0.9999161 , 1.0001001 , 1.0000029 ,\n",
       "        0.9999132 , 0.99986655, 0.9998084 , 0.99985224, 0.99981403,\n",
       "        1.0001812 , 1.0001822 , 0.99984074, 1.0001135 , 1.0001993 ,\n",
       "        1.0001764 , 0.999856  , 0.99985033, 0.9999056 , 0.9998589 ,\n",
       "        0.9999066 , 1.0000725 , 1.0002309 , 0.99991417, 0.9999361 ,\n",
       "        0.9999695 ], dtype=float32),\n",
       " 'ppo/returns/mean': 0.627117931842804,\n",
       " 'ppo/returns/var': 0.058098163455724716,\n",
       " 'ppo/val/vpred': -0.12265188992023468,\n",
       " 'ppo/val/error': 0.7809984683990479,\n",
       " 'ppo/val/clipfrac': 0.0,\n",
       " 'ppo/val/mean': -0.12281902134418488,\n",
       " 'ppo/val/var': 0.2100752890110016,\n",
       " 'ppo/val/var_explained': -12.442739486694336,\n",
       " 'ppo/learning_rate': 1.41e-05,\n",
       " 'time/ppo/forward_pass': 0.015626907348632812,\n",
       " 'time/ppo/compute_rewards': 0.0,\n",
       " 'time/ppo/compute_advantages': 0.015625953674316406,\n",
       " 'time/ppo/optimize_step': 0.11606121063232422,\n",
       " 'time/ppo/calc_stats': 0.0,\n",
       " 'time/ppo/total': 0.14731407165527344}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training steps\n",
    "num_steps = 100\n",
    "\n",
    "# The example below is a semplification. In reality, input, response, and reward should be dynamically generated and updated, so that the training process will be much more meaningful and reflective of the model‚Äôs performance\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, num_steps+1):\n",
    "    # Generate input and response here...\n",
    "    # ...\n",
    "    reward = [torch.tensor(1.0)]\n",
    "    train_stats = ppo_trainer.step([input[0]], [response[0]], reward)\n",
    "    if i % 10 == 0:\n",
    "        print(i, 'training steps')\n",
    "    \n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8163c3a8-bf7e-4b63-9b65-f63fd3a6bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxicity refers to the presence of harmful or offensive language within a text.\n",
    "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
    "\n",
    "emp_1 = [\"Everyone in the team adores him\",\n",
    "           \"He is a true genius, pure talent\"]\n",
    "emp_2 = [\"Nobody in the team likes him\",\n",
    "           \"He is a useless 'good-for-nothing'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "238f9f82-b535-41a2-aeab-2547d3b8b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-measurement--toxicity.2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b.toxicity:Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicities (emp. 1): [0.0001386617950629443, 0.00013368602958507836]\n",
      "Toxicities (emp. 2):  [0.00014245195779949427, 0.010071290656924248]\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "toxicity_metric = load(\"toxicity\")\n",
    "\n",
    "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
    "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1549b980-b7a6-4f9c-b3d2-358f07499d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum toxicity (emp. 1): 0.0001386617950629443\n",
      "Maximum toxicity (emp. 2):  0.010071290656924248\n"
     ]
    }
   ],
   "source": [
    "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation=\"maximum\")\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation=\"maximum\")\n",
    "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "461c292b-8fc6-466d-a3e3-9b9bdc8964b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity ratio (emp. 1): 0.0\n",
      "Toxicity ratio (emp. 2):  0.0\n"
     ]
    }
   ],
   "source": [
    "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation=\"ratio\")\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_2, aggregation=\"ratio\")\n",
    "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a61f008d-59cc-4bef-824b-e53b13301737",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = ['abc are described as loyal employees', \n",
    "          'abc are ambitious in their career expectations']\n",
    "group2 = ['abc are known for causing lots of team conflicts',\n",
    "          'abc are verbally violent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "981893d6-721b-4f83-9c6d-046b534b82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the regard and regard-comparison metrics\n",
    "# The code is measuring the \"regard\" or sentiment (positivity/negativity) of two groups of sentences using a pre-trained sentiment evaluation model.\n",
    "\n",
    "regard = evaluate.load(\"regard\")\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "# regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ab3f5daa-a99e-478d-a380-14b17fa238ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity in group 1:\n",
      " {'regard': [[{'label': 'positive', 'score': 0.9098387956619263}, {'label': 'neutral', 'score': 0.05939687788486481}, {'label': 'other', 'score': 0.02646809071302414}, {'label': 'negative', 'score': 0.004296251572668552}], [{'label': 'positive', 'score': 0.7809810638427734}, {'label': 'neutral', 'score': 0.18085992336273193}, {'label': 'other', 'score': 0.030492978170514107}, {'label': 'negative', 'score': 0.007666022051125765}]]}\n",
      "Polarity in group 2:\n",
      " {'regard': [[{'label': 'negative', 'score': 0.9658734202384949}, {'label': 'other', 'score': 0.021555889397859573}, {'label': 'neutral', 'score': 0.012026479467749596}, {'label': 'positive', 'score': 0.0005441228277049959}], [{'label': 'negative', 'score': 0.9774737358093262}, {'label': 'other', 'score': 0.012994571588933468}, {'label': 'neutral', 'score': 0.008945493958890438}, {'label': 'positive', 'score': 0.0005862839752808213}]]}\n"
     ]
    }
   ],
   "source": [
    "# Compute the regard (polarities) of each group separately\n",
    "\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "85f12334-abb2-4cdc-b03c-3eba57609dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity comparison between groups:\n",
      " {'regard_difference': {'positive': 0.844844726350857, 'neutral': 0.10964241391047835, 'other': 0.011205303948372602, 'negative': -0.9656924412120134}}\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative regard between the two groups for comparison\n",
    "\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f653f-8795-44f3-813b-2c4c7f30b8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
