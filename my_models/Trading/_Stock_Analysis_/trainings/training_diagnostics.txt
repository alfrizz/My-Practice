
------------------------------------------------------------------------------------------------------------------------------------------------------
RUN START: 2025-10-25T12:38:03.027893Z

BASELINES:
  TRAIN mean RMSE        = 0.04190
  TRAIN persistence RMSE = 0.00941
  VAL   mean RMSE        = 0.06583
  VAL   persistence RMSE = 0.01214

HYPERPARAMS:
  LOOK_BACK = 60
  USE_CONV = False
  CONV_K = 3
  CONV_DILATION = 1
  USE_TCN = False
  TCN_LAYERS = 2
  TCN_KERNEL = 3
  USE_SHORT_LSTM = True
  SHORT_UNITS = 64
  DROPOUT_SHORT = 0.0
  USE_TRANSFORMER = True
  TRANSFORMER_LAYERS = 1
  TRANSFORMER_HEADS = 4
  TRANSFORMER_FF_MULT = 4
  DROPOUT_LONG = 0.0
  USE_LONG_LSTM = False
  LONG_UNITS = 64
  FLATTEN_MODE = last
  PRED_HIDDEN = 64
  ALPHA_SMOOTH = 0.0
  MAX_EPOCHS = 90
  EARLY_STOP_PATIENCE = 9
  WEIGHT_DECAY = 1e-05
  CLIPNORM = 10
  ONECYCLE_MAX_LR = 0.001
  ONECYCLE_DIV_FACTOR = 10
  ONECYCLE_FINAL_DIV = 50
  ONECYCLE_PCT_START = 0.2
  ONECYCLE_STRATEGY = cos
  FREEZE_TILL = 5
  TRAIN_BATCH = 64
  VAL_BATCH = 1
  TRAIN_WORKERS = 8
  TRAIN_PREFETCH_FACTOR = 4
  MICRO_SAMPLE_K = 16

PER-EPOCH LOG FORMAT (explanatory):
  E{ep:02d}                : epoch number formatted with two digits
  OPTS[{groups}:{lrs}]     : optimizer groups and learning rates (list of group LR strings)
  GN[reg,cls,ter,tot]      : gradient norms for regularizer, classification, term, and total (per-epoch summary)
  GD[med,p90,max]         : gradient delta statistics (median, 90th percentile, maximum) used for stability checks
  UR[med,max]             : update ratio statistics (median,max) measuring step/param magnitude ratio
  lr={lr:.1e}             : current learning rate (main/backbone) reported in scientific notation
  TR[rmse,r2,mae]         : training metrics (RMSE, R^2, MAE) aggregated across training set
  VL[rmse,r2,mae]         : validation metrics (RMSE, R^2, MAE) aggregated on the validation set
  SR={slope_rmse:.3f}     : slope RMSE diagnostic measuring calibration of trend predictions
  SL={slip:.2f},HR={hub_max:.3f} : slip and hub max indicators for recent prediction horizons
  topK(g/u)=param:grad_norm/update_ratio,... : the top-k parameter entries by gradient norm and their update ratios
  Additional single-run diagnostics printed once in the header:
    DEBUG_SHAPES raw_reg=(N,1,1): shapes from a detached first batch snapshot
    GROUP_NONZERO_COUNTS [k1,k2,...]             : per-group nonzero counts used for diagnostics
    DEBUG_GRADS backbone=... head=...    : boolean flags indicating whether gradients were observed in those parts
    MICRODETAIL ms: full_forward=...ms preds_cpu=...ms nseg=... seg/s=... mean_len=... gpuMB=... cpuB=... syncs=...
      -> full_forward: wall-clock ms for the sampled forward over prepared windows
      -> preds_cpu: ms to detach and copy predictions to CPU (post-forward host cost)
      -> nseg: number of flattened segments forwarded (useful to normalize throughput)
      -> seg/s: inferred segments per second (throughput) = nseg / full_forward_seconds
      -> mean_len: average per-segment time-series length in timesteps
      -> gpuMB: approximate peak GPU memory (MB) observed during forward
      -> cpuB: bytes copied to CPU for predictions (human-readable KB/MB)
      -> syncs: explicit torch.cuda.synchronize() counts used for timing accuracy
      -> gpuRes: peak GPU reserved memory in MB (helps detect fragmentation)
      -> actMB: estimated activation footprint (MB) computed from allocated delta
      -> out_shape/out_dtype/out_numel: model output shape, dtype and number of elements
      -> collector_ms: wall-clock ms spent by the collector (init-log overhead)
      -> dataloader_ms: ms to fetch the sampled batch from the dataloader
      -> param_bytes: total parameter memory in bytes

DEBUG_OPT GROUPS=1 LRS=['1.1e-04'] COUNTS=[36] MODEL_STATIC: total_params=76,802 trainable=72,512 frozen=4,290 MODEL_SAMPLE_PARAMS: ['short_lstm.weight_ih_l0', 'short_lstm.weight_hh_l0', 'short_lstm.bias_ih_l0', 'short_lstm.bias_hh_l0', 'short_lstm.weight_ih_l0_reverse', 'short_lstm.weight_hh_l0_reverse', 'short_lstm.bias_ih_l0_reverse', 'short_lstm.bias_hh_l0_reverse', 'ln_short.weight', 'ln_short.bias'] # PER-EPOCH LOG FORMAT: #  E{ep:02d} | OPTS[{groups}:{lrs}] | GN[reg,cls,ter,tot] | GD[med,p90,max] | UR[med,max] | lr={lr:.1e} | TR[rmse,r2,mae] | VL[rmse,r2,mae] | SR={slope_rmse:.3f} | SL={slip:.2f},HR={hub_max:.3f} | topK(g/u)=param:grad_norm/update_ratio,...
BATCH_INFO x0_shape=(64, 451, 60, 20)
[micro-collector] batch type=<class 'list'> len=9
BATCH_SHAPE B=64 groups=451 seq_len_full=60 feat=20
[micro-collector] x_batch.shape=(64, 451, 60, 20) B=64 groups=451 seq_len_full=60 feat=20
DEBUG_SHAPES raw_reg=(28864, 1, 1)
GROUP_NONZERO_COUNTS [30]
DEBUG_GRADS backbone=True head=False
MICRODETAIL ms: full_forward=56.20ms preds_cpu=0.25ms nseg=23903 seg/s=425356 mean_len=2.53 gpuMB=6479 gpuRes=13828 gpuAllocBytes=5030702592 gpuResBytes=14499708928 cpuB=46KB syncs=35 p50seg=2.78ms p90seg=5.44ms actMB=4163 out_shape=(23903, 1, 1) out_dtype=torch.float16 out_numel=23903 out_bytes=46KB windows_bytes=112045KB collector_ms=837.3ms dataloader_ms=580.0ms param_bytes=300KB env=torch=2.7.0a0+ecf3bae40a.nv25.02 cuda=12.8 dev=NVIDIA GeForce RTX 5080 Laptop GPU

E01 | OPTS[1:1.1e-04|cnts=[36]] | GN[0.000,0.000,0.000,0.044] | GD[2.0e-03,9.4e-03,2.9e-02] | UR[2.2e-07,2.2e-05] | LR_MAIN=1.1e-04 | lr=1.1e-04 | TR[0.162,-1.05,0.115] | VL[0.111,-0.01,0.076] | SR=0.016 | SL=0.00,HR=0.000 | topK(g/u)=short2long.weight:0.029/6.8e-07, feature_proj.weight:0.020/4.6e-07, linear2.weight:0.014/3.2e-07, out_proj.weight:0.009/2.2e-07, short_lstm.weight_ih_l0:0.008/1.7e-07, self_attn.in_proj_weight:0.008/8.3e-08, short_lstm.weight_ih_l0_reverse:0.007/1.5e-07, linear1.weight:0.006/7.1e-08, short_lstm.bias_ih_l0:0.005/5.1e-07, short_lstm.bias_hh_l0:0.005/5.0e-07, short_lstm.bias_ih_l0_reverse:0.005/4.1e-07, short_lstm.bias_hh_l0_reverse:0.005/4.2e-07, short2long.bias:0.003/6.5e-07, short_lstm.weight_hh_l0:0.003/4.4e-08, ln_proj.weight:0.003/3.3e-08, ln_flat.weight:0.002/3.3e-08, feature_proj.bias:0.002/3.8e-07, norm1.weight:0.002/2.7e-08, ln_flat.bias:0.002/2.2e-05, norm2.weight:0.002/2.7e-08, ln_proj.bias:0.002/2.0e-05, norm2.bias:0.002/1.6e-05, out_proj.bias:0.002/1.6e-05, norm1.bias:0.002/1.4e-05, linear2.bias:0.002/5.7e-07, ln_short.bias:0.001/1.3e-05, ln_short.weight:0.001/1.6e-08, self_attn.in_proj_bias:0.001/8.9e-06, short_lstm.weight_hh_l0_reverse:0.001/1.8e-08, linear1.bias:0.001/6.6e-08, 0.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00, 2.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00 | T=22.1s,TP=78463.3s/s | GPU=6.33GiB | LAYER_GN[weight_ih_l0:8.284e-03/1.00,weight:2.944e-02/1.00,weight:1.990e-02/1.00]

E02 | OPTS[1:1.3e-04|cnts=[36]] | GN[0.000,0.000,0.000,0.027] | GD[1.4e-03,5.8e-03,1.9e-02] | UR[1.6e-07,1.5e-05] | LR_MAIN=1.3e-04 | lr=1.3e-04 | TR[0.114,-0.01,0.080] | VL[0.104,0.11,0.072] | SR=0.015 | SL=0.00,HR=0.000 | topK(g/u)=short2long.weight:0.019/5.2e-07, feature_proj.weight:0.012/3.3e-07, linear2.weight:0.008/2.3e-07, out_proj.weight:0.006/1.6e-07, short_lstm.weight_ih_l0:0.005/1.2e-07, self_attn.in_proj_weight:0.004/5.5e-08, short_lstm.weight_ih_l0_reverse:0.004/1.0e-07, linear1.weight:0.004/5.7e-08, short_lstm.bias_ih_l0:0.003/3.8e-07, short_lstm.bias_hh_l0:0.003/3.8e-07, short_lstm.bias_ih_l0_reverse:0.003/3.2e-07, short_lstm.bias_hh_l0_reverse:0.003/3.2e-07, short2long.bias:0.002/5.0e-07, short_lstm.weight_hh_l0:0.002/3.0e-08, ln_proj.weight:0.002/2.4e-08, ln_flat.weight:0.001/2.3e-08, ln_flat.bias:0.001/1.5e-05, ln_proj.bias:0.001/1.4e-05, feature_proj.bias:0.001/2.8e-07, norm1.weight:0.001/1.9e-08, norm2.weight:0.001/1.8e-08, norm2.bias:0.001/1.0e-05, out_proj.bias:0.001/9.9e-06, norm1.bias:0.001/9.1e-06, linear2.bias:0.001/4.1e-07, ln_short.weight:0.001/1.1e-08, ln_short.bias:0.001/8.1e-06, self_attn.in_proj_bias:0.001/4.7e-06, short_lstm.weight_hh_l0_reverse:0.001/1.2e-08, linear1.bias:0.000/5.3e-08, 0.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00, 2.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00 | T=18.0s,TP=96386.5s/s | GPU=8.46GiB *CHKPT | LAYER_GN[weight_ih_l0:4.828e-03/0.58,weight:1.892e-02/0.64,weight:1.214e-02/0.61]

E03 | OPTS[1:1.6e-04|cnts=[36]] | GN[0.000,0.000,0.000,0.080] | GD[4.4e-03,1.7e-02,5.8e-02] | UR[6.0e-07,6.1e-05] | LR_MAIN=1.6e-04 | lr=1.6e-04 | TR[0.107,0.11,0.075] | VL[0.101,0.16,0.066] | SR=0.017 | SL=0.00,HR=0.000 | topK(g/u)=short2long.weight:0.058/2.0e-06, feature_proj.weight:0.032/1.1e-06, linear2.weight:0.024/8.4e-07, out_proj.weight:0.017/6.0e-07, short_lstm.weight_ih_l0:0.016/4.9e-07, self_attn.in_proj_weight:0.013/2.1e-07, short_lstm.weight_ih_l0_reverse:0.012/3.8e-07, linear1.weight:0.011/1.9e-07, short_lstm.bias_ih_l0:0.008/1.1e-06, short_lstm.bias_hh_l0:0.008/1.1e-06, short2long.bias:0.007/2.1e-06, short_lstm.bias_ih_l0_reverse:0.006/8.1e-07, short_lstm.bias_hh_l0_reverse:0.006/8.2e-07, short_lstm.weight_hh_l0:0.005/1.2e-07, ln_proj.weight:0.005/9.6e-08, ln_flat.weight:0.005/9.3e-08, ln_flat.bias:0.005/6.1e-05, ln_proj.bias:0.004/5.4e-05, feature_proj.bias:0.004/1.1e-06, norm2.bias:0.004/4.1e-05, out_proj.bias:0.004/4.0e-05, norm1.bias:0.003/3.5e-05, linear2.bias:0.003/1.7e-06, norm2.weight:0.003/6.1e-08, norm1.weight:0.003/5.9e-08, ln_short.bias:0.002/3.6e-05, self_attn.in_proj_bias:0.002/1.9e-05, ln_short.weight:0.002/3.7e-08, short_lstm.weight_hh_l0_reverse:0.002/3.9e-08, linear1.bias:0.001/1.9e-07, 0.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00, 2.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00 | T=18.1s,TP=95869.5s/s | GPU=8.46GiB *CHKPT | LAYER_GN[weight_ih_l0:1.579e-02/1.91,weight:5.795e-02/1.97,weight:3.195e-02/1.61]
