
------------------------------------------------------------------------------------------------------------------------------------------------------
RUN START: 2025-10-25T14:33:05.952957Z

BASELINES:
  TRAIN mean RMSE        = 0.04190
  TRAIN persistence RMSE = 0.00941
  VAL   mean RMSE        = 0.06583
  VAL   persistence RMSE = 0.01214

HYPERPARAMS:
  LOOK_BACK = 60
  USE_CONV = False
  CONV_K = 3
  CONV_DILATION = 1
  USE_TCN = False
  TCN_LAYERS = 2
  TCN_KERNEL = 3
  USE_SHORT_LSTM = True
  SHORT_UNITS = 64
  DROPOUT_SHORT = 0.0
  USE_TRANSFORMER = True
  TRANSFORMER_LAYERS = 1
  TRANSFORMER_HEADS = 4
  TRANSFORMER_FF_MULT = 4
  DROPOUT_LONG = 0.0
  USE_LONG_LSTM = False
  LONG_UNITS = 64
  FLATTEN_MODE = last
  PRED_HIDDEN = 64
  ALPHA_SMOOTH = 0.5
  MAX_EPOCHS = 90
  EARLY_STOP_PATIENCE = 9
  WEIGHT_DECAY = 1e-05
  CLIPNORM = 10
  ONECYCLE_MAX_LR = 0.001
  ONECYCLE_DIV_FACTOR = 10
  ONECYCLE_FINAL_DIV = 50
  ONECYCLE_PCT_START = 0.2
  ONECYCLE_STRATEGY = cos
  FREEZE_TILL = 5
  TRAIN_BATCH = 64
  VAL_BATCH = 1
  TRAIN_WORKERS = 8
  TRAIN_PREFETCH_FACTOR = 4
  MICRO_SAMPLE_K = 16

PER-EPOCH LOG FORMAT (explanatory):
  E{ep:02d}                : epoch number formatted with two digits
  OPTS[{groups}:{lrs}]     : optimizer groups and learning rates (list of group LR strings)
  GN[reg,cls,ter,tot]      : gradient norms for regularizer, classification, term, and total (per-epoch summary)
  GD[med,p90,max]         : gradient delta statistics (median, 90th percentile, maximum) used for stability checks
  UR[med,max]             : update ratio statistics (median,max) measuring step/param magnitude ratio
  lr={lr:.1e}             : current learning rate (main/backbone) reported in scientific notation
  TR[rmse,r2,mae]         : training metrics (RMSE, R^2, MAE) aggregated across training set
  VL[rmse,r2,mae]         : validation metrics (RMSE, R^2, MAE) aggregated on the validation set
  SR={slope_rmse:.3f}     : slope RMSE diagnostic measuring calibration of trend predictions
  SL={slip:.2f},HR={hub_max:.3f} : slip and hub max indicators for recent prediction horizons
  topK(g/u)=param:grad_norm/update_ratio,... : the top-k parameter entries by gradient norm and their update ratios
  Additional single-run diagnostics printed once in the header:
    DEBUG_SHAPES raw_reg=(N,1,1): shapes from a detached first batch snapshot
    GROUP_NONZERO_COUNTS [k1,k2,...]             : per-group nonzero counts used for diagnostics
    DEBUG_GRADS backbone=... head=...    : boolean flags indicating whether gradients were observed in those parts
    MICRODETAIL ms: full_forward=...ms preds_cpu=...ms nseg=... seg/s=... mean_len=... gpuMB=... cpuB=... syncs=...
      -> full_forward: wall-clock ms for the sampled forward over prepared windows
      -> preds_cpu: ms to detach and copy predictions to CPU (post-forward host cost)
      -> nseg: number of flattened segments forwarded (useful to normalize throughput)
      -> seg/s: inferred segments per second (throughput) = nseg / full_forward_seconds
      -> mean_len: average per-segment time-series length in timesteps
      -> gpuMB: approximate peak GPU memory (MB) observed during forward
      -> cpuB: bytes copied to CPU for predictions (human-readable KB/MB)
      -> syncs: explicit torch.cuda.synchronize() counts used for timing accuracy
      -> gpuRes: peak GPU reserved memory in MB (helps detect fragmentation)
      -> actMB: estimated activation footprint (MB) computed from allocated delta
      -> out_shape/out_dtype/out_numel: model output shape, dtype and number of elements
      -> collector_ms: wall-clock ms spent by the collector (init-log overhead)
      -> dataloader_ms: ms to fetch the sampled batch from the dataloader
      -> param_bytes: total parameter memory in bytes

DEBUG_OPT GROUPS=1 LRS=['1.1e-04'] COUNTS=[36] MODEL_STATIC: total_params=76,802 trainable=72,512 frozen=4,290 MODEL_SAMPLE_PARAMS: ['short_lstm.weight_ih_l0', 'short_lstm.weight_hh_l0', 'short_lstm.bias_ih_l0', 'short_lstm.bias_hh_l0', 'short_lstm.weight_ih_l0_reverse', 'short_lstm.weight_hh_l0_reverse', 'short_lstm.bias_ih_l0_reverse', 'short_lstm.bias_hh_l0_reverse', 'ln_short.weight', 'ln_short.bias'] # PER-EPOCH LOG FORMAT: #  E{ep:02d} | OPTS[{groups}:{lrs}] | GN[reg,cls,ter,tot] | GD[med,p90,max] | UR[med,max] | lr={lr:.1e} | TR[rmse,r2,mae] | VL[rmse,r2,mae] | SR={slope_rmse:.3f} | SL={slip:.2f},HR={hub_max:.3f} | topK(g/u)=param:grad_norm/update_ratio,...
BATCH_INFO x0_shape=(64, 451, 60, 20)
[micro-collector] batch type=<class 'list'> len=9
BATCH_SHAPE B=64 groups=451 seq_len_full=60 feat=20
[micro-collector] x_batch.shape=(64, 451, 60, 20) B=64 groups=451 seq_len_full=60 feat=20
DEBUG_SHAPES raw_reg=(28864, 1, 1)
GROUP_NONZERO_COUNTS [30]
DEBUG_GRADS backbone=True head=False
MICRODETAIL ms: full_forward=57.56ms preds_cpu=0.20ms nseg=23903 seg/s=415236 mean_len=2.53 gpuMB=6479 gpuRes=13828 gpuAllocBytes=5030587392 gpuResBytes=14499708928 cpuB=46KB syncs=35 p50seg=2.44ms p90seg=4.00ms actMB=4163 out_shape=(23903, 1, 1) out_dtype=torch.float16 out_numel=23903 out_bytes=46KB windows_bytes=112045KB collector_ms=845.3ms dataloader_ms=480.6ms param_bytes=300KB env=torch=2.7.0a0+ecf3bae40a.nv25.02 cuda=12.8 dev=NVIDIA GeForce RTX 5080 Laptop GPU

E01 | OPTS[1:1.1e-04|cnts=[36]] | GN[0.000,0.000,0.000,0.149] | GD[8.9e-03,3.4e-02,1.0e-01] | UR[7.9e-07,1.8e-04] | LR_MAIN=1.1e-04 | lr=1.1e-04 | TR[0.108,0.09,0.075] | VL[0.105,0.08,0.069] | SR=0.015 | SL=0.00,HR=0.000 | topK(g/u)=short2long.weight:0.104/2.4e-06, feature_proj.weight:0.064/1.5e-06, linear2.weight:0.041/9.6e-07, out_proj.weight:0.034/7.9e-07, short_lstm.weight_ih_l0:0.030/6.3e-07, self_attn.in_proj_weight:0.029/3.1e-07, linear1.weight:0.020/2.3e-07, short_lstm.weight_ih_l0_reverse:0.018/3.8e-07, short_lstm.bias_ih_l0:0.016/1.4e-06, short_lstm.bias_hh_l0:0.016/1.4e-06, ln_flat.weight:0.014/1.8e-07, short2long.bias:0.013/2.4e-06, ln_proj.weight:0.013/1.7e-07, short_lstm.weight_hh_l0:0.011/1.8e-07, short_lstm.bias_ih_l0_reverse:0.010/8.9e-07, short_lstm.bias_hh_l0_reverse:0.010/9.6e-07, ln_flat.bias:0.009/1.7e-04, ln_proj.bias:0.009/1.8e-04, feature_proj.bias:0.008/1.6e-06, out_proj.bias:0.007/1.3e-04, norm2.weight:0.007/9.4e-08, norm2.bias:0.007/1.0e-04, norm1.bias:0.006/1.2e-04, norm1.weight:0.006/8.6e-08, linear2.bias:0.006/2.3e-06, ln_short.weight:0.005/7.3e-08, ln_short.bias:0.005/9.9e-05, self_attn.in_proj_bias:0.005/5.5e-05, short_lstm.weight_hh_l0_reverse:0.003/5.3e-08, linear1.bias:0.003/2.2e-07, 0.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00, 2.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00 | T=20.6s,TP=84056.0s/s | GPU=6.33GiB | LAYER_GN[weight_ih_l0:3.033e-02/1.00,weight:1.035e-01/1.00,weight:6.397e-02/1.00]

E02 | OPTS[1:1.3e-04|cnts=[36]] | GN[0.000,0.000,0.000,0.147] | GD[9.1e-03,3.3e-02,1.0e-01] | UR[9.2e-07,1.9e-04] | LR_MAIN=1.3e-04 | lr=1.3e-04 | TR[0.105,0.14,0.073] | VL[0.104,0.10,0.068] | SR=0.016 | SL=0.00,HR=0.000 | topK(g/u)=short2long.weight:0.103/2.9e-06, feature_proj.weight:0.063/1.7e-06, linear2.weight:0.040/1.1e-06, out_proj.weight:0.033/9.2e-07, short_lstm.weight_ih_l0:0.029/7.3e-07, self_attn.in_proj_weight:0.028/3.6e-07, linear1.weight:0.020/2.8e-07, short_lstm.weight_ih_l0_reverse:0.020/4.9e-07, short_lstm.bias_ih_l0:0.016/1.7e-06, short_lstm.bias_hh_l0:0.016/1.7e-06, ln_flat.weight:0.014/2.2e-07, ln_proj.weight:0.013/2.1e-07, short2long.bias:0.013/2.8e-06, short_lstm.bias_ih_l0_reverse:0.011/1.2e-06, short_lstm.bias_hh_l0_reverse:0.011/1.3e-06, short_lstm.weight_hh_l0:0.010/2.0e-07, ln_flat.bias:0.009/1.7e-04, ln_proj.bias:0.009/1.9e-04, feature_proj.bias:0.008/1.9e-06, out_proj.bias:0.007/1.5e-04, norm2.weight:0.007/1.1e-07, norm2.bias:0.007/1.0e-04, norm1.bias:0.006/1.3e-04, norm1.weight:0.006/1.0e-07, linear2.bias:0.006/2.6e-06, ln_short.weight:0.006/9.3e-08, ln_short.bias:0.005/1.1e-04, self_attn.in_proj_bias:0.004/5.0e-05, short_lstm.weight_hh_l0_reverse:0.003/6.6e-08, linear1.bias:0.003/2.7e-07, 0.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00, 2.bias:0.000/0.0e+00, weight.original0:0.000/0.0e+00, weight.original1:0.000/0.0e+00 | T=17.5s,TP=99054.4s/s | GPU=8.46GiB *CHKPT | LAYER_GN[weight_ih_l0:2.939e-02/0.97,weight:1.032e-01/1.00,weight:6.312e-02/0.99]
