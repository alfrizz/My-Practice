
------------------------------------------------------------------------------------------------------------------------------------------------------
RUN START: 2025-11-12T18:09:07.391033Z

SINGLE RUN DIAGNOSTIC FORMAT (explanatory)
    BATCH_SHAPE B=7216 groups=1 seq_len_full=20 feat=20 : canonical input geometry used to build forwarded windows (snapshot.B, snapshot.groups, snapshot.seq_len_full, snapshot.feat_dim)
    MICRODETAIL ms: <k=v ...> : single-line deterministic dump of snapshot keys printed as key=value (sorted); human-readable units used where applicable
    -> B: number of batch samples used to build windows (snapshot.B)
    -> groups: number of logical groups used when flattening windows (snapshot.groups)
    -> seq_len_full: nominal full sequence length used when padding/truncating windows (snapshot.seq_len_full)
    -> feat / feat_dim: feature dimension used to build windows (snapshot.feat_dim)
    -> activation_mb: estimated activation footprint in MB computed from GPU allocation delta (snapshot.activation_mb)
    -> backward_ms: backward pass ms placeholder (snapshot.backward_ms)
    -> collector_ms: total wall-clock ms spent by the collector including sampling, CPU work and device synchronizations (snapshot.collector_ms)
    -> cpu_copy_bytes: bytes copied to host for predictions shown human-readable in the log (snapshot.cpu_copy_bytes)
    -> dataloader_ms: ms spent fetching the sampled batch from the dataloader (snapshot.dataloader_ms)
    -> device_syncs_count: number of explicit device synchronizations performed during snapshot (snapshot.device_syncs_count)
    -> env: small dict with python/torch/cuda/device_name strings (snapshot.env)
    -> expected_segments: nominal B * groups used to estimate workload (snapshot.expected_segments)
    -> full_forward_ms: wall-clock ms for the sampled forward over prepared windows (snapshot.full_forward_ms)
    -> pred_extra_ms: estimated CPU-side post-forward ms = collector_ms - full_forward_ms - summed per-seg time (snapshot.pred_extra_ms)
    -> gpu_allocated_bytes: raw GPU bytes allocated after forward (snapshot.gpu_allocated_bytes)
    -> gpu_peak_mb: peak GPU memory in MB (snapshot.gpu_peak_mb)
    -> gpu_reserved_bytes / gpu_reserved_mb: reserved GPU bytes and MB (snapshot.gpu_reserved_bytes, snapshot.gpu_reserved_mb)
    -> grads: dict {'backbone': bool, 'head': bool} indicating gradient presence by name-bucket (snapshot.grads)
    -> group_nonzero_counts: per-optimizer-group counts of parameters with non-None .grad (snapshot.group_nonzero_counts)
    -> mean_seg_len: average per-segment time-series length in timesteps computed as sum_seg_lens / num_segments (snapshot.mean_seg_len)
    -> num_segments: actual number of flattened segments forwarded (snapshot.num_segments)
    -> sum_seg_lens: sum of non-empty segment lengths used to compute mean_seg_len (snapshot.sum_seg_lens)
    -> out_bytes / out_dtype / out_numel / out_shape: model output bytes, dtype string, element count, and tuple shape (snapshot.out_bytes, snapshot.out_dtype, snapshot.out_numel, snapshot.out_shape)
    -> param_bytes: total parameter memory in bytes (human-readable in log; raw int in snapshot.param_bytes)
    -> total_params / trainable_params: canonical parameter counts stamped by init_log (snapshot.total_params, snapshot.trainable_params)
    -> per_segment_p50_ms / per_segment_p90_ms: empirical per-segment forward-ms percentiles when sampling enabled (snapshot.per_segment_p50_ms, snapshot.per_segment_p90_ms)
    -> raw_reg_shape: the raw detached regression output shape (snapshot.raw_reg_shape)
    -> segments_per_sec: inferred throughput = num_segments / (full_forward_ms/1000.0) (snapshot.segments_per_sec)
    -> windows_bytes: total bytes for the windows tensor (human-readable in log; raw int in snapshot.windows_bytes)

PER-EPOCH LOG FORMAT (explanatory):
  E{ep:02d}                : epoch number formatted with two digits
  OPTS[{groups}:{lr_main}|cnts=[c1,c2,...]] : optimizer groups count; lr_main is the representative LR (first group); cnts lists per-group parameter counts
  GN[name:val,...,TOT=val] : per-bucket gradient L2 norms printed as short_name:curr_norm; TOT is sqrt(sum squares over reported buckets)
  GD[med,p90,max]         : gradient-norm distribution statistics printed as median, index-based 90th-percentile, and maximum
  UR[med,max]             : update-ratio statistics (median,max) where update_ratio = lr * grad_norm / max(weight_norm,1e-8)
  LR_MAIN={lr:.1e} | lr={lr:.1e} : representative main LR and explicit first-group lr printed in scientific notation
  TR[...metrics...]       : training metrics reported for the epoch (rmse, mae, r2, etc.; see TR composition in code)
  VAL[...metrics...]      : validation metrics reported for the epoch (rmse, mae, r2, etc.; see VAL composition in code)
  SR={slope_rmse:.3f}     : slope RMSE computed on model.last_val_tot_preds/model.last_val_targs (trend calibration)
  SL={slip:.2f},HR={hub_max:.3f} : slip fraction and hub max indicators derived from model.last_hub (defaults 0.00/0.000 when missing)
  FMB={val:.4f}           : first-mini-batch diagnostic metric from the first-batch snapshot if set (snapshot.FMB)
  T={elapsed:.1f}s,TP={throughput:.1f} : epoch elapsed seconds and throughput (segments/sec inferred from sampled forward)
  chk={val}               : checkpoint marker token printed as chk in the line (implementation-specific; integerized when boolean)
  GPU={GiB:.2f}GiB        : optional GPU memory high-water printed in GiB when CUDA available (torch.cuda.max_memory_allocated / (1024**3))
  *CHKPT                  : optional marker when model._last_epoch_checkpoint is truthy (separate visual marker outside numeric chk token)
  LAYER_GN[...]           : optional small set of monitored layer norms printed as name_short:curr_norm/ratio where ratio = curr / baseline_observed_on_first_epoch
  G/U=name:grad/update_ratio,... : parameter buckets printed sorted by descending gradient norm; every short-name bucket is emitted in compact form
      - format for each token is short_name:{g:.3e}/{u:.1e} where g is the gradient norm and u is the lr-scaled update ratio
      - short_name uses the last 1â€“3 name segments (see short_name logic); entries are deduped by short_name keeping the highest-g representative
      - the G/U list is printed every epoch (values change per epoch) and contains both large and very small gradients in scientific notation

BASELINES:
  TRAIN mean RMSE        = 0.11379
  TRAIN persistence RMSE = 0.00941
  VAL   mean RMSE        = 0.10862
  VAL   persistence RMSE = 0.01200

HYPERPARAMS:
  USE_CONV = False
  CONV_K = 3
  CONV_DILATION = 1
  CONV_CHANNELS = 64
  USE_TCN = False
  TCN_LAYERS = 3
  TCN_KERNEL = 3
  TCN_CHANNELS = 64
  USE_SHORT_LSTM = False
  SHORT_UNITS = 128
  DROPOUT_SHORT = 0.1
  USE_TRANSFORMER = True
  TRANSFORMER_D_MODEL = 64
  TRANSFORMER_LAYERS = 1
  TRANSFORMER_HEADS = 4
  TRANSFORMER_FF_MULT = 4
  DROPOUT_TRANS = 0.05
  USE_LONG_LSTM = False
  DROPOUT_LONG = 0.1
  LONG_UNITS = 128
  FLATTEN_MODE = pool
  PRED_HIDDEN = 64
  ALPHA_SMOOTH = 0.0
  WARMUP_STEPS = 5
  USE_HUBER = False
  HUBER_DELTA = 0.1
  USE_DELTA = False
  LAMBDA_DELTA = 0.1
  MAX_EPOCHS = 90
  EARLY_STOP_PATIENCE = 9
  WEIGHT_DECAY = 1e-06
  CLIPNORM = 3
  ONECYCLE_MAX_LR = 0.0003
  ONECYCLE_DIV_FACTOR = 10
  ONECYCLE_FINAL_DIV = 100
  ONECYCLE_PCT_START = 0.1
  ONECYCLE_STRATEGY = cos
  TRAIN_BATCH = 16
  VAL_BATCH = 1
  TRAIN_WORKERS = 8
  TRAIN_PREFETCH_FACTOR = 4
  LOOK_BACK = 60
  MICRO_SAMPLE_K = 16

DEBUG_OPT GROUPS=1 LRS=['3.8e-05'] COUNTS=[28] | MODEL_STATIC: total_params=74,371 trainable=74,371 frozen=0 | param_bytes=290.5KB
BATCH_SHAPE B=7216 groups=1 seq_len_full=60 feat=109
MICRODETAIL ms: B=7216 activation_mb=6 backward_ms=29.32 collector_ms=3290.76 cpu_copy_bytes=68B dataloader_ms=3050.71 device_syncs_count=2 env={'python': '3.12.3', 'torch': '2.7.0a0+ecf3bae40a.nv25.02', 'cuda': '12.8', 'device_name': 'NVIDIA GeForce RTX 5080 Laptop GPU'} expected_segments=7216 feat_dim=109 full_forward_ms=9.09 gpu_allocated_bytes=563.86MB gpu_peak_mb=599 gpu_reserved_bytes=3448.00MB gpu_reserved_mb=3448 grads={'backbone': True, 'head': True} group_nonzero_counts=[26] groups=1 mean_seg_len=1.00 num_segments=34 out_bytes=68B out_dtype=torch.float16 out_numel=34 out_shape=(34, 1) param_bytes=290.5KB per_segment_p50_ms=1.11 per_segment_p90_ms=1.73 pred_extra_ms=3259.18 raw_reg_shape=(34, 1) segments_per_sec=3741.95 seq_len_full=60 sum_seg_lens=34 total_params=74371 trainable_params=74371 windows_bytes=868.6KB

E01 | OPTS[1:3.8e-05|cnts=[28]] | GN[head_flat=0.065,short2long=0.039,transformer=0.030,feature_proj=0.020,ln_proj=0.004,ln_flat=0.003,attn_pool=0.000,TOT=0.084] | GD[4.5e-03,2.0e-02,5.0e-02] | UR[2.3e-07,2.3e-03] | lr=3.8e-05 | TR[0.1154,0.0804,-0.0291,BASE_RMSE=0.1154,DELTA_RMSE=nan] | VAL[0.1034,0.0728,0.0942,BASE_RMSE=0.1034] | BASE_LOSS=1.3326e-02,DELTA_LOSS=0.0000e+00,DELTA_RATIO=0.000e+00,BASE_RMSE=0.11544,DELTA_RMSE=nan | SCHED_PCT=1.1% | SR=0.010 | SL=0.00,HR=0.000 | FMB=0.0000 | T=185.2s,TP=9235.3seg/s | chk=0 | GPU=0.59GiB | 
G/U=short2long.weight:3.9e-02/2.3e-07,feature_proj.weight:2.0e-02/1.6e-07,0.linear2.weight:1.8e-02/1.5e-07,self_attn.out_proj.weight:1.6e-02/1.3e-07,0.self_attn.in_proj_weight:1.3e-02/5.2e-08,head_flat.2.bias:1.1e-02/2.3e-03,0.linear1.weight:7.3e-03/3.0e-08,short2long.bias:6.3e-03/2.9e-07,feature_proj.bias:4.9e-03/4.0e-07,self_attn.out_proj.bias:4.5e-03/4.6e-05,head_flat.0.bias:4.5e-03/4.1e-07,ln_proj.bias:3.3e-03/2.5e-05,0.norm2.bias:3.3e-03/3.3e-05,0.norm1.bias:3.3e-03/3.3e-05,0.linear2.bias:3.2e-03/4.1e-07,0.self_attn.in_proj_bias:2.6e-03/1.5e-05,ln_flat.bias:2.5e-03/1.9e-05,ln_flat.weight:2.4e-03/8.1e-09,ln_proj.weight:2.4e-03/8.0e-09,0.norm1.weight:2.3e-03/1.1e-08,0.norm2.weight:2.1e-03/1.0e-08,0.linear1.bias:1.2e-03/3.8e-08,attn_pool.weight:0.0e+00/0.0e+00,attn_pool.bias:0.0e+00/0.0e+00 | 
FEAT_TOP=roc_112:9.66e-01,ema_dev_56:9.34e-01,eng_ema_cross_down:8.83e-01,obv_diff_56:8.81e-01,rsi_56:8.79e-01,roc_14:8.66e-01,lag3_ret:8.62e-01,dist_high_14:8.43e-01,adx_56:8.19e-01,dist_low_28:8.13e-01,eng_ma:8.10e-01,vol_z_14:8.04e-01,obv_z_14:7.91e-01,plus_di_14:7.90e-01,eng_obv:7.87e-01,plus_di_7:7.78e-01,z_bbw_40:7.73e-01,dist_high_56:7.70e-01,bb_w_10:7.60e-01,ret_5:7.50e-01,eng_macd:7.38e-01,obv_z_56:7.03e-01,dist_low_56:6.95e-01,bb_w_20:6.88e-01,roc_7:6.85e-01,obv:6.85e-01,atr_pct_14:6.83e-01,mom_std_60:6.81e-01,dist_low_112:6.75e-01,dist_high_28:6.74e-01,hour:6.72e-01,sma_pct_14:6.70e-01,day_of_week:6.65e-01,adx_14:6.65e-01,obv_z_7:6.57e-01,z_obv_diff:6.56e-01,bb_w_80:6.51e-01,eng_bb_mid:6.43e-01,plus_di_28:6.43e-01,bb_w_40:6.39e-01,ret_std_14:6.29e-01,sma_pct_112:6.26e-01,adx_28:6.26e-01,mom_sum_5:6.22e-01,mom_sum_1:6.17e-01,obv_pct_56:6.13e-01,z_vwap_dev:6.12e-01,atr_pct_56:6.07e-01,ema_dev_7:6.04e-01,hour_sin:5.99e-01,roc_28:5.93e-01,z_bbw_10:5.84e-01,obv_diff_28:5.80e-01,obv_diff_14:5.74e-01,obv_z_28:5.73e-01,ret_std_7:5.70e-01,atr_pct_7:5.63e-01,eng_rsi:5.31e-01,obv_pct_14:5.30e-01,minus_di_56:5.26e-01,sma_pct_56:5.23e-01,minus_di_14:5.21e-01,z_atr_7:5.21e-01,hour_cos:5.20e-01,ema_dev_112:5.14e-01,obv_sma_28:5.08e-01,z_eng_atr:5.06e-01,mom_std_15:5.04e-01,lag1_ret:5.02e-01,ret_std_56:4.99e-01,eng_vwap:4.91e-01,eng_adx:4.89e-01,mom_sum_60:4.86e-01,plus_di_56:4.81e-01,lag2_ret:4.80e-01,rsi_28:4.80e-01,adx_7:4.57e-01,minus_di_28:4.52e-01,ema_dev_28:4.52e-01,ema_dev_14:4.50e-01,obv_pct_7:4.50e-01,rsi_7:4.49e-01,rsi_14:4.43e-01,vol_spike_14:4.38e-01,ret_std_28:4.37e-01,sma_pct_7:4.35e-01,ret_56:4.31e-01,mom_sum_15:4.31e-01,z_bbw_80:4.27e-01,eng_atr_div:4.24e-01,sma_pct_28:4.21e-01,dist_low_14:4.18e-01,vwap_dev_pct_14:4.17e-01,dist_high_112:4.11e-01,obv_sma_14:4.09e-01,ret_15:4.07e-01,obv_sma_56:3.93e-01,atr_pct_28:3.78e-01,body_pct:3.67e-01,minus_di_7:3.59e-01,mom_std_5:3.42e-01,obv_sma_7:3.25e-01,obv_diff_7:3.19e-01,range_pct:3.19e-01,z_bbw_20:3.14e-01,month:3.13e-01,z_bb_w:2.79e-01,ret_60:2.18e-01,obv_pct_28:2.50e-02
