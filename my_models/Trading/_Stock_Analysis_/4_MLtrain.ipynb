{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed6c184-8438-497e-8ae7-82cc35ac4819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libs.models_custom' from '/workspace/my_models/Trading/_Stock_Analysis_/libs/models_custom.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# 1) Wipe out your namespace\n",
    "%reset -f\n",
    "\n",
    "# 2) Clear Jupyter’s stored outputs (and inputs if you like)\n",
    "try:\n",
    "    Out.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    In.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) Force Python GC\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 4) Free any GPU buffers\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "from libs import params, trades, feats, plots, models_core, models_custom\n",
    "importlib.reload(params)\n",
    "importlib.reload(trades)\n",
    "importlib.reload(feats)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(models_core)\n",
    "importlib.reload(models_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy  as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, Dropout\n",
    "import torch.nn.functional as Funct\n",
    "from torch_lr_finder import LRFinder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sma_pct_14</th>\n",
       "      <th>atr_pct_14</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>bb_w_20</th>\n",
       "      <th>plus_di_14</th>\n",
       "      <th>range_pct</th>\n",
       "      <th>eng_ma</th>\n",
       "      <th>minus_di_14</th>\n",
       "      <th>eng_macd</th>\n",
       "      <th>macd_diff_12_26_9</th>\n",
       "      <th>body_pct</th>\n",
       "      <th>macd_line_12_26_9</th>\n",
       "      <th>volume</th>\n",
       "      <th>obv_diff_14</th>\n",
       "      <th>eng_rsi</th>\n",
       "      <th>eng_atr_div</th>\n",
       "      <th>eng_adx</th>\n",
       "      <th>adx_14</th>\n",
       "      <th>hour</th>\n",
       "      <th>body</th>\n",
       "      <th>close_raw</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-02 09:33:00</th>\n",
       "      <td>0.499798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624765</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.489532</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.488177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.519392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>0.506538</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>6.967909e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 09:34:00</th>\n",
       "      <td>0.499798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624765</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.489532</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.488177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.519392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>0.506538</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>7.448270e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 09:35:00</th>\n",
       "      <td>0.499798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624765</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.489532</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.488177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.519392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>0.506538</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>7.961747e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 09:36:00</th>\n",
       "      <td>0.499798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624765</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.489532</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.488177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.519392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>0.506538</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>8.510623e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 09:37:00</th>\n",
       "      <td>0.499798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624765</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.489532</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.488177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.519392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>0.506538</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>9.097338e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>0.751690</td>\n",
       "      <td>0.323886</td>\n",
       "      <td>0.672166</td>\n",
       "      <td>0.273759</td>\n",
       "      <td>0.363125</td>\n",
       "      <td>0.254109</td>\n",
       "      <td>0.532431</td>\n",
       "      <td>0.074852</td>\n",
       "      <td>0.634772</td>\n",
       "      <td>0.888941</td>\n",
       "      <td>0.624850</td>\n",
       "      <td>0.971125</td>\n",
       "      <td>0.362651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664288</td>\n",
       "      <td>0.537709</td>\n",
       "      <td>0.301944</td>\n",
       "      <td>-0.975989</td>\n",
       "      <td>0.854984</td>\n",
       "      <td>196.815000</td>\n",
       "      <td>1.654708e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>0.650094</td>\n",
       "      <td>0.336016</td>\n",
       "      <td>0.616686</td>\n",
       "      <td>0.290460</td>\n",
       "      <td>0.347207</td>\n",
       "      <td>0.420131</td>\n",
       "      <td>0.532016</td>\n",
       "      <td>0.067044</td>\n",
       "      <td>0.633511</td>\n",
       "      <td>0.840741</td>\n",
       "      <td>0.374057</td>\n",
       "      <td>0.981544</td>\n",
       "      <td>0.434283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.676887</td>\n",
       "      <td>0.541855</td>\n",
       "      <td>0.328685</td>\n",
       "      <td>-0.975989</td>\n",
       "      <td>0.158091</td>\n",
       "      <td>196.675000</td>\n",
       "      <td>1.484384e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>0.631153</td>\n",
       "      <td>0.322169</td>\n",
       "      <td>0.614734</td>\n",
       "      <td>0.304199</td>\n",
       "      <td>0.336272</td>\n",
       "      <td>0.121620</td>\n",
       "      <td>0.531618</td>\n",
       "      <td>0.064933</td>\n",
       "      <td>0.631866</td>\n",
       "      <td>0.777896</td>\n",
       "      <td>0.494766</td>\n",
       "      <td>0.983006</td>\n",
       "      <td>0.496049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584662</td>\n",
       "      <td>0.545337</td>\n",
       "      <td>0.353515</td>\n",
       "      <td>-0.975989</td>\n",
       "      <td>0.493632</td>\n",
       "      <td>196.670000</td>\n",
       "      <td>1.477119e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>0.375519</td>\n",
       "      <td>0.346959</td>\n",
       "      <td>0.475401</td>\n",
       "      <td>0.305249</td>\n",
       "      <td>0.290578</td>\n",
       "      <td>0.565110</td>\n",
       "      <td>0.531157</td>\n",
       "      <td>0.160021</td>\n",
       "      <td>0.623450</td>\n",
       "      <td>0.456523</td>\n",
       "      <td>0.090577</td>\n",
       "      <td>0.884486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645760</td>\n",
       "      <td>0.531521</td>\n",
       "      <td>0.348960</td>\n",
       "      <td>-0.975989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196.240000</td>\n",
       "      <td>1.166764e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>0.535073</td>\n",
       "      <td>0.374297</td>\n",
       "      <td>0.551738</td>\n",
       "      <td>0.303059</td>\n",
       "      <td>0.249733</td>\n",
       "      <td>0.630630</td>\n",
       "      <td>0.531913</td>\n",
       "      <td>0.176984</td>\n",
       "      <td>0.622570</td>\n",
       "      <td>0.422962</td>\n",
       "      <td>0.462226</td>\n",
       "      <td>0.868408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708780</td>\n",
       "      <td>0.525575</td>\n",
       "      <td>0.336212</td>\n",
       "      <td>-0.993518</td>\n",
       "      <td>0.403294</td>\n",
       "      <td>196.540000</td>\n",
       "      <td>8.667170e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3715200 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sma_pct_14  atr_pct_14    rsi_14   bb_w_20  plus_di_14  \\\n",
       "2004-01-02 09:33:00    0.499798    0.000000  1.000000  0.000000    0.000000   \n",
       "2004-01-02 09:34:00    0.499798    0.000000  1.000000  0.000000    0.000000   \n",
       "2004-01-02 09:35:00    0.499798    0.000000  1.000000  0.000000    0.000000   \n",
       "2004-01-02 09:36:00    0.499798    0.000000  1.000000  0.000000    0.000000   \n",
       "2004-01-02 09:37:00    0.499798    0.000000  1.000000  0.000000    0.000000   \n",
       "...                         ...         ...       ...       ...         ...   \n",
       "2025-06-18 20:56:00    0.751690    0.323886  0.672166  0.273759    0.363125   \n",
       "2025-06-18 20:57:00    0.650094    0.336016  0.616686  0.290460    0.347207   \n",
       "2025-06-18 20:58:00    0.631153    0.322169  0.614734  0.304199    0.336272   \n",
       "2025-06-18 20:59:00    0.375519    0.346959  0.475401  0.305249    0.290578   \n",
       "2025-06-18 21:00:00    0.535073    0.374297  0.551738  0.303059    0.249733   \n",
       "\n",
       "                     range_pct    eng_ma  minus_di_14  eng_macd  \\\n",
       "2004-01-02 09:33:00   0.000000  0.516453     0.000000  0.624765   \n",
       "2004-01-02 09:34:00   0.000000  0.516453     0.000000  0.624765   \n",
       "2004-01-02 09:35:00   0.000000  0.516453     0.000000  0.624765   \n",
       "2004-01-02 09:36:00   0.000000  0.516453     0.000000  0.624765   \n",
       "2004-01-02 09:37:00   0.000000  0.516453     0.000000  0.624765   \n",
       "...                        ...       ...          ...       ...   \n",
       "2025-06-18 20:56:00   0.254109  0.532431     0.074852  0.634772   \n",
       "2025-06-18 20:57:00   0.420131  0.532016     0.067044  0.633511   \n",
       "2025-06-18 20:58:00   0.121620  0.531618     0.064933  0.631866   \n",
       "2025-06-18 20:59:00   0.565110  0.531157     0.160021  0.623450   \n",
       "2025-06-18 21:00:00   0.630630  0.531913     0.176984  0.622570   \n",
       "\n",
       "                     macd_diff_12_26_9  body_pct  macd_line_12_26_9    volume  \\\n",
       "2004-01-02 09:33:00           0.506696  0.499412           0.489532  0.020724   \n",
       "2004-01-02 09:34:00           0.506696  0.499412           0.489532  0.020724   \n",
       "2004-01-02 09:35:00           0.506696  0.499412           0.489532  0.020724   \n",
       "2004-01-02 09:36:00           0.506696  0.499412           0.489532  0.020724   \n",
       "2004-01-02 09:37:00           0.506696  0.499412           0.489532  0.020724   \n",
       "...                                ...       ...                ...       ...   \n",
       "2025-06-18 20:56:00           0.888941  0.624850           0.971125  0.362651   \n",
       "2025-06-18 20:57:00           0.840741  0.374057           0.981544  0.434283   \n",
       "2025-06-18 20:58:00           0.777896  0.494766           0.983006  0.496049   \n",
       "2025-06-18 20:59:00           0.456523  0.090577           0.884486  1.000000   \n",
       "2025-06-18 21:00:00           0.422962  0.462226           0.868408  1.000000   \n",
       "\n",
       "                     obv_diff_14  eng_rsi  eng_atr_div   eng_adx    adx_14  \\\n",
       "2004-01-02 09:33:00     0.488177      1.0     0.353456  0.519392  0.000000   \n",
       "2004-01-02 09:34:00     0.488177      1.0     0.353456  0.519392  0.000000   \n",
       "2004-01-02 09:35:00     0.488177      1.0     0.353456  0.519392  0.000000   \n",
       "2004-01-02 09:36:00     0.488177      1.0     0.353456  0.519392  0.000000   \n",
       "2004-01-02 09:37:00     0.488177      1.0     0.353456  0.519392  0.000000   \n",
       "...                          ...      ...          ...       ...       ...   \n",
       "2025-06-18 20:56:00     1.000000      0.0     0.664288  0.537709  0.301944   \n",
       "2025-06-18 20:57:00     0.000000      0.0     0.676887  0.541855  0.328685   \n",
       "2025-06-18 20:58:00     0.000000      0.0     0.584662  0.545337  0.353515   \n",
       "2025-06-18 20:59:00     0.000000      0.0     0.645760  0.531521  0.348960   \n",
       "2025-06-18 21:00:00     1.000000      0.0     0.708780  0.525575  0.336212   \n",
       "\n",
       "                         hour      body   close_raw        signal  \n",
       "2004-01-02 09:33:00  1.002425  0.506538    0.764286  6.967909e-09  \n",
       "2004-01-02 09:34:00  1.002425  0.506538    0.764286  7.448270e-09  \n",
       "2004-01-02 09:35:00  1.002425  0.506538    0.764286  7.961747e-09  \n",
       "2004-01-02 09:36:00  1.002425  0.506538    0.764286  8.510623e-09  \n",
       "2004-01-02 09:37:00  1.002425  0.506538    0.764286  9.097338e-09  \n",
       "...                       ...       ...         ...           ...  \n",
       "2025-06-18 20:56:00 -0.975989  0.854984  196.815000  1.654708e-01  \n",
       "2025-06-18 20:57:00 -0.975989  0.158091  196.675000  1.484384e-01  \n",
       "2025-06-18 20:58:00 -0.975989  0.493632  196.670000  1.477119e-01  \n",
       "2025-06-18 20:59:00 -0.975989  0.000000  196.240000  1.166764e-01  \n",
       "2025-06-18 21:00:00 -0.993518  0.403294  196.540000  8.667170e-02  \n",
       "\n",
       "[3715200 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_sel = pd.read_csv(params.feat_all_csv, index_col=0, parse_dates=True)[params.features_cols_tick + ['close_raw'] + [params.label_col]]\n",
    "    \n",
    "df_feat_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8d933-8402-45de-a838-4335b2a37d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside build_tensors, features: ['sma_pct_14', 'atr_pct_14', 'rsi_14', 'bb_w_20', 'plus_di_14', 'range_pct', 'eng_ma', 'minus_di_14', 'eng_macd', 'macd_diff_12_26_9', 'body_pct', 'macd_line_12_26_9', 'volume', 'obv_diff_14', 'eng_rsi', 'eng_atr_div', 'eng_adx', 'adx_14', 'hour', 'body']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f0bdf8954a4055b28e8fb0ba53bbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing days:   0%|          | 0/5400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, end_times_tr, end_times_val, end_times_te = models_core.model_core_pipeline(\n",
    "    df             = df_feat_sel,\n",
    "    look_back      = params.hparams[\"LOOK_BACK\"],\n",
    "    sess_start     = params.sess_start_pred_tick,\n",
    "    train_prop     = params.train_prop,\n",
    "    val_prop       = params.val_prop,\n",
    "    train_batch    = params.hparams[\"TRAIN_BATCH\"],\n",
    "    train_workers  = params.hparams[\"TRAIN_WORKERS\"],\n",
    "    prefetch_factor= params.hparams[\"TRAIN_PREFETCH_FACTOR\"],\n",
    "    signal_thresh  = params.best_optuna_params[\"buy_threshold\"],\n",
    "    return_thresh  = params.return_threshold_tick\n",
    ")\n",
    "\n",
    "for name, ld, tm in zip(\n",
    "    [\"train\",\"val\",\"test\"],\n",
    "    [train_loader, val_loader, test_loader],\n",
    "    [end_times_tr, end_times_val, end_times_te]\n",
    "):\n",
    "    models_core.summarize_split(name, ld, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(models_custom) #############\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the ModelClass & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model = models_custom.ModelClass(\n",
    "    n_feats         = len(params.features_cols_tick),\n",
    "    short_units     = params.hparams[\"SHORT_UNITS\"],\n",
    "    long_units      = params.hparams[\"LONG_UNITS\"],\n",
    "    dropout_short   = params.hparams[\"DROPOUT_SHORT\"],\n",
    "    dropout_long    = params.hparams[\"DROPOUT_LONG\"],\n",
    "    dropout_trans   = params.hparams[\"DROPOUT_TRANS\"],\n",
    "    pred_hidden     = params.hparams[\"PRED_HIDDEN\"],\n",
    "    window_len      = params.hparams[\"LOOK_BACK\"],\n",
    "\n",
    "    # Gating flags\n",
    "    use_conv          = params.hparams[\"USE_CONV\"],\n",
    "    use_tcn           = params.hparams[\"USE_TCN\"],\n",
    "    use_short_lstm    = params.hparams[\"USE_SHORT_LSTM\"],\n",
    "    use_transformer   = params.hparams[\"USE_TRANSFORMER\"],\n",
    "    use_long_lstm     = params.hparams[\"USE_LONG_LSTM\"],\n",
    "    use_delta         = params.hparams[\"USE_DELTA\"],\n",
    "    flatten_mode      = params.hparams[\"FLATTEN_MODE\"]\n",
    ")\n",
    "\n",
    "model.to(params.device)  \n",
    "\n",
    "# # robust zeroing (covers parametrizations) for delta head\n",
    "with torch.no_grad():\n",
    "    if getattr(model, \"delta_head\", None) is not None:\n",
    "        torch.nn.init.zeros_(model.delta_head.weight)\n",
    "        if hasattr(model.delta_head, \"bias\"):\n",
    "            torch.nn.init.zeros_(model.delta_head.bias)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60683d75-5de6-4685-9593-f451788ebbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model check: overfit one batch: force your model to train on the exact same small set of examples over and over.\n",
    "# # You should see loss → 0 in a few dozen steps on a single batch.\n",
    "\n",
    "# # 1) Grab a single batch (no shuffle issues)\n",
    "# batch = next(iter(train_loader))\n",
    "# x_pad, y_sig, *_, lengths = batch\n",
    "\n",
    "# # 2) Move to device\n",
    "# device = next(model.parameters()).device\n",
    "# x_pad = x_pad.to(device)\n",
    "# y_sig = y_sig.to(device)\n",
    "\n",
    "# # 3) Extract just the first day’s valid windows\n",
    "# #    lengths[0] might be a tensor or int\n",
    "# W = lengths[0].item() if isinstance(lengths[0], torch.Tensor) else lengths[0]\n",
    "# x_day = x_pad[0, :W]       # shape (W, features…)\n",
    "# y_day = y_sig[0, :W]       # shape (W,)\n",
    "\n",
    "# # 4) Our single target is the last tick of that day\n",
    "# target_val = y_day[-1].unsqueeze(0)   # shape (1,)\n",
    "\n",
    "# # 5) Turn off dropout so we can memorize perfectly\n",
    "# for m in model.modules():\n",
    "#     if isinstance(m, Dropout):\n",
    "#         m.p = 0.0\n",
    "\n",
    "# # 6) Clear any saved LSTM state (if your model uses h_short/h_long)\n",
    "# if hasattr(model, \"h_short\"): model.h_short = None\n",
    "# if hasattr(model, \"h_long\"):  model.h_long  = None\n",
    "\n",
    "# # 7) Set up optimizer & loss\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "# criterion = MSELoss()\n",
    "\n",
    "# # 8) Overfit loop: reset state every pass, predict last‐tick, compare scalar→scalar\n",
    "# model.train()\n",
    "# for step in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # reset hidden state each iteration\n",
    "#     if hasattr(model, \"h_short\"): model.h_short = None\n",
    "#     if hasattr(model, \"h_long\"):  model.h_long  = None\n",
    "\n",
    "#     # forward on the full day sequence\n",
    "#     raw_out = model(x_day)  \n",
    "#     raw_reg = raw_out[0] if isinstance(raw_out, (tuple, list)) else raw_out\n",
    "\n",
    "#     # collapse to shape (W,)\n",
    "#     if raw_reg.dim() == 3:\n",
    "#         raw_reg = raw_reg[0].squeeze(-1)\n",
    "#     elif raw_reg.dim() == 2:\n",
    "#         raw_reg = raw_reg.squeeze(-1)\n",
    "\n",
    "#     # take *only* the final-tick prediction → scalar\n",
    "#     pred_val = raw_reg[-1].unsqueeze(0)   # shape (1,)\n",
    "\n",
    "#     # compute scalar loss\n",
    "#     loss = criterion(pred_val, target_val)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (step + 1) % 10 == 0 or step == 0:\n",
    "#         print(f\"Step {step+1:02d}  loss={loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7ec8c-5882-486b-bdfd-371b8db260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate([batch[1].cpu().numpy().ravel() for batch in train_loader])\n",
    "y_val = np.concatenate([batch[1].cpu().numpy().ravel() for batch in val_loader])\n",
    "\n",
    "# Visualize the true‐signal distributions on train vs. validation\n",
    "plt.hist(y_train, bins=50, alpha=0.5, label=\"train true\")\n",
    "plt.hist(y_val,   bins=50, alpha=0.5, label=\"val true\")\n",
    "plt.xlabel(\"Signal value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"True Signal Distribution: Train vs. Validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(params) #############\n",
    "importlib.reload(models_custom) #############\n",
    "importlib.reload(models_core) #############\n",
    "\n",
    "n_days = len(train_loader.dataset)\n",
    "print(f\"Training sees {n_days} unique trading days per epoch.\\n\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: total={total_params:,}, trainable={trainable_params:,}\\n\")\n",
    "\n",
    "print('Using HyperParameters:\\n', params.hparams)\n",
    "\n",
    "optimizer = AdamW(\n",
    "  model.parameters(),\n",
    "  lr           = params.hparams[\"ONECYCLE_MAX_LR\"],\n",
    "  weight_decay = params.hparams[\"WEIGHT_DECAY\"]\n",
    ")\n",
    "\n",
    "batches_per_epoch = len(train_loader)\n",
    "total_steps = batches_per_epoch * params.hparams[\"MAX_EPOCHS\"]\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "  optimizer,\n",
    "  max_lr           = params.hparams[\"ONECYCLE_MAX_LR\"],\n",
    "  total_steps      = total_steps,\n",
    "  pct_start        = params.hparams[\"ONECYCLE_PCT_START\"],\n",
    "  div_factor       = params.hparams[\"ONECYCLE_DIV_FACTOR\"],\n",
    "  final_div_factor = params.hparams[\"ONECYCLE_FINAL_DIV\"],\n",
    "  anneal_strategy  = params.hparams[\"ONECYCLE_STRATEGY\"],\n",
    ")\n",
    "optimizer.scheduler = scheduler # necessary to log sched_field\n",
    "\n",
    "if getattr(scheduler, \"total_steps\", None) != total_steps:\n",
    "    raise RuntimeError(f\"Scheduler total_steps mismatch: scheduler={getattr(scheduler,'total_steps',None)} expected={total_steps}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse  = models_custom.model_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    scheduler           = scheduler,\n",
    "    scaler              = GradScaler(),\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = params.hparams['MAX_EPOCHS'],\n",
    "    early_stop_patience = params.hparams['EARLY_STOP_PATIENCE'],\n",
    "    clipnorm            = params.hparams['CLIPNORM'],\n",
    "    alpha_smooth        = params.hparams['ALPHA_SMOOTH'],\n",
    "    lambda_delta        = params.hparams['LAMBDA_DELTA'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ff8b7-413b-409f-aaae-75f21bdfe1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"train shape:\", y_train.shape, \"val shape:\", y_val.shape)\n",
    "# print(\"train sum len:\", np.sum([a.size for a in y_train]))  # if y_train is concatenated already skip\n",
    "# import numpy as np\n",
    "# for name, arr in [(\"train\", y_train), (\"val\", y_val)]:\n",
    "#     arr = np.asarray(arr).ravel()\n",
    "#     print(name, \"count\", arr.size,\n",
    "#           \"nan\", np.isnan(arr).sum(),\n",
    "#           \"inf\", np.isinf(arr).sum(),\n",
    "#           \"finite\", np.isfinite(arr).sum())\n",
    "#     if arr.size:\n",
    "#         print(name, \"min,max,median:\",\n",
    "#               np.nanmin(arr), np.nanmax(arr), np.nanmedian(arr))\n",
    "# arr = np.asarray(y_train).ravel()\n",
    "# bad_idx = np.where(~np.isfinite(arr))[0]\n",
    "# print(\"first bad train idx\", bad_idx[:10])\n",
    "# good = np.isfinite(y_train)\n",
    "# y_train_f = np.asarray(y_train).ravel()[good]\n",
    "# y_val_f   = np.asarray(y_val).ravel()[np.isfinite(y_val)]\n",
    "\n",
    "# vmin = np.nanpercentile(y_train_f, 0.5)\n",
    "# vmax = np.nanpercentile(y_train_f, 99.5)\n",
    "# plt.hist(y_train_f, bins=50, alpha=0.5, label=\"train true\", range=(vmin, vmax))\n",
    "# plt.hist(y_val_f,   bins=50, alpha=0.5, label=\"val true\",   range=(vmin, vmax))\n",
    "# # # inside debug run of build_tensors for one day (or after building payloads)\n",
    "# # print(\"wins_count\", wins.shape[0], \"ends_count\", ends_np.shape[0], \"mask_count\", mask.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af1db9-d502-43cb-a703-8cb51b1f914c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
