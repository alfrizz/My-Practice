{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed6c184-8438-497e-8ae7-82cc35ac4819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libs.models' from '/workspace/my_models/Trading/_Stock_Analysis_/libs/models.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 1) Wipe out your namespace\n",
    "%reset -f\n",
    "\n",
    "# 2) Clear Jupyter’s stored outputs (and inputs if you like)\n",
    "try:\n",
    "    Out.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    In.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) Force Python GC\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 4) Free any GPU buffers\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import importlib\n",
    "from libs import trades, plots, params, models\n",
    "importlib.reload(trades)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy  as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "from torch_lr_finder import LRFinder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_feat_sel \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_all_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[params\u001b[38;5;241m.\u001b[39mfeatures_cols_tick \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [params\u001b[38;5;241m.\u001b[39mlabel_col]]\n\u001b[1;32m      3\u001b[0m df_feat_sel\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_feat_sel = pd.read_csv(params.feat_all_csv, index_col=0, parse_dates=True)[params.features_cols_tick + ['bid','ask'] + [params.label_col]]\n",
    "    \n",
    "df_feat_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM input tensors (disk-backed memmaps)\n",
    "X, y_sig, y_ret, raw_close, raw_bid, raw_ask, end_times = models.build_lstm_tensors(\n",
    "    df            = df_feat_sel,\n",
    "    sess_start    = params.sess_start_pred_tick \n",
    ")\n",
    "\n",
    "# quick shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"  X         =\", X.shape,    \"(samples, look_back, features)\")\n",
    "print(\"  y_sig     =\", y_sig.shape, \"(samples,)\")\n",
    "print(\"  y_ret     =\", y_ret.shape, \"(samples,)\")\n",
    "print(\"  raw_close =\", raw_close.shape)\n",
    "print(\"  raw_bid   =\", raw_bid.shape)\n",
    "print(\"  raw_ask   =\", raw_ask.shape)\n",
    "print(\"  end_times =\", end_times.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test by calendar day\n",
    "(\n",
    "(X_tr,  y_sig_tr,  y_ret_tr),\n",
    "(X_val, y_sig_val, y_ret_val),\n",
    "(X_te,  y_sig_te,  y_ret_te,  raw_close_te, raw_bid_te, raw_ask_te),\n",
    "samples_per_day,\n",
    "day_id_tr, day_id_val, day_id_te\n",
    ") = models.chronological_split(\n",
    "    X, y_sig, y_ret,\n",
    "    raw_close, raw_bid, raw_ask,\n",
    "    end_times   = end_times,\n",
    "    train_prop  = params.train_prop,\n",
    "    val_prop    = params.val_prop,\n",
    "    train_batch = params.hparams['TRAIN_BATCH']\n",
    ")\n",
    "\n",
    "# Print shapes of all tensors\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_tr  =\", X_tr.shape)\n",
    "print(\"  y_sig_tr, y_ret_tr =\", y_sig_tr.shape, y_ret_tr.shape)\n",
    "print(\"  X_val =\", X_val.shape)\n",
    "print(\"  y_sig_val, y_ret_val =\", y_sig_val.shape, y_ret_val.shape)\n",
    "print(\"  X_te  =\", X_te.shape)\n",
    "print(\"  y_sig_te, y_ret_te =\", y_sig_te.shape, y_ret_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9b6c4-3d79-45c0-b2c0-c4f46f1ad866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carve `end_times` into the same three splits:\n",
    "n_tr  = day_id_tr .shape[0] \n",
    "n_val = day_id_val.shape[0]\n",
    "i_tr  = n_tr\n",
    "i_val = n_tr + n_val\n",
    "\n",
    "end_times_tr  = end_times[:i_tr]\n",
    "end_times_val = end_times[i_tr:i_val]\n",
    "end_times_te  = end_times[i_val:]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Build DataLoaders over calendar‐days\n",
    "# -----------------------------------------------------------------------------\n",
    "train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "    # train split:   \n",
    "    X_tr,            y_sig_tr,     y_ret_tr,   end_times_tr,\n",
    "    # val split:\n",
    "    X_val,           y_sig_val,    y_ret_val,  end_times_val,\n",
    "    # test split + raw‐prices\n",
    "    X_te,            y_sig_te,     y_ret_te,   end_times_te,\n",
    "    raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    \n",
    "    sess_start_time       = params.sess_start_pred_tick,\n",
    "    signal_thresh         = params.best_optuna_params[\"buy_threshold\"],\n",
    "    return_thresh         = 0.01,  # flat‐zone threshold for returns (to tune)\n",
    "    train_batch           = params.hparams[\"TRAIN_BATCH\"],\n",
    "    train_workers         = params.hparams[\"NUM_WORKERS\"],\n",
    "    train_prefetch_factor = params.hparams[\"TRAIN_PREFETCH_FACTOR\"]\n",
    ")\n",
    "\n",
    "print(f\"Days  → train={len(train_loader.dataset)}, val={len(val_loader.dataset)}, test={len(test_loader.dataset)}\")\n",
    "print(f\"Batches → train={len(train_loader)},   val={len(val_loader)},   test={len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the stateful DualMemoryLSTM & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "model = models.DualMemoryLSTM(\n",
    "    n_feats        = X.shape[-1],                          \n",
    "    short_units    = params.hparams['SHORT_UNITS'],    \n",
    "    long_units     = params.hparams['LONG_UNITS'],     \n",
    "    dropout_short  = params.hparams['DROPOUT_SHORT'],  \n",
    "    dropout_long   = params.hparams['DROPOUT_LONG'],   \n",
    "    att_heads      = params.hparams['ATT_HEADS'],\n",
    "    att_drop       = params.hparams['ATT_DROPOUT']\n",
    ")\n",
    "model.to(params.device)  \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512a0dd-d2c8-418e-bfca-4580fb4be995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------\n",
    "# # Compute plateau_sched timing parameters\n",
    "# # -----------------------------------------------------------------------------\n",
    "# # Total training samples = total windows in X_tr (one window per row)\n",
    "# n_train_samples = X_tr.shape[0]\n",
    "\n",
    "# # How many optimizer steps (day‐bundles) constitute one epoch?\n",
    "# steps_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build optimizer, LR scheduler, AMP scaler, and gradient‐clip norm\n",
    "# -----------------------------------------------------------------------------\n",
    "optimizer, plateau_sched, cosine_sched, scaler, clipnorm = models.make_optimizer_and_scheduler(\n",
    "    model,\n",
    "    initial_lr        = params.hparams['INITIAL_LR'],       \n",
    "    weight_decay      = params.hparams['WEIGHT_DECAY'],     \n",
    "    clipnorm          = params.hparams['CLIPNORM']   \n",
    ")\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5fc27-f31d-4adc-a247-5468674f83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Helper: extract the true “signal” values from any loader into a flat array\n",
    "# -----------------------------------------------------------------------------\n",
    "def extract_y(loader):\n",
    "    return np.concatenate([batch[1].cpu().numpy().ravel() for batch in loader])\n",
    "\n",
    "# Pull out train & validation targets\n",
    "y_train = extract_y(train_loader)\n",
    "y_val   = extract_y(val_loader)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Zero‐forecast baseline RMSE (predict 0 always)\n",
    "#    RMSE_zero = √(mean(y²))\n",
    "# -----------------------------------------------------------------------------\n",
    "rmse_zero_train = np.sqrt(np.mean(y_train**2))\n",
    "rmse_zero_val   = np.sqrt(np.mean(y_val**2))\n",
    "print(f\"Zero‐forecast RMSE (predict 0): train = {rmse_zero_train:.6f},  val = {rmse_zero_val:.6f}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Compute mean, variance & std for train/validation targets\n",
    "#    and derive the mean‐predictor baseline (R² = 0)\n",
    "# -----------------------------------------------------------------------------\n",
    "for split, y in [(\"Train\", y_train), (\"Validation\", y_val)]:\n",
    "    mean_y    = y.mean()\n",
    "    std_y     = y.std(ddof=0)     # population std = √variance\n",
    "    var_y     = std_y**2\n",
    "    rmse_mean = std_y            # RMSE_baseline = std(target)\n",
    "\n",
    "    print(f\"{split} target stats:\")\n",
    "    print(f\"  mean = {mean_y:.4f},  var = {var_y:.4f},  std = {std_y:.4f}\")\n",
    "    print(f\"{split} mean‐predictor baseline:\")\n",
    "    print(f\"  RMSE_baseline = {rmse_mean:.6f}\")\n",
    "    print(\"  R²_baseline   = 0.00\\n\")\n",
    "\n",
    "    if split == \"Validation\":\n",
    "        rmse_mean_val = rmse_mean # used afterwards for the final reporting\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7ec8c-5882-486b-bdfd-371b8db260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the true‐signal distributions on train vs. validation\n",
    "plt.hist(y_train, bins=100, alpha=0.5, label=\"train true\")\n",
    "plt.hist(y_val,   bins=100, alpha=0.5, label=\"val true\")\n",
    "plt.xlabel(\"Signal value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"True Signal Distribution: Train vs. Validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique trading days does each epoch see?\n",
    "n_days = len(train_loader.dataset)\n",
    "print(f\"Training sees {n_days} unique trading days per epoch.\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse  = models.custom_stateful_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    cosine_sched        = cosine_sched,\n",
    "    plateau_sched       = plateau_sched,\n",
    "    scaler              = scaler,\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = params.hparams['MAX_EPOCHS'],\n",
    "    early_stop_patience = params.hparams['EARLY_STOP_PATIENCE'],\n",
    "    clipnorm            = clipnorm,\n",
    "    device              = params.device\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final reporting: best RMSE and relative improvement\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "\n",
    "improvement_zero = 100.0 * (1.0 - best_val_rmse / rmse_zero_val)\n",
    "print(f\"Improvement over zero‐baseline = {improvement_zero:5.1f}%\")\n",
    "\n",
    "improvement_mean = 100.0 * (1.0 - best_val_rmse / rmse_mean_val)\n",
    "print(f\"Improvement over mean‐baseline = {improvement_mean:5.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b25c89-05c3-4a16-b013-97143f173e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
