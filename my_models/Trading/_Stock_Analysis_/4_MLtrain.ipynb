{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed6c184-8438-497e-8ae7-82cc35ac4819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libs.models_core' from '/workspace/my_models/Trading/_Stock_Analysis_/libs/models_core.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 1) Wipe out your namespace\n",
    "%reset -f\n",
    "\n",
    "# 2) Clear Jupyter’s stored outputs (and inputs if you like)\n",
    "try:\n",
    "    Out.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    In.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) Force Python GC\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 4) Free any GPU buffers\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "from libs import params, trades, feats, plots, models_core\n",
    "importlib.reload(params)\n",
    "importlib.reload(trades)\n",
    "importlib.reload(feats)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(models_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy  as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, Dropout\n",
    "import torch.nn.functional as Funct\n",
    "from torch_lr_finder import LRFinder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_sel = pd.read_csv(params.feat_all_csv, index_col=0, parse_dates=True)[params.features_cols_tick + ['close_raw'] + [params.label_col]]\n",
    "    \n",
    "df_feat_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8d933-8402-45de-a838-4335b2a37d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, end_times_tr, end_times_val, end_times_te = models_core.model_core_pipeline(\n",
    "    df             = df_feat_sel,\n",
    "    look_back      = params.hparams[\"LOOK_BACK\"],\n",
    "    sess_start     = params.sess_start_pred_tick,\n",
    "    train_prop     = params.train_prop,\n",
    "    val_prop       = params.val_prop,\n",
    "    train_batch    = params.hparams[\"TRAIN_BATCH\"],\n",
    "    train_workers  = params.hparams[\"TRAIN_WORKERS\"],\n",
    "    prefetch_factor= params.hparams[\"TRAIN_PREFETCH_FACTOR\"],\n",
    "    signal_thresh  = params.best_optuna_params[\"buy_threshold\"],\n",
    "    return_thresh  = params.return_threshold_tick\n",
    ")\n",
    "\n",
    "for name, ld, tm in zip(\n",
    "    [\"train\",\"val\",\"test\"],\n",
    "    [train_loader, val_loader, test_loader],\n",
    "    [end_times_tr, end_times_val, end_times_te]\n",
    "):\n",
    "    models_core.summarize_split(name, ld, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(params.model_selected) #############\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the ModelClass & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model = params.model_selected.ModelClass(\n",
    "    n_feats         = len(params.features_cols_tick),\n",
    "    short_units     = params.hparams[\"SHORT_UNITS\"],\n",
    "    long_units      = params.hparams[\"LONG_UNITS\"],\n",
    "    dropout_short   = params.hparams[\"DROPOUT_SHORT\"],\n",
    "    dropout_long    = params.hparams[\"DROPOUT_LONG\"],\n",
    "    pred_hidden     = params.hparams[\"PRED_HIDDEN\"],\n",
    "    window_len      = params.hparams[\"LOOK_BACK\"],\n",
    "\n",
    "    # Gating flags\n",
    "    use_conv          = params.hparams[\"USE_CONV\"],\n",
    "    use_tcn           = params.hparams[\"USE_TCN\"],\n",
    "    use_short_lstm    = params.hparams[\"USE_SHORT_LSTM\"],\n",
    "    use_transformer   = params.hparams[\"USE_TRANSFORMER\"],\n",
    "    use_long_lstm     = params.hparams[\"USE_LONG_LSTM\"],\n",
    "    flatten_mode      = params.hparams[\"FLATTEN_MODE\"]\n",
    ")\n",
    "\n",
    "model.to(params.device)  \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60683d75-5de6-4685-9593-f451788ebbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model check: overfit one batch: force your model to train on the exact same small set of examples over and over.\n",
    "# # You should see loss → 0 in a few dozen steps on a single batch.\n",
    "\n",
    "# # 1) Grab a single batch (no shuffle issues)\n",
    "# batch = next(iter(train_loader))\n",
    "# x_pad, y_sig, *_, lengths = batch\n",
    "\n",
    "# # 2) Move to device\n",
    "# device = next(model.parameters()).device\n",
    "# x_pad = x_pad.to(device)\n",
    "# y_sig = y_sig.to(device)\n",
    "\n",
    "# # 3) Extract just the first day’s valid windows\n",
    "# #    lengths[0] might be a tensor or int\n",
    "# W = lengths[0].item() if isinstance(lengths[0], torch.Tensor) else lengths[0]\n",
    "# x_day = x_pad[0, :W]       # shape (W, features…)\n",
    "# y_day = y_sig[0, :W]       # shape (W,)\n",
    "\n",
    "# # 4) Our single target is the last tick of that day\n",
    "# target_val = y_day[-1].unsqueeze(0)   # shape (1,)\n",
    "\n",
    "# # 5) Turn off dropout so we can memorize perfectly\n",
    "# for m in model.modules():\n",
    "#     if isinstance(m, Dropout):\n",
    "#         m.p = 0.0\n",
    "\n",
    "# # 6) Clear any saved LSTM state (if your model uses h_short/h_long)\n",
    "# if hasattr(model, \"h_short\"): model.h_short = None\n",
    "# if hasattr(model, \"h_long\"):  model.h_long  = None\n",
    "\n",
    "# # 7) Set up optimizer & loss\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "# criterion = MSELoss()\n",
    "\n",
    "# # 8) Overfit loop: reset state every pass, predict last‐tick, compare scalar→scalar\n",
    "# model.train()\n",
    "# for step in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # reset hidden state each iteration\n",
    "#     if hasattr(model, \"h_short\"): model.h_short = None\n",
    "#     if hasattr(model, \"h_long\"):  model.h_long  = None\n",
    "\n",
    "#     # forward on the full day sequence\n",
    "#     raw_out = model(x_day)  \n",
    "#     raw_reg = raw_out[0] if isinstance(raw_out, (tuple, list)) else raw_out\n",
    "\n",
    "#     # collapse to shape (W,)\n",
    "#     if raw_reg.dim() == 3:\n",
    "#         raw_reg = raw_reg[0].squeeze(-1)\n",
    "#     elif raw_reg.dim() == 2:\n",
    "#         raw_reg = raw_reg.squeeze(-1)\n",
    "\n",
    "#     # take *only* the final-tick prediction → scalar\n",
    "#     pred_val = raw_reg[-1].unsqueeze(0)   # shape (1,)\n",
    "\n",
    "#     # compute scalar loss\n",
    "#     loss = criterion(pred_val, target_val)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (step + 1) % 10 == 0 or step == 0:\n",
    "#         print(f\"Step {step+1:02d}  loss={loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7ec8c-5882-486b-bdfd-371b8db260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate([batch[1].cpu().numpy().ravel() for batch in train_loader])\n",
    "y_val = np.concatenate([batch[1].cpu().numpy().ravel() for batch in val_loader])\n",
    "\n",
    "# Visualize the true‐signal distributions on train vs. validation\n",
    "plt.hist(y_train, bins=50, alpha=0.5, label=\"train true\")\n",
    "plt.hist(y_val,   bins=50, alpha=0.5, label=\"val true\")\n",
    "plt.xlabel(\"Signal value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"True Signal Distribution: Train vs. Validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(params) #############\n",
    "importlib.reload(params.model_selected) #############\n",
    "importlib.reload(models_core) #############\n",
    "\n",
    "# How many unique trading days does each epoch see?\n",
    "n_days = len(train_loader.dataset)\n",
    "print(f\"Training sees {n_days} unique trading days per epoch.\\n\")\n",
    "\n",
    "print('Using HyperParameters:\\n', params.hparams)\n",
    "\n",
    "optimizer = AdamW(\n",
    "  model.parameters(),\n",
    "  lr           = params.hparams[\"ONECYCLE_MAX_LR\"],\n",
    "  weight_decay = params.hparams[\"WEIGHT_DECAY\"]\n",
    ")\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "  optimizer,\n",
    "  max_lr           = params.hparams[\"ONECYCLE_MAX_LR\"],\n",
    "  total_steps      = len(train_loader)*params.hparams[\"MAX_EPOCHS\"], # batches × epochs\n",
    "  pct_start        = params.hparams[\"ONECYCLE_PCT_START\"],\n",
    "  div_factor       = params.hparams[\"ONECYCLE_DIV_FACTOR\"],\n",
    "  final_div_factor = params.hparams[\"ONECYCLE_FINAL_DIV\"],\n",
    "  anneal_strategy  = params.hparams[\"ONECYCLE_STRATEGY\"],\n",
    ")\n",
    "\n",
    "globals()[\"collect_or_run_forward_micro_snapshot\"] = models_core.collect_or_run_forward_micro_snapshot\n",
    "if hasattr(model, \"_micro_snapshot\"):\n",
    "    delattr(model, \"_micro_snapshot\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse  = params.model_selected.model_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    scheduler           = scheduler,\n",
    "    scaler              = GradScaler(),\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = params.hparams['MAX_EPOCHS'],\n",
    "    early_stop_patience = params.hparams['EARLY_STOP_PATIENCE'],\n",
    "    clipnorm            = params.hparams['CLIPNORM'],\n",
    "    alpha_smooth        = params.hparams['ALPHA_SMOOTH']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
