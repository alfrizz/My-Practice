{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f06a4f-691a-4a84-a305-e7212eb879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python’s garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import importlib\n",
    "from libs import trades, plots, params, models\n",
    "importlib.reload(trades)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "importlib.reload(models)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import math\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Funct\n",
    "from torch_lr_finder import LRFinder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obv</th>\n",
       "      <th>hour</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>vwap_dev</th>\n",
       "      <th>open</th>\n",
       "      <th>ma_20</th>\n",
       "      <th>ma_5</th>\n",
       "      <th>close</th>\n",
       "      <th>atr_14</th>\n",
       "      <th>macd_12_26</th>\n",
       "      <th>bb_width_20</th>\n",
       "      <th>in_trading</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>signal_smooth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-04-03 10:49:00</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.650</td>\n",
       "      <td>1.240040e-16</td>\n",
       "      <td>28.650</td>\n",
       "      <td>28.650000</td>\n",
       "      <td>28.65000</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28.641405</td>\n",
       "      <td>28.658595</td>\n",
       "      <td>0.131792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 10:50:00</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.650</td>\n",
       "      <td>1.240040e-16</td>\n",
       "      <td>28.650</td>\n",
       "      <td>28.650000</td>\n",
       "      <td>28.65000</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28.641405</td>\n",
       "      <td>28.658595</td>\n",
       "      <td>0.132326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 10:51:00</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.650</td>\n",
       "      <td>1.240040e-16</td>\n",
       "      <td>28.650</td>\n",
       "      <td>28.650000</td>\n",
       "      <td>28.65000</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28.641405</td>\n",
       "      <td>28.658595</td>\n",
       "      <td>0.132862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 10:52:00</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.650</td>\n",
       "      <td>1.240040e-16</td>\n",
       "      <td>28.650</td>\n",
       "      <td>28.650000</td>\n",
       "      <td>28.65000</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28.641405</td>\n",
       "      <td>28.658595</td>\n",
       "      <td>0.133399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03 10:53:00</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>28.650</td>\n",
       "      <td>1.240040e-16</td>\n",
       "      <td>28.650</td>\n",
       "      <td>28.650000</td>\n",
       "      <td>28.65000</td>\n",
       "      <td>28.6500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28.641405</td>\n",
       "      <td>28.658595</td>\n",
       "      <td>0.133939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>3.294477e+08</td>\n",
       "      <td>20</td>\n",
       "      <td>173.6771</td>\n",
       "      <td>173.215</td>\n",
       "      <td>1.249034e+00</td>\n",
       "      <td>173.375</td>\n",
       "      <td>174.838390</td>\n",
       "      <td>173.91300</td>\n",
       "      <td>173.5650</td>\n",
       "      <td>0.304529</td>\n",
       "      <td>-0.422065</td>\n",
       "      <td>0.014819</td>\n",
       "      <td>1</td>\n",
       "      <td>173.512900</td>\n",
       "      <td>173.617100</td>\n",
       "      <td>0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>3.288235e+08</td>\n",
       "      <td>20</td>\n",
       "      <td>173.5900</td>\n",
       "      <td>173.240</td>\n",
       "      <td>1.246621e+00</td>\n",
       "      <td>173.565</td>\n",
       "      <td>174.736890</td>\n",
       "      <td>173.73700</td>\n",
       "      <td>173.3800</td>\n",
       "      <td>0.317029</td>\n",
       "      <td>-0.466939</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>1</td>\n",
       "      <td>173.328000</td>\n",
       "      <td>173.432000</td>\n",
       "      <td>0.005465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>3.283690e+08</td>\n",
       "      <td>20</td>\n",
       "      <td>173.4100</td>\n",
       "      <td>173.200</td>\n",
       "      <td>1.245701e+00</td>\n",
       "      <td>173.390</td>\n",
       "      <td>174.634390</td>\n",
       "      <td>173.53500</td>\n",
       "      <td>173.3100</td>\n",
       "      <td>0.323814</td>\n",
       "      <td>-0.502359</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>1</td>\n",
       "      <td>173.258000</td>\n",
       "      <td>173.362000</td>\n",
       "      <td>0.007152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>3.272742e+08</td>\n",
       "      <td>20</td>\n",
       "      <td>173.4000</td>\n",
       "      <td>173.230</td>\n",
       "      <td>1.245284e+00</td>\n",
       "      <td>173.315</td>\n",
       "      <td>174.527890</td>\n",
       "      <td>173.38100</td>\n",
       "      <td>173.2800</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>-0.526778</td>\n",
       "      <td>0.018221</td>\n",
       "      <td>1</td>\n",
       "      <td>173.228000</td>\n",
       "      <td>173.332000</td>\n",
       "      <td>0.007898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>3.349241e+08</td>\n",
       "      <td>21</td>\n",
       "      <td>174.0500</td>\n",
       "      <td>173.170</td>\n",
       "      <td>1.249352e+00</td>\n",
       "      <td>173.300</td>\n",
       "      <td>174.442375</td>\n",
       "      <td>173.42894</td>\n",
       "      <td>173.6097</td>\n",
       "      <td>0.374521</td>\n",
       "      <td>-0.513606</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>0</td>\n",
       "      <td>173.557600</td>\n",
       "      <td>173.661800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1779401 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              obv  hour      high      low      vwap_dev  \\\n",
       "2014-04-03 10:49:00  0.000000e+00    10   28.6500   28.650  1.240040e-16   \n",
       "2014-04-03 10:50:00  0.000000e+00    10   28.6500   28.650  1.240040e-16   \n",
       "2014-04-03 10:51:00  0.000000e+00    10   28.6500   28.650  1.240040e-16   \n",
       "2014-04-03 10:52:00  0.000000e+00    10   28.6500   28.650  1.240040e-16   \n",
       "2014-04-03 10:53:00  0.000000e+00    10   28.6500   28.650  1.240040e-16   \n",
       "...                           ...   ...       ...      ...           ...   \n",
       "2025-06-18 20:56:00  3.294477e+08    20  173.6771  173.215  1.249034e+00   \n",
       "2025-06-18 20:57:00  3.288235e+08    20  173.5900  173.240  1.246621e+00   \n",
       "2025-06-18 20:58:00  3.283690e+08    20  173.4100  173.200  1.245701e+00   \n",
       "2025-06-18 20:59:00  3.272742e+08    20  173.4000  173.230  1.245284e+00   \n",
       "2025-06-18 21:00:00  3.349241e+08    21  174.0500  173.170  1.249352e+00   \n",
       "\n",
       "                        open       ma_20       ma_5     close    atr_14  \\\n",
       "2014-04-03 10:49:00   28.650   28.650000   28.65000   28.6500  0.000000   \n",
       "2014-04-03 10:50:00   28.650   28.650000   28.65000   28.6500  0.000000   \n",
       "2014-04-03 10:51:00   28.650   28.650000   28.65000   28.6500  0.000000   \n",
       "2014-04-03 10:52:00   28.650   28.650000   28.65000   28.6500  0.000000   \n",
       "2014-04-03 10:53:00   28.650   28.650000   28.65000   28.6500  0.000000   \n",
       "...                      ...         ...        ...       ...       ...   \n",
       "2025-06-18 20:56:00  173.375  174.838390  173.91300  173.5650  0.304529   \n",
       "2025-06-18 20:57:00  173.565  174.736890  173.73700  173.3800  0.317029   \n",
       "2025-06-18 20:58:00  173.390  174.634390  173.53500  173.3100  0.323814   \n",
       "2025-06-18 20:59:00  173.315  174.527890  173.38100  173.2800  0.322743   \n",
       "2025-06-18 21:00:00  173.300  174.442375  173.42894  173.6097  0.374521   \n",
       "\n",
       "                     macd_12_26  bb_width_20  in_trading         bid  \\\n",
       "2014-04-03 10:49:00    0.000000     0.000000           0   28.641405   \n",
       "2014-04-03 10:50:00    0.000000     0.000000           0   28.641405   \n",
       "2014-04-03 10:51:00    0.000000     0.000000           0   28.641405   \n",
       "2014-04-03 10:52:00    0.000000     0.000000           0   28.641405   \n",
       "2014-04-03 10:53:00    0.000000     0.000000           0   28.641405   \n",
       "...                         ...          ...         ...         ...   \n",
       "2025-06-18 20:56:00   -0.422065     0.014819           1  173.512900   \n",
       "2025-06-18 20:57:00   -0.466939     0.016242           1  173.328000   \n",
       "2025-06-18 20:58:00   -0.502359     0.017430           1  173.258000   \n",
       "2025-06-18 20:59:00   -0.526778     0.018221           1  173.228000   \n",
       "2025-06-18 21:00:00   -0.513606     0.018282           0  173.557600   \n",
       "\n",
       "                            ask  signal_smooth  \n",
       "2014-04-03 10:49:00   28.658595       0.131792  \n",
       "2014-04-03 10:50:00   28.658595       0.132326  \n",
       "2014-04-03 10:51:00   28.658595       0.132862  \n",
       "2014-04-03 10:52:00   28.658595       0.133399  \n",
       "2014-04-03 10:53:00   28.658595       0.133939  \n",
       "...                         ...            ...  \n",
       "2025-06-18 20:56:00  173.617100       0.001063  \n",
       "2025-06-18 20:57:00  173.432000       0.005465  \n",
       "2025-06-18 20:58:00  173.362000       0.007152  \n",
       "2025-06-18 20:59:00  173.332000       0.007898  \n",
       "2025-06-18 21:00:00  173.661800       0.000000  \n",
       "\n",
       "[1779401 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back      = params.look_back_tick \n",
    "features_cols  = params.features_cols_tick\n",
    "label_col      = params.label_col\n",
    "\n",
    "date = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "regular_start_pred  = params.regular_start_pred\n",
    "\n",
    "# USE GPU if available, otherwise fallback to CPU\n",
    "device = params.device\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "df_raw = pd.read_csv(params.ready_csv, index_col=0, parse_dates=True)\n",
    "df = models.feature_engineering(df_raw, features_cols, label_col)\n",
    "df.to_csv(params.final_csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b805fa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X         = torch.Size([1441001, 120, 13]) (samples, look_back, features)\n",
      "  y         = torch.Size([1441001]) (samples,)\n",
      "  raw_close = torch.Size([1441001])\n",
      "  raw_bid   = torch.Size([1441001])\n",
      "  raw_ask   = torch.Size([1441001])\n"
     ]
    }
   ],
   "source": [
    "X, y, raw_close, raw_bid, raw_ask = models.build_lstm_tensors(\n",
    "    df=df,\n",
    "    look_back=look_back,\n",
    "    features_cols=features_cols,\n",
    "    label_col=label_col,\n",
    "    regular_start=regular_start_pred\n",
    ")\n",
    "\n",
    "# quick shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"  X         =\", X.shape,    \"(samples, look_back, features)\")\n",
    "print(\"  y         =\", y.shape,    \"(samples,)\")\n",
    "print(\"  raw_close =\", raw_close.shape)\n",
    "print(\"  raw_bid   =\", raw_bid.shape)\n",
    "print(\"  raw_ask   =\", raw_ask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4f1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_tr        = torch.Size([1013805, 120, 13])\n",
      "  y_tr        = torch.Size([1013805])\n",
      "  raw_close_te= torch.Size([215642])\n",
      "  raw_bid_te  = torch.Size([215642])\n",
      "  raw_ask_te  = torch.Size([215642])\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val/test by calendar day\n",
    "(X_tr, y_tr), \\\n",
    "(X_val, y_val), \\\n",
    "(X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "samples_per_day, day_id_tr, day_id_val, day_id_te = models.chronological_split(\n",
    "    X, y, raw_close, raw_bid, raw_ask, df,\n",
    "    look_back   = look_back,\n",
    "    regular_start   = regular_start_pred,\n",
    "    train_prop  = params.train_prop,\n",
    "    val_prop    = params.val_prop,\n",
    "    train_batch = params.hparams['TRAIN_BATCH']\n",
    ")\n",
    "\n",
    "# Print shapes of all tensors\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_tr        =\", X_tr.shape)\n",
    "print(\"  y_tr        =\", y_tr.shape)\n",
    "print(\"  raw_close_te=\", raw_close_te.shape)\n",
    "print(\"  raw_bid_te  =\", raw_bid_te.shape)\n",
    "print(\"  raw_ask_te  =\", raw_ask_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad9b6c4-3d79-45c0-b2c0-c4f46f1ad866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Entered split_to_day_datasets\n",
      "1) building weekday arrays\n",
      "   Weekdays counts → tr=1013805, val=211554, te=215642\n",
      "2) moving all splits to CPU\n",
      "   CPU casts done\n",
      "3) zero-bas­ing day_id for val & test\n",
      "   val_day_id ∈ [0..413], total days=414\n",
      "   te_day_id  ∈ [0..421], total days=422\n",
      "4) instantiating DayWindowDatasets\n",
      "   ds_tr days: 1984\n",
      "   ds_val days: 414\n",
      "   ds_te days: 422\n",
      "5) building DataLoaders\n",
      "   train_loader ready\n",
      "   val_loader ready\n",
      "   test_loader ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Build DataLoaders over calendar‐days\n",
    "# -----------------------------------------------------------------------------\n",
    "train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "    # Training split arrays (from chronological_split)\n",
    "    X_tr, y_tr, day_id_tr,\n",
    "    # Validation split arrays\n",
    "    X_val, y_val, day_id_val,\n",
    "    # Test split arrays + raw prices for post‐tracking\n",
    "    X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    # Original minute‐bar DataFrame for weekday mapping\n",
    "    df=df,\n",
    "    train_batch=params.hparams['TRAIN_BATCH'],\n",
    "    train_workers=params.hparams['NUM_WORKERS'],\n",
    "    train_prefetch_factor=params.hparams['TRAIN_PREFETCH_FACTOR']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualMemoryLSTM(\n",
       "  (short_lstm): LSTM(13, 64, batch_first=True)\n",
       "  (attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (do_short): Dropout(p=0.2, inplace=False)\n",
       "  (ln_short): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (long_lstm): LSTM(64, 96, batch_first=True)\n",
       "  (do_long): Dropout(p=0.25, inplace=False)\n",
       "  (ln_long): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  (pred): Linear(in_features=96, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate the stateful DualMemoryLSTM & move to device\n",
    "# -----------------------------------------------------------------------------\n",
    "model = models.DualMemoryLSTM(\n",
    "    n_feats        = len(features_cols),                          \n",
    "    short_units    = params.hparams['SHORT_UNITS'],    \n",
    "    long_units     = params.hparams['LONG_UNITS'],     \n",
    "    dropout_short  = params.hparams['DROPOUT_SHORT'],  \n",
    "    dropout_long   = params.hparams['DROPOUT_LONG'],   \n",
    "    att_heads      = params.hparams['ATT_HEADS'],\n",
    "    att_drop       = params.hparams['ATT_DROPOUT']\n",
    ")\n",
    "model.to(device)   # place model parameters on GPU or CPU as specified\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5512a0dd-d2c8-418e-bfca-4580fb4be995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.0005\n",
       "    lr: 0.0005\n",
       "    maximize: False\n",
       "    weight_decay: 1e-05\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute plateau_sched timing parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# Total training samples = total windows in X_tr (one window per row)\n",
    "n_train_samples = X_tr.shape[0]\n",
    "\n",
    "# How many optimizer steps (day‐bundles) constitute one epoch?\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build optimizer, LR scheduler, AMP scaler, and gradient‐clip norm\n",
    "# -----------------------------------------------------------------------------\n",
    "optimizer, plateau_sched, cosine_sched, scaler, clipnorm = models.make_optimizer_and_scheduler(\n",
    "    model,\n",
    "    initial_lr        = params.hparams['INITIAL_LR'],       \n",
    "    weight_decay      = params.hparams['WEIGHT_DECAY'],     \n",
    "    clipnorm          = params.hparams['CLIPNORM']   \n",
    ")\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e70105f-bbe5-4ce0-aabe-acf9193ef401",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer_cpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m      8\u001b[0m     model_cpu\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m      9\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,        \u001b[38;5;66;03m# placeholder; the finder will override this\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2) Create a tiny DataLoader (batch_size=1) to save memory\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m small_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(\n\u001b[1;32m     15\u001b[0m     train_loader\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[1;32m     16\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 3) Define an aligned MSE that permutes/expands your [1,1,D] or [D,1,1]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#    target → [D, T, 1] to match output shape exactly.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maligned_mse\u001b[39m(output, target):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# output: [D, T, 1]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# target might come in as [D,1,1] or [1,1,D]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# function to find the optimal learning rate\n",
    "###########################################\n",
    "\n",
    "# 1) Move model to CPU and build a fresh optimizer (no scheduler metadata)\n",
    "model_cpu = model.cpu()\n",
    "optimizer_cpu = torch.optim.AdamW(\n",
    "    model_cpu.parameters(),\n",
    "    lr=1e-3,        # placeholder; the finder will override this\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "# 2) Create a tiny DataLoader (batch_size=1) to save memory\n",
    "small_loader = DataLoader(\n",
    "    train_loader.dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# 3) Define an aligned MSE that permutes/expands your [1,1,D] or [D,1,1]\n",
    "#    target → [D, T, 1] to match output shape exactly.\n",
    "def aligned_mse(output, target):\n",
    "    # output: [D, T, 1]\n",
    "    # target might come in as [D,1,1] or [1,1,D]\n",
    "    tgt = target\n",
    "\n",
    "    # Case A: target == [D, 1, 1] → expand middle dim to T\n",
    "    if tgt.dim() == 3 and tgt.shape[0] == output.shape[0] \\\n",
    "       and tgt.shape[1] == 1 and tgt.shape[2] == 1:\n",
    "        tgt = tgt.expand(-1, output.size(1), -1)\n",
    "\n",
    "    # Case B: target == [1, 1, D] → permute to [D,1,1] then expand\n",
    "    elif tgt.dim() == 3 and tgt.shape[0] == 1 \\\n",
    "         and tgt.shape[1] == 1 and tgt.shape[2] == output.shape[0]:\n",
    "        # permute (0,1,2) → (2,1,0) to get [D,1,1]\n",
    "        tgt = tgt.permute(2, 1, 0)\n",
    "        tgt = tgt.expand(-1, output.size(1), -1)\n",
    "\n",
    "    else:\n",
    "        # fallback: broadcast to exactly output.shape\n",
    "        tgt = tgt.expand(output.shape)\n",
    "\n",
    "    return Funct.mse_loss(output, tgt)\n",
    "\n",
    "# 4) Free any lingering GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 5) Run the LR‐Finder on CPU for just 30 mini‐batches\n",
    "lr_finder = LRFinder(\n",
    "    model_cpu,\n",
    "    optimizer_cpu,\n",
    "    aligned_mse,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "lr_finder.range_test(\n",
    "    small_loader,\n",
    "    end_lr=1,     # maximum LR to try\n",
    "    num_iter=30   # number of batches\n",
    ")\n",
    "lr_finder.plot()   # examine loss vs. LR curve\n",
    "lr_finder.reset()  # restore original model & optimizer states\n",
    "\n",
    "# 6) Move model back to GPU for your main training\n",
    "model = model_cpu.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94780-a876-4bf4-ad27-6abc2da1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Count how many calendar days we see each epoch and Compute baseline RMSE on validation (zero forecast)\n",
    "# -----------------------------------------------------------------------------\n",
    "n_train_days = len(train_loader.dataset)  # dataset length = # unique days\n",
    "print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "\n",
    "baseline_val_rmse = models.naive_rmse(val_loader)\n",
    "print(f\"Baseline (zero‐forecast) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse  = models.custom_stateful_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    cosine_sched        = cosine_sched,\n",
    "    plateau_sched       = plateau_sched,\n",
    "    scaler              = scaler,\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    "    max_epochs          = params.hparams['MAX_EPOCHS'],\n",
    "    early_stop_patience = params.hparams['EARLY_STOP_PATIENCE'],\n",
    "    baseline_val_rmse   = baseline_val_rmse,\n",
    "    clipnorm            = clipnorm,\n",
    "    device              = device\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Final reporting: best RMSE and relative improvement\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\\nChampion validation RMSE = {best_val_rmse:.6f}\")\n",
    "\n",
    "improvement = 100.0 * (1.0 - best_val_rmse / baseline_val_rmse)\n",
    "print(f\"Improvement over zero‐baseline = {improvement:5.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8660dd-d2db-434a-aa59-17814d343fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c3b45-e00a-4a85-860d-4f87b71cbbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
