{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83643614-b6ed-4209-97bf-be942fd07b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python’s garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import importlib\n",
    "from libs import trades, plots, params, models\n",
    "importlib.reload(trades)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "importlib.reload(models)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression, RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance, partial_dependence, PartialDependenceDisplay\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "\n",
    "import shap\n",
    "from shap import SamplingExplainer\n",
    "\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa382fc8-28a5-4696-aca1-6786b840aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = params.feats_cols_all\n",
    "label_col = params.label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627c81bb-036e-4b6d-8fcb-d3964edd32fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>r_1</th>\n",
       "      <th>r_5</th>\n",
       "      <th>r_15</th>\n",
       "      <th>vol_15</th>\n",
       "      <th>volume_spike</th>\n",
       "      <th>atr_14</th>\n",
       "      <th>vwap_dev</th>\n",
       "      <th>rsi_14</th>\n",
       "      <th>bb_width_20</th>\n",
       "      <th>stoch_k_14</th>\n",
       "      <th>stoch_d_3</th>\n",
       "      <th>ma_5</th>\n",
       "      <th>ma_20</th>\n",
       "      <th>ma_diff</th>\n",
       "      <th>macd_12_26</th>\n",
       "      <th>macd_signal_9</th>\n",
       "      <th>obv</th>\n",
       "      <th>in_trading</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-02 13:09:00</th>\n",
       "      <td>0.764235</td>\n",
       "      <td>0.764235</td>\n",
       "      <td>0.764235</td>\n",
       "      <td>0.764235</td>\n",
       "      <td>48081.25</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.567313</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.764266</td>\n",
       "      <td>0.764281</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.241625e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 13:10:00</th>\n",
       "      <td>0.764219</td>\n",
       "      <td>0.764219</td>\n",
       "      <td>0.764219</td>\n",
       "      <td>0.764219</td>\n",
       "      <td>54775.00</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.687332</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.764252</td>\n",
       "      <td>0.764277</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-1.789375e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 13:11:00</th>\n",
       "      <td>0.764202</td>\n",
       "      <td>0.764202</td>\n",
       "      <td>0.764202</td>\n",
       "      <td>0.764202</td>\n",
       "      <td>61468.75</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.771753</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.764235</td>\n",
       "      <td>0.764273</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-2.404062e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.928260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 13:12:00</th>\n",
       "      <td>0.764185</td>\n",
       "      <td>0.764185</td>\n",
       "      <td>0.764185</td>\n",
       "      <td>0.764185</td>\n",
       "      <td>68162.50</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.823929</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.764219</td>\n",
       "      <td>0.764268</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-3.085688e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-01-02 13:13:00</th>\n",
       "      <td>0.764169</td>\n",
       "      <td>0.764169</td>\n",
       "      <td>0.764169</td>\n",
       "      <td>0.764169</td>\n",
       "      <td>74856.25</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.848531</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.764202</td>\n",
       "      <td>0.764262</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-3.834250e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:56:00</th>\n",
       "      <td>196.680000</td>\n",
       "      <td>196.860000</td>\n",
       "      <td>196.630000</td>\n",
       "      <td>196.815000</td>\n",
       "      <td>385695.00</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>1.703441</td>\n",
       "      <td>0.276029</td>\n",
       "      <td>9.688403</td>\n",
       "      <td>69.675681</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>72.169811</td>\n",
       "      <td>62.051653</td>\n",
       "      <td>196.593000</td>\n",
       "      <td>196.325795</td>\n",
       "      <td>0.267205</td>\n",
       "      <td>0.175662</td>\n",
       "      <td>0.134526</td>\n",
       "      <td>1.998180e+10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:57:00</th>\n",
       "      <td>196.810000</td>\n",
       "      <td>196.940000</td>\n",
       "      <td>196.560000</td>\n",
       "      <td>196.675000</td>\n",
       "      <td>460630.00</td>\n",
       "      <td>-0.000712</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>1.841981</td>\n",
       "      <td>0.291029</td>\n",
       "      <td>9.680776</td>\n",
       "      <td>62.080501</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>58.567483</td>\n",
       "      <td>64.412431</td>\n",
       "      <td>196.650000</td>\n",
       "      <td>196.348545</td>\n",
       "      <td>0.301455</td>\n",
       "      <td>0.179462</td>\n",
       "      <td>0.143513</td>\n",
       "      <td>1.998134e+10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:58:00</th>\n",
       "      <td>196.675000</td>\n",
       "      <td>196.740000</td>\n",
       "      <td>196.630000</td>\n",
       "      <td>196.670000</td>\n",
       "      <td>525245.00</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>1.883214</td>\n",
       "      <td>0.291743</td>\n",
       "      <td>9.680476</td>\n",
       "      <td>60.449896</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>58.091247</td>\n",
       "      <td>62.942847</td>\n",
       "      <td>196.672000</td>\n",
       "      <td>196.369045</td>\n",
       "      <td>0.302955</td>\n",
       "      <td>0.179996</td>\n",
       "      <td>0.150810</td>\n",
       "      <td>1.998082e+10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 20:59:00</th>\n",
       "      <td>196.680000</td>\n",
       "      <td>196.750000</td>\n",
       "      <td>196.240000</td>\n",
       "      <td>196.240000</td>\n",
       "      <td>2075503.00</td>\n",
       "      <td>-0.002189</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>5.053932</td>\n",
       "      <td>0.313971</td>\n",
       "      <td>9.657013</td>\n",
       "      <td>51.403049</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>17.134965</td>\n",
       "      <td>44.597898</td>\n",
       "      <td>196.618000</td>\n",
       "      <td>196.367545</td>\n",
       "      <td>0.250455</td>\n",
       "      <td>0.144060</td>\n",
       "      <td>0.149460</td>\n",
       "      <td>1.997874e+10</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-18 21:00:00</th>\n",
       "      <td>196.580000</td>\n",
       "      <td>196.650000</td>\n",
       "      <td>196.080000</td>\n",
       "      <td>196.540000</td>\n",
       "      <td>15600625.00</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>10.811366</td>\n",
       "      <td>0.344693</td>\n",
       "      <td>9.672467</td>\n",
       "      <td>58.461474</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>44.927536</td>\n",
       "      <td>40.051249</td>\n",
       "      <td>196.588000</td>\n",
       "      <td>196.385795</td>\n",
       "      <td>0.202205</td>\n",
       "      <td>0.138196</td>\n",
       "      <td>0.147207</td>\n",
       "      <td>1.999434e+10</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2742735 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           open        high         low       close  \\\n",
       "2004-01-02 13:09:00    0.764235    0.764235    0.764235    0.764235   \n",
       "2004-01-02 13:10:00    0.764219    0.764219    0.764219    0.764219   \n",
       "2004-01-02 13:11:00    0.764202    0.764202    0.764202    0.764202   \n",
       "2004-01-02 13:12:00    0.764185    0.764185    0.764185    0.764185   \n",
       "2004-01-02 13:13:00    0.764169    0.764169    0.764169    0.764169   \n",
       "...                         ...         ...         ...         ...   \n",
       "2025-06-18 20:56:00  196.680000  196.860000  196.630000  196.815000   \n",
       "2025-06-18 20:57:00  196.810000  196.940000  196.560000  196.675000   \n",
       "2025-06-18 20:58:00  196.675000  196.740000  196.630000  196.670000   \n",
       "2025-06-18 20:59:00  196.680000  196.750000  196.240000  196.240000   \n",
       "2025-06-18 21:00:00  196.580000  196.650000  196.080000  196.540000   \n",
       "\n",
       "                          volume       r_1       r_5      r_15    vol_15  \\\n",
       "2004-01-02 13:09:00     48081.25 -0.000022 -0.000066 -0.000066  0.000009   \n",
       "2004-01-02 13:10:00     54775.00 -0.000022 -0.000088 -0.000088  0.000010   \n",
       "2004-01-02 13:11:00     61468.75 -0.000022 -0.000110 -0.000110  0.000011   \n",
       "2004-01-02 13:12:00     68162.50 -0.000022 -0.000110 -0.000131  0.000011   \n",
       "2004-01-02 13:13:00     74856.25 -0.000022 -0.000110 -0.000153  0.000011   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "2025-06-18 20:56:00    385695.00  0.000635  0.002846  0.003639  0.000804   \n",
       "2025-06-18 20:57:00    460630.00 -0.000712  0.001450  0.003081  0.000836   \n",
       "2025-06-18 20:58:00    525245.00 -0.000025  0.000559  0.002291  0.000823   \n",
       "2025-06-18 20:59:00   2075503.00 -0.002189 -0.001375 -0.000255  0.001017   \n",
       "2025-06-18 21:00:00  15600625.00  0.001528 -0.000763  0.001833  0.001078   \n",
       "\n",
       "                     volume_spike    atr_14  vwap_dev     rsi_14  bb_width_20  \\\n",
       "2004-01-02 13:09:00      1.567313  0.000004 -0.000061   0.000000     0.000070   \n",
       "2004-01-02 13:10:00      1.687332  0.000005 -0.000079   0.000000     0.000101   \n",
       "2004-01-02 13:11:00      1.771753  0.000006 -0.000096   0.000000     0.000133   \n",
       "2004-01-02 13:12:00      1.823929  0.000007 -0.000112   0.000000     0.000167   \n",
       "2004-01-02 13:13:00      1.848531  0.000008 -0.000127   0.000000     0.000202   \n",
       "...                           ...       ...       ...        ...          ...   \n",
       "2025-06-18 20:56:00      1.703441  0.276029  9.688403  69.675681     0.004172   \n",
       "2025-06-18 20:57:00      1.841981  0.291029  9.680776  62.080501     0.004427   \n",
       "2025-06-18 20:58:00      1.883214  0.291743  9.680476  60.449896     0.004636   \n",
       "2025-06-18 20:59:00      5.053932  0.313971  9.657013  51.403049     0.004652   \n",
       "2025-06-18 21:00:00     10.811366  0.344693  9.672467  58.461474     0.004619   \n",
       "\n",
       "                     stoch_k_14  stoch_d_3        ma_5       ma_20   ma_diff  \\\n",
       "2004-01-02 13:09:00    0.000000   0.000000    0.764266    0.764281 -0.000015   \n",
       "2004-01-02 13:10:00    0.000000   0.000000    0.764252    0.764277 -0.000025   \n",
       "2004-01-02 13:11:00    0.000000   0.000000    0.764235    0.764273 -0.000038   \n",
       "2004-01-02 13:12:00    0.000000   0.000000    0.764219    0.764268 -0.000049   \n",
       "2004-01-02 13:13:00    0.000000   0.000000    0.764202    0.764262 -0.000060   \n",
       "...                         ...        ...         ...         ...       ...   \n",
       "2025-06-18 20:56:00   72.169811  62.051653  196.593000  196.325795  0.267205   \n",
       "2025-06-18 20:57:00   58.567483  64.412431  196.650000  196.348545  0.301455   \n",
       "2025-06-18 20:58:00   58.091247  62.942847  196.672000  196.369045  0.302955   \n",
       "2025-06-18 20:59:00   17.134965  44.597898  196.618000  196.367545  0.250455   \n",
       "2025-06-18 21:00:00   44.927536  40.051249  196.588000  196.385795  0.202205   \n",
       "\n",
       "                     macd_12_26  macd_signal_9           obv  in_trading  \\\n",
       "2004-01-02 13:09:00   -0.000007      -0.000002 -1.241625e+05           0   \n",
       "2004-01-02 13:10:00   -0.000011      -0.000004 -1.789375e+05           0   \n",
       "2004-01-02 13:11:00   -0.000015      -0.000006 -2.404062e+05           0   \n",
       "2004-01-02 13:12:00   -0.000019      -0.000009 -3.085688e+05           0   \n",
       "2004-01-02 13:13:00   -0.000024      -0.000012 -3.834250e+05           0   \n",
       "...                         ...            ...           ...         ...   \n",
       "2025-06-18 20:56:00    0.175662       0.134526  1.998180e+10           1   \n",
       "2025-06-18 20:57:00    0.179462       0.143513  1.998134e+10           1   \n",
       "2025-06-18 20:58:00    0.179996       0.150810  1.998082e+10           1   \n",
       "2025-06-18 20:59:00    0.144060       0.149460  1.997874e+10           1   \n",
       "2025-06-18 21:00:00    0.138196       0.147207  1.999434e+10           0   \n",
       "\n",
       "                     hour  day_of_week  month    signal  \n",
       "2004-01-02 13:09:00    13            4      1  0.915187  \n",
       "2004-01-02 13:10:00    13            4      1  0.921701  \n",
       "2004-01-02 13:11:00    13            4      1  0.928260  \n",
       "2004-01-02 13:12:00    13            4      1  0.934864  \n",
       "2004-01-02 13:13:00    13            4      1  0.941513  \n",
       "...                   ...          ...    ...       ...  \n",
       "2025-06-18 20:56:00    20            2      6  0.000000  \n",
       "2025-06-18 20:57:00    20            2      6  0.000000  \n",
       "2025-06-18 20:58:00    20            2      6  0.000000  \n",
       "2025-06-18 20:59:00    20            2      6  0.000000  \n",
       "2025-06-18 21:00:00    21            2      6  0.000000  \n",
       "\n",
       "[2742735 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_sign = pd.read_csv(params.sign_csv, index_col=0, parse_dates=True)\n",
    "\n",
    "# apply feature engineering to all features\n",
    "df_raw = models.feature_engineering(df_sign, features_cols, label_col)\n",
    "df_raw_drop = df_raw.drop(['bid','ask'],axis=1)\n",
    "df_raw_drop # df after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1316b-3f38-4457-91c1-50ec4eaa5f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc9436146f84a20be8f8f8c9a5e34b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scaling price per day (train):   0%|          | 0/3790 [00:00<?, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_feat = models.scale_with_splits(\n",
    "                            df = df_raw,\n",
    "                            features_cols = features_cols,\n",
    "                            label_col = params.label_col,\n",
    "                            train_prop = params.train_prop,\n",
    "                            val_prop = params.val_prop\n",
    ")\n",
    "print('saving df ...')\n",
    "df_feat.to_csv(params.feat_csv) # saving also bid and ask for future use\n",
    "df_feat.drop(['bid','ask'],axis=1,inplace=True)\n",
    "df_feat # df after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf5891-d196-418b-9bf5-acd54d995cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_dual_histograms(\n",
    "    df_before = df_raw_drop,\n",
    "    df_after  = df_feat,\n",
    "    features  = features_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7317c-6c53-49df-9cb6-b1cfa8945cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83795e52-9848-4d40-9d8f-d534697984b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the master dict of feature importances\n",
    "features_importances = {\n",
    "    feat: {}\n",
    "    for feat in features_cols\n",
    "} \n",
    "\n",
    "X = df_feat[features_cols]\n",
    "y = df_feat[label_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afb911-feee-48d2-9c42-49136066d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: update the dict\n",
    "def update_feature_importances(fi_dict, importance_type, values: pd.Series):\n",
    "    \"\"\"\n",
    "    fi_dict: master dict\n",
    "    importance_type: one of \"corr\",\"mi\",\"perm\",\"shap\",\"lasso\"\n",
    "    values: pd.Series indexed by feature name\n",
    "    \"\"\"\n",
    "    for feat, val in values.items():\n",
    "        if feat in fi_dict:\n",
    "            fi_dict[feat][importance_type] = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795c2d05-c105-4b89-bf79-779417d7e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_feature_importance(\n",
    "    df: pd.DataFrame,\n",
    "    features: list[str],\n",
    "    label: str,\n",
    "    method: str,               # \"corr\", \"mi\", \"perm\", \"shap\", \"lasso\"\n",
    "    compute_fn,                # function(feature_name) → importance_value\n",
    "    threshold: float = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic live plotter for a sequence of feature importances.\n",
    "    \n",
    "    - df        : full dataframe\n",
    "    - features  : list of column names to score\n",
    "    - label     : target column name (for context in title)\n",
    "    - method    : text legend (\"Corr\", \"Mutual Info\", etc.)\n",
    "    - compute_fn: a callable f → score[f]\n",
    "    - threshold : optional vertical line in the bar chart\n",
    "    \"\"\"\n",
    "    # accumulator\n",
    "    scores = {}\n",
    "\n",
    "    # loop with progress bar\n",
    "    for f in tqdm(features, desc=f\"{method}\"):\n",
    "        # compute\n",
    "        val = compute_fn(f)\n",
    "        scores[f] = val\n",
    "\n",
    "        # update on‐screen plot\n",
    "        clear_output(wait=True)\n",
    "        series = pd.Series(scores).sort_values()\n",
    "        plt.figure(figsize=(6, max(3, len(series)*0.25)))\n",
    "        sns.barplot(x=series.values, y=series.index, palette=\"vlag\")\n",
    "        if threshold is not None:\n",
    "            plt.axvline(threshold, color=\"gray\", linestyle=\"--\")\n",
    "            if method==\"Corr\":\n",
    "                plt.axvline(-threshold, color=\"gray\", linestyle=\"--\")\n",
    "        plt.title(f\"{method} Importance (partial) for {label}\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "\n",
    "        # tiny pause so the UI can breathe (optional)\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    # final return\n",
    "    return pd.Series(scores).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eb23c-faf4-48db-8a74-e7d0c676578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 1) Correlation live\n",
    "\n",
    "This method uses Pearson’s correlation coefficient to quantify the linear relationship\n",
    "between each input feature and the target variable.  A coefficient ρ close to ±1\n",
    "indicates a strong linear association (positive or negative), whereas a value near 0\n",
    "suggests little to no linear dependency.\n",
    "\n",
    "How this function works:\n",
    "  • For every feature f in your dataset, compute ρ = corr(f, y), the correlation with y.  \n",
    "  • The helper live_feature_importance streams an updating bar chart as you loop\n",
    "    through features, hiding any whose |ρ| falls below a threshold (here 0.05).  \n",
    "  • At the end, you get a pd.Series of all ρ values, sorted by magnitude.  \n",
    "  • Finally, update_feature_importances stores that series under the key \"corr\"\n",
    "    in your master dictionary, so you can compare correlation importance alongside\n",
    "    other measures.\n",
    "'''\n",
    "\n",
    "\n",
    "def corr_fn(feat):\n",
    "    return df_feat[feat].corr(df_feat[label_col])\n",
    "\n",
    "corr_scores = live_feature_importance(\n",
    "    df_feat,\n",
    "    features_cols,\n",
    "    label_col,\n",
    "    method=\"Corr\",\n",
    "    compute_fn=corr_fn,\n",
    "    threshold=0.05\n",
    ")\n",
    "\n",
    "update_feature_importances(features_importances, \"corr\", corr_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16928f21-c6bc-4235-adef-c86246022d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 2) Mutual Information live\n",
    "\n",
    "Mutual Information (MI) measures any statistical dependency—linear or non‐linear—\n",
    "between a feature and the target.  It captures how much knowing the feature\n",
    "reduces uncertainty about the target, with higher MI indicating stronger dependency.\n",
    "\n",
    "How this function works:\n",
    "  • For each feature, call scikit‐learn’s mutual_info_regression, which\n",
    "    nonparametrically estimates MI between that feature and y.  \n",
    "  • live_feature_importance streams a bar chart as MI values accumulate,\n",
    "    highlighting features in order of increasing MI.  \n",
    "  • A threshold of zero ensures all features appear in the live plot.  \n",
    "  • Finally, we record the resulting MI scores in your master dict under \"mi\",\n",
    "    ready for downstream comparison.\n",
    "'''\n",
    "\n",
    "\n",
    "def mi_fn(feat):\n",
    "    return mutual_info_regression(\n",
    "        df_feat[[feat]].fillna(0),\n",
    "        df_feat[label_col],\n",
    "        discrete_features=False,\n",
    "        random_state=0\n",
    "    )[0]\n",
    "\n",
    "mi_scores = live_feature_importance(\n",
    "    df_feat,\n",
    "    features_cols,\n",
    "    label_col,\n",
    "    method=\"Mutual Info\",\n",
    "    compute_fn=mi_fn,\n",
    "    threshold=0.0\n",
    ")\n",
    "\n",
    "update_feature_importances(features_importances, \"mi\", mi_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137e5a8-f73e-4d84-924f-dbe3c7a2752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 3) Lasso live (coordinate descent)\n",
    "\n",
    "Lasso regression augments ordinary least‐squares with an ℓ₁ penalty on coefficients,\n",
    "forcing many weights to zero and performing built‐in feature selection.  Through\n",
    "cross‐validation, it picks the regularization strength α that best balances bias\n",
    "vs. sparsity.\n",
    "\n",
    "How this function works:\n",
    "  1. Standardize X and center y so penalties are comparable across features.  \n",
    "  2. Fit LassoCV over a logarithmic grid of α values with k‐fold CV to choose α.  \n",
    "  3. Once α is selected, take the absolute value of each coefficient as its “importance.”  \n",
    "  4. Stream‐plot these absolute coefficients with live_feature_importance, letting\n",
    "     you watch features sorted by their weight magnitude.  \n",
    "  5. Store the final |coefficients| Series under the key \"lasso\" in your\n",
    "     features_importances dict.\n",
    "'''\n",
    "\n",
    "\n",
    "# 1) Standardize X and y\n",
    "scaler = StandardScaler()\n",
    "Xz = scaler.fit_transform(X)\n",
    "yz = y - y.mean()\n",
    "\n",
    "# 2) Fit LassoCV once for all features\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=np.logspace(-4, -1, 20), # Densify your α grid (np.logspace(-4, -1, 20) → np.logspace(-6, 0, 100))\n",
    "    cv=10, # increase the number of CV folds (cv=5 → cv=10 or 20) to reduce variance in α\n",
    "    max_iter=10000, # Raise max_iter or tighten tol so the solver truly converges on each α.\n",
    "    random_state=0\n",
    ").fit(Xz, yz)\n",
    "\n",
    "# 3) Build a Series of |coefficients|\n",
    "lasso_scores = pd.Series(\n",
    "    np.abs(lasso_cv.coef_),\n",
    "    index=features_cols\n",
    ").sort_values()\n",
    "\n",
    "# 4) Stream‐plot via our helper\n",
    "def lasso_cv_fn(feat):\n",
    "    return lasso_scores[feat]\n",
    "\n",
    "_ = live_feature_importance(\n",
    "    df_feat,\n",
    "    features_cols,\n",
    "    label_col,\n",
    "    method=\"LassoCV\",\n",
    "    compute_fn=lasso_cv_fn,\n",
    "    threshold=0.0\n",
    ")\n",
    "\n",
    "# 5) Store\n",
    "update_feature_importances(features_importances, \"lasso\", lasso_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d62cff-0acf-4ea4-a1be-7f284d5459a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────\n",
    "# Incremental RF fit with warm_start + tqdm\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "'''\n",
    "Initialize a warm‐start RandomForest,\n",
    "grow it one tree at a time up to TARGET_TREES,\n",
    "so we can reuse the same fitted model for RFE, permutation importance, and SHAP.\n",
    "'''\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    warm_start=True,\n",
    "    n_estimators=30, # Using a larger forest (n_estimators) to stabilize .feature_importances_ (slower)\n",
    "    max_depth=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "TARGET_TREES = 25\n",
    "for i in tqdm(range(TARGET_TREES), desc=\"Training RF trees\"):\n",
    "    rf.set_params(n_estimators=i + 1)\n",
    "    rf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6415127-1c42-4c77-b7df-3fe6567001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 4) RFE live (recursive feature elimination with CV)\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a wrapper approach that repeatedly fits\n",
    "a model, ranks features by importance, drops the weakest feature, and refits\n",
    "until only the desired number remain.  This process exposes interactions and\n",
    "collinearity effects as the feature set shrinks.\n",
    "\n",
    "How this function works:\n",
    "  • Start with your full feature list and clone a fresh RF each iteration.  \n",
    "  • Fit the RF on the current subset, extract raw .feature_importances_.  \n",
    "  • Use live_feature_importance to stream‐plot importance bars—dropped features\n",
    "    show zero importance once eliminated.  \n",
    "  • Drop the single least‐important feature and repeat until you keep\n",
    "    n_features_to_keep survivors.  \n",
    "  • Fit one final RF on those survivors, stream the final plot, and then\n",
    "    call update_feature_importances exactly once, saving that last importance\n",
    "    Series under \"rfe\".\n",
    "'''\n",
    "\n",
    "remaining = features_cols.copy()\n",
    "# how many features you want at the end\n",
    "n_features_to_keep = len(remaining)  # this way it runs only once to save time (but a lower number should be set here)\n",
    "\n",
    "rounds = len(remaining) - n_features_to_keep\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "for i in range(rounds):\n",
    "    # cross-validate RF on current subset to get stable importances\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    imps_list = []\n",
    "    for train_idx, val_idx in kf.split(df_feat):\n",
    "        model_cv = clone(rf)\n",
    "        model_cv.warm_start = False\n",
    "        model_cv.fit(df_feat.iloc[train_idx][remaining], df_feat.iloc[train_idx][label_col])\n",
    "        imps_list.append(pd.Series(\n",
    "            model_cv.feature_importances_,\n",
    "            index=remaining\n",
    "        ))\n",
    "\n",
    "    # average importances across folds\n",
    "    imps = pd.concat(imps_list, axis=1).mean(axis=1)\n",
    "\n",
    "    # compute_fn returns 0.0 for dropped features\n",
    "    def compute_fn(feat, imps=imps):\n",
    "        return float(imps.get(feat, 0.0))\n",
    "\n",
    "    # live‐plot\n",
    "    _ = live_feature_importance(\n",
    "        df=df_feat,\n",
    "        features=features_cols,             # full list for consistent axis\n",
    "        label=label_col,                    # <-- correct arg name\n",
    "        method=f\"RFE round {i+1}\",\n",
    "        compute_fn=compute_fn,\n",
    "        threshold=0.0\n",
    "    )\n",
    "    plt.show()                              # force display\n",
    "\n",
    "    # drop the least‐important feature\n",
    "    worst = imps.idxmin()\n",
    "    remaining.remove(worst)\n",
    "\n",
    "# final fit on survivors\n",
    "final = clone(rf)\n",
    "final.warm_start = False\n",
    "final.fit(df_feat[remaining], df_feat[label_col])\n",
    "\n",
    "final_imps = pd.Series(final.feature_importances_, index=remaining)\n",
    "\n",
    "# final live‐plot\n",
    "_ = live_feature_importance(\n",
    "    df=df_feat,\n",
    "    features=features_cols,\n",
    "    label=label_col,\n",
    "    method=\"RFE final\",\n",
    "    compute_fn=lambda feat: float(final_imps.get(feat, 0.0)),\n",
    "    threshold=0.0\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# save only the final importances\n",
    "update_feature_importances(\n",
    "    fi_dict=features_importances,\n",
    "    importance_type=\"rfe\",\n",
    "    values=final_imps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f950389-0694-48dc-a0a5-72a92eb67b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 5) Permutation Importance live (RF) — CV-based\n",
    "\n",
    "Permutation importance measures feature impact on model performance by\n",
    "randomly shuffling one feature’s values and observing the increase in\n",
    "prediction error.  Unlike impurity importances, it reflects the model’s\n",
    "dependence on each feature in held‐out data.\n",
    "\n",
    "How this function works:\n",
    "  A) Impurity importances:\n",
    "     • Grab rf.feature_importances_ from the in‐memory forest and store them\n",
    "       under \"impurity\" (never zero because every tree splits somewhere).  \n",
    "  B) CV-based permutation:\n",
    "     • Perform k‐fold cross‐validation.  In each fold, fit the RF on train,\n",
    "       permute one feature in test, measure the Δ‐MSE, and record the mean\n",
    "       drop over multiple repeats.  \n",
    "     • Average these Δ‐MSE drops across folds to get a stable permutation score.  \n",
    "  C) Streaming:\n",
    "     • Use live_feature_importance to display these CV‐based permutation scores\n",
    "       in real time.  \n",
    "  D) Storage:\n",
    "     • Store the final averaged permutation Series under \"perm\" in your dict.\n",
    "'''\n",
    "\n",
    "\n",
    "# A) Impurity-based importance (never zero)\n",
    "impurity_imp = pd.Series(rf.feature_importances_, index=features_cols)\n",
    "update_feature_importances(features_importances, \"perm_imp\", impurity_imp)\n",
    "\n",
    "# B) CV-based permutation importance\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=0) # boost stability by increasing n_splits\n",
    "perm_scores = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    rf.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    pi = permutation_importance(\n",
    "        rf,\n",
    "        X.iloc[test_idx],\n",
    "        y.iloc[test_idx],\n",
    "        n_repeats=30, # boost stability by increasing the number of repeats (n_repeats=10 → 30 or 50)\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        random_state=0\n",
    "    )\n",
    "    perm_scores.append(pi.importances_mean)\n",
    "\n",
    "perm_mean = np.mean(perm_scores, axis=0)\n",
    "perm_scores_mean = pd.Series(perm_mean, index=features_cols)\n",
    "update_feature_importances(features_importances, \"perm_CV\", perm_scores_mean)\n",
    "\n",
    "# C) live plotting via our helper\n",
    "def perm_fn(feat):\n",
    "    return perm_scores_mean[feat]\n",
    "\n",
    "perm_scores_live = live_feature_importance(\n",
    "    df_feat,\n",
    "    features_cols,\n",
    "    label_col,\n",
    "    method=\"CV-Permute RF\",\n",
    "    compute_fn=perm_fn,\n",
    "    threshold=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516524c5-6604-4c71-aff0-6dc919b595bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 6) SHAP live (sampling‐style via PermutationExplainer)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) uses game‐theoretic Shapley values to\n",
    "attribute each model prediction to individual feature contributions. The\n",
    "mean absolute SHAP value per feature approximates its global importance.\n",
    "\n",
    "How this function works:\n",
    "  • Randomly sample a subset of rows (e.g., 2,000) as a reference for\n",
    "    “missing” value permutations.  \n",
    "  • Instantiate a high‐level shap.Explainer with algorithm=\"permutation\",\n",
    "    wrapping rf.predict and the reference sample.  \n",
    "  • Compute SHAP values on the sample, limiting total model calls per instance\n",
    "    (max_evals=100) for a speed–variance trade‐off.  \n",
    "  • Take the mean of absolute SHAP values across all instances for each feature.  \n",
    "  • Stream these mean |SHAP| values with live_feature_importance.  \n",
    "  • Store the final Series under \"shap\" for comparison with other methods.\n",
    "'''\n",
    "\n",
    "# 1) build a small reference set for “missing” value draws\n",
    "X_sample = shap.sample(X, 2000, random_state=0)\n",
    "# smaller reference → fewer model calls, coarser feature-distribution approximation\n",
    "\n",
    "# 2) instantiate via the high-level API,\n",
    "#    forcing a permutation-based Monte-Carlo explainer\n",
    "explainer = shap.Explainer(\n",
    "    rf.predict,\n",
    "    X_sample,\n",
    "    algorithm=\"permutation\"\n",
    ")\n",
    "\n",
    "# 3) compute SHAP on X_sample,\n",
    "#    limiting the total model calls per row for speed/variance trade-off\n",
    "shap_out = explainer(\n",
    "    X_sample,\n",
    "    max_evals=100          # total evals per instance; lower → faster but noisier\n",
    ")\n",
    "\n",
    "# 4) extract & aggregate\n",
    "shap_array  = shap_out.values          # shape: (n_rows, n_features)\n",
    "shap_scores = pd.Series(\n",
    "    np.abs(shap_array).mean(axis=0),\n",
    "    index=features_cols\n",
    ")\n",
    "\n",
    "# 5) wire it into your live_feature_importance stream\n",
    "def shap_fn(feat):\n",
    "    return shap_scores[feat]\n",
    "\n",
    "shap_scores_live = live_feature_importance(\n",
    "    df_feat,\n",
    "    features_cols,\n",
    "    label_col,\n",
    "    method=\"SHAP (perm-sampling)\",\n",
    "    compute_fn=shap_fn,\n",
    "    threshold=None\n",
    ")\n",
    "\n",
    "update_feature_importances(features_importances, \"shap\", shap_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa556bf-22ca-4df0-b8a9-6037cbcb8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 7) PDP live (partial dependence)\n",
    "\n",
    "Partial dependence plots isolate the marginal effect of one feature on the\n",
    "prediction by averaging out all other features.  The range (max–min) of the\n",
    "PDP curve quantifies how much that feature can move the model’s output.\n",
    "\n",
    "How this function works:\n",
    "  • Loop over each feature and compute its PDP with sklearn’s partial_dependence\n",
    "    (handling different return formats for compatibility).  \n",
    "  • Extract the averaged predictions vs. grid_values for that feature.  \n",
    "  • Compute importance as the vertical range of the PDP curve.  \n",
    "  • Stream the resulting importance scores using live_feature_importance.  \n",
    "  • Finally, save the PDP‐based Series under \"pdp\" in your feature‐importance dict.\n",
    "'''\n",
    "\n",
    "\n",
    "pdp_importances = {}\n",
    "pdp_raw = {}\n",
    "\n",
    "for feat in features_cols:\n",
    "    pdp_res = partial_dependence(\n",
    "        estimator=rf,\n",
    "        X=df_feat[features_cols],\n",
    "        features=[feat],\n",
    "        grid_resolution=200, # increase grid_resolution=50 → 100, 200, 500 to capture finer‐grained curve shapes.\n",
    "        kind=\"average\"\n",
    "    )\n",
    "\n",
    "    # 1) Extract grid & values robustly\n",
    "    if hasattr(pdp_res, \"average\") and hasattr(pdp_res, \"grid_values\"):\n",
    "        # sklearn ≥1.2\n",
    "        values = pdp_res.average[0]\n",
    "        grid   = pdp_res.grid_values[0]\n",
    "    elif isinstance(pdp_res, dict) and \"values\" in pdp_res:\n",
    "        # older Bunch: 'values' key\n",
    "        values = pdp_res[\"average\"][0]\n",
    "        grid   = pdp_res[\"values\"][0]\n",
    "    elif isinstance(pdp_res, tuple) and len(pdp_res) == 2:\n",
    "        # tuple return: (avg, axes)\n",
    "        vals, axes = pdp_res\n",
    "        values = vals[0]\n",
    "        grid   = axes[0]\n",
    "    else:\n",
    "        # fallback: use display to grab data\n",
    "        from sklearn.inspection import PartialDependenceDisplay\n",
    "        disp = PartialDependenceDisplay.from_estimator(\n",
    "            estimator=rf,\n",
    "            X=df_feat[features_cols],\n",
    "            features=[feat],\n",
    "            grid_resolution=50,\n",
    "            kind=\"average\",\n",
    "            plot=False\n",
    "        )\n",
    "        line   = disp.lines_[0][0]\n",
    "        grid   = line.get_xdata()\n",
    "        values = line.get_ydata()\n",
    "\n",
    "    # 2) Compute importance = range of the average PDP curve\n",
    "    imp = float(np.max(values) - np.min(values))\n",
    "    pdp_importances[feat] = imp\n",
    "    pdp_raw[feat] = (grid, values)\n",
    "\n",
    "# Convert to Series\n",
    "pdp_scores = pd.Series(pdp_importances, index=features_cols)\n",
    "\n",
    "# Live‐plot helper expects compute_fn(feat) → float\n",
    "def pdp_fn(feat):\n",
    "    return pdp_scores[feat]\n",
    "\n",
    "pdp_scores_live = live_feature_importance(\n",
    "    df_feat,\n",
    "    features_cols,\n",
    "    label_col,\n",
    "    method=\"PDP\",\n",
    "    compute_fn=pdp_fn,\n",
    "    threshold=0.0\n",
    ")\n",
    "\n",
    "# Store final PDP scores\n",
    "update_feature_importances(features_importances, \"pdp\", pdp_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe937b-d327-4cc1-86bb-58661c80de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally inspect the combined table\n",
    "fi_df = pd.DataFrame.from_dict(features_importances, orient=\"index\")\n",
    "fi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389d0e4-70f2-4b56-95cf-60315c92ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_importance(\n",
    "    fi_df: pd.DataFrame,\n",
    "    method_weights: dict[str, float] | None = None,\n",
    "    plot: bool = False,\n",
    "    top_n: int | None = None,\n",
    "    figsize: tuple[float, float] = (10, 6),\n",
    "    bar_kwargs: dict = None\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Combine multiple feature‐importance measures into one global score,\n",
    "    and optionally plot the results.\n",
    "\n",
    "    Args:\n",
    "      fi_df: DataFrame with index=features and columns=importance methods\n",
    "      method_weights: dict of {method: weight}. Missing methods → weight 0. \n",
    "      plot: if True, draw a horizontal bar plot of global importance\n",
    "      top_n: if set, only plot the top_n features\n",
    "      figsize: figure size passed to plt.figure\n",
    "      bar_kwargs: extra kwargs passed to plt.barh (e.g., color, alpha)\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping feature names to their combined importance, sorted high→low.\n",
    "    \"\"\"\n",
    "    # 0) fill missing\n",
    "    fi_df = fi_df.fillna(0.0)\n",
    "\n",
    "    # 1) Prepare weights\n",
    "    methods = fi_df.columns.tolist()\n",
    "    if method_weights is None:\n",
    "        weights = {m: 1.0 for m in methods}\n",
    "    else:\n",
    "        weights = {m: method_weights.get(m, 0.0) for m in methods}\n",
    "\n",
    "    # 2) Normalize each column to [0,1]\n",
    "    norm_df = pd.DataFrame(index=fi_df.index)\n",
    "    for m in methods:\n",
    "        vals = fi_df[m].abs()\n",
    "        max_val = vals.max()\n",
    "        norm_df[m] = vals / max_val if max_val > 0 else vals\n",
    "\n",
    "    # 3) Weighted sum\n",
    "    weighted_sum = pd.Series(0.0, index=fi_df.index)\n",
    "    for m, w in weights.items():\n",
    "        weighted_sum += norm_df[m] * w\n",
    "\n",
    "    # 4) Rescale to [0,1]\n",
    "    total_w = sum(weights.values())\n",
    "    if total_w > 0:\n",
    "        weighted_sum /= total_w\n",
    "\n",
    "    # 5) Sort descending\n",
    "    global_importance = (\n",
    "        weighted_sum\n",
    "        .sort_values(ascending=False)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # 6) Plot if requested\n",
    "    if plot:\n",
    "        _plot_global_importance(global_importance,\n",
    "                                top_n=top_n,\n",
    "                                figsize=figsize,\n",
    "                                bar_kwargs=bar_kwargs or {})\n",
    "\n",
    "    return global_importance\n",
    "\n",
    "\n",
    "def _plot_global_importance(\n",
    "    global_imp: dict[str, float],\n",
    "    top_n: int | None = None,\n",
    "    figsize: tuple[float, float] = (10, 6),\n",
    "    bar_kwargs: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Internal helper to draw a horizontal bar chart of feature importances.\n",
    "    \"\"\"\n",
    "    # Convert to Series for easy slicing\n",
    "    imp_series = pd.Series(global_imp)\n",
    "    if top_n is not None:\n",
    "        imp_series = imp_series.iloc[:top_n]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(imp_series.index, imp_series.values, **(bar_kwargs or {}))\n",
    "    plt.xlabel(\"Global Importance (0–1)\")\n",
    "    plt.title(\"Feature Global Importance\")\n",
    "    plt.gca().invert_yaxis()  # highest at top\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b01fa-525a-483c-ba93-b9e973ad4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_weights = {\n",
    "    'corr':    0.5,\n",
    "    'mi':      0.5,\n",
    "    'perm_CV': 1.0,\n",
    "    'shap':    1.0,\n",
    "    'lasso':   0.2,\n",
    "    'rfe':     1.0,\n",
    "    'perm_imp':0.8,\n",
    "    'pdp':     0.3\n",
    "}\n",
    "\n",
    "# assuming compute_global_importance is already defined (with fillna inside)\n",
    "global_imp = compute_global_importance(\n",
    "    fi_df,\n",
    "    method_weights=my_weights,\n",
    "    plot=True,         # draws bar chart\n",
    "    top_n=15,          # show top 15 features\n",
    "    bar_kwargs={\n",
    "        'color': 'teal',\n",
    "        'alpha': 0.8\n",
    "    }\n",
    ")\n",
    "\n",
    "features_cols = []\n",
    "# display top features and scores\n",
    "for feat, score in list(global_imp.items())[:15]:\n",
    "    print(f\"{feat:20s} → {score:.3f}\")\n",
    "    features_cols.append(feat)\n",
    "\n",
    "\n",
    "# features_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228dd8c8-d459-40e7-a94e-ee2af0bd95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_screen(fi_df: pd.DataFrame,\n",
    "                       threshold: float = 0.9,\n",
    "                       plot: bool = True) -> list[tuple[str,str]]:\n",
    "    \"\"\"\n",
    "    Correlation measures how two variables move together.\n",
    "\n",
    "    It ranges from –1 (perfect opposite) through 0 (no linear link) to +1 (perfect together).\n",
    "    \n",
    "    Returns list of feature pairs whose absolute correlation exceeds threshold.\n",
    "    Optionally plots a heatmap.\n",
    "    \"\"\"\n",
    "    # 1. compute correlation matrix\n",
    "    corr = fi_df.corr().abs()\n",
    "\n",
    "    # 2. mask self‐correlations\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "\n",
    "    # 3. optionally plot\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr, mask=~mask, annot=False, cmap='vlag', center=0,\n",
    "                    linewidths=0.5, cbar_kws={'shrink':0.75})\n",
    "        plt.title(\"Feature Correlation Heatmap\")\n",
    "        plt.show()\n",
    "\n",
    "    # 4. find high‐corr pairs\n",
    "    high_corr = []\n",
    "    for i, j in zip(*np.where((corr > threshold) & mask)):\n",
    "        feat_i = corr.index[i]\n",
    "        feat_j = corr.columns[j]\n",
    "        high_corr.append((feat_i, feat_j))\n",
    "\n",
    "    return high_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5ba69-0a62-497d-bc48-22608f9fa9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vif(\n",
    "    fi_df: pd.DataFrame,\n",
    "    thresh: float = 10.0,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    VIF (Variance Inflation Factor) shows how much a single predictor’s variance is inflated by its linear relationships with all the other predictors.\n",
    "\n",
    "    Computes VIF for each numeric, non‐constant feature in fi_df.\n",
    "    Drops constant columns, fills NaNs→0, adds an intercept, then computes VIF.\n",
    "    Flags features with VIF > thresh.\n",
    "    \n",
    "    Returns a DataFrame with columns: feature, VIF, flag.\n",
    "    \"\"\"\n",
    "    # 1) Fill missing\n",
    "    df = fi_df.copy().fillna(0.0)\n",
    "\n",
    "    # 2) Keep only float/int cols\n",
    "    df_num = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # 3) Drop constant columns (zero variance)\n",
    "    variances = df_num.var(axis=0)\n",
    "    non_const_cols = variances[variances > 0.0].index.tolist()\n",
    "    df_num = df_num[non_const_cols]\n",
    "\n",
    "    # 4) If fewer than 2 features, we can’t compute VIF\n",
    "    if df_num.shape[1] < 2:\n",
    "        if verbose:\n",
    "            print(\"Less than 2 non‐constant numeric features → no VIF to compute.\")\n",
    "        return pd.DataFrame({\n",
    "            'feature': df_num.columns,\n",
    "            'VIF': [0.0]*df_num.shape[1],\n",
    "            'flag': [False]*df_num.shape[1]\n",
    "        })\n",
    "\n",
    "    # 5) Add intercept (constant) column\n",
    "    X = add_constant(df_num, has_constant='add')\n",
    "\n",
    "    # 6) Compute VIF for each feature (skip constant at idx=0)\n",
    "    vif_records = []\n",
    "    for i, feature in enumerate(df_num.columns, start=1):\n",
    "        try:\n",
    "            v = variance_inflation_factor(X.values, i)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  → Could not compute VIF for {feature}: {e}\")\n",
    "            v = np.nan\n",
    "        vif_records.append({\n",
    "            'feature': feature,\n",
    "            'VIF': v,\n",
    "            'flag': bool(v and v > thresh)\n",
    "        })\n",
    "\n",
    "    vif_df = pd.DataFrame(vif_records).sort_values('VIF', ascending=False)\n",
    "    return vif_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533641a-d024-4736-a5d8-b6cd6010bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Run correlation / VIF on X_df\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "high_corr_pairs = correlation_screen(df, threshold=0.9, plot=True)\n",
    "print(\"High‐corr feature pairs:\", high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f227bdf-0ce7-472f-bbd3-306d9d3d1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = compute_vif(df, thresh=10.0)\n",
    "print(vif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032a001-6295-4cd2-867d-8f3a91e89bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
