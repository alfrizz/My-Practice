{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a018ed14-7d55-46c5-9fe9-f394a35c1bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libs.feats' from '/workspace/my_models/Trading/_Stock_Analysis_/libs/feats.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 1) Wipe out your namespace\n",
    "%reset -f\n",
    "\n",
    "# 2) Clear Jupyter’s stored outputs (and inputs if you like)\n",
    "try:\n",
    "    Out.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    In.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) Force Python GC\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 4) Free any GPU buffers\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import importlib\n",
    "from libs import trades, plots, params, feats\n",
    "importlib.reload(trades)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "importlib.reload(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83643614-b6ed-4209-97bf-be942fd07b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import time\n",
    "import datetime as dt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6216c1-e03a-4d79-b7b0-6cae5a19d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating all indicator (standard + engineered) …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e11529f361b4fd796937488c5a8d46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Days:   0%|          | 0/5400 [00:00<?, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read sign timestamps (with pre-market included)\n",
    "df_sign = pd.read_csv(params.sign_csv, index_col=0, parse_dates=True)\n",
    "\n",
    "all_inds = []\n",
    "print(\"Generating all indicator (standard + engineered) …\")\n",
    "\n",
    "for day, day_df in tqdm(df_sign.groupby(df_sign.index.normalize()), desc=\"Days\", unit=\"day\"):\n",
    "    # 1) Compute raw standard + custom-window features\n",
    "    stand_inds = feats.standard_indicators(df=day_df, mult_inds_win=params.mult_inds_win)\n",
    "\n",
    "    # 2) Build stationary, ratio-based eng_* signals\n",
    "    eng_inds = feats.engineered_indicators(stand_inds)\n",
    "\n",
    "    # Merge raw/custom + engineered and drop rows that still have any NaN\n",
    "    day_inds = pd.concat([stand_inds, eng_inds], axis=1).dropna()\n",
    "\n",
    "    all_inds.append(day_inds)\n",
    "\n",
    "# Glue back into one long DataFrame\n",
    "df_inds_unsc = pd.concat(all_inds).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e623389d-102e-42bd-b4ff-704549774c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick engineered features visualization\n",
    "\n",
    "# # 1) Identify engineered‐feature columns and the raw close price column\n",
    "# eng_features = [c for c in df_feat_unsc.columns if c.startswith(\"eng_\")]\n",
    "\n",
    "# # 2) Pick a random trading day and convert to pandas Timestamp\n",
    "# all_days   = df_feat_unsc.index.normalize().unique()\n",
    "# random_day = np.random.choice(all_days)\n",
    "# day_ts     = pd.to_datetime(random_day)\n",
    "\n",
    "# # 3) Subset the DataFrame to that single day\n",
    "# mask         = df_feat_unsc.index.normalize() == random_day\n",
    "# df_day_feats = df_feat_unsc.loc[mask, eng_features]\n",
    "# df_day_close = df_feat_unsc.loc[mask, 'close']\n",
    "\n",
    "# # 4) Create one subplot per engineered feature\n",
    "# fig, axes = plt.subplots(\n",
    "#     nrows   = len(eng_features),\n",
    "#     ncols   = 1,\n",
    "#     figsize = (12, 2 * len(eng_features)),\n",
    "#     sharex  = True\n",
    "# )\n",
    "# fig.suptitle(f\"Engineered Features vs. Close on {day_ts.date()}\", fontsize=16)\n",
    "\n",
    "# # 5) Plot each feature on its own left‐y axis, and close price on a right‐y axis\n",
    "# for ax, feat in zip(axes, eng_features):\n",
    "#     # left axis: engineered feature\n",
    "#     ax.plot(df_day_feats.index, df_day_feats[feat],\n",
    "#             color=\"C0\", label=feat)\n",
    "#     ax.set_ylabel(feat, color=\"C0\")\n",
    "#     ax.tick_params(axis=\"y\", colors=\"C0\")\n",
    "#     ax.grid(True)\n",
    "\n",
    "#     # right axis: raw close price\n",
    "#     ax2 = ax.twinx()\n",
    "#     ax2.plot(df_day_close.index, df_day_close,\n",
    "#              color=\"k\", alpha=0.6, label=\"close\")\n",
    "#     ax2.set_ylabel(\"close\", color=\"k\")\n",
    "#     ax2.tick_params(axis=\"y\", colors=\"k\")\n",
    "\n",
    "# # 6) Final formatting\n",
    "# axes[-1].set_xlabel(\"Time of Day\")\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4a81bb-4680-4097-bf51-65d70cc5a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importlib.reload(feats) ###################\n",
    "\n",
    "# feat_cols    = [c for c in df_feat_unsc.columns if c!=\"signal\"]\n",
    "\n",
    "# overrides = {\n",
    "#   # raw geometry & MACD → light tails\n",
    "#   \"body\":              \"robust_tails_light\",\n",
    "#   \"macd_line_12_26_9\": \"robust_tails_light\",\n",
    "#   \"macd_signal_12_26_9\":\"robust_tails_light\",\n",
    "#   \"macd_diff_12_26_9\": \"robust_tails_light\",\n",
    "#   # OBV & related → heavy tails\n",
    "#   \"obv\":         \"robust_tails_heavy\",\n",
    "#   \"obv_diff_14\": \"robust_tails_heavy\",\n",
    "#   \"obv_sma_14\":  \"robust_tails_heavy\",\n",
    "#   \"eng_obv\":     \"robust_tails_heavy\",\n",
    "#   \"volume\":      \"robust_tails_heavy\",\n",
    "#   # continuous ratio/unbounded\n",
    "#   \"ret\":        \"unbounded\",\n",
    "#   \"log_ret\":    \"unbounded\",\n",
    "#   \"body_pct\":   \"unbounded\",\n",
    "#   \"range_pct\":  \"unbounded\",\n",
    "#   \"sma_pct_14\": \"unbounded\",\n",
    "#   \"sma_pct_28\": \"unbounded\",\n",
    "#   \"bb_w_20\":    \"unbounded\",\n",
    "#   \"atr_pct_14\": \"unbounded\",\n",
    "# }\n",
    "\n",
    "\n",
    "# assign_df = feats.assign_feature_groups(\n",
    "#     df        = df_feat_unsc,\n",
    "#     cols      = feat_cols,\n",
    "#     # ratio_range:     float = 0.15,\n",
    "#     # heavy_thresh:    float = 1e7,\n",
    "#     # skew_thresh:     float = 3.0,\n",
    "#     # kurtosis_thresh: float = 5.0,\n",
    "#     # discrete_thresh: int   = 10,\n",
    "#     overrides    = overrides\n",
    "# )\n",
    "\n",
    "# # print group lists\n",
    "# for group, features in assign_df['group_final'].groupby(assign_df['group_final']).groups.items():\n",
    "#     print(f\"{group:10s}\", len(features), \":\\n\", features, \"\\n\")\n",
    "\n",
    "# assign_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aebeb862-baa1-4fd6-ab86-150765a1800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------\n",
    "# #   This plot shows, for each trading day, the first 1-minute bar when *every* feature has a non-null value.  \n",
    "# #   Plotting the histogram of those hours tells how long the indicators take to “warm up” each morning before the model can run.\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n",
    "# #  For each calendar day, find the first timestamp where *all* feat_ cols are non-null\n",
    "# first_valid = (\n",
    "#     df_feat_unsc\n",
    "#       .groupby(df_feat_unsc.index.normalize())\n",
    "#       .apply(lambda grp: grp.dropna(subset=feat_cols).index.min())\n",
    "# )\n",
    "\n",
    "# #  Extract the hour (0–23) of that first fully-populated bar\n",
    "# first_valid_hours = first_valid.dt.hour\n",
    "\n",
    "# #  Plot the histogram\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.hist(\n",
    "#     first_valid_hours,\n",
    "#     bins=range(0, 25),       # 24 one-hour bins\n",
    "#     align='left',\n",
    "#     color='skyblue',\n",
    "#     edgecolor='black'\n",
    "# )\n",
    "# plt.xticks(range(0, 24))\n",
    "# plt.xlabel('Hour of Day (0 = midnight)')\n",
    "# plt.ylabel('Number of Days')\n",
    "# plt.title('Histogram of First Fully-Populated Feature Bar per Session')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd704760-097c-4af6-ba02-fb514d12c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # candidate raw-level names/patterns to consider dropping\n",
    "# drop_exact = {\"open\", \"high\", \"low\", \"close\", \"volume\", \"body\", \"upper_shad\", \"lower_shad\"}\n",
    "# drop_patterns = [\n",
    "#     r\"^ema_\\d+$\",        # raw EMA levels\n",
    "#     r\"^sma_\\d+$\",        # raw SMA levels (keep sma_*_pct)\n",
    "#     r\"^atr_\\d+$\",        # raw ATR levels (keep atr_pct_*)\n",
    "#     r\"^bb_lband_\", r\"^bb_hband_\",  # raw BB levels (keep bb_w_)\n",
    "#     r\"^vwap_(?!.*_dev)\", # raw VWAP levels (keep vwap_dev_*)\n",
    "#     r\"^rolling_max_close_\", r\"^rolling_min_close_\"\n",
    "# ]\n",
    "\n",
    "# # find candidates present in df\n",
    "# cands = set(c for c in df_feat_unsc.columns if c in drop_exact)\n",
    "# for p in drop_patterns:\n",
    "#     cands.update([c for c in df_feat_unsc.columns if re.match(p, c)])\n",
    "# cands = sorted(cands)\n",
    "\n",
    "# # helper checks\n",
    "# def has_col(name):\n",
    "#     return name in df_feat_unsc.columns\n",
    "\n",
    "# def has_any(pattern):\n",
    "#     pat = pattern.replace(\"*\", \".*\")\n",
    "#     return any(re.match(pat, col) for col in df_feat_unsc.columns)\n",
    "\n",
    "# safe = []\n",
    "# keep = []\n",
    "\n",
    "# for col in cands:\n",
    "#     ok = False\n",
    "\n",
    "#     # raw EMA -> require ema_dev_N OR both eng_ema_cross flags\n",
    "#     m = re.match(r\"^ema_(\\d+)$\", col)\n",
    "#     if m:\n",
    "#         w = m.group(1)\n",
    "#         ok = (f\"ema_dev_{w}\" in df_feat_unsc.columns) or (\"eng_ema_cross_up\" in df_feat_unsc.columns and \"eng_ema_cross_down\" in df_feat_unsc.columns)\n",
    "\n",
    "#     # raw SMA -> require sma_N_pct\n",
    "#     elif re.match(r\"^sma_(\\d+)$\", col):\n",
    "#         w = re.search(r\"_(\\d+)$\", col).group(1)\n",
    "#         ok = (f\"sma_{w}_pct\" in df_feat_unsc.columns) or has_any(\"sma_*_pct\")\n",
    "\n",
    "#     # ATR raw -> require atr_pct_N\n",
    "#     elif re.match(r\"^atr_(\\d+)$\", col):\n",
    "#         w = re.search(r\"_(\\d+)$\", col).group(1)\n",
    "#         ok = (f\"atr_pct_{w}\" in df_feat_unsc.columns) or has_any(\"atr_pct_\")\n",
    "\n",
    "#     # BB raw bands -> require bb_w_N or eng_bb\n",
    "#     elif re.match(r\"^bb_(lband|hband)_\", col):\n",
    "#         ok = has_any(\"bb_w_\") or (\"eng_bb\" in df_feat_unsc.columns)\n",
    "\n",
    "#     # VWAP raw -> require vwap_dev_N or eng_vwap\n",
    "#     elif re.match(r\"^vwap_(?!.*_dev)\", col):\n",
    "#         ok = has_any(\"vwap_dev_\") or (\"eng_vwap\" in df_feat_unsc.columns)\n",
    "\n",
    "#     # rolling max/min -> require dist_high_*/dist_low_*\n",
    "#     elif re.match(r\"^rolling_(max|min)_close_\", col):\n",
    "#         ok = has_any(\"dist_high_\") or has_any(\"dist_low_\")\n",
    "\n",
    "#     # OBV raw -> require obv_pct_* or obv_z_*\n",
    "#     elif col == \"obv\":\n",
    "#         ok = has_any(\"obv_pct_\") or has_any(\"obv_z_\") or has_any(\"obv_diff_\")\n",
    "\n",
    "#     # body/shadows -> require body_pct or range_pct\n",
    "#     elif col in {\"body\", \"upper_shad\", \"lower_shad\"}:\n",
    "#         ok = has_col(\"body_pct\") or has_col(\"range_pct\")\n",
    "\n",
    "#     # open/high/low/close/volume -> require returns/normalized volume exist\n",
    "#     elif col in {\"open\", \"high\", \"low\", \"close\"}:\n",
    "#         ok = has_col(\"ret\") or has_col(\"log_ret\") or has_any(\"ret_\")\n",
    "#     elif col == \"volume\":\n",
    "#         ok = has_any(\"vol_z_\") or has_any(\"vol_spike_\") or has_any(\"obv_\") or has_any(\"obv_pct_\")\n",
    "\n",
    "#     if ok:\n",
    "#         safe.append(col)\n",
    "#     else:\n",
    "#         keep.append(col)\n",
    "\n",
    "# # report\n",
    "# print(\"SAFE TO DROP:\", len(safe))\n",
    "# for c in safe: print(\"  -\", c)\n",
    "# print(\"\\nKEEP FOR NOW (missing derived alternatives):\", len(keep))\n",
    "# for c in keep: print(\"  -\", c)\n",
    "\n",
    "# # If safe list looks correct, uncomment to drop:\n",
    "# # df_feat_unsc = df_feat_unsc.drop(columns=safe, errors=\"ignore\")\n",
    "# # print(\"Dropped\", len(safe), \"columns; new shape:\", df_feat_unsc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a417d1-dac3-4724-bc9c-0bac3893dc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbc902fd5004ebc82f78e757326fb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prune_and_percentiles:   0%|          | 0/177 [00:00<?, ?feat/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(feats) ########\n",
    "\n",
    "df_feat_unsc, to_drop, diag = feats.prune_and_percentiles(\n",
    "    df_unsc = df_inds_unsc,\n",
    "    train_prop = params.train_prop\n",
    ")\n",
    "\n",
    "print(\"dropped features:\\n\", to_drop)\n",
    "\n",
    "# show remaining features only (exclude dropped) and sort by pct_low (ascending -> most aggressive clipping first)\n",
    "diag_alive = diag[diag[\"status\"] != \"DROP\"].copy()\n",
    "display(diag_alive.sort_values(\"pct_pair\", ascending=False).head(25))\n",
    "\n",
    "print(\"features remaining:\", diag_alive.shape[0], \"dropped:\", len(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1316b-3f38-4457-91c1-50ec4eaa5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(feats) #########\n",
    "\n",
    "df_feat_scal = feats.scaling_with_percentiles(\n",
    "    df = df_feat_unsc,\n",
    "    label_col = params.label_col,\n",
    "    diag = diag,\n",
    "    train_prop = params.train_prop,\n",
    "    val_prop = params.val_prop,\n",
    "    winsorize = True\n",
    ")\n",
    "\n",
    "df_feat_scal  # scaled DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6991d-57aa-44e6-89f8-84a5be59b48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(feats) ###################\n",
    "\n",
    "diag = feats.scaling_diagnostics(df_unscaled = df_feat_unsc,\n",
    "                           df_scaled = df_feat_scal,\n",
    "                           train_prop = params.train_prop,\n",
    "                           clip_thresh = 0.05)\n",
    "\n",
    "diag[diag[\"suggested_action\"] != \"ok\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe8bfa-bc42-4784-99c2-438ffea9822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(plots) ###################\n",
    "\n",
    "plots.plot_dual_histograms(\n",
    "    df_before = df_feat_unsc,\n",
    "    df_after  = df_feat_scal,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07806c0f-5acb-42c5-a29b-b19709f3fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(params) #############\n",
    "importlib.reload(plots) #############\n",
    "\n",
    "df_plot = df_feat_scal.copy()\n",
    "\n",
    "df_plot['close_raw'] = df_inds_unsc['close']\n",
    "\n",
    "df_month = df_plot[\n",
    "    df_plot.index.to_period('M') == params.month_to_check\n",
    "].copy()\n",
    "\n",
    "for day in df_month.index.normalize().unique():\n",
    "    # select all timestamps on this day\n",
    "    df_day = df_month[df_month.index.normalize() == day]\n",
    "    if df_day.empty:\n",
    "        continue\n",
    "\n",
    "    plots.plot_trades(\n",
    "      df_day,\n",
    "      col_signal1 = params.label_col,\n",
    "      col_close   = 'close_raw',\n",
    "      start_plot  = None)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f5f83-de1e-49e2-b912-2f35105bd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving df …\")\n",
    "df_feat_scal.to_csv(params.feat_all_csv)\n",
    "print(\"saved df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eaedb8-f482-4d69-b0ee-5988b935f91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
