{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d702d5b-d0fb-48ab-9a48-134bb847bab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_WAIT_POLICY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPASSIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plots, params, models\n\u001b[1;32m     20\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(plots)\n\u001b[1;32m     21\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(params)\n",
      "File \u001b[0;32m/workspace/my_models/Trading/_Stock_Analysis_/libs/plots.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m params, trades\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/my_models/Trading/_Stock_Analysis_/libs/params.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py:379\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    378\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python’s garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = \"PASSIVE\"\n",
    "\n",
    "import importlib\n",
    "from libs import plots, params, models\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366b422-dfac-4830-8f50-db7b522f2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off interactive plotting globally (we’ll manage our own display)\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # safe, headless-friendly\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.exceptions import TrialPruned\n",
    "from optuna.importance import get_param_importances\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9498b30-9abb-4123-87f1-0ed191478e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols  = params.features_cols_tick\n",
    "label_col      = params.label_col\n",
    "device         = params.device\n",
    "\n",
    "df_feat = pd.read_csv(params.feat_csv, index_col=0, parse_dates=True)\n",
    "\n",
    "df_features = df_feat[features_cols + ['signal','ask','bid']]\n",
    "df_features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2e58b-3140-4a76-9473-1fab56c18d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate look_backs under half the interval between the day’s first and sess_start\n",
    "\n",
    "first_time = (\n",
    "    df_features.index\n",
    "        .to_series()\n",
    "        .groupby(df_features.index.normalize())\n",
    "        .min()\n",
    "        .dt.time\n",
    "        .mode()[0]\n",
    ")\n",
    "\n",
    "# convert both times to minutes since midnight\n",
    "fm = first_time.hour * 60 + first_time.minute\n",
    "sm = params.sess_start.hour * 60 + params.sess_start.minute\n",
    "\n",
    "# half the difference, count full 30-min slots, and build multiples\n",
    "n_steps    = int(((sm - fm) / 2) // 30)      # e.g. floor(165/30) = 5\n",
    "look_backs = [30 * i for i in range(1, n_steps + 1)]\n",
    "look_backs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4612b-f5bb-4e4a-8784-6d5689de67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Optuna objective definition\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    hp = {\n",
    "    # ── Architecture ────────────────────────────────────────────────\n",
    "    \"look_back\"    : trial.suggest_categorical(\"look_back\", look_backs),\n",
    "    \"DROPOUT_SHORT\": trial.suggest_float(\"DROPOUT_SHORT\", 0.05, 0.35),\n",
    "    \"DROPOUT_LONG\":  trial.suggest_float(\"DROPOUT_LONG\",  0.05, 0.35),\n",
    "    \"ATT_DROPOUT\":   trial.suggest_float(\"ATT_DROPOUT\",   0.05, 0.35),\n",
    "    \n",
    "    # ── Optimizer & Scheduler ──────────────────────────────────────\n",
    "    \"INITIAL_LR\":    trial.suggest_float(\"INITIAL_LR\",    1e-4, 1e-3),\n",
    "    \"ETA_MIN\":       trial.suggest_float(\"ETA_MIN\",       1e-6, 1e-5),\n",
    "    \"WEIGHT_DECAY\":  trial.suggest_float(\"WEIGHT_DECAY\",  1e-6, 1e-4),\n",
    "    \"CLIPNORM\":      trial.suggest_float(\"CLIPNORM\",      0.5, 2),\n",
    "    }\n",
    "\n",
    "    \n",
    "    print(f\"\\n▶ Trial {trial.number} starting with:\\n{hp}\\n\")#\n",
    "    # Build model\n",
    "    model = models.DualMemoryLSTM(\n",
    "        n_feats       = len(features_cols),\n",
    "        short_units   = params.hparams[\"SHORT_UNITS\"],\n",
    "        long_units    = params.hparams[\"LONG_UNITS\"],\n",
    "        dropout_short = hp[\"DROPOUT_SHORT\"],\n",
    "        dropout_long  = hp[\"DROPOUT_LONG\"],\n",
    "        att_heads     = params.hparams[\"ATT_HEADS\"],\n",
    "        att_drop      = hp[\"ATT_DROPOUT\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # Build optimizer + schedulers + scaler\n",
    "    optimizer, plateau_sched, _ , scaler, clipnorm = \\\n",
    "        models.make_optimizer_and_scheduler(\n",
    "            model            = model,\n",
    "            initial_lr       = hp[\"INITIAL_LR\"],\n",
    "            weight_decay     = hp[\"WEIGHT_DECAY\"],\n",
    "            clipnorm         = hp[\"CLIPNORM\"]\n",
    "        )\n",
    "\n",
    "    cosine_sched = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=params.hparams['T_0'], \n",
    "        T_mult=params.hparams['T_MULT'], \n",
    "        eta_min=hp['ETA_MIN']\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Build LSTM input tensors (disk-backed memmaps)\n",
    "    #    Returns five tensors on `device`\n",
    "    X, y, raw_close, raw_bid, raw_ask = models.build_lstm_tensors(\n",
    "        df             = df_features,\n",
    "        look_back      = hp[\"look_back\"],\n",
    "        features_cols  = features_cols,\n",
    "        label_col      = label_col,\n",
    "        sess_start     = False # if we want the predictions not to start from sess_start, but from sess_start_pred\n",
    "    )\n",
    "    print(\"Full tensors Shapes:\")\n",
    "    print(\"  X         =\", X.shape,    \"(samples, look_back, features)\")\n",
    "    print(\"  y         =\", y.shape,    \"(samples,)\")\n",
    "    \n",
    "    # Split the full sliding‐window dataset (X, y, raw_*) into train, validation,    and test sets by calendar day,    \n",
    "    (X_tr, y_tr), \\\n",
    "    (X_val, y_val), \\\n",
    "    (X_te, y_te, raw_close_te, raw_bid_te, raw_ask_te), \\\n",
    "    samples_per_day, day_id_tr, day_id_val, day_id_te = models.chronological_split(\n",
    "        X, y, raw_close, raw_bid, raw_ask, df_features,\n",
    "        look_back       = hp[\"look_back\"],\n",
    "        train_prop      = params.train_prop,\n",
    "        val_prop        = params.val_prop,\n",
    "        train_batch     = params.hparams['TRAIN_BATCH'],\n",
    "        sess_start     = False # if we want the predictions not to start from sess_start, but from sess_start_pred\n",
    "\n",
    "    )\n",
    "    print(\"Split tensors Shapes:\")\n",
    "    print(\"  X_tr =\", X_tr.shape)\n",
    "    print(\"  X_val =\", X_val.shape)\n",
    "    print(\"  X_te =\", X_te.shape)\n",
    "\n",
    "    \n",
    "    #  Build DataLoaders over calendar‐days\n",
    "    train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "        # Training split arrays (from chronological_split)\n",
    "        X_tr, y_tr, day_id_tr,\n",
    "        # Validation split arrays\n",
    "        X_val, y_val, day_id_val,\n",
    "        # Test split arrays + raw prices for post‐tracking\n",
    "        X_te, y_te, day_id_te, raw_close_te, raw_bid_te, raw_ask_te,\n",
    "        # Original minute‐bar DataFrame for weekday mapping\n",
    "        df=df_features,\n",
    "        train_batch=params.hparams['TRAIN_BATCH'],\n",
    "        train_workers=params.hparams['NUM_WORKERS'],\n",
    "        train_prefetch_factor=params.hparams['TRAIN_PREFETCH_FACTOR']\n",
    "    )\n",
    "\n",
    "    # Count how many calendar days we see each epoch and Compute baseline RMSE on validation (zero forecast)\n",
    "    n_train_days = len(train_loader.dataset)  # dataset length = # unique days\n",
    "    print(f\"Training sees {n_train_days} calendar days per epoch\\n\")\n",
    "    baseline_val_rmse = models.naive_rmse(val_loader)\n",
    "    print(f\"Baseline (zero‐forecast) RMSE on validation = {baseline_val_rmse:.6f}\")\n",
    "    \n",
    "    # Run training & return best validation RMSE\n",
    "    best_rmse = models.custom_stateful_training_loop(\n",
    "        model               = model,\n",
    "        optimizer           = optimizer,\n",
    "        cosine_sched        = cosine_sched,\n",
    "        plateau_sched       = plateau_sched,\n",
    "        scaler              = scaler,\n",
    "        train_loader        = train_loader,\n",
    "        val_loader          = val_loader,\n",
    "        max_epochs          = params.hparams[\"MAX_EPOCHS\"],\n",
    "        early_stop_patience = params.hparams[\"EARLY_STOP_PATIENCE\"],\n",
    "        baseline_val_rmse   = baseline_val_rmse,\n",
    "        clipnorm            = clipnorm,\n",
    "        device              = device,\n",
    "    )\n",
    "\n",
    "    del model, optimizer, plateau_sched, cosine_sched, scaler, clipnorm\n",
    "    del X, y, raw_close, raw_bid, raw_ask\n",
    "    del X_tr, y_tr, X_val, y_val, X_te, y_te\n",
    "    del raw_close_te, raw_bid_te, raw_ask_te\n",
    "    del train_loader, val_loader, test_loader\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce185de-1d62-49f9-9a96-00385e813521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build blank figure & line\n",
    "fig, ax = plt.subplots(figsize=(7,3))\n",
    "line, = ax.plot([], [], \"bo-\")\n",
    "ax.set(xlabel=\"Trial #\", ylabel=\"Objective\",\n",
    "       title=\"Optuna optimization progress\")\n",
    "ax.grid(True)\n",
    "\n",
    "# display once and grab the handle\n",
    "handle = display(fig, display_id=True)\n",
    "plt.close(fig)\n",
    "\n",
    "# ask plots.py for a callback bound to these objects\n",
    "live_cb = plots.make_live_plot_callback(fig, ax, line, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658050b-a604-4331-ac64-72b2c7fd3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Create Optuna study and run optimization\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "study = optuna.create_study(\n",
    "    storage=\"sqlite:///optuna_study.db\",    # Point it at an SQLite file so it writes out each result immediately instead of buffering in RAM\n",
    "    load_if_exists=True,\n",
    "    direction=\"minimize\",\n",
    "    pruner=MedianPruner(n_startup_trials=6, n_warmup_steps=12),\n",
    ")\n",
    "\n",
    "        \n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials = 100,\n",
    "    n_jobs   = 1,\n",
    "    callbacks=[live_cb, plots.cleanup_callback],\n",
    ")\n",
    "\n",
    "plt.close('all')   # safe here; the final image remains displayed in the notebook output\n",
    "gc.collect()       # optional extra sweep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312366c-7f12-4c10-9f34-0647d6be022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Print out the best hyperparameters & result\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation RMSE:\", study.best_value)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Compute and print parameter importances\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "imps = get_param_importances(study)\n",
    "print(\"\\nHyperparameter importances (higher ⇒ more impact on RMSE):\")\n",
    "for name, score in sorted(imps.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name:20s} : {score:.3f}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Dump study results to JSON\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Build your session‐only DataFrame once\n",
    "session_df = df.between_time(params.regular_start,\n",
    "                             params.regular_end)\n",
    "\n",
    "# 2) Derive the trading‐day boundaries\n",
    "first_day = session_df.index.normalize().min()\n",
    "last_day  = session_df.index.normalize().max()\n",
    "\n",
    "# 3) Format your file name\n",
    "start_date = first_day.strftime(\"%Y%m%d\")\n",
    "end_date   = last_day.strftime(\"%Y%m%d\")\n",
    "file_name  = f\"{params.ticker}_{start_date}-{end_date}_optuna_model_hpars.json\"\n",
    "file_path  = os.path.join(results_folder, file_name)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Dump study results (including importances)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_params\": study.best_params,\n",
    "            \"best_value\" : study.best_value,\n",
    "            \"importances\": imps,\n",
    "            \"trials\": [\n",
    "                {\"number\": t.number, \"value\": t.value, \"params\": t.params, \n",
    "                 \"state\": t.state.name}\n",
    "                for t in study.trials\n",
    "            ],\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )\n",
    "\n",
    "print(f\"\\nOptuna results (and importances) saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97c435-e800-483e-b2ab-f9412ef6954e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865782e-fc07-4354-a5b9-2cdeed1ebad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa9fb2-411b-4d9f-9a33-e6f6c59de7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
