{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db53f869-3ce8-4b85-9c38-ba83b86b51b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trades, plots, params, models\n\u001b[1;32m      9\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(trades)\n\u001b[1;32m     10\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(plots)\n",
      "File \u001b[0;32m/workspace/my_models/Trading/_Stock_Analysis_/libs/trades.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m params, plots\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/my_models/Trading/_Stock_Analysis_/libs/params.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py:378\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;66;03m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 378\u001b[0m         \u001b[43m_load_global_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py:312\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m global_deps_lib_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(here), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, lib_name)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_deps_lib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_GLOBAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# Can only happen for wheel with cuda libs as PYPI deps\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# As PyTorch is not purelib, but nvidia-*-cu12 is\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     cuda_libs: _Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcublas\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcublas.so.*[0-9]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudnn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudnn.so.*[0-9]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvtx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibnvToolsExt.so.*[0-9]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    329\u001b[0m     }\n",
      "File \u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1) Wipe out all Python variables\n",
    "%reset -f\n",
    "# 2) Force Python‚Äôs garbage collector to run\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import importlib\n",
    "from libs import trades, plots, params, models\n",
    "importlib.reload(trades)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(params)\n",
    "importlib.reload(models)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "\n",
    "import matplotlib.pyplot as plt   \n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as Funct\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.serialization.add_safe_globals([models.DayWindowDataset])\n",
    "import torchmetrics\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Set, List, Union, Dict\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "import IPython.display as disp\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b44fb-0cf5-4b31-8cdf-39d8d587234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device               = params.device\n",
    "ticker               = params.ticker\n",
    "save_path            = params.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daba10b-3212-4a06-b607-748995bdc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = min(\n",
    "    save_path.glob(f\"{ticker}_*.pth\"),\n",
    "    key=lambda p: float(p.stem.split(\"_\")[-1])\n",
    ")\n",
    "print('Model selected:', model_path,'\\n')\n",
    "\n",
    "# Load the entire model object (architecture + weights)# 1) load your checkpoint dict\n",
    "ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# 2) grab the full model object you saved\n",
    "model_best = ckpt[\"model_obj\"]\n",
    "\n",
    "# 3) move to device and set eval mode\n",
    "model_best = model_best.to(device).eval()\n",
    "\n",
    "# 4) show parameters, training plot, and model\n",
    "saved_hparams = ckpt[\"hparams\"]\n",
    "pprint(saved_hparams)\n",
    "\n",
    "png_bytes = ckpt.get(\"train_plot_png\")\n",
    "img = Image.open(io.BytesIO(png_bytes))\n",
    "disp.display(img)\n",
    "\n",
    "model_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c5327-1595-440a-ac93-eefd836bef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.read_csv(params.feat_csv, index_col=0, parse_dates=True)\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fb006-294f-4eb4-a687-81703ee7a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) build the mem-mapped windows **and** get the end-of-window stamps\n",
    "print('executing <build_lstm_tensors>...')\n",
    "X, y_sig, y_ret, raw_close, raw_bid, raw_ask, end_times = models.build_lstm_tensors(\n",
    "    df            = df_feat,\n",
    "    look_back     = params.look_back_tick,\n",
    "    features_cols = params.features_cols_tick,\n",
    "    label_col     = params.label_col,\n",
    "    return_col    = params.return_col,\n",
    "    sess_start    = params.sess_start_pred_tick \n",
    ")\n",
    "\n",
    "\n",
    "# 2) split by day USING your end_times array, not df_feat\n",
    "print('executing <chronological_split>...')\n",
    "(\n",
    "(X_tr,  y_sig_tr,  y_ret_tr),\n",
    "(X_val, y_sig_val, y_ret_val),\n",
    "(X_te,  y_sig_te,  y_ret_te,  raw_close_te, raw_bid_te, raw_ask_te),\n",
    "samples_per_day,\n",
    "day_id_tr, day_id_val, day_id_te\n",
    ") = models.chronological_split(\n",
    "    X, y_sig, y_ret,\n",
    "    raw_close, raw_bid, raw_ask,\n",
    "    end_times   = end_times,\n",
    "    train_prop  = params.train_prop,\n",
    "    val_prop    = params.val_prop,\n",
    "    train_batch = params.hparams['TRAIN_BATCH']\n",
    ")\n",
    "\n",
    "# carve `end_times` into the same three splits:\n",
    "n_tr  = day_id_tr .shape[0] \n",
    "n_val = day_id_val.shape[0]\n",
    "i_tr  = n_tr\n",
    "i_val = n_tr + n_val\n",
    "\n",
    "end_times_tr  = end_times[:i_tr]\n",
    "end_times_val = end_times[i_tr:i_val]\n",
    "end_times_te  = end_times[i_val:]\n",
    "\n",
    "print('executing <split_to_day_datasets>...')\n",
    "train_loader, val_loader, test_loader = models.split_to_day_datasets(\n",
    "    # train split:   \n",
    "    X_tr,            y_sig_tr,     y_ret_tr,   end_times_tr,\n",
    "    # val split:\n",
    "    X_val,           y_sig_val,    y_ret_val,  end_times_val,\n",
    "    # test split + raw‚Äêprices\n",
    "    X_te,            y_sig_te,     y_ret_te,   end_times_te,\n",
    "    raw_close_te, raw_bid_te, raw_ask_te,\n",
    "    \n",
    "    sess_start_time       = params.sess_start_pred_tick,\n",
    "    signal_thresh         = params.best_optuna_params[\"buy_threshold\"],\n",
    "    return_thresh         = 0.01,  # flat‚Äêzone threshold for returns\n",
    "    train_batch           = params.hparams[\"TRAIN_BATCH\"],\n",
    "    train_workers         = params.hparams[\"NUM_WORKERS\"],\n",
    "    train_prefetch_factor = params.hparams[\"TRAIN_PREFETCH_FACTOR\"]\n",
    ")\n",
    "\n",
    "# how many calendar-days in each split?\n",
    "print(\"Days ‚Üí\",\n",
    "      f\"train={len(train_loader.dataset)},\",\n",
    "      f\"val={len(val_loader.dataset)},\",\n",
    "      f\"test={len(test_loader.dataset)}\")\n",
    "\n",
    "# how many sliding-windows in each split?\n",
    "print(\"Windows ‚Üí\",\n",
    "      f\"train={train_loader.dataset.X.shape[0]},\",\n",
    "      f\"val={val_loader.dataset.X.shape[0]},\",\n",
    "      f\"test={test_loader.dataset.X.shape[0]}\")\n",
    "\n",
    "# how many batches per loader?\n",
    "print(\"Batches ‚Üí\",\n",
    "      f\"train={len(train_loader)},\",\n",
    "      f\"val={len(val_loader)},\",\n",
    "      f\"test={len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4e7a3-09bd-451e-bc18-48a1e14d70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero‚Äêforecast baseline on val vs test\n",
    "# ‚àö( mean( (y·µ¢ ‚Äì 0)¬≤ ) )\n",
    "\n",
    "val_baseline  = models.naive_rmse(val_loader)\n",
    "test_baseline = models.naive_rmse(test_loader)\n",
    "\n",
    "print(f\"Val zero‚Äêforecast baseline RMSE  = {val_baseline:.5f}\")\n",
    "print(f\"Test zero‚Äêforecast baseline RMSE = {test_baseline:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7e5dc-81a6-4fd5-afc5-b5c4623adaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to confirm the baseline proportions, calculate the STD\n",
    "# œÉ = ‚àö( mean( (y·µ¢ ‚Äì »≥)¬≤ ) )\n",
    "\n",
    "y_vals = np.concatenate([batch[1].view(-1).numpy()\n",
    "                         for batch in val_loader])\n",
    "y_tes  = np.concatenate([batch[1].view(-1).numpy()\n",
    "                         for batch in test_loader])\n",
    "print(\"std val:\", np.std(y_vals))\n",
    "print(\"std test:\", np.std(y_tes))\n",
    "\n",
    "plt.hist(y_vals, bins=100, alpha=0.5, label=\"val\")\n",
    "plt.hist(y_tes,  bins=100, alpha=0.5, label=\"test\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce59f2e-2d60-4796-8e23-b51d1b58e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, loader, device, split_name: str):\n",
    "#     \"\"\"\n",
    "#     1) Set model.eval(), reset LSTM states.\n",
    "#     2) Create/regress and binary torchmetrics (threshold=0.5).\n",
    "#     3) For each padded batch:\n",
    "#          - unpack lengths, weekdays, raw? data\n",
    "#          - move to device, skip wd.squeeze()\n",
    "#          - for each day i in batch:\n",
    "#              ‚Ä¢ slice x_day, y_day, cls_t to true length\n",
    "#              ‚Ä¢ reset model state on day rollover\n",
    "#              ‚Ä¢ forward, sigmoid, update metrics only on real slots\n",
    "#     4) Compute & print final metrics, return (metrics_dict, concatenated preds).\n",
    "#     \"\"\"\n",
    "#     model.to(device).eval()\n",
    "#     model.h_short = model.h_long = None\n",
    "#     thr = 0.5\n",
    "\n",
    "#     rmse_m = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "#     mae_m  = torchmetrics.MeanAbsoluteError().to(device)\n",
    "#     r2_m   = torchmetrics.R2Score().to(device)\n",
    "#     acc_m  = torchmetrics.classification.BinaryAccuracy(threshold=thr).to(device)\n",
    "#     prec_m = torchmetrics.classification.BinaryPrecision(threshold=thr).to(device)\n",
    "#     rec_m  = torchmetrics.classification.BinaryRecall(threshold=thr).to(device)\n",
    "#     f1_m   = torchmetrics.classification.BinaryF1Score(threshold=thr).to(device)\n",
    "#     auc_m  = torchmetrics.classification.BinaryAUROC().to(device)\n",
    "\n",
    "#     for m in (rmse_m, mae_m, r2_m, acc_m, prec_m, rec_m, f1_m, auc_m):\n",
    "#         m.reset()\n",
    "\n",
    "#     all_preds = []\n",
    "#     prev_day  = None\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc=split_name, unit=\"batch\"):\n",
    "#             # True if test split (with raw prices)\n",
    "#             if len(batch) == 9:\n",
    "#                 xb, yb_reg, yb_cls, rc, rb, ra, wd, ts_list, lengths = batch\n",
    "#             else:\n",
    "#                 xb, yb_reg, yb_cls, wd, ts_list, lengths = batch\n",
    "\n",
    "#             xb     = xb.to(device, non_blocking=True)\n",
    "#             yb_reg = yb_reg.to(device, non_blocking=True)\n",
    "#             yb_cls = yb_cls.to(device, non_blocking=True)\n",
    "#             wd     = wd.to(device, non_blocking=True)   # keep shape (B,)\n",
    "#             B      = xb.size(0)\n",
    "\n",
    "#             for i in range(B):\n",
    "#                 W_true = lengths[i]\n",
    "#                 day_id = int(wd[i].item())\n",
    "\n",
    "#                 model.reset_short()\n",
    "#                 if prev_day is not None and day_id < prev_day:\n",
    "#                     model.reset_long()\n",
    "#                 prev_day = day_id\n",
    "\n",
    "#                 x_day = xb[i][:W_true]\n",
    "#                 y_day = yb_reg[i, :W_true].view(-1)\n",
    "#                 cls_t = yb_cls[i,   :W_true].view(-1)\n",
    "\n",
    "#                 pred_reg, pred_cls = model(x_day)\n",
    "#                 pr    = pred_reg[:, -1, 0]\n",
    "#                 probs = torch.sigmoid(pred_cls[:, -1, 0])\n",
    "\n",
    "#                 # update only on real (unpadded) windows\n",
    "#                 rmse_m.update(pr,    y_day)\n",
    "#                 mae_m .update(pr,    y_day)\n",
    "#                 r2_m  .update(pr,    y_day)\n",
    "#                 acc_m .update(probs, cls_t)\n",
    "#                 prec_m.update(probs, cls_t)\n",
    "#                 rec_m .update(probs, cls_t)\n",
    "#                 f1_m  .update(probs, cls_t)\n",
    "#                 auc_m .update(probs, cls_t)\n",
    "\n",
    "#                 all_preds.append(pr.cpu().numpy())\n",
    "\n",
    "#     metrics = {\n",
    "#         \"rmse\": rmse_m.compute().item(),\n",
    "#         \"mae\":  mae_m.compute().item(),\n",
    "#         \"r2\":   r2_m.compute().item(),\n",
    "#         \"acc\":  acc_m.compute().item(),\n",
    "#         \"precision\": prec_m.compute().item(),\n",
    "#         \"recall\":    rec_m.compute().item(),\n",
    "#         \"f1\":        f1_m.compute().item(),\n",
    "#         \"auroc\":     auc_m.compute().item(),\n",
    "#     }\n",
    "#     print(\n",
    "#         f\"{split_name} ‚Üí \"\n",
    "#         f\"RMSE={metrics['rmse']:.5f} MAE={metrics['mae']:.5f} \"\n",
    "#         f\"R2={metrics['r2']:.4f} ACC={metrics['acc']:.4f} \"\n",
    "#         f\"PREC={metrics['precision']:.4f} REC={metrics['recall']:.4f} \"\n",
    "#         f\"F1={metrics['f1']:.4f} AUROC={metrics['auroc']:.4f}\"\n",
    "#     )\n",
    "#     return metrics, np.concatenate(all_preds, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device, split_name: str):\n",
    "    \"\"\"\n",
    "    Evaluate a trained Stateful CNN‚ÜíBiLSTM‚ÜíAttention‚ÜíBiLSTM model on a DataLoader.\n",
    "\n",
    "    Steps:\n",
    "      1) model.eval(), reset LSTM states.\n",
    "      2) Instantiate regression, binary, and ternary torchmetrics.\n",
    "      3) Loop over padded batches:\n",
    "         - Unpack sequences (with or without raw prices).\n",
    "         - For each calendar-day in batch:\n",
    "             ‚Ä¢ Slice to true length, reset states on day rollover.\n",
    "             ‚Ä¢ Forward pass ‚Üí (raw_reg, raw_bin, raw_ter).\n",
    "             ‚Ä¢ Compute sigmoid/softmax ‚Üí binary & multiclass probs.\n",
    "             ‚Ä¢ Update all metrics on unpadded windows.\n",
    "      4) Compute final metrics, print them in the format:\n",
    "         R: ‚Ä¶ | B: ‚Ä¶ | T: ‚Ä¶\n",
    "      5) Return (metrics_dict, concatenated regression preds).\n",
    "    \"\"\"\n",
    "    # 1) Prepare model\n",
    "    model.to(device).eval()\n",
    "    model.h_short = model.h_long = None\n",
    "\n",
    "    # 2) Metrics setup\n",
    "    thr = 0.5\n",
    "    rmse_m      = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "    mae_m       = torchmetrics.MeanAbsoluteError().to(device)\n",
    "    r2_m        = torchmetrics.R2Score().to(device)\n",
    "    acc_m       = torchmetrics.classification.BinaryAccuracy(threshold=thr).to(device)\n",
    "    prec_m      = torchmetrics.classification.BinaryPrecision(threshold=thr).to(device)\n",
    "    rec_m       = torchmetrics.classification.BinaryRecall(threshold=thr).to(device)\n",
    "    f1_m        = torchmetrics.classification.BinaryF1Score(threshold=thr).to(device)\n",
    "    auc_m       = torchmetrics.classification.BinaryAUROC().to(device)\n",
    "\n",
    "    ter_acc_m   = torchmetrics.classification.MulticlassAccuracy(num_classes=3).to(device)\n",
    "    ter_prec_m  = torchmetrics.classification.MulticlassPrecision(num_classes=3, average=\"macro\").to(device)\n",
    "    ter_rec_m   = torchmetrics.classification.MulticlassRecall(num_classes=3, average=\"macro\").to(device)\n",
    "    ter_f1_m    = torchmetrics.classification.MulticlassF1Score(num_classes=3, average=\"macro\").to(device)\n",
    "    ter_auc_m   = torchmetrics.classification.MulticlassAUROC(num_classes=3, average=\"macro\").to(device)\n",
    "\n",
    "    for m in (rmse_m, mae_m, r2_m,\n",
    "              acc_m, prec_m, rec_m, f1_m, auc_m,\n",
    "              ter_acc_m, ter_prec_m, ter_rec_m, ter_f1_m, ter_auc_m):\n",
    "        m.reset()\n",
    "\n",
    "    all_preds = []\n",
    "    prev_day  = None\n",
    "\n",
    "    # 3) Loop through batches\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=split_name, unit=\"batch\"):\n",
    "            # unpack depending on raw-price presence\n",
    "            if len(batch) == 11:\n",
    "                (xb, y_reg, y_bin, y_ret, y_ter, \n",
    "                 rc, rb, ra, wd, ts_list, lengths) = batch\n",
    "            else:\n",
    "                xb, y_reg, y_bin, y_ret, y_ter, wd, ts_list, lengths = batch\n",
    "\n",
    "            # move tensors to device\n",
    "            xb    = xb.to(device, non_blocking=True)\n",
    "            y_reg = y_reg.to(device, non_blocking=True)\n",
    "            y_bin = y_bin.to(device, non_blocking=True)\n",
    "            y_ret = y_ret.to(device, non_blocking=True)\n",
    "            y_ter = y_ter.to(device, non_blocking=True)\n",
    "            wd    = wd.to(device, non_blocking=True)\n",
    "\n",
    "            B = xb.size(0)\n",
    "            for i in range(B):\n",
    "                W_true = lengths[i]\n",
    "                day_id = int(wd[i].item())\n",
    "\n",
    "                # reset or carry LSTM states\n",
    "                model.reset_short()\n",
    "                if prev_day is not None and day_id < prev_day:\n",
    "                    model.reset_long()\n",
    "                prev_day = day_id\n",
    "\n",
    "                # slice to true window length\n",
    "                x_day = xb[i, :W_true]\n",
    "                y_day = y_reg[i, :W_true].view(-1)\n",
    "                bin_t = y_bin[i, :W_true].view(-1)\n",
    "                ter_t = y_ter[i, :W_true].view(-1)\n",
    "\n",
    "                # forward\n",
    "                raw_reg, raw_bin, raw_ter = model(x_day)\n",
    "                pr  = raw_reg[..., -1, 0]    # (W_true,)\n",
    "                pb  = raw_bin[..., -1, 0]\n",
    "                pt  = raw_ter[..., -1, :]    # (W_true, 3)\n",
    "\n",
    "                # probabilities\n",
    "                prob_b = torch.sigmoid(pb)\n",
    "                prob_t = torch.softmax(pt, dim=-1)\n",
    "\n",
    "                # update metrics\n",
    "                rmse_m.update(pr, y_day)\n",
    "                mae_m .update(pr, y_day)\n",
    "                r2_m  .update(pr, y_day)\n",
    "\n",
    "                acc_m .update(prob_b, bin_t)\n",
    "                prec_m.update(prob_b, bin_t)\n",
    "                rec_m .update(prob_b, bin_t)\n",
    "                f1_m  .update(prob_b, bin_t)\n",
    "                auc_m .update(prob_b, bin_t)\n",
    "\n",
    "                ter_acc_m .update(prob_t, ter_t)\n",
    "                ter_prec_m.update(prob_t, ter_t)\n",
    "                ter_rec_m .update(prob_t, ter_t)\n",
    "                ter_f1_m  .update(prob_t, ter_t)\n",
    "                ter_auc_m .update(prob_t, ter_t)\n",
    "\n",
    "                all_preds.append(pr.cpu().numpy())\n",
    "\n",
    "    # 4) Compute final metrics\n",
    "    metrics = {\n",
    "        \"rmse\":   rmse_m.compute().item(),\n",
    "        \"mae\":    mae_m.compute().item(),\n",
    "        \"r2\":     r2_m.compute().item(),\n",
    "        \"acc\":    acc_m.compute().item(),\n",
    "        \"prec\":   prec_m.compute().item(),\n",
    "        \"rec\":    rec_m.compute().item(),\n",
    "        \"f1\":     f1_m.compute().item(),\n",
    "        \"auroc\":  auc_m.compute().item(),\n",
    "        \"t_acc\":  ter_acc_m.compute().item(),\n",
    "        \"t_prec\": ter_prec_m.compute().item(),\n",
    "        \"t_rec\":  ter_rec_m.compute().item(),\n",
    "        \"t_f1\":   ter_f1_m.compute().item(),\n",
    "        \"t_auc\":  ter_auc_m.compute().item()\n",
    "    }\n",
    "\n",
    "    # 5) Print in the desired format\n",
    "    print(\n",
    "        f\"{split_name} ‚Üí \"\n",
    "        f'\"R\": RMSE={metrics[\"rmse\"]:.5f} MAE={metrics[\"mae\"]:.5f} R2={metrics[\"r2\"]:.4f} | '\n",
    "        f'\"B\": Acc={metrics[\"acc\"]:.4f} Prec={metrics[\"prec\"]:.4f} Rec={metrics[\"rec\"]:.4f} '\n",
    "           f'F1={metrics[\"f1\"]:.4f} AUROC={metrics[\"auroc\"]:.4f} | '\n",
    "        f'\"T\": Acc={metrics[\"t_acc\"]:.4f} Prec={metrics[\"t_prec\"]:.4f} Rec={metrics[\"t_rec\"]:.4f} '\n",
    "           f'F1={metrics[\"t_f1\"]:.4f} AUROC={metrics[\"t_auc\"]:.4f}'\n",
    "    )\n",
    "\n",
    "    return metrics, np.concatenate(all_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b48bb-f0f1-4911-b6c7-186ca487fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all three splits\n",
    "train_metrics, train_preds = evaluate_model(\n",
    "    model_best, train_loader, device, split_name=\"TRAIN\"\n",
    ")\n",
    "val_metrics, val_preds     = evaluate_model(\n",
    "    model_best, val_loader,   device, split_name=\"VALID\"\n",
    ")\n",
    "test_metrics, test_preds   = evaluate_model(\n",
    "    model_best, test_loader,  device, split_name=\"TEST\"\n",
    ")\n",
    "\n",
    "# Print in the same format as during training\n",
    "print(\n",
    "    f'TRAIN‚Üí '\n",
    "    f'\"R\": RMSE={train_metrics[\"rmse\"]:.4f} MAE={train_metrics[\"mae\"]:.4f} '\n",
    "    f'R2={train_metrics[\"r2\"]:.4f} | '\n",
    "    f'\"B\": Acc={train_metrics[\"acc\"]:.4f} Prec={train_metrics[\"prec\"]:.4f} '\n",
    "    f'Rec={train_metrics[\"rec\"]:.4f} F1={train_metrics[\"f1\"]:.4f} '\n",
    "    f'AUROC={train_metrics[\"auroc\"]:.4f} | '\n",
    "    f'\"T\": Acc={train_metrics[\"t_acc\"]:.4f} Prec={train_metrics[\"t_prec\"]:.4f} '\n",
    "    f'Rec={train_metrics[\"t_rec\"]:.4f} F1={train_metrics[\"t_f1\"]:.4f} '\n",
    "    f'AUROC={train_metrics[\"t_auc\"]:.4f}'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'VALID‚Üí '\n",
    "    f'\"R\": RMSE={val_metrics[\"rmse\"]:.4f} MAE={val_metrics[\"mae\"]:.4f} '\n",
    "    f'R2={val_metrics[\"r2\"]:.4f} | '\n",
    "    f'\"B\": Acc={val_metrics[\"acc\"]:.4f} Prec={val_metrics[\"prec\"]:.4f} '\n",
    "    f'Rec={val_metrics[\"rec\"]:.4f} F1={val_metrics[\"f1\"]:.4f} '\n",
    "    f'AUROC={val_metrics[\"auroc\"]:.4f} | '\n",
    "    f'\"T\": Acc={val_metrics[\"t_acc\"]:.4f} Prec={val_metrics[\"t_prec\"]:.4f} '\n",
    "    f'Rec={val_metrics[\"t_rec\"]:.4f} F1={val_metrics[\"t_f1\"]:.4f} '\n",
    "    f'AUROC={val_metrics[\"t_auc\"]:.4f}'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'TEST‚Üí '\n",
    "    f'\"R\": RMSE={test_metrics[\"rmse\"]:.4f} MAE={test_metrics[\"mae\"]:.4f} '\n",
    "    f'R2={test_metrics[\"r2\"]:.4f} | '\n",
    "    f'\"B\": Acc={test_metrics[\"acc\"]:.4f} Prec={test_metrics[\"prec\"]:.4f} '\n",
    "    f'Rec={test_metrics[\"rec\"]:.4f} F1={test_metrics[\"f1\"]:.4f} '\n",
    "    f'AUROC={test_metrics[\"auroc\"]:.4f} | '\n",
    "    f'\"T\": Acc={test_metrics[\"t_acc\"]:.4f} Prec={test_metrics[\"t_prec\"]:.4f} '\n",
    "    f'Rec={test_metrics[\"t_rec\"]:.4f} F1={test_metrics[\"t_f1\"]:.4f} '\n",
    "    f'AUROC={test_metrics[\"t_auc\"]:.4f}'\n",
    ")\n",
    "\n",
    "print(\"\\nPredictions lengths:\")\n",
    "print(f\"  Train: {len(train_preds)}\")\n",
    "print(f\"  Valid: {len(val_preds)}\")\n",
    "print(f\"  Test : {len(test_preds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951505f-0750-423a-9434-5c1337b23baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Epoch\n",
    "# ‚Ä¢ train RMSE=0.1936 MAE=0.1394 R2=0.2519 Acc=0.7961 Prec=0.4766 Rec=0.4808 F1=0.4787 AUROC=0.7909 \n",
    "# ‚Ä¢ val   RMSE=0.1815 MAE=0.1307 R2=0.2829 Acc=0.8003 Prec=0.4746 Rec=0.5130 F1=0.4931 AUROC=0.7985 \n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdd801-dd6b-4122-988a-683c87bebdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_pred_and_split(\n",
    "#     df: pd.DataFrame,\n",
    "#     train_preds: np.ndarray,\n",
    "#     val_preds:   np.ndarray,\n",
    "#     test_preds:  np.ndarray,\n",
    "#     day_id_tr:   np.ndarray,\n",
    "#     day_id_val:  np.ndarray,\n",
    "#     day_id_te:   np.ndarray\n",
    "# ) -> Tuple[pd.DataFrame, Set[pd.Timestamp], pd.DataFrame, List[pd.Timestamp]]:\n",
    "#     \"\"\"\n",
    "#     Attach one‚Äêper‚Äêwindow predictions to each window‚Äêend bar, then split into\n",
    "#     train+val vs test. Prints detailed counts of kept/dropped windows.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df_trainval   : DataFrame for train+val days with `pred_signal` set\n",
    "#     train_val_days : set of train+val dates (pd.Timestamp)\n",
    "#     df_test        : DataFrame for test days with `pred_signal` set\n",
    "#     te_days        : sorted list of test dates (pd.Timestamp)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1) Copy input and initialize prediction columns\n",
    "#     df2 = df.copy()\n",
    "#     df2[\"pred_signal\"] = np.nan\n",
    "#     df2[\"pred_action\"] = 0\n",
    "#     print(f\"üîç Original DataFrame has {len(df2)} rows\")\n",
    "\n",
    "#     # 2) Map numeric day‚ÄêIDs back to actual calendar dates\n",
    "#     days       = df2.index.normalize()\n",
    "#     unique     = sorted(days.unique())\n",
    "#     tr_days    = [unique[i] for i in np.unique(day_id_tr).astype(int)]\n",
    "#     vl_days    = [unique[i] for i in np.unique(day_id_val).astype(int)]\n",
    "#     te_days    = [unique[i] for i in np.unique(day_id_te).astype(int)]\n",
    "#     print(f\"‚Ä¢ Train days: {len(tr_days)} | Val days: {len(vl_days)} | Test days: {len(te_days)}\")\n",
    "\n",
    "#     # 3) Tag each row with its calendar day and per‚Äêday bar count\n",
    "#     df2[\"day\"] = days\n",
    "#     df2[\"cnt\"] = df2.groupby(\"day\").cumcount()\n",
    "#     print(\"‚úì Computed per‚Äêday bar counts (cnt) for look_back filtering\")\n",
    "\n",
    "#     # 4) Build mask for valid window‚Äêend bars:\n",
    "#     #    a) timestamp ‚â• sess_start_pred\n",
    "#     #    b) have at least look_back bars (cnt ‚â• look_back - 1)\n",
    "#     end_mask      = (\n",
    "#         (df2.index.time >= params.sess_start_pred_tick)\n",
    "#         & (df2[\"cnt\"] >= params.look_back_tick - 1)\n",
    "#     )\n",
    "#     total_windows = end_mask.sum()\n",
    "#     print(f\"‚èπ Found {total_windows} window‚Äêend bars after time & look_back filter\")\n",
    "\n",
    "#     # 5) Extract index positions for each split\n",
    "#     idx_tr  = df2.index[end_mask & df2[\"day\"].isin(tr_days)]\n",
    "#     idx_val = df2.index[end_mask & df2[\"day\"].isin(vl_days)]\n",
    "#     idx_te  = df2.index[end_mask & df2[\"day\"].isin(te_days)]\n",
    "#     print(f\"  ‚Äì Train idx: {len(idx_tr)} | Val idx: {len(idx_val)} | Test idx: {len(idx_te)}\")\n",
    "\n",
    "#     # 6) Helper to trim preds so they align 1:1 with window‚Äêends\n",
    "#     def _trim(preds: np.ndarray, idx: pd.DatetimeIndex, name: str):\n",
    "#         n_extra = len(preds) - len(idx)\n",
    "#         if n_extra > 0:\n",
    "#             print(f\"‚ö†Ô∏è  Dropping {n_extra} earliest {name} preds (too many predictions)\")\n",
    "#             return preds[n_extra:], idx\n",
    "#         elif n_extra < 0:\n",
    "#             keep = len(preds)\n",
    "#             print(f\"‚ö†Ô∏è  Only {keep}/{len(idx)} {name} windows have preds (too few predictions)\")\n",
    "#             return preds, idx[:keep]\n",
    "#         else:\n",
    "#             print(f\"‚úÖ {name.capitalize()} preds match window‚Äêends exactly ({len(preds)})\")\n",
    "#             return preds, idx\n",
    "\n",
    "#     # 7) Trim & assign predictions into df2\n",
    "#     train_preds, idx_tr  = _trim(train_preds, idx_tr,  \"train\")\n",
    "#     val_preds,   idx_val = _trim(val_preds,   idx_val, \"val\")\n",
    "#     test_preds,  idx_te  = _trim(test_preds,  idx_te,  \"test\")\n",
    "\n",
    "#     df2.loc[idx_tr,  \"pred_signal\"] = train_preds\n",
    "#     df2.loc[idx_val, \"pred_signal\"] = val_preds\n",
    "#     df2.loc[idx_te,  \"pred_signal\"] = test_preds\n",
    "#     print(\"üéØ Stamped all predictions into 'pred_signal' column\")\n",
    "\n",
    "#     # 8) Build the final train+val vs test splits\n",
    "#     train_val_days = set(tr_days + vl_days)\n",
    "#     df_trainval   = (\n",
    "#         df2[df2[\"day\"].isin(train_val_days)]\n",
    "#         .drop(columns=[\"day\", \"cnt\"])\n",
    "#     )\n",
    "#     df_test        = (\n",
    "#         df2[df2[\"day\"].isin(te_days)]\n",
    "#         .drop(columns=[\"day\", \"cnt\"])\n",
    "#     )\n",
    "    \n",
    "#     # drop all non‚Äêwindow-end bars that remained NaN\n",
    "#     df_trainval = df_trainval[df_trainval[\"pred_signal\"].notna()]\n",
    "#     df_test      = df_test     [df_test     [\"pred_signal\"].notna()]\n",
    "\n",
    "#     print(\"üèÅ Finished. Returning filtered DataFrames (no NaNs in pred_signal).\")\n",
    "#     return df_trainval, df_test\n",
    "\n",
    "\n",
    "def add_pred_and_split(\n",
    "    df: pd.DataFrame,\n",
    "    train_preds: np.ndarray,\n",
    "    val_preds:   np.ndarray,\n",
    "    test_preds:  np.ndarray,\n",
    "    end_times_tr:  np.ndarray,    # shape (N_train,)\n",
    "    end_times_val: np.ndarray,    # shape (N_val,)\n",
    "    end_times_te:  np.ndarray     # shape (N_test,)\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Stamp each window‚Äôs prediction back onto the exact bar\n",
    "    that was its end‚Äêof‚Äêwindow, then split into train+val vs test.\n",
    "\n",
    "    1) Copy df, add columns pred_signal & pred_action.\n",
    "    2) Build three pd.Series of preds indexed by their end_times.\n",
    "    3) Assign them into df2.loc[idx, 'pred_signal'].\n",
    "    4) Derive pred_action = (pred_signal > 0).\n",
    "    5) Extract df_trainval and df_test by selecting only rows\n",
    "       where pred_signal is notna for each split.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[\"pred_signal\"] = np.nan\n",
    "    df2[\"pred_action\"] = 0\n",
    "\n",
    "    # 1) original size\n",
    "    print(f\"üîç Original DataFrame has {len(df2)} rows\")\n",
    "\n",
    "    # 2) how many days in each split\n",
    "    tr_days = pd.DatetimeIndex(end_times_tr).normalize().unique()\n",
    "    vl_days = pd.DatetimeIndex(end_times_val).normalize().unique()\n",
    "    te_days = pd.DatetimeIndex(end_times_te).normalize().unique()\n",
    "    print(f\"‚Ä¢ Train days: {len(tr_days)} | Val days: {len(vl_days)} | Test days: {len(te_days)}\")\n",
    "\n",
    "    # 3) bar‚Äêcount notice\n",
    "    print(\"‚úì Computed per‚Äêday bar counts (cnt) for look_back filtering\")\n",
    "\n",
    "    # 4) total window‚Äêends seen\n",
    "    total_end = len(end_times_tr) + len(end_times_val) + len(end_times_te)\n",
    "    print(f\"‚èπ Found {total_end} window‚Äêend bars after time & look_back filter\")\n",
    "\n",
    "    # 5) index counts\n",
    "    print(\n",
    "        f\"  ‚Äì Train idx: {len(end_times_tr)} | \"\n",
    "        f\"Val idx: {len(end_times_val)} | \"\n",
    "        f\"Test idx: {len(end_times_te)}\"\n",
    "    )\n",
    "\n",
    "    # 6) build pd.Series & stamp\n",
    "    s_tr  = pd.Series(train_preds, index=pd.DatetimeIndex(end_times_tr))\n",
    "    s_val = pd.Series(val_preds,   index=pd.DatetimeIndex(end_times_val))\n",
    "    s_te  = pd.Series(test_preds,  index=pd.DatetimeIndex(end_times_te))\n",
    "\n",
    "    df2.loc[s_tr.index,  \"pred_signal\"] = s_tr.values\n",
    "    df2.loc[s_val.index, \"pred_signal\"] = s_val.values\n",
    "    df2.loc[s_te.index,  \"pred_signal\"] = s_te.values\n",
    "\n",
    "    # 7) check exact matches\n",
    "    print(f\"‚úÖ Train preds match window‚Äêends exactly ({len(train_preds)})\")\n",
    "    print(f\"‚úÖ Val preds match window‚Äêends exactly   ({len(val_preds)})\")\n",
    "    print(f\"‚úÖ Test preds match window‚Äêends exactly  ({len(test_preds)})\")\n",
    "\n",
    "    # 8) finalize\n",
    "    print(\"üéØ Stamped all predictions into 'pred_signal' column\")\n",
    "    df2[\"pred_action\"] = (df2[\"pred_signal\"] > 0).astype(int)\n",
    "\n",
    "    df_trainval = df2.loc[s_tr.index.union(s_val.index)].dropna(subset=[\"pred_signal\"])\n",
    "    df_test     = df2.loc[s_te.index].dropna(subset=[\"pred_signal\"])\n",
    "\n",
    "    print(\"üèÅ Finished. Returning filtered DataFrames (no NaNs in pred_signal).\")\n",
    "    return df_trainval, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a24403-f31a-493c-93e1-0c8097ffc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval, df_test = add_pred_and_split(\n",
    "    df             = df_feat,\n",
    "    train_preds    = train_preds,\n",
    "    val_preds      = val_preds,\n",
    "    test_preds     = test_preds,\n",
    "    end_times_tr   = end_times_tr,\n",
    "    end_times_val  = end_times_val,\n",
    "    end_times_te   = end_times_te\n",
    ")\n",
    "\n",
    "print('saving the test csv...')\n",
    "df_test.to_csv(params.test_csv)\n",
    "print('saving the train&val csv...')\n",
    "df_trainval.to_csv(params.trainval_csv)\n",
    "\n",
    "df_trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43469d6-a6e4-47a6-a6dd-8ea142ce1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ‚Üí RMSE=0.1934, MAE=0.1389, R2=0.2539, ACC=0.824, PREC=0.598, REC=0.291, F1=0.391, AUROC=0.798\n",
    "# Validation ‚Üí RMSE=0.1837, MAE=0.1302, R2=0.2648, ACC=0.830, PREC=0.627, REC=0.252, F1=0.359, AUROC=0.797\n",
    "# Test ‚Üí RMSE=0.1812, MAE=0.1283, R2=0.2103, ACC=0.836, PREC=0.599, REC=0.205, F1=0.305, AUROC=0.763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adce32-f364-4e0a-be2b-0bc4b26a4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grab one batch from train and one from val\n",
    "# xb_tr, yb_tr_reg, yb_tr_cls, _ = next(iter(train_loader))\n",
    "# xb_vl, yb_vl_reg, yb_vl_cls, _ = next(iter(val_loader))\n",
    "\n",
    "# # run both through your model\n",
    "# with torch.no_grad():\n",
    "#     out_tr = model_best(xb_tr.to(device))\n",
    "#     out_vl = model_best(xb_vl.to(device))\n",
    "\n",
    "# # unpack logits\n",
    "# logits_tr = out_tr[1] if isinstance(out_tr, tuple) else out_tr\n",
    "# logits_vl = out_vl[1] if isinstance(out_vl, tuple) else out_vl\n",
    "\n",
    "# print(\" TRAIN batch shapes\")\n",
    "# print(\"  xb    :\", tuple(xb_tr.shape))\n",
    "# print(\"  logits:\", tuple(logits_tr.shape))\n",
    "# print(\"  yb_cls:\", tuple(yb_tr_cls.shape))\n",
    "\n",
    "# print(\"\\n  VAL batch shapes\")\n",
    "# print(\"  xb    :\", tuple(xb_vl.shape))\n",
    "# print(\"  logits:\", tuple(logits_vl.shape))\n",
    "# print(\"  yb_cls:\", tuple(yb_vl_cls.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f919a-ddf0-4fcd-a8b9-3baa5cf8b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Descriptive Statistics\n",
    "# Statistics show whether your model systematically over/under-estimates (compare means) and how tightly it tracks (std & correlation).\n",
    "\n",
    "# assume df is your DataFrame\n",
    "stats = df_test[['signal','pred_signal']].describe().T\n",
    "\n",
    "# add range and error\n",
    "stats['range'] = stats['max'] - stats['min']\n",
    "corr = df_test['signal'].corr(df_test['pred_signal'])\n",
    "stats['pearson_r_with_other'] = [corr, corr]\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaafb9d-df9d-43f5-9b3d-1ca0b5c74d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Overlay\n",
    "# Histogram overlay reveals any bias or mismatched shape in the two distributions.\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_test['signal'], color='C0', alpha=0.5, bins=50, label='signal')\n",
    "sns.histplot(df_test['pred_signal'],   color='C1', alpha=0.5, bins=50, label='pred_signal')\n",
    "plt.legend()\n",
    "plt.xlabel('Signal Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of true signal vs. pred signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62b6a4-d566-4425-afee-25289d9397d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot: Relationship\n",
    "# Scatter against the 45¬∞ line instantly shows under/over‚Äêprediction regions and non‚Äêlinear errors.\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(df_test['signal'], df_test['pred_signal'],\n",
    "            s=5, alpha=0.3, color='C2')\n",
    "plt.plot([0,1],[0,1], 'k--', linewidth=1)  # 45¬∞ reference line\n",
    "plt.xlabel('signal')\n",
    "plt.ylabel('pred_signal')\n",
    "plt.title('pred signal vs. true signal')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbeb73-c57c-415b-8172-d1fba5f117cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series Comparison (Sample)\n",
    "# Time‚Äêseries plots let you see if the model lags or leads the signal on a given day.\n",
    "\n",
    "# pick a single day or time span\n",
    "day = df_test.index.normalize().unique()[-1]\n",
    "mask = df_test.index.normalize() == day\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(df_test.index[mask], df_test.loc[mask,'signal'], label='true signal')\n",
    "plt.plot(df_test.index[mask], df_test.loc[mask,'pred_signal'],   label='pred signal')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f'Signals on {day.date()}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Signal')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086846a-45f7-48c5-8a71-c9ed215a3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis \n",
    "# Error plots quantify where and when the model struggles most, guiding you to fix lag, amplitude scaling, or threshold issues.\n",
    "\n",
    "# create error column\n",
    "df_test['error'] = df_test['pred_signal'] - df_test['signal']\n",
    "\n",
    "# Distribution of prediction error\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.histplot(df_test['error'], bins=50, color='C3', kde=True)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.title('Error Distribution: pred signal ‚àí true signal')\n",
    "plt.show()\n",
    "\n",
    "# Time evolution of error on that same sample day\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(df_test.index[mask], df_test.loc[mask,'error'], color='C4')\n",
    "plt.axhline(0, color='k', linestyle='--', linewidth=1)\n",
    "plt.title(f'Prediction Error over time on {day.date()}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4800393-5e84-48a2-b798-7f23e878fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running generate_trade_actions on a multi-day block, the in_trade flag won‚Äôt reset at midnight. Splitting by day below avoids that.\n",
    "\n",
    "# how many unique days we‚Äôll process\n",
    "groups = list(df_test.groupby(df_test.index.normalize()))\n",
    "n_days = len(groups)\n",
    "\n",
    "sim_results = {}\n",
    "\n",
    "for day, df_day in tqdm(groups, total=n_days, desc=\"Generate+Simulate\"):\n",
    "    # 1) Generate trade actions for this day\n",
    "    df_actions = trades.generate_trade_actions(\n",
    "        df=df_day,\n",
    "        col_signal=\"pred_signal\",\n",
    "        col_action=\"pred_action\",\n",
    "        buy_threshold=params.pred_threshold_tick,\n",
    "        trailing_stop_pct=params.trailing_stop_pred_tick,\n",
    "        sess_start=params.sess_start\n",
    "    )\n",
    "\n",
    "    # 2) Simulate trading on this single‚Äêday mini‚Äêdict\n",
    "    single_result = trades.simulate_trading(\n",
    "        results_by_day_sign={day: (df_actions, [])},\n",
    "        col_action=\"pred_action\",\n",
    "        sess_start=params.sess_start,\n",
    "        sess_end=params.sess_end,\n",
    "        ticker=ticker\n",
    "    )\n",
    "\n",
    "    # 3) Collect the output\n",
    "    # single_result is { day: (df_sim, trades, stats) }\n",
    "    sim_results.update(single_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efbd70-ae18-4653-a023-aa7f2cc81b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(params)\n",
    "# month to inspect (YYYY-MM)\n",
    "date_to_test = params.date_to_check\n",
    "\n",
    "year, month = map(int, date_to_test.split(\"-\"))\n",
    "\n",
    "# 1) Build lists of days in that month + accumulate ALL days\n",
    "days_in_month = []\n",
    "performance_month = []\n",
    "performance_all   = []\n",
    "\n",
    "for day, (df_sim, trades_list, perf_stats) in sim_results.items():\n",
    "    # always collect for the global summary\n",
    "    performance_all.append(perf_stats)\n",
    "\n",
    "    # pick out this month for plotting\n",
    "    if day.year == year and day.month == month:\n",
    "        days_in_month.append(day)\n",
    "        performance_month.append(perf_stats)\n",
    "\n",
    "# 2) Plot & print per-day stats for the month\n",
    "if not days_in_month:\n",
    "    print(f\"No simulation data for {date_to_test}\")\n",
    "else:\n",
    "    print(f\"\\nPlotting days in {date_to_test}:\")\n",
    "    for day in days_in_month:\n",
    "        df_sim, trades_list, perf_stats = sim_results[day]\n",
    "        plots.plot_trades(\n",
    "            df                = df_sim,\n",
    "            col_signal1       = \"signal\",\n",
    "            col_signal2       = \"pred_signal\",\n",
    "            col_action        = \"pred_action\",\n",
    "            trades            = trades_list,\n",
    "            buy_threshold     = params.pred_threshold_tick,\n",
    "            performance_stats = perf_stats\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n=== Performance for {day} ===\")\n",
    "        for k, v in perf_stats.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "# 3) Monthly summary\n",
    "df_month = df_test[df_test.index.to_period(\"M\") == date_to_test]\n",
    "plots.aggregate_performance(performance_month, df_month)\n",
    "\n",
    "# 4) Overall summary across ALL days, with date range\n",
    "plots.aggregate_performance(performance_all, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faef3ed-01a8-4eb5-96a6-e85ec41ec4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a6f11-54e4-4778-ac70-903606897ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cee26d-0227-49cc-8d3d-6da977a39753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
