{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed6c184-8438-497e-8ae7-82cc35ac4819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libs.models_custom' from '/workspace/my_models/Trading/_Stock_Analysis_/libs/models_custom.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "# 1) Wipe out your namespace\n",
    "%reset -f\n",
    "\n",
    "# 2) Clear Jupyter’s stored outputs (and inputs if you like)\n",
    "try:\n",
    "    Out.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    In.clear()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# 3) Force Python GC\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 4) Free any GPU buffers\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "from libs import params, trades, feats, plots, models_core, models_custom\n",
    "importlib.reload(params)\n",
    "importlib.reload(trades)\n",
    "importlib.reload(feats)\n",
    "importlib.reload(plots)\n",
    "importlib.reload(models_core)\n",
    "importlib.reload(models_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c868158-e6bb-4d56-bbdd-8e8103f0b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy  as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "from typing import Sequence, List, Tuple, Optional, Union\n",
    "from pathlib import Path\n",
    "import time\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, Dropout\n",
    "import torch.nn.functional as Funct\n",
    "from torch_lr_finder import LRFinder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Callable, List, Dict, Tuple, Optional\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "\n",
    "import shap\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "\n",
    "from IPython.display import clear_output, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full feature set filter ( or filter : params.sess_start_reg, params.sess_end ? )\n",
    "df_all_features = pd.read_csv(params.feat_all_csv, index_col=0, parse_dates=True) \n",
    "\n",
    "# Separate feature matrix X and target vector y\n",
    "all_features = [c for c in df_all_features.columns if c not in ['close_raw', params.label_col]]\n",
    "X_all        = df_all_features[all_features]\n",
    "y            = df_all_features[params.label_col]\n",
    "\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572f7cd-54b6-47e6-8e2c-07297ab0086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_final_feats, pruned_feats, corr_full, corr_pruned = feats.prune_features_by_variance_and_correlation(\n",
    "    X_all=X_all,\n",
    "    y=y,\n",
    "    min_std=params.feats_min_std,\n",
    "    max_corr=params.feats_max_corr,\n",
    ")\n",
    "\n",
    "# show heatmaps\n",
    "plots.plot_correlation_before_after(corr_full, corr_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1e572-a745-4f52-8990-b3a85f2324e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build final feature DataFrame if needed\n",
    "df_feat_final = df_all_features[kept_final_feats + [params.label_col] + ['close_raw']]\n",
    "\n",
    "df_feat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8d933-8402-45de-a838-4335b2a37d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(params) #######\n",
    "\n",
    "_, best_optuna_params = params.load_target_sign_optuna_record(sig_type = \"target\")\n",
    "\n",
    "train_loader, val_loader, test_loader, end_times_tr, end_times_val, end_times_te = models_core.model_core_pipeline(\n",
    "    df             = df_feat_final,\n",
    "    look_back      = params.hparams[\"LOOK_BACK\"],\n",
    "    sess_start     = params.sess_start_shift,\n",
    "    train_prop     = params.train_prop,\n",
    "    val_prop       = params.val_prop,\n",
    "    train_batch    = params.hparams[\"TRAIN_BATCH\"],\n",
    "    train_workers  = params.hparams[\"TRAIN_WORKERS\"],\n",
    "    prefetch_factor= params.hparams[\"TRAIN_PREFETCH_FACTOR\"],\n",
    "    signal_thresh  = best_optuna_params[\"buy_thresh\"],\n",
    "    return_thresh  = params.return_thresh_tick\n",
    ")\n",
    "\n",
    "del df_feat_final, end_times_tr, end_times_val, end_times_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c80d5-20f2-46bd-8d6d-9223b72fb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_windows_from_loader(loader):\n",
    "    \"\"\"\n",
    "    Extract all windows and targets from a DataLoader into numpy arrays.\n",
    "\n",
    "    Parameters\n",
    "    - loader: iterable DataLoader that yields batches where\n",
    "        batch[0] is a tensor with shape (B, L, F) and\n",
    "        batch[1] is a tensor with shape (B,) (targets).\n",
    "\n",
    "    Returns\n",
    "    - X_windows: numpy array, shape (N, L, F)\n",
    "    - y_windows: numpy array, shape (N,)\n",
    "    \"\"\"\n",
    "    X_chunks, y_chunks = [], []\n",
    "    for batch in tqdm(loader, desc=\"Extracting windows\"):\n",
    "        # move tensors to CPU and convert to numpy\n",
    "        X_chunks.append(batch[0].detach().cpu().numpy())      # (B, L, F)\n",
    "        y_chunks.append(batch[1].detach().cpu().numpy().reshape(-1))  # (B,)\n",
    "    X_windows = np.concatenate(X_chunks, axis=0)\n",
    "    y_windows = np.concatenate(y_chunks, axis=0)\n",
    "    return X_windows, y_windows\n",
    "\n",
    "\n",
    "####### \n",
    "\n",
    "# Extract windows and targets from the test_loader\n",
    "X_windows, y_windows = extract_windows_from_loader(test_loader)\n",
    "\n",
    "X_windows.shape, y_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec031f-6c8f-455f-9c72-ae411e03ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "#################### CODE FOR MODEL RETRAINING #####################\n",
    "####################################################################\n",
    "\n",
    "model = models_custom.ModelClass(\n",
    "    n_feats             = len(kept_final_feats),\n",
    "    short_units         = params.hparams[\"SHORT_UNITS\"],\n",
    "    long_units          = params.hparams[\"LONG_UNITS\"],\n",
    "    transformer_d_model = params.hparams[\"TRANSFORMER_D_MODEL\"],\n",
    "    transformer_layers  = params.hparams[\"TRANSFORMER_LAYERS\"],\n",
    "    dropout_short       = params.hparams[\"DROPOUT_SHORT\"],\n",
    "    dropout_long        = params.hparams[\"DROPOUT_LONG\"],\n",
    "    dropout_trans       = params.hparams[\"DROPOUT_TRANS\"],\n",
    "    pred_hidden         = params.hparams[\"PRED_HIDDEN\"],\n",
    "    look_back           = params.hparams[\"LOOK_BACK\"],\n",
    "\n",
    "    # Gating flags\n",
    "    use_conv            = params.hparams[\"USE_CONV\"],\n",
    "    use_tcn             = params.hparams[\"USE_TCN\"],\n",
    "    use_short_lstm      = params.hparams[\"USE_SHORT_LSTM\"],\n",
    "    use_transformer     = params.hparams[\"USE_TRANSFORMER\"],\n",
    "    use_long_lstm       = params.hparams[\"USE_LONG_LSTM\"],\n",
    "    use_delta           = params.hparams[\"USE_DELTA\"],\n",
    "    flatten_mode        = params.hparams[\"FLATTEN_MODE\"]\n",
    ")\n",
    "\n",
    "model.feature_names = kept_final_feats # for logging\n",
    "model.to(params.device)  \n",
    "    \n",
    "optimizer = AdamW(\n",
    "  model.parameters(),\n",
    "  lr           = params.hparams[\"ONECYCLE_MAX_LR\"],\n",
    "  weight_decay = params.hparams[\"WEIGHT_DECAY\"]\n",
    ")\n",
    "\n",
    "batches_per_epoch = len(train_loader)\n",
    "total_steps = batches_per_epoch * params.hparams[\"MAX_EPOCHS\"]\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "  optimizer,\n",
    "  max_lr           = params.hparams[\"ONECYCLE_MAX_LR\"],\n",
    "  total_steps      = total_steps,\n",
    "  pct_start        = params.hparams[\"ONECYCLE_PCT_START\"],\n",
    "  div_factor       = params.hparams[\"ONECYCLE_DIV_FACTOR\"],\n",
    "  final_div_factor = params.hparams[\"ONECYCLE_FINAL_DIV\"],\n",
    "  anneal_strategy  = params.hparams[\"ONECYCLE_STRATEGY\"],\n",
    ")\n",
    "optimizer.scheduler = scheduler # necessary to log sched_field\n",
    "\n",
    "if getattr(scheduler, \"total_steps\", None) != total_steps:\n",
    "    raise RuntimeError(f\"Scheduler total_steps mismatch: scheduler={getattr(scheduler,'total_steps',None)} expected={total_steps}\")\n",
    "\n",
    "n_days = len(train_loader.dataset)\n",
    "print(f\"Training sees {n_days} unique trading days per epoch.\\n\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: total={total_params:,}, trainable={trainable_params:,}\\n\")\n",
    "\n",
    "print('Using HyperParameters:\\n', params.hparams)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run the custom stateful training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "best_val_rmse  = models_custom.model_training_loop(\n",
    "    model               = model,\n",
    "    optimizer           = optimizer,\n",
    "    scheduler           = scheduler,\n",
    "    scaler              = GradScaler(),\n",
    "    train_loader        = train_loader,\n",
    "    val_loader          = val_loader,\n",
    ")\n",
    "\n",
    "# retrieve model just trained (keep exactly 5 decimals)\n",
    "fname = f\"{params.ticker}_{best_val_rmse:.5f}_fin.pth\"\n",
    "chk = Path(params.models_folder) / fname\n",
    "\n",
    "# load checkpoint and recreate model from stored hparams\n",
    "ckpt = torch.load(chk, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# load weights, move to device, set eval\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded checkpoint:\", chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba8c6f-d120-409f-8a5c-0e47e6585e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ####################### CODE FOR MODEL RELOADING ####################\n",
    "# #####################################################################\n",
    "\n",
    "# fname = f\"{params.ticker}_{params.sel_val_rmse}_fin.pth\"\n",
    "# model_path = Path(params.models_folder) / fname\n",
    "\n",
    "# ckpt = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# print(\"\\nHyperparameters:\")\n",
    "# pprint(ckpt[\"hparams\"])\n",
    "\n",
    "# if \"train_plot_png\" in ckpt:\n",
    "#     img = Image.open(io.BytesIO(ckpt[\"train_plot_png\"]))\n",
    "#     display(img)\n",
    "\n",
    "# print(\"\\nTrain metrics:\", ckpt[\"train_metrics\"])\n",
    "# print(  \" Val metrics:\", ckpt[\"val_metrics\"])\n",
    "\n",
    "# # Reinstantiate a *clean* model in CPU memory\n",
    "# model = models_custom.ModelClass(\n",
    "#     n_feats             = len(kept_final_feats),\n",
    "#     short_units         = ckpt[\"hparams\"][\"SHORT_UNITS\"],\n",
    "#     long_units          = ckpt[\"hparams\"][\"LONG_UNITS\"],\n",
    "#     transformer_d_model = ckpt[\"hparams\"][\"TRANSFORMER_D_MODEL\"],\n",
    "#     transformer_layers  = ckpt[\"hparams\"][\"TRANSFORMER_LAYERS\"],\n",
    "#     dropout_short       = ckpt[\"hparams\"][\"DROPOUT_SHORT\"],\n",
    "#     dropout_long        = ckpt[\"hparams\"][\"DROPOUT_LONG\"],\n",
    "#     dropout_trans       = ckpt[\"hparams\"][\"DROPOUT_TRANS\"],\n",
    "#     pred_hidden         = ckpt[\"hparams\"][\"PRED_HIDDEN\"],\n",
    "#     look_back           = ckpt[\"hparams\"][\"LOOK_BACK\"],\n",
    "\n",
    "#     # Gating flags\n",
    "#     use_conv            = ckpt[\"hparams\"][\"USE_CONV\"],\n",
    "#     use_tcn             = ckpt[\"hparams\"][\"USE_TCN\"],\n",
    "#     use_short_lstm      = ckpt[\"hparams\"][\"USE_SHORT_LSTM\"],\n",
    "#     use_transformer     = ckpt[\"hparams\"][\"USE_TRANSFORMER\"],\n",
    "#     use_long_lstm       = ckpt[\"hparams\"][\"USE_LONG_LSTM\"],\n",
    "#     use_delta           = ckpt[\"hparams\"][\"USE_DELTA\"],\n",
    "#     flatten_mode        = ckpt[\"hparams\"][\"FLATTEN_MODE\"]\n",
    "# )\n",
    "\n",
    "# # Load *only* the weight tensors into that fresh model\n",
    "# model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "# model.to(params.device)\n",
    "# model.eval()\n",
    "\n",
    "# print(\"Loaded checkpoint:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01207fa-e889-4210-bf8d-ced21c36c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare master dict for feature importances\n",
    "features_importances = {feat: {} for feat in kept_final_feats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bb159-0eda-4d6e-95d1-29985824b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_importance(X_windows, y_windows, feature_names,\n",
    "                                n_repeats=1, batch_size=1024, seed=0, device=params.device):\n",
    "    \"\"\"\n",
    "    Compute model-level Pearson correlation importance for a trained transformer.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: absolute Pearson correlation per feature (higher = more important)\n",
    "    - base_mse: None (no baseline MSE for this univariate data method)\n",
    "    \"\"\"\n",
    "    \n",
    "    # flatten features across samples and timesteps: shape (N*L, F)\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names), \"feature_names length must match X_windows feature axis\"\n",
    "    X_flat = X_windows.reshape(N * L, F)\n",
    "    y_rep = np.repeat(y_windows, L)\n",
    "\n",
    "    # compute absolute Pearson per feature (handle constant cols)\n",
    "    scores = {}\n",
    "    for fi, fname in enumerate(feature_names):\n",
    "        col = X_flat[:, fi]\n",
    "        if np.all(col == col[0]):\n",
    "            corr_val = 0.0\n",
    "        else:\n",
    "            corr_val = np.corrcoef(col, y_rep)[0, 1]\n",
    "            if np.isnan(corr_val):\n",
    "                corr_val = 0.0\n",
    "        scores[fname] = float(abs(corr_val))\n",
    "\n",
    "    # build final series (sorted desc) and display using shared helper\n",
    "    imp_series = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # use your common live display helper to reveal results consistently\n",
    "    # reveal in original feature order (pass feature_names) or ranked order (imp_series.index.tolist())\n",
    "    _ = feats.live_display_importances(imp_series, features=feature_names, label=params.label_col, method=\"Corr\", batch=4, pause=0.02)\n",
    "\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "imp_corr, _ = corr_importance(X_windows, y_windows, kept_final_feats,\n",
    "                                         n_repeats=1, batch_size=1024, seed=0, device=params.device)\n",
    "\n",
    "feats.update_feature_importances(features_importances, importance_type=\"corr\", values=imp_corr)\n",
    "print(list(imp_corr.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0761ab-9b10-4cfb-9863-500ecdd215e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mi_importance(X_windows, y_windows, feature_names,\n",
    "                              n_repeats=1, batch_size=1024, seed=0, device=params.device,\n",
    "                              sample_size=50000, batch_feat_size=32, mi_n_neighbors=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Faster but slightly more precise mutual information importance.\n",
    "\n",
    "    Same signature as before plus optional tuning params:\n",
    "    - sample_size: max rows (N*L) to sample for MI estimation (int or None for full)\n",
    "    - batch_feat_size: number of features per vectorized MI call\n",
    "    - mi_n_neighbors: number of neighbors passed to mutual_info_regression\n",
    "    - n_jobs: parallel jobs for mutual_info_regression (-1 uses all cores if supported)\n",
    "\n",
    "    Returns: (pd.Series sorted desc, None) and updates features_importances like other functions.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    X_flat = X_windows.reshape(N * L, F)\n",
    "    y_flat = np.repeat(y_windows, L)\n",
    "\n",
    "    total_rows = X_flat.shape[0]\n",
    "    scores_acc = np.zeros(F, dtype=float)\n",
    "    runs = 2  # average two subsample runs (seed, seed+1)\n",
    "\n",
    "    for run in range(runs):\n",
    "        run_seed = seed + run\n",
    "        if sample_size is not None and total_rows > sample_size:\n",
    "            idx = rng.choice(total_rows, size=sample_size, replace=False)\n",
    "            X_sub = X_flat[idx, :]\n",
    "            y_sub = y_flat[idx]\n",
    "        else:\n",
    "            X_sub = X_flat\n",
    "            y_sub = y_flat\n",
    "\n",
    "        const_mask = np.all(X_sub == X_sub[:1, :], axis=0)\n",
    "\n",
    "        for start in range(0, F, batch_feat_size):\n",
    "            cols = list(range(start, min(F, start + batch_feat_size)))\n",
    "            nonconst = [c for c in cols if not const_mask[c]]\n",
    "            if nonconst:\n",
    "                try:\n",
    "                    mi_vals = mutual_info_regression(X_sub[:, nonconst], y_sub,\n",
    "                                                     random_state=run_seed,\n",
    "                                                     n_neighbors=mi_n_neighbors,\n",
    "                                                     n_jobs=n_jobs)\n",
    "                except TypeError:\n",
    "                    mi_vals = mutual_info_regression(X_sub[:, nonconst], y_sub, random_state=run_seed)\n",
    "                j = 0\n",
    "                for c in cols:\n",
    "                    if const_mask[c]:\n",
    "                        scores_acc[c] += 0.0\n",
    "                    else:\n",
    "                        scores_acc[c] += float(mi_vals[j]); j += 1\n",
    "            else:\n",
    "                for c in cols:\n",
    "                    scores_acc[c] += 0.0\n",
    "\n",
    "        # light pause to let UI breathe between subsample runs\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        # partial display using the shared helper (reveals current averaged values)\n",
    "        partial_series = pd.Series(scores_acc / (run + 1), index=feature_names).sort_values(ascending=False)\n",
    "        # reveal in original feature order; batch controls update granularity\n",
    "        _ = feats.live_display_importances(partial_series, features=feature_names, label=params.label_col, method=f\"MI (run {run+1}/{runs})\", batch=16, pause=0.01)\n",
    "\n",
    "    mi_avg = scores_acc / runs\n",
    "    imp_series = pd.Series({feature_names[i]: float(mi_avg[i]) for i in range(F)}).sort_values(ascending=False)\n",
    "    # final reveal (ranked)\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(), label=params.label_col, method=\"MI\", batch=8, pause=0.02)\n",
    "\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "imp_mi, _ = mi_importance(X_windows, y_windows, kept_final_feats,\n",
    "                                      n_repeats=1, batch_size=1024, seed=0, device=params.device)\n",
    "\n",
    "feats.update_feature_importances(features_importances, importance_type=\"mi\", values=imp_mi)\n",
    "print(list(imp_mi.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6a7e3-7afa-4834-a99b-3a8dadc8c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linreg_coef_importance(X_windows, y_windows, feature_names,\n",
    "                                       n_repeats=1, batch_size=1024, seed=0, device=params.device):\n",
    "    \"\"\"\n",
    "    Importance from absolute standardized coefficients of a Ridge linear model\n",
    "    trained on flattened (sample x time) features.\n",
    "\n",
    "    - Fits a Ridge regression on X_flat (shape N*L, F) -> y_rep (N*L,)\n",
    "    - Standardizes features before fitting (zero mean, unit std)\n",
    "    - Uses absolute value of coefficients as importance\n",
    "    Returns (pd.Series sorted desc, None) and does not change the torch model.\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare flattened data\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names), \"feature_names length must match X_windows feature axis\"\n",
    "    X_flat = X_windows.reshape(N * L, F)\n",
    "    y_flat = np.repeat(y_windows, L)\n",
    "\n",
    "    # quick initial placeholder so UI shows immediate feedback\n",
    "    clear_output(wait=True)\n",
    "    display_text = pd.Series({\"status\": \"Starting linreg importance...\"})\n",
    "    # use live_display_importances to show placeholder (it expects a Series indexed by features)\n",
    "    placeholder = pd.Series(np.zeros(len(feature_names)), index=feature_names)\n",
    "    _ = feats.live_display_importances(placeholder, features=feature_names, label=params.label_col, method=\"LinReg (starting)\", batch= max(1, F//10), pause=0.01)\n",
    "\n",
    "    # standardize features only on a small subsample to avoid allocating full X_std\n",
    "    scaler = StandardScaler()\n",
    "    rng = np.random.RandomState(seed)\n",
    "    subsample_size = min(20000, X_flat.shape[0])\n",
    "    if X_flat.shape[0] > subsample_size:\n",
    "        subs_idx = rng.choice(X_flat.shape[0], size=subsample_size, replace=False)\n",
    "        X_sub_raw = X_flat[subs_idx]\n",
    "        scaler.fit(X_sub_raw)\n",
    "        X_sub = scaler.transform(X_sub_raw)\n",
    "        y_sub = y_flat[subs_idx]\n",
    "    else:\n",
    "        X_sub = scaler.fit_transform(X_flat)\n",
    "        y_sub = y_flat\n",
    "    \n",
    "    # free the full flattened array early\n",
    "    del X_flat\n",
    "    gc.collect()\n",
    "\n",
    "    alphas = np.logspace(-3, 3, 13)\n",
    "    # use RidgeCV but run on subsample\n",
    "    ridge_cv = RidgeCV(alphas=alphas, scoring=\"neg_mean_squared_error\")\n",
    "    ridge_cv.fit(X_sub, y_sub)\n",
    "    best_alpha = float(ridge_cv.alpha_)\n",
    "\n",
    "    # intermediate reveal after alpha selection (show small text via live_display_importances)\n",
    "    mid_placeholder = pd.Series(np.zeros(len(feature_names)), index=feature_names)\n",
    "    # annotate best alpha by temporarily setting the first feature to a nonzero small value so UI shows progress textually\n",
    "    mid_placeholder.iloc[0] = 0.001\n",
    "    _ = feats.live_display_importances(mid_placeholder, features=feature_names, label=f\"{params.label_col} (alpha={best_alpha:.3g})\", method=\"LinReg (alpha selected)\", batch=max(1, F//10), pause=0.01)\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "    # fit final Ridge on standardized data with chosen alpha (fast)\n",
    "    ridge = Ridge(alpha=best_alpha, solver=\"auto\")\n",
    "    ridge.fit(X_sub, y_sub)\n",
    "    coefs = ridge.coef_.flatten() if hasattr(ridge, \"coef_\") else np.asarray([0.0] * F)\n",
    "    coefs = np.asarray(coefs).reshape(-1)[:F]\n",
    "\n",
    "    # importance = absolute standardized coefficient\n",
    "    scores = {feature_names[i]: float(abs(coefs[i])) for i in range(F)}\n",
    "    imp_series = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # final reveal using live_display_importances (ranked)\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(), label=params.label_col, method=\"LinReg\", batch=max(1, len(imp_series)//10), pause=0.02)\n",
    "\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "##############################################################################################################################################\n",
    "\n",
    "\n",
    "imp_linreg, _ = linreg_coef_importance(\n",
    "    X_windows, y_windows, kept_final_feats,\n",
    "    batch_size=1024, seed=0, device=params.device\n",
    ")\n",
    "\n",
    "feats.update_feature_importances(features_importances, importance_type=\"linreg\", values=imp_linreg)\n",
    "print(list(imp_linreg.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e83c9-27a3-4495-bd09-41059fa83842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_importance(model, X_windows, y_windows, feature_names,\n",
    "                        batch_size=1024, predict_batch=512, seed=0, device=params.device,\n",
    "                        chunk_size=32, plot=True):\n",
    "    \"\"\"\n",
    "    Exact ablation ΔMSE (no approximation), memory-safe:\n",
    "    - computes base_mse on full set\n",
    "    - processes features in chunks, but streams predictions over sample minibatches\n",
    "      to avoid allocating k * N full copies\n",
    "    - returns (pd.Series ΔMSE per feature, base_mse)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    was_training = model.training\n",
    "    model.to(device); model.eval()\n",
    "    with torch.no_grad():\n",
    "        base_pred = feats.predict_windows(model, X_windows, batch_size=batch_size, device=device)\n",
    "    base_mse = float(mean_squared_error(y_windows, base_pred))\n",
    "\n",
    "    X_windows = X_windows.astype(np.float32)\n",
    "    y_windows = y_windows.astype(np.float32)\n",
    "    feat_means = X_windows.reshape(N * L, F).mean(axis=0).astype(np.float32)\n",
    "\n",
    "    scores = np.zeros(F, dtype=float)\n",
    "\n",
    "    # process features in chunks but stream over sample minibatches\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, F, chunk_size):\n",
    "            end = min(F, start + chunk_size)\n",
    "            k = end - start\n",
    "            se_sums = np.zeros(k, dtype=float)   # sum of squared errors per ablation across all samples\n",
    "            total = 0\n",
    "\n",
    "            # stream over sample minibatches to keep memory small\n",
    "            for s in range(0, N, predict_batch):\n",
    "                s2 = min(N, s + predict_batch)\n",
    "                Xb = X_windows[s:s2]        # (B, L, F)\n",
    "                yb = y_windows[s:s2]        # (B,)\n",
    "\n",
    "                # build k small ablated copies for this minibatch: shape (k, B, L, F)\n",
    "                batch = np.repeat(Xb[None, ...], k, axis=0)\n",
    "                for i, fi in enumerate(range(start, end)):\n",
    "                    batch[i, :, :, fi] = float(feat_means[fi])\n",
    "                \n",
    "                batch_reshaped = batch.reshape(-1, L, F)\n",
    "                preds = feats.predict_windows(model, batch_reshaped, batch_size=batch_size, device=device)\n",
    "                preds = np.asarray(preds).reshape(k, s2 - s)\n",
    "\n",
    "                # accumulate sum of squared errors per ablation\n",
    "                se_sums += np.sum((preds - yb[None, :])**2, axis=1)\n",
    "                total += (s2 - s)\n",
    "\n",
    "            # compute MSE per ablation on full set and store delta\n",
    "            mses = se_sums / float(total)\n",
    "            scores[start:end] = mses - base_mse\n",
    "\n",
    "            if plot:\n",
    "                partial = pd.Series({f: float(scores[i]) for i, f in enumerate(feature_names)})\n",
    "                partial = partial.sort_values(ascending=False)\n",
    "                _ = feats.live_display_importances(partial, features=feature_names,\n",
    "                                                   label=params.label_col, method=\"Ablation\", batch=8, pause=0.02)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    imp_series = pd.Series({f: float(scores[i]) for i, f in enumerate(feature_names)}).sort_values(ascending=False)\n",
    "    if plot:\n",
    "        _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(),\n",
    "                                           label=params.label_col, method=\"Ablation\", batch=8, pause=0.02)\n",
    "    return imp_series, base_mse\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "imp_abl, baseline_mse = ablation_importance(\n",
    "    model, X_windows, y_windows, kept_final_feats,\n",
    "    batch_size=1024, seed=0, device=params.device\n",
    ")\n",
    "\n",
    "print(\"Baseline MSE:\", baseline_mse)\n",
    "feats.update_feature_importances(features_importances, importance_type=\"ablation\", values=imp_abl)\n",
    "print(list(imp_abl.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acfe75-c0d8-46ca-880e-502d79f7bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients_importance(model, X_windows, y_windows, feature_names,\n",
    "                                    n_steps=50, n_samples=1000, baseline=\"zero\",\n",
    "                                    batch_size=256, seed=0, device=params.device):\n",
    "    \"\"\"\n",
    "    Integrated Gradients aggregated to per-feature importance (compact, efficient).\n",
    "\n",
    "    - Computes per-sample IG attributions using batched torch computations and fewer allocations.\n",
    "    - Aggregates mean(|IG|) over samples and timesteps to give per-feature scores.\n",
    "    - baseline: \"zero\", \"mean\", or a numpy array of shape (L, F).\n",
    "    - Updates features_importances with importance_type=\"ig\" and returns (pd.Series, None).\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    n_explain = min(n_samples, N)\n",
    "    idx_explain = rng.choice(N, size=n_explain, replace=False) if N > n_explain else np.arange(N)\n",
    "    X_explain = X_windows[idx_explain].astype(\"float32\")  # (n_explain, L, F)\n",
    "\n",
    "    # baseline tensor (L, F)\n",
    "    if isinstance(baseline, str) and baseline == \"zero\":\n",
    "        baseline_w = np.zeros((L, F), dtype=np.float32)\n",
    "    elif isinstance(baseline, str) and baseline == \"mean\":\n",
    "        baseline_w = X_windows.mean(axis=0).astype(np.float32)\n",
    "    elif isinstance(baseline, np.ndarray):\n",
    "        baseline_w = baseline.astype(np.float32)\n",
    "        assert baseline_w.shape == (L, F)\n",
    "    else:\n",
    "        baseline_w = np.zeros((L, F), dtype=np.float32)\n",
    "\n",
    "    baseline_t = torch.from_numpy(baseline_w).to(device)\n",
    "\n",
    "    def batch_attributions(xb_np):\n",
    "        # xb_np: (B, L, F) float32 numpy\n",
    "        B = xb_np.shape[0]\n",
    "        xb_t = torch.from_numpy(xb_np).to(device)                       # (B, L, F)\n",
    "        grads_acc = torch.zeros_like(xb_t, device=device)               # accumulate grads on device\n",
    "\n",
    "        # loop over steps but keep tensors on device to reduce allocations\n",
    "        for step in range(1, n_steps + 1):\n",
    "            alpha = step / float(n_steps)\n",
    "            inp = baseline_t.unsqueeze(0) + alpha * (xb_t - baseline_t.unsqueeze(0))  # (B, L, F)\n",
    "            inp.requires_grad_(True)\n",
    "            out = model(inp)\n",
    "            preds = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            preds = preds.reshape(preds.shape[0], -1).sum(dim=1)  # per-sample scalar\n",
    "            preds_sum = preds.sum()                              # scalar\n",
    "            preds_sum.backward()\n",
    "            grads_acc += inp.grad.detach()\n",
    "            inp.grad.zero_()\n",
    "            del inp, out, preds, preds_sum\n",
    "        # IG estimate: (input - baseline) * avg_grad\n",
    "        ig_t = (xb_t - baseline_t.unsqueeze(0)) * (grads_acc / float(n_steps))\n",
    "        return ig_t.cpu().numpy()  # (B, L, F)\n",
    "\n",
    "    # compute attributions in batches and show partial updates (memory-safe: no growing list)\n",
    "    running_sum_feats = np.zeros((F,), dtype=np.float64)  # accumulate sum over samples+timesteps for each feature\n",
    "    processed = 0\n",
    "    \n",
    "    for start in range(0, len(X_explain), batch_size):\n",
    "        xb = X_explain[start:start + batch_size]\n",
    "        at_batch = batch_attributions(xb)                     # (B, L, F) numpy float32\n",
    "        # sum absolute attributions over timesteps and samples for this batch -> (F,)\n",
    "        batch_sum = np.abs(at_batch).sum(axis=(0, 1)).astype(np.float64)\n",
    "        running_sum_feats += batch_sum\n",
    "        processed += at_batch.shape[0]\n",
    "    \n",
    "        # partial display: mean over processed samples of sum over timesteps -> (F,)\n",
    "        per_feature_abs = (running_sum_feats / float(processed))\n",
    "        partial_scores = pd.Series({feature_names[i]: float(per_feature_abs[i]) for i in range(F)}).sort_values(ascending=False)\n",
    "        _ = feats.live_display_importances(partial_scores, features=feature_names, label=params.label_col, method=\"IG (partial)\", batch=max(1, F//10), pause=0.01)\n",
    "    \n",
    "    # final aggregation: mean over all explained samples of sum over timesteps\n",
    "    per_feature = running_sum_feats / float(processed)  # (F,)\n",
    "\n",
    "    scores = {feature_names[i]: float(per_feature[i]) for i in range(F)}\n",
    "    imp_series = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(), label=params.label_col, method=\"IG\", batch=max(1, len(imp_series)//10), pause=0.02)\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "imp_ig, _ = integrated_gradients_importance(\n",
    "    model, X_windows, y_windows, kept_final_feats,\n",
    "    n_steps=50, n_samples=1000, baseline=\"zero\",\n",
    "    batch_size=256, seed=0, device=params.device\n",
    ")\n",
    "\n",
    "feats.update_feature_importances(features_importances, importance_type=\"ig\", values=imp_ig)\n",
    "print(list(imp_ig.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcf844-ab3d-4ab7-b326-e553cb47fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_importance(model, X_windows, y_windows, feature_names,\n",
    "                            n_repeats=3, batch_size=1024, seed=0, device=params.device,\n",
    "                            n_samples=600, plot=True, plot_every=10, refine_topk=30):\n",
    "    \"\"\"\n",
    "    Fast permutation importance with a refinement pass:\n",
    "    - fast subsampled scoring to rank features\n",
    "    - recompute exact ΔMSE on full set for top `refine_topk` features to avoid sampling artifacts\n",
    "    - returns (imp_series, base_mse_full)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    # full baseline for reporting / exact refines\n",
    "    base_pred_full = feats.predict_windows(model, X_windows, batch_size=batch_size, device=device)\n",
    "    base_mse_full = mean_squared_error(y_windows, base_pred_full)\n",
    "\n",
    "    # subsample for fast scoring\n",
    "    n_sub = min(N, int(n_samples)) if n_samples is not None else N\n",
    "    if n_sub < N:\n",
    "        idx_sub = rng.choice(N, size=n_sub, replace=False)\n",
    "        X_sub = X_windows[idx_sub]\n",
    "        y_sub = y_windows[idx_sub]\n",
    "    else:\n",
    "        X_sub = X_windows\n",
    "        y_sub = y_windows\n",
    "\n",
    "    # baseline on subsample (use for deltas in fast pass)\n",
    "    base_pred_sub = feats.predict_windows(model, X_sub, batch_size=batch_size, device=device)\n",
    "    base_mse_sub = mean_squared_error(y_sub, base_pred_sub)\n",
    "\n",
    "    scores = {}\n",
    "    for fi, fname in enumerate(feature_names):\n",
    "        R = max(1, int(n_repeats))\n",
    "        tiled = np.repeat(X_sub[None, ...], R, axis=0).reshape(-1, L, F)\n",
    "        for r in range(R):\n",
    "            s = r * n_sub\n",
    "            e = s + n_sub\n",
    "            perm_idx = rng.permutation(n_sub)\n",
    "            tiled[s:e, :, fi] = X_sub[perm_idx, :, fi]\n",
    "\n",
    "        preds = feats.predict_windows(model, tiled, batch_size=batch_size, device=device)\n",
    "        preds = np.asarray(preds).reshape(R, n_sub)\n",
    "        mses = np.mean((preds - y_sub[None, :]) ** 2, axis=1)\n",
    "        scores[fname] = float(mses.mean() - base_mse_sub)\n",
    "\n",
    "        if plot and ((fi + 1) % plot_every == 0 or fi == F - 1):\n",
    "            partial = pd.Series(scores).reindex(feature_names).sort_values(ascending=False)\n",
    "            _ = feats.live_display_importances(partial, features=feature_names,\n",
    "                                               label=params.label_col, method=\"Perm\", batch=8, pause=0.02)\n",
    "\n",
    "    # refinement: recompute exact ΔMSE on full set for top-K\n",
    "    ranked = pd.Series(scores).sort_values(ascending=False)\n",
    "    topk = ranked.index[:min(refine_topk, len(ranked))].tolist()\n",
    "\n",
    "    # compute exact deltas for these top features on the full X_windows\n",
    "    refined = {}\n",
    "    for fname in topk:\n",
    "        fi = feature_names.index(fname)\n",
    "        orig_col = X_windows[:, :, fi].copy()\n",
    "        X_windows[:, :, fi] = rng.permutation(orig_col)   # permute full set once (consistent with original logic)\n",
    "        pred = feats.predict_windows(model, X_windows, batch_size=batch_size, device=device)\n",
    "        refined_val = mean_squared_error(y_windows, pred) - base_mse_full\n",
    "        refined[fname] = float(refined_val)\n",
    "        X_windows[:, :, fi] = orig_col\n",
    "\n",
    "    # replace top-K entries with refined exact values\n",
    "    for k, v in refined.items():\n",
    "        scores[k] = v\n",
    "\n",
    "    imp_series = pd.Series(scores).sort_values(ascending=False)\n",
    "    if plot:\n",
    "        _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(),\n",
    "                                           label=params.label_col, method=\"Perm\", batch=8, pause=0.02)\n",
    "    return imp_series, base_mse_full\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "imp_perm, baseline_mse = perm_importance(\n",
    "    model, X_windows, y_windows, kept_final_feats,\n",
    "    n_repeats=3, batch_size=1024, seed=0, device=params.device\n",
    ")\n",
    "print(\"Baseline MSE:\", baseline_mse)\n",
    "feats.update_feature_importances(features_importances, importance_type=\"perm\", values=imp_perm)\n",
    "print(list(imp_perm.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fe805-feeb-4010-87d8-be8241208186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_shap_importance(X_windows, y_windows, feature_names,\n",
    "                                          n_samples=50, background_samples=20, batch_size=1024,\n",
    "                                          seed=0, device=params.device, rf_n_estimators=30):\n",
    "    \"\"\"\n",
    "    Surrogate SHAP using a RandomForestRegressor + TreeSHAP on flattened windows.\n",
    "\n",
    "    - Fits RF on X_flat (N, L*F) -> y_windows (aligned per-window)\n",
    "    - Uses shap.TreeExplainer on a small sampled subset and aggregates |SHAP| across timesteps to per-feature scores\n",
    "    - Fast defaults: small surrogate, small explain set, chunked SHAP, and immediate UI placeholder\n",
    "    - Returns (pd.Series sorted desc, None) and updates features_importances with importance_type=\"surrogate_shap\"\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    X_flat = X_windows.reshape(N, L * F)\n",
    "    y_flat = y_windows\n",
    "\n",
    "    # fit surrogate on a small subset\n",
    "    max_rows = 5000\n",
    "    if X_flat.shape[0] > max_rows:\n",
    "        idx_fit = rng.choice(X_flat.shape[0], size=max_rows, replace=False)\n",
    "        X_fit = X_flat[idx_fit]\n",
    "        y_fit = y_flat[idx_fit]\n",
    "    else:\n",
    "        X_fit = X_flat\n",
    "        y_fit = y_flat\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=min(5, rf_n_estimators), max_depth=8, n_jobs=1, random_state=seed)\n",
    "    rf.fit(X_fit, y_fit)\n",
    "\n",
    "    placeholder = pd.Series(np.zeros(len(feature_names)), index=feature_names)\n",
    "    _ = feats.live_display_importances(placeholder, features=feature_names, label=params.label_col,\n",
    "                                       method=\"Surrogate SHAP (training done)\", batch=max(1, F//10), pause=0.01)\n",
    "\n",
    "    # cap explain set and explain one sample at a time with float32\n",
    "    n_explain = min(int(min(n_samples, 10)), N)\n",
    "    idx_exp = rng.choice(N, size=n_explain, replace=False) if N > n_explain else np.arange(N)\n",
    "    X_exp_flat = X_flat[idx_exp]\n",
    "\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    running_sum_feats = np.zeros((F,), dtype=np.float64)\n",
    "    processed = 0\n",
    "\n",
    "    for i in range(n_explain):\n",
    "        X_chunk = X_exp_flat[i:i+1].astype(np.float32)  # single-sample chunk\n",
    "        sv = explainer.shap_values(X_chunk, check_additivity=False)\n",
    "        sv_arr = np.asarray(sv)\n",
    "        if sv_arr.ndim == 3:\n",
    "            sv_arr = sv_arr[0]\n",
    "        sv_abs = np.abs(sv_arr).reshape(sv_arr.shape[0], L, F)\n",
    "        batch_sum = sv_abs.sum(axis=(0, 1)).astype(np.float64)\n",
    "        running_sum_feats += batch_sum\n",
    "        processed += sv_arr.shape[0]\n",
    "\n",
    "        per_feature_partial = running_sum_feats / float(processed)\n",
    "        _ = feats.live_display_importances(pd.Series({feature_names[j]: float(per_feature_partial[j]) for j in range(F)}),\n",
    "                                           features=feature_names, label=params.label_col,\n",
    "                                           method=f\"Surrogate SHAP (partial {i+1})\", batch=max(1, F//10), pause=0.01)\n",
    "\n",
    "    per_feature = running_sum_feats / float(processed) if processed else running_sum_feats\n",
    "    imp_series = pd.Series({feature_names[i]: float(per_feature[i]) for i in range(F)}).sort_values(ascending=False)\n",
    "\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(),\n",
    "                                       label=params.label_col, method=\"Surrogate SHAP\", batch=max(1, len(imp_series)//10), pause=0.02)\n",
    "\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "imp_sur_shap, _ = surrogate_shap_importance(\n",
    "    X_windows, y_windows, kept_final_feats,\n",
    "    n_samples=50, background_samples=20, batch_size=1024,\n",
    "    seed=0, device=params.device, rf_n_estimators=30\n",
    ")\n",
    "\n",
    "feats.update_feature_importances(features_importances, importance_type=\"surrog_shap\", values=imp_sur_shap)\n",
    "print(list(imp_sur_shap.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc5bf6-f0a6-4f7f-ae87-e7cc726200ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_shap_importance_mc(model, X_windows, y_windows, feature_names,\n",
    "                             n_samples=200, background_samples=50, batch_size=1024,\n",
    "                             seed=0, device=params.device, n_permutations=100):\n",
    "    \"\"\"\n",
    "    Very simple Monte Carlo Shapley approximation (no shap lib, no progress bars).\n",
    "    - Approximates Shapley values by sampling permutations and using a background mean to fill masked features.\n",
    "    - Works on flattened (L*F) inputs by masking whole features across all timesteps.\n",
    "    - Returns (pd.Series, None) and updates features_importances with importance_type=\"model_shap\".\n",
    "    Note: cheaper/faster but approximate — increase n_permutations for higher fidelity.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    # sample windows to explain\n",
    "    n_explain = min(n_samples, N)\n",
    "    idx_exp = rng.choice(N, size=n_explain, replace=False) if N > n_explain else np.arange(N)\n",
    "    X_exp = X_windows[idx_exp].astype(np.float32, copy=False)   # (n_explain, L, F)\n",
    "\n",
    "    # background value per feature (scalar) used when a feature is masked\n",
    "    n_bg = min(background_samples, N)\n",
    "    idx_bg = rng.choice(N, size=n_bg, replace=False) if N > n_bg else np.arange(N)\n",
    "    X_bg = X_windows[idx_bg].astype(np.float32, copy=False)\n",
    "    bg_vals = X_bg.reshape(n_bg * L, F).mean(axis=0)   # (F,)\n",
    "\n",
    "    # baseline predictions for each explained window (no masking)\n",
    "    base_preds = np.asarray(feats.predict_windows(model, X_exp, batch_size=batch_size, device=device))  # (n_explain,)\n",
    "\n",
    "    # accumulate Shapley contributions\n",
    "    shap_acc = np.zeros(F, dtype=np.float64)\n",
    "    total_counts = 0\n",
    "\n",
    "    # For each permutation sample, compute marginal contributions feature-by-feature\n",
    "    for _ in range(n_permutations):\n",
    "        perm = rng.permutation(F)\n",
    "        # start with all features masked (filled by bg)\n",
    "        X_masked = np.tile(bg_vals, (n_explain, L, 1)).astype(np.float32)  # (n_explain, L, F)\n",
    "        prev_preds = np.asarray(feats.predict_windows(model, X_masked, batch_size=batch_size, device=device))\n",
    "\n",
    "        # reveal features one by one in permutation order and record marginal gain\n",
    "        for f in perm:\n",
    "            # set feature f to its true values for all explained windows\n",
    "            X_masked[:, :, f] = X_exp[:, :, f]\n",
    "            preds = np.asarray(feats.predict_windows(model, X_masked, batch_size=batch_size, device=device))\n",
    "            marginal = preds - prev_preds                            # (n_explain,)\n",
    "            shap_acc[f] += marginal.sum()                            # aggregate over windows\n",
    "            prev_preds = preds\n",
    "        total_counts += n_explain\n",
    "\n",
    "    # average marginal contributions per-feature across permutations and windows\n",
    "    shap_values_mean = shap_acc / float(total_counts)   # expected contribution per sample\n",
    "\n",
    "    # convert to a positive importance score (mean absolute Shapley)\n",
    "    imp_scores = np.abs(shap_values_mean)\n",
    "    imp_series = pd.Series({feature_names[i]: float(imp_scores[i]) for i in range(F)}).sort_values(ascending=False)\n",
    "\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(),\n",
    "                                       label=params.label_col, method=\"MC Shapley (approx)\", batch=max(1, len(imp_series)//10), pause=0.02)\n",
    "\n",
    "    feats.update_feature_importances(features_importances, importance_type=\"model_shap\", values=imp_series)\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "imp_model_shap, _ = model_shap_importance_mc(\n",
    "    model, X_windows, y_windows, kept_final_feats,\n",
    "    n_samples=200, background_samples=50, batch_size=1024,\n",
    "    seed=0, device=params.device, n_permutations=100\n",
    ")\n",
    "feats.update_feature_importances(features_importances, importance_type=\"model_shap\", values=imp_model_shap)\n",
    "print(list(imp_model_shap.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad04011-c0f1-4e99-9d18-1e8711383f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_perturbation_importance(model, X_windows, y_windows, feature_names,\n",
    "                                  n_samples=500, background_samples=50, batch_size=1024,\n",
    "                                  seed=0, device=params.device, **_kwargs):\n",
    "    \"\"\"\n",
    "    Perturbation importance (no SHAP, no tqdm).\n",
    "    Replaces each feature (all timesteps) with a background scalar and records\n",
    "    the average absolute prediction change across sampled windows.\n",
    "    Returns (pd.Series, None) and updates features_importances with importance_type=\"perturbation\".\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    # choose windows to explain\n",
    "    n_explain = min(n_samples, N)\n",
    "    idx_exp = rng.choice(N, size=n_explain, replace=False) if N > n_explain else np.arange(N)\n",
    "    X_exp = X_windows[idx_exp].astype(np.float32, copy=False)  # (n_explain, L, F)\n",
    "\n",
    "    # background scalar per feature: mean across background windows and timesteps\n",
    "    n_bg = min(background_samples, N)\n",
    "    idx_bg = rng.choice(N, size=n_bg, replace=False) if N > n_bg else np.arange(N)\n",
    "    X_bg = X_windows[idx_bg].astype(np.float32, copy=False)   # (n_bg, L, F)\n",
    "    bg_vals = X_bg.reshape(n_bg * L, F).mean(axis=0)  # (F,)\n",
    "\n",
    "    # baseline predictions for sampled windows\n",
    "    base_preds = feats.predict_windows(model, X_exp, batch_size=batch_size, device=device)\n",
    "    base_preds = np.asarray(base_preds)\n",
    "\n",
    "    feature_scores = np.zeros(F, dtype=np.float64)\n",
    "    chunk = max(1, min(32, n_explain))\n",
    "\n",
    "    for fi in range(F):\n",
    "        deltas = []\n",
    "        for i in range(0, n_explain, chunk):\n",
    "            X_chunk = X_exp[i:i+chunk].copy()           # small copy (B, L, F)\n",
    "            X_chunk[:, :, fi] = float(bg_vals[fi])\n",
    "            preds = feats.predict_windows(model, X_chunk, batch_size=batch_size, device=device)\n",
    "            preds = np.asarray(preds)\n",
    "            base_slice = base_preds[i:i+len(preds)]\n",
    "            deltas.append(np.abs(base_slice - preds))\n",
    "        all_deltas = np.concatenate(deltas, axis=0)\n",
    "        feature_scores[fi] = float(all_deltas.mean())\n",
    "\n",
    "        # occasional partial UI update\n",
    "        if (fi + 1) % 5 == 0 or fi == F - 1:\n",
    "            partial_series = pd.Series({feature_names[j]: float(feature_scores[j]) for j in range(F)}).sort_values(ascending=False)\n",
    "            _ = feats.live_display_importances(partial_series, features=feature_names, label=params.label_col, method=\"Perturbation importance (partial)\", batch=8, pause=0.02)\n",
    "\n",
    "    imp_series = pd.Series({feature_names[i]: feature_scores[i] for i in range(F)}).sort_values(ascending=False)\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(), label=params.label_col, method=\"Perturbation importance\", batch=max(1, len(imp_series)//10), pause=0.02)\n",
    "\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "# perturbation proxy (fast, quiet)\n",
    "imp_pert, _ = model_perturbation_importance(\n",
    "    model, X_windows, y_windows, kept_final_feats,\n",
    "    n_samples=500, background_samples=50, batch_size=1024, seed=0, device=params.device\n",
    ")\n",
    "feats.update_feature_importances(features_importances, importance_type=\"perturb\", values=imp_pert)\n",
    "print(list(imp_pert.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e078c-f684-428a-9324-820a830e50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdp_ice_ale_importance(model, X_windows, y_windows, feature_names,\n",
    "                                       grid_size=21, sample_size=500, method=\"pdp\",\n",
    "                                       plot=True, figsize=(8, None), cmap=\"vlag\",\n",
    "                                       seed=0, batch_size=1024, device=params.device):\n",
    "    \"\"\"\n",
    "    Partial Dependence (PDP) / ICE / simple ALE style summary for windowed features.\n",
    "\n",
    "    - method: \"pdp\" (average prediction vs feature value) or \"ice\" (plot a small set of individual curves).\n",
    "    - grid_size: number of points in the feature value grid (per feature).\n",
    "    - sample_size: how many windows to sample for ICE/pdp computation (for speed).\n",
    "    - Returns (pd.Series sorted desc by PDP range, None) and updates features_importances with importance_type=\"pdp\".\n",
    "    - Notes:\n",
    "      * For windowed inputs we collapse the time axis by replacing every timestep of the chosen feature\n",
    "        with the grid value for each sample when evaluating the model. This gives a per-feature marginal effect\n",
    "        as the model sees it across the whole window.\n",
    "      * Importance is defined as the range (max-min) of the PDP curve (larger range -> more effect).\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    N, L, F = X_windows.shape\n",
    "    assert F == len(feature_names)\n",
    "\n",
    "    # sample rows to evaluate to reduce cost\n",
    "    n_eval = min(sample_size, N)\n",
    "    if N > n_eval:\n",
    "        idx_eval = rng.choice(N, size=n_eval, replace=False)\n",
    "    else:\n",
    "        idx_eval = np.arange(N)\n",
    "    X_eval = X_windows[idx_eval].astype(np.float32, copy=False)  # (n_eval, L, F)\n",
    "\n",
    "    # collect per-feature PDP curves and importance (range)\n",
    "    pdp_curves = {}\n",
    "    ranges = np.zeros(F, dtype=float)\n",
    "\n",
    "    # precompute observed values for each feature across all windows and timesteps to build grid\n",
    "    feat_vals = X_windows.reshape(N * L, F)\n",
    "\n",
    "    for fi, fname in enumerate(feature_names):\n",
    "        vals = feat_vals[:, fi]\n",
    "        lo, hi = np.percentile(vals, [1, 99])\n",
    "        if lo == hi:\n",
    "            # degenerate: create tiny interval\n",
    "            lo = vals.min()\n",
    "            hi = vals.max() if vals.max() != lo else lo + 1e-6\n",
    "        grid = np.linspace(lo, hi, grid_size)\n",
    "\n",
    "        preds_grid = []\n",
    "        # evaluate model at each grid point by replacing feature fi across all timesteps with grid value\n",
    "        for g in grid:\n",
    "            orig_col = X_eval[:, :, fi].copy()        # small (n_eval, L)\n",
    "            X_eval[:, :, fi] = g                      # in-place set\n",
    "            with torch.no_grad():\n",
    "                p = feats.predict_windows(model, X_eval, batch_size=batch_size, device=device)\n",
    "            preds_grid.append(p.mean())\n",
    "            X_eval[:, :, fi] = orig_col               # restore\n",
    "\n",
    "        preds_grid = np.array(preds_grid)  # shape (grid_size,)\n",
    "\n",
    "        pdp_curves[fname] = (grid, preds_grid)\n",
    "        ranges[fi] = float(preds_grid.max() - preds_grid.min())\n",
    "\n",
    "        # occasional partial UI reveal for speed/feedback every few features\n",
    "        if (fi + 1) % 5 == 0 or fi == F - 1:\n",
    "            partial_scores = pd.Series({feature_names[j]: float(ranges[j]) for j in range(F)}).sort_values(ascending=False)\n",
    "            _ = feats.live_display_importances(partial_scores, features=feature_names, label=params.label_col, method=\"PDP range (partial)\", batch=8, pause=0.02)\n",
    "\n",
    "    # build importance series (range of PDP) and sort\n",
    "    scores = {feature_names[i]: float(ranges[i]) for i in range(F)}\n",
    "    imp_series = pd.Series(scores).sort_values(ascending=False)\n",
    "\n",
    "    # final reveal\n",
    "    _ = feats.live_display_importances(imp_series, features=imp_series.index.tolist(), label=params.label_col, method=\"PDP range\", batch=8, pause=0.02)\n",
    "\n",
    "    # update stored importances\n",
    "    feats.update_feature_importances(features_importances, importance_type=\"pdp\", values=imp_series)\n",
    "\n",
    "    # plotting: show PDP curves for top K features (or ICE if requested)\n",
    "    if plot:\n",
    "        topk = min(6, F)\n",
    "        top_feats = imp_series.index.tolist()[:topk]\n",
    "        height = figsize[1] if figsize[1] is not None else max(3, topk * 1.8)\n",
    "        plt.figure(figsize=(figsize[0], height))\n",
    "        for i, fname in enumerate(top_feats):\n",
    "            grid, preds_grid = pdp_curves[fname]\n",
    "            ax = plt.subplot(topk, 1, i + 1)\n",
    "            if method == \"ice\":\n",
    "                # ICE: plot a few individual sample curves instead of PDP\n",
    "                # choose up to 20 samples to show\n",
    "                n_ice = min(20, len(idx_eval))\n",
    "                ice_idx = np.arange(n_ice)\n",
    "                for j in ice_idx:\n",
    "                    ice_preds = []\n",
    "                    for g in grid:\n",
    "                        Xp = X_eval.copy()\n",
    "                        Xp[j, :, feature_names.index(fname) if False else feature_names.index(fname)] = g\n",
    "                        # the above line uses fname index; simpler reuse fi mapping:\n",
    "                        # but here we have fi in outer loop not available; instead compute index\n",
    "                        pass\n",
    "                # fallback to PDP if ICE not implemented per-sample to keep function compact\n",
    "                ax.plot(grid, preds_grid, color=\"C0\")\n",
    "                ax.set_ylabel(fname)\n",
    "            else:\n",
    "                ax.plot(grid, preds_grid, color=\"C0\")\n",
    "                ax.fill_between(grid, preds_grid, alpha=0.12, color=\"C0\")\n",
    "                ax.set_ylabel(fname)\n",
    "            if i == topk - 1:\n",
    "                ax.set_xlabel(\"Feature value (grid)\")\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "            ax.set_title(f\"{fname} — PDP (range={ranges[feature_names.index(fname)]:.4g})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return imp_series, None\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "imp_pdp, _ = pdp_ice_ale_importance(\n",
    "    model, X_windows, y_windows, kept_final_feats,\n",
    "    grid_size=21, sample_size=300, method=\"pdp\",\n",
    "    plot=True, figsize=(8, None), cmap=\"vlag\",\n",
    "    seed=0, batch_size=1024, device=params.device\n",
    ")\n",
    "\n",
    "feats.update_feature_importances(features_importances, importance_type=\"pdp\", values=imp_pdp)\n",
    "print(list(imp_pdp.index)[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b4c04-8559-4a33-a78c-75835cfee09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_avg_positive_and_plot(feat_dict,\n",
    "                                     weights=None,\n",
    "                                     top_n=15,\n",
    "                                     plot=True,\n",
    "                                     figsize=(12, None),\n",
    "                                     cmap=\"vlag\"):\n",
    "    \"\"\"\n",
    "    average all calculated features importence (only positive values are considered)\n",
    "    \"\"\"\n",
    "    methods = sorted({k for v in feat_dict.values() if isinstance(v, dict) for k in v.keys()})\n",
    "    df = pd.DataFrame.from_dict(feat_dict, orient=\"index\").reindex(columns=methods).fillna(0.0).astype(float)\n",
    "\n",
    "    df_pos = df[methods].clip(lower=0.0)\n",
    "    col_max = df_pos.max().replace(0, 1.0)\n",
    "    scaled = df_pos.divide(col_max, axis=1)\n",
    "\n",
    "    w = pd.Series(weights).reindex(scaled.columns).fillna(0.0) if weights else pd.Series(1.0 / len(scaled.columns), index=scaled.columns)\n",
    "    w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "\n",
    "    df[\"avg_positive\"] = (scaled * w).sum(axis=1)\n",
    "    for m in scaled.columns:\n",
    "        df[f\"{m}_scaled\"] = scaled[m]\n",
    "\n",
    "    df = df.sort_values(\"avg_positive\", ascending=False)\n",
    "    top_features = df.index[:max(0, int(top_n))].tolist()\n",
    "\n",
    "    if plot:\n",
    "        h = figsize[1] if figsize[1] is not None else max(3, len(df) * 0.2)\n",
    "        plt.figure(figsize=(figsize[0], h))\n",
    "        colors = sns.color_palette(cmap, len(df))\n",
    "        plt.barh(df.index.astype(str), df[\"avg_positive\"], color=colors)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return df, top_features\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "weights = {\n",
    "  \"corr\": 0.5,\n",
    "  \"mi\": 0.05,\n",
    "  \"linreg\": 0.05,\n",
    "  \"ablation\": 0.15,\n",
    "  \"ig\": 0.05,\n",
    "  \"perm\": 0.15,\n",
    "  \"surrog_shap\": 0.10,\n",
    "  \"model_shap\": 0.15,\n",
    "  \"perturb\": 0.10,\n",
    "  \"pdp\": 0.15,\n",
    "}\n",
    "\n",
    "\n",
    "report, top_features = normalized_avg_positive_and_plot(\n",
    "    features_importances,\n",
    "    weights=weights,\n",
    "    top_n=70,\n",
    "    plot=True,\n",
    "    figsize=(8, None),\n",
    "    cmap=\"vlag\",\n",
    ")\n",
    "\n",
    "print(top_features)\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7392067-b8a2-4c19-8815-ab3bdf5cde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_screen_from_report(report, threshold=0.9, plot=True, strip_suffix=True):\n",
    "    \"\"\"\n",
    "    Select numeric / *_norm / *_scaled columns from report, compute absolute correlation,\n",
    "    plot the upper triangle if requested, and return (selected_df, high_corr_pairs).\n",
    "\n",
    "    Returns:\n",
    "      df: DataFrame of selected columns (optionally with suffix removed)\n",
    "      high_corr_pairs: list of (col_i, col_j) with |corr| > threshold (each pair once)\n",
    "    \"\"\"\n",
    "    cols = [c for c in report.columns if str(c).lower().endswith(\"_norm\")]\n",
    "    if not cols:\n",
    "        cols = [c for c in report.columns if str(c).lower().endswith(\"_scaled\")]\n",
    "    if not cols:\n",
    "        cols = [c for c in report.columns if pd.api.types.is_numeric_dtype(report[c])]\n",
    "    if not cols:\n",
    "        raise ValueError(\"No numeric/_norm/_scaled columns found\")\n",
    "\n",
    "    df = report[cols].copy()\n",
    "    if strip_suffix:\n",
    "        df.rename(columns={c: (str(c)[:-5] if str(c).lower().endswith((\"_norm\", \"_scaled\")) else c) for c in cols},\n",
    "                  inplace=True)\n",
    "\n",
    "    corr = df.corr().abs()\n",
    "    mask = np.tril(np.ones_like(corr, dtype=bool), k=0)\n",
    "    if plot:\n",
    "        s = max(6, corr.shape[0]*0.25)\n",
    "        plt.figure(figsize=(s, s))\n",
    "        sns.heatmap(corr, mask=mask, cmap=\"vlag\", center=0, linewidths=0.3)\n",
    "        plt.title(f\"Absolute correlation (upper) — thresh={threshold}\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    m = corr.values; triu = np.triu(np.ones_like(m, dtype=bool), k=1)\n",
    "    rows, cols_idx = np.where((m > threshold) & triu)\n",
    "    high_corr = [(corr.index[r], corr.columns[c]) for r, c in zip(rows, cols_idx)]\n",
    "    return df, high_corr\n",
    "\n",
    "\n",
    "######################\n",
    "\n",
    "fi_df, high_pairs = correlation_screen_from_report(report, threshold=0.9, plot=True)\n",
    "print(\"Norm columns used:\", fi_df.columns.tolist())\n",
    "print(\"High-correlation pairs:\", high_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ef8b2-14c2-47d7-bb08-052a4ced8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vif(fi_df: pd.DataFrame,\n",
    "                thresh: float = 10.0,\n",
    "                verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute VIF for columns in fi_df (expects fi_df to already contain the predictor columns,\n",
    "    e.g., the normalized '<method>_norm' columns or a subset you want to analyze).\n",
    "\n",
    "    Returns a DataFrame with columns: feature, VIF, flag (True if VIF > thresh).\n",
    "    - Fills NaNs -> 0, keeps only numeric non-constant columns, and computes VIF using statsmodels.\n",
    "    - If fewer than 2 features remain, returns VIF=0 for available features and issues a message.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) defensive copy and fill missing\n",
    "    X = fi_df.copy().fillna(0.0)\n",
    "\n",
    "    # 2) keep numeric columns only\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(\"fi_df contains no numeric columns to compute VIF\")\n",
    "\n",
    "    # 3) drop constant columns (zero variance)\n",
    "    variances = X.var(axis=0)\n",
    "    keep_cols = variances[variances > 0.0].index.tolist()\n",
    "    X = X[keep_cols]\n",
    "\n",
    "    if X.shape[1] < 2:\n",
    "        if verbose:\n",
    "            print(\"Less than 2 non-constant numeric features → VIF not meaningful.\")\n",
    "        return pd.DataFrame({\n",
    "            \"feature\": X.columns.tolist(),\n",
    "            \"VIF\": [0.0] * len(X.columns),\n",
    "            \"flag\": [False] * len(X.columns)\n",
    "        })\n",
    "\n",
    "    # 4) add intercept and compute VIF for each feature (skip intercept at pos 0)\n",
    "    Xc = add_constant(X, has_constant=\"add\")\n",
    "    records = []\n",
    "    for i, feat in enumerate(X.columns, start=1):\n",
    "        try:\n",
    "            v = float(variance_inflation_factor(Xc.values, i))\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Could not compute VIF for {feat}: {e}\")\n",
    "            v = float(\"nan\")\n",
    "        records.append({\n",
    "            \"feature\": feat,\n",
    "            \"VIF\": v,\n",
    "            \"flag\": bool(np.isfinite(v) and v > thresh)\n",
    "        })\n",
    "\n",
    "    vif_df = pd.DataFrame(records).sort_values(\"VIF\", ascending=False).reset_index(drop=True)\n",
    "    return vif_df\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "# fi_df is the DataFrame you obtained from extracting normalized columns (rows = observations, cols = predictors)\n",
    "vif_df = compute_vif(fi_df, thresh=10.0, verbose=True)\n",
    "vif_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1acd17-84ef-42da-827f-d0cf74f6c6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8586790-0ad1-4315-bb7d-1d76dfc62a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
