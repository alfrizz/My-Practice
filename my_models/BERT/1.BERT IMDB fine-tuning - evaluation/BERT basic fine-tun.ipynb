{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fd7d0f-14f1-44d5-8a57-e698c5def01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://brainyx.co/journal/journal4/\n",
    "\n",
    "\n",
    "# One of the most common uses of BERT is to download a model that has been pre-trained with a large amount of text and fine tuning it with a small amount of data. In this article, we will show you how to download a pre-trained model from hugginfface and fine tune it with sample code.\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, recall_score \n",
    "from sklearn.metrics import precision_score, f1_score\n",
    " \n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    " \n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698012d2-2b89-40b5-958f-dbcb109f36b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\")\n",
    "# raw_datasets = load_dataset(\"imdb\", split=['train', 'test'])\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01139b1-dbb7-4100-8321-149442de013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(raw_datasets['train']) # 0 indicates a negative review and 1 indicates a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee57fd87-c690-4ba2-8a45-cbea5b2ddfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DarkWolf tells the tale of a young waitress na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is just not worth your time. Its re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's actually a good thing Sean Connery retire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Wooden Horse was one of the first \"great\" ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rudy does it again with this hot off the stree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Evidently, not many people have seen this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>A huge disappointment from writer Hamm and dir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Robert Siodmak does a fabulous job with this B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Now, I won't deny that when I purchased this o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>First animated feature film from Ireland is al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    DarkWolf tells the tale of a young waitress na...      0\n",
       "1    This movie is just not worth your time. Its re...      0\n",
       "2    It's actually a good thing Sean Connery retire...      1\n",
       "3    The Wooden Horse was one of the first \"great\" ...      1\n",
       "4    Rudy does it again with this hot off the stree...      1\n",
       "..                                                 ...    ...\n",
       "495  Evidently, not many people have seen this movi...      1\n",
       "496  A huge disappointment from writer Hamm and dir...      0\n",
       "497  Robert Siodmak does a fabulous job with this B...      1\n",
       "498  Now, I won't deny that when I purchased this o...      0\n",
       "499  First animated feature film from Ireland is al...      1\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Samples for Train and Test\n",
    "\n",
    "sample_train_val = raw_datasets['train'].shuffle().select(range(0,500)).to_pandas() #2000\n",
    "sample_test = raw_datasets['test'].shuffle().select(range(0,500)).to_pandas()\n",
    "\n",
    "sample_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a839cda-2c5f-4546-863f-aa45ed290772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Pretrained Tokenizer and Model\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cc2289-2f5c-4094-9ad9-1ce7189b8f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e298e5-6622-4bc3-b5f0-abd99891ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Dataset\n",
    "# Define a simple class inherited from torch dataset\n",
    "# This is a custom dataset class for PyTorch, which is often used when you want to load your own data into a PyTorch model.\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset): # The class Dataset is inheriting from torch.utils.data.Dataset. This is a base class provided by PyTorch for representing a dataset.\n",
    "    \n",
    "    def __init__(self, encodings, labels=None): # The __init__ method is the initializer for the class. It takes as input encodings and labels. encodings are the encoded representations of your text data (for example, tokenized and converted to IDs), and labels are the corresponding labels for your data.\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    " \n",
    "    def __getitem__(self, idx): # The __getitem__ method is used to get the data (encodings and labels) for a specific item in the dataset. idx is the index of the item. This method returns a dictionary where the keys are the names of the encoding components (like “input_ids”, “attention_mask”, etc.) and the values are the corresponding tensors. If labels are provided, a “labels” key is also added to the dictionary.\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    " \n",
    "    def __len__(self): # The __len__ method returns the number of items in the dataset\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    " \n",
    "\n",
    "# The item dictionary would look like this:\n",
    "# {\n",
    "#     'input_ids': tensor([101, 2001, 2019, 6207, 102]),\n",
    "#     'token_type_ids': tensor([0, 0, 0, 0, 0]),\n",
    "#     'attention_mask': tensor([1, 1, 1, 1, 1]),\n",
    "#     'labels': tensor(0)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51cc3c79-9ca2-43f3-91e6-da72baf45db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"DarkWolf tells the tale of a young waitress named Josie (Samaire Armstrong) who had been leading a pretty ordinary life until her friend Mary (Tippi Hedren) is killed by a Werewolf, you see Werewolves actually exist in modern day America & there is even a special organisation within the police force to fight the Werewolf threat headed up by Detective Steve Turley (Ryan Olosio) who has the difficult task of telling Josie that she is in fact a pure blooded Werewolf herself & that a so-called 'dark prince' Werewolf (Kane Hodder) wants to mate with her & create a new breed of pure blood Werewolves that will take over the entire world, or something like that. Understandably Josie has a hard time believing it, that is until she sees the evidence with her own eyes. It's up to Werewolf cop Steve to save Josie, the day & the world...<br /><br />Co-executive produced & directed Richard Friedman I thought DarkWolf was a pretty bad low budget shot on a digital camcorder horror film that didn't really do anything for me. The script by Geoffrey Alan Holliday starts out promisingly enough being set in a strip club with plenty of naked breasts on show & then there's a Werewolf attack which leaves someone splattered everywhere but after this decent opening sequence it's pretty much down hill all the way I'm afraid. For a start it's slow going, it's dull, it's predictable & it's populated with highly annoying character's who come out with lots of bad dialogue & do stupid things like when they have the opportunity to shoot the Werewolf they don't, I have no idea why but they prefer to just stand there instead. The script is dumb & doesn't explain itself, why has Josie never turned into a Werewolf before? Is she really the only one? Why can't this 'dark prince' find another female Werewolf? There are also lots of other things which make little or no sense like an ancient book which at fist seems quite important but is then totally forgotten about half-way through but you get the idea anyway, as a whole the film plods along in very linear fashion to a very predictable climatic showdown that is underwhelming to say the least.<br /><br />Director Friedman lights the film quite well with bright neon but this is noting new or original & doesn't really improve the film as a spectacle. Now let's talk special effects or rather the lack of them because the effects in DarkWolf are far from special, the Werewolf transformation is achieved using CGI & it's among the worst looking CGI I've ever seen, seriously a Playstation would be embarrassed about these computer graphics. It's easily the worst Werewolf transformation I've ever seen, An American Werewolf in London (1981) was made over 25 years ago yet the special effects in that are literally light-years ahead of the ones seen in DarkWolf, who says special effects have improved over the years? The animatronic puppet effects aren't much better either although at least there's something psychical on screen. The gore isn't up to much after a gory opening kill there's some blood splatter & plenty of dead bodies but not much else. Thre's a fair amount of female nudity if that's your thing but don't get too excited because you still have to sit through a terrible film to see it, is it really worth it?<br /><br />Technically DarkWolf is alright apart from possibly the worst CGI effect ever, it's reasonably well made & it at least seems to have production values. The acting is what you'd expect really.<br /><br />DarkWolf is yet another low budget piece of crap horror film that litter video shop shelves & clutter the schedules of obscure cable TV stations, I didn't think it was as bad as some but it's like saying going to the dentist is slightly more fun than going to a funeral although when all said & done they're both horrible still...\",\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = list(sample_train_val[\"text\"])\n",
    "sample_y = list(sample_train_val[\"label\"])\n",
    " \n",
    "sample_x[0], sample_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b115b1eb-ef95-4b35-badd-723836f30141",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(sample_x, sample_y, test_size=0.2)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d02190-69ca-4d59-b6c2-783af61dc621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2034, 3191, 7247, 1055, 10131, 1005, 1055, 21459]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2b48313-ad10-4211-b8ac-130f04116df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized['token_type_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ae5577-9670-4c02-8b1d-78c45aee4907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized['attention_mask'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d0e36b3-713d-4a3a-851c-b90f78f09968",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = Dataset(X_train_tokenized, Y_train)\n",
    "input_val = Dataset(X_val_tokenized, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9beceb2e-004c-4026-9f8f-f5ed1d973c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1045,  2034,  3191,  7247,  1055, 10131,  1005,  1055, 21459,\n",
       "          3117,  1999,  2026,  6619,  3694,  2381,  2465,  1010,  1998,  1045,\n",
       "          5632,  2296, 26162,  3931,  1997,  2009,  1012,  2009,  2001,  2471,\n",
       "         13418,  2008,  5365,  2052,  2131,  2907,  1997,  2009,  1010,  1998,\n",
       "          6195,  2008,  2009,  2001,  2081,  1999,  4347,  1010,  1996,  3463,\n",
       "          2024,  6581,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  3056,  2477,  2031,  2000,  2022,  3970,  1024,  1999,  4347,\n",
       "          2045,  2001,  2053,  3160,  1997,  9179,  4004,  5889,  1999,  1037,\n",
       "          2350,  5365,  2143,  1012,  1999,  1037,  2126,  2023, 17552,  2015,\n",
       "          1996,  2203,  4031,  2738,  2062,  5875,  2084,  2065,  2027,  2018,\n",
       "          2042,  2583,  2000,  2224,  1037,  2062, 14469,  1011,  2559,  3459,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2007,\n",
       "          2008, 18355,  2000,  9462,  1010,  3237,  3135, 12415, 22794, 22927,\n",
       "          1998,  2472, 11430,  5951,  1006,  2426,  2500,  1007,  2165,  1996,\n",
       "          4390,  2000,  2192,  1011,  4060,  1037, 21459,  1998, 17227,  3459,\n",
       "          1012,  2703, 14163,  3490,  3248,  7418, 11192,  1012, 14163,  3490,\n",
       "          2001,  2012,  1996,  4672,  1997,  2010,  4204,  2004,  2019,  3364,\n",
       "          2076,  2023,  2558,  1010,  1998,  2071,  2200,  3053,  2377,  2505,\n",
       "          2002,  2404,  2010,  2568,  2000,  1012,  2320,  2017,  2131,  2627,\n",
       "          1996,  5789,  1006,  2009,  1005,  1055,  2204,  1010,  2021,  2053,\n",
       "          2028,  2003,  2183,  2000,  2428,  6707,  2032,  2005,  1037,  2822,\n",
       "          2158,  1007,  1010,  2010,  2836,  2038,  2035,  1996,  2310,  6935,\n",
       "         27605, 15909, 12672,  1997,  2010,  2190,  2147,  1012,  1026,  7987,\n",
       "          1013,  1028,  1026,  7987,  1013,  1028,  2059,  2045,  2003,  6446,\n",
       "          2063, 28035,  1012,  2746,  2125,  2019,  7436,  2663,  1996,  3025,\n",
       "          2095,  2005,  2014,  2836,  1999,  1996,  2307,  1062, 28872,  8151,\n",
       "          1010,  1996, 20098, 10087,  3366,  3883,  1005,  1055,  2732,  2001,\n",
       "          2006,  1996,  4125,  1998,  2016,  2001,  2445,  1996, 22088,  2535,\n",
       "          1997,  1051,  1011, 17595,  2750,  2014,  3768,  1997,  3325,  1999,\n",
       "          5365,  1012,  2014,  2836,  2180,  2014,  1037,  2117,  5486,  7436,\n",
       "          1010,  1996,  2034,  2051,  1999,  2381,  2008,  3047,  1012,  1026,\n",
       "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  2172,  6256,  2038,\n",
       "          2042, 22915,  2012, 28035,  1005,  1055,  2836,  1010,  1998,  2014,\n",
       "          7436,  2663,  2182,  1012,  2016,  2038,  2042,  2170,  4799,  1998,\n",
       "          2028,  1011,  3602,  1012,  2045,  2003,  1037,  2235,  8982,  1997,\n",
       "          3606,  1999,  2008,  1012,  2174,  1010,  2008,  2108,  2056,  1010,\n",
       "          2035,  2017,  2342,  2000,  2079,  2003,  2175,  2067,  2000,  1996,\n",
       "          2338,  1012,  2005, 28035,  1010,  2295,  2025,  2822,  1010,  2209,\n",
       "          1051,  1011, 17595,  3492,  2172,  2004, 10131,  2626,  2014,  1025,\n",
       "          2009,  2003,  1999,  2755,  1037, 21459,  2836,  1010,  1998,  2028,\n",
       "          1997,  1996,  2190, 15210,  2013,  2338,  2000,  3898,  1045,  2031,\n",
       "          2412,  9741,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  2004,  2005,  1996,  2717,  1997,  1996,  3459,  1010,  2092,\n",
       "          2023,  2001, 15418,  1012,  2027,  2018,  1996,  5221,  9238,  1997,\n",
       "          3340,  1998,  2839,  5889,  1999,  5365,  2012,  1996,  2051,  1010,\n",
       "          1998,  1037,  2502,  5166,  2000,  3477,  2005,  1996,  2190,  1010,\n",
       "          1998,  1999,  1996,  2203,  2027,  2288,  1996,  2190,  1012,  1026,\n",
       "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  2143,  3730,\n",
       "          6132,  7418, 11192,  1005,  1055,  3510,  2000,  1051,  1011, 17595,\n",
       "          5399,  1012,  1999,  1996,  3117,  1010,  2007,  7177,  2272,  1996,\n",
       "         11516,  2015,  1997,  1996,  5771,  1998,  2002,  3138,  2006,  1037,\n",
       "          9530,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f946c4-8b24-415f-a00a-651f0337affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Evaluation Metrics\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # pred, labels = p\n",
    "    # pred = np.argmax(pred, axis=1)\n",
    "    labels = p.label_ids\n",
    "    pred = p.predictions.argmax(-1)\n",
    "    print(classification_report(labels, pred))\n",
    " \n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred) \n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    " \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17155e06-0d3e-4939-b68a-11be3bcfed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py309\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 1:02:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647931</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.369284</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.854369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.311130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.318277</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463904</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.847458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.89      0.71        47\n",
      "           1       0.83      0.45      0.59        53\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.71      0.67      0.65       100\n",
      "weighted avg       0.72      0.66      0.64       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85        47\n",
      "           1       0.88      0.83      0.85        53\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.85      0.85      0.85       100\n",
      "weighted avg       0.85      0.85      0.85       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90        47\n",
      "           1       0.92      0.89      0.90        53\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87        47\n",
      "           1       0.87      0.91      0.89        53\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.88      0.88      0.88       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.68      0.78        47\n",
      "           1       0.77      0.94      0.85        53\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.84      0.81      0.81       100\n",
      "weighted avg       0.84      0.82      0.82       100\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.3780446116129557, metrics={'train_runtime': 3766.8056, 'train_samples_per_second': 0.319, 'train_steps_per_second': 0.02, 'total_flos': 315733266432000.0, 'train_loss': 0.3780446116129557, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a GPU is available, otherwise fall back to CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define Training Arguments for fine-tuning BERT\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",                   # Directory to save the model and logs\n",
    "    evaluation_strategy=\"steps\",           # Evaluate the model every 'eval_steps'\n",
    "    eval_steps=15,                        # Number of steps between evaluations\n",
    "    save_strategy=\"steps\",                 # Save the model every 'save_steps'\n",
    "    save_steps=15,                        # Number of steps between saving the model\n",
    "    per_device_train_batch_size=16,        # Batch size for training\n",
    "    per_device_eval_batch_size=16,         # Batch size for evaluation\n",
    "    num_train_epochs=3,                    # Number of training epochs\n",
    "    seed=0,                                # Seed for reproducibility\n",
    "    load_best_model_at_end=True            # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# The Trainer from the transformers library abstracts away the need to manually create DataLoader objects.\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),                # Move the model to GPU if available\n",
    "    args=args,                             # Pass the training arguments\n",
    "    train_dataset=input_train,             # Training dataset\n",
    "    eval_dataset=input_val,                # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,       # Metrics for evaluation\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping callback to stop training if no improvement\n",
    ")\n",
    "\n",
    "# Fine-tune the pre-trained BERT model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c50a48-61b1-4ffa-b3aa-93c2f8949d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_train) # global_step=75 (above) makes sense because 400 samples divided into batches of 16 results in 75 batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c861faa9-ba07-4eca-9322-d6b99b4836f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2004,  1045,  2031,  2056,  2077,  1010,  1045,  2572,  2025,\n",
       "          1037,  5470,  1997,  1056, 28394,  3723,  1012,  2002,  1005,  1055,\n",
       "          2074,  2061, 12943, 17643, 26477,  1045,  4299, 20016,  2052,  2074,\n",
       "          4521,  2032,  1998,  2131,  2009,  2058,  2007,  1012,  1026,  7987,\n",
       "          1013,  1028,  1026,  7987,  1013,  1028,  1999,  2023,  9476, 18230,\n",
       "          2003, 11573,  2320,  2153,  1998,  2003,  2067,  2000,  8521,  2041,\n",
       "          1997, 13044, 18484,  1012,  2002,  7516,  1037,  8592,  2911,  2975,\n",
       "          3417,  3518,  1998,  7288,  2000,  6154,  7548,  2043,  2002,  8645,\n",
       "          2015,  1056, 28394,  3723,  2003,  2028,  1997,  1996,  5467,  1012,\n",
       "          1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  2911,\n",
       "          3640,  2019, 11706,  2754,  2005,  1996,  2206,  7632, 14642,  5705,\n",
       "          1998, 18230,  9652,  5363,  2000,  4608,  1996, 15703,  4743,  1998,\n",
       "          4468, 11915,  6799,  2791,  1012,  2065,  2069,  2005,  2320,  2002,\n",
       "          1005,  1055,  9510,  2011, 17492,  1998, 18468,  1010,  2947,  5131,\n",
       "          1997,  1996, 29348, 17154,  5091,  1012,  2021,  1010,  2004, 13941,\n",
       "          4626,  1056, 28394,  3723,  2175,  1010,  2023,  2028,  2003,  3243,\n",
       "          2204,  1010,  2021,  2009,  1005,  1055,  2035,  4283,  2000,  2008,\n",
       "          8235,  4937,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Fine-tuned BERT and Run Prediction\n",
    "\n",
    "# Load test data\n",
    "X_test = list(sample_test[\"text\"])\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    " \n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    " \n",
    "test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82681141-c748-4d54-a242-817fe37f0e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "model_path = \"output/checkpoint-60\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    " \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbb1e748-f33c-4680-8965-e2884e91709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alienware\\miniconda3\\envs\\py309\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    " \n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    " \n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed1fa78-8da4-45c1-82cb-0c9b11ecc2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = sample_test.label.to_numpy()\n",
    "\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d11e85c-195c-413b-b92f-1f3ad7450fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[212  39]\n",
      " [ 27 222]]\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77bc86e1-986b-42c6-b776-96fbc2f33e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.87       251\n",
      "           1       0.85      0.89      0.87       249\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute classification report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print('\\nClassification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbfdc7-fd8d-4119-894e-4f9361e78334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bfee0-88e6-429d-aaf8-332a78d831f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599d661-4d86-4c12-b7af-1be0af545413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f31a82-0818-4038-aef3-e21590b9321e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d4727-5641-4721-a83c-8e3dbaeb20c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b68e85-7421-4d67-8cf1-ecdc0ccecb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f922cc5-b577-41d0-bd1b-cce92f17e481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52464f12-af45-4e75-803e-0490be383e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44ad67-b1eb-4d52-ac1e-ee890edca50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c2704-8620-41c6-9ce5-bc05f267b474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py309",
   "language": "python",
   "name": "py309"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
